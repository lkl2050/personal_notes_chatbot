{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import Document\n",
    "from typing import List\n",
    "import re\n",
    "from pathlib import Path\n",
    "from llama_index.llms.ollama import Ollama\n",
    "import os\n",
    "import qdrant_client\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    StorageContext\n",
    ")\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import Settings\n",
    "\n",
    "from llama_index.readers.obsidian import ObsidianReader\n",
    "from llama_index.core.readers.base import BaseReader\n",
    "from llama_index.readers.file.markdown import MarkdownReader\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ObsidianProcessor:\n",
    "    def __init__(self, chunk_size: int = 512, chunk_overlap: int = 50):\n",
    "        self.node_parser = SentenceSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean Obsidian-specific markdown and formatting\"\"\"\n",
    "        # Remove Obsidian internal links [[...]]\n",
    "        text = re.sub(r'\\[\\[([^\\]]+)\\]\\]', r'\\1', text)\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "        # Remove empty lines\n",
    "        text = '\\n'.join(line for line in text.split('\\n') if line.strip())\n",
    "        return text\n",
    "\n",
    "    def process_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"Process and chunk documents\"\"\"\n",
    "        cleaned_docs = []\n",
    "        for doc in documents:\n",
    "            if doc.text.strip():  # Skip empty documents\n",
    "                cleaned_text = self.clean_text(doc.text)\n",
    "                if cleaned_text:\n",
    "                    doc.text = cleaned_text\n",
    "                    cleaned_docs.append(doc)\n",
    "\n",
    "        # Chunk documents\n",
    "        nodes = self.node_parser.get_nodes_from_documents(cleaned_docs)\n",
    "        return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import ServiceContext\n",
    "from datetime import datetime\n",
    "\n",
    "def create_enhanced_index(documents: List[Document], embed_model, llm):\n",
    "    # Create processor and process documents\n",
    "    processor = ObsidianProcessor(chunk_size=512, chunk_overlap=50)\n",
    "    processed_docs = processor.process_documents(documents)\n",
    "\n",
    "    # Create service context\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "        llm=llm,\n",
    "        embed_model=embed_model,\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "\n",
    "    # Create index with metadata\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        processed_docs,\n",
    "        service_context=service_context,\n",
    "        show_progress=True\n",
    "    )\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "class PersonalObsidianChat:\n",
    "    def __init__(self, index, llm):\n",
    "        self.index = index\n",
    "        self.llm = llm\n",
    "\n",
    "        # Custom prompt for context retrieval\n",
    "        self.custom_prompt = PromptTemplate(\n",
    "            \"You are a helpful AI assistant with access to my personal notes. \"\n",
    "            \"Based on the context provided, please give a thoughtful and accurate response. \"\n",
    "            \"If you're not sure about something, please say so.\\n\\n\"\n",
    "            \"Context: {context}\\n\"\n",
    "            \"Question: {query}\\n\\n\"\n",
    "            \"Response: \"\n",
    "        )\n",
    "\n",
    "        # Create simple chat engine without unsupported parameters\n",
    "        self.chat_engine = self.index.as_chat_engine(\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "    def chat(self, query: str) -> str:\n",
    "        try:\n",
    "            response = self.chat_engine.chat(query)\n",
    "            return response.response\n",
    "        except Exception as e:\n",
    "            return f\"Error processing query: {str(e)}\"\n",
    "\n",
    "    def search_notes(self, query: str, top_k: int = 3):\n",
    "        \"\"\"Search through notes and return most relevant passages\"\"\"\n",
    "        retriever = self.index.as_retriever(similarity_top_k=top_k)\n",
    "        nodes = retriever.retrieve(query)\n",
    "        return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyObsidianReader(BaseReader):\n",
    "    \"\"\"Utilities for loading data from an Obsidian Vault.\n",
    "\n",
    "    Args:\n",
    "        input_dir (str): Path to the vault.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dir: str):\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self.input_dir = Path(input_dir)\n",
    "\n",
    "    def my_load_data(self, *args, **load_kwargs):\n",
    "        \"\"\"Load data from the input directory.\"\"\"\n",
    "        docs = []\n",
    "        for dirpath, dirnames, filenames in os.walk(self.input_dir):\n",
    "            # Exclude 'image_media' from directory traversal\n",
    "            if \"Images_Media\" in dirnames:\n",
    "                dirnames.remove(\"Images_Media\")\n",
    "            dirnames[:] = [d for d in dirnames if not d.startswith(\".\")]\n",
    "            for filename in filenames:\n",
    "                if filename.endswith(\".md\"):\n",
    "                    filepath = os.path.join(dirpath, filename)\n",
    "                    content = MarkdownReader().load_data(Path(filepath))\n",
    "                    docs.extend(content)\n",
    "        return docs\n",
    "\n",
    "    def load_langchain_documents(self, **load_kwargs):\n",
    "        \"\"\"Load data in LangChain document format.\"\"\"\n",
    "        docs = self.load_data(**load_kwargs)\n",
    "        return [d.to_langchain_format() for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/50/7cf7c2h51d56mzs5q8vmn77h0000gn/T/ipykernel_58938/929690816.py:10: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e129a02b311b4656b9a5cf0465617487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: (Implicit) I can answer without any more tools!\n",
      "Answer:  Sure, I can help with that. Can you please provide me with your username or any relevant information to identify you in a conversation?\n",
      "\u001b[0m Sure, I can help with that. Can you please provide me with your username or any relevant information to identify you in a conversation?\n"
     ]
    }
   ],
   "source": [
    "# Initialize components\n",
    "llm = Ollama(model=\"Tinydolphin\", request_timeout=120.0)\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# Set up settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Create index\n",
    "reader = MyObsidianReader(input_dir=\"/path/to/obsidian/vault\")\n",
    "documents = reader.my_load_data()\n",
    "index = create_enhanced_index(documents, embed_model, llm)\n",
    "\n",
    "# Create chat interface\n",
    "obsidian_chat = PersonalObsidianChat(index, llm)\n",
    "\n",
    "# Example usage\n",
    "query = \"What are my notes about social perception?\"\n",
    "response = obsidian_chat.chat(query)\n",
    "print(response)\n",
    "\n",
    "# Search specific notes\n",
    "relevant_notes = obsidian_chat.search_notes(\"Nice Guys\")\n",
    "for note in relevant_notes:\n",
    "    print(f\"Relevance: {note.score:.2f}\")\n",
    "    print(note.text[:200] + \"...\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
