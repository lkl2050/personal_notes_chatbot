{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    Settings,\n",
    "    Document\n",
    ")\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    Settings,\n",
    "    Document\n",
    ")\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.chat_engine import SimpleChatEngine\n",
    "from llama_index.core.prompts import PromptTemplate, MessageRole\n",
    "\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.readers.base import BaseReader\n",
    "from llama_index.readers.file.markdown import MarkdownReader\n",
    "\n",
    "class ObsidianProcessor:\n",
    "    def __init__(self, chunk_size: int = 512, chunk_overlap: int = 50):\n",
    "        self.node_parser = SentenceSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean Obsidian-specific markdown and formatting\"\"\"\n",
    "        # Remove Obsidian internal links [[...]]\n",
    "        text = re.sub(r'\\[\\[([^\\]]+)\\]\\]', r'\\1', text)\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "        # Remove empty lines\n",
    "        text = '\\n'.join(line for line in text.split('\\n') if line.strip())\n",
    "        return text\n",
    "\n",
    "    def process_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"Process and chunk documents\"\"\"\n",
    "        cleaned_docs = []\n",
    "        for doc in documents:\n",
    "            if doc.text.strip():  # Skip empty documents\n",
    "                cleaned_text = self.clean_text(doc.text)\n",
    "                if cleaned_text:\n",
    "                    doc.text = cleaned_text\n",
    "                    cleaned_docs.append(doc)\n",
    "\n",
    "        # Convert nodes back to documents\n",
    "        nodes = self.node_parser.get_nodes_from_documents(cleaned_docs)\n",
    "        return [Document(text=node.text) for node in nodes]\n",
    "\n",
    "class MyObsidianReader(BaseReader):\n",
    "    def __init__(self, input_dir: str):\n",
    "        self.input_dir = Path(input_dir)\n",
    "\n",
    "    def my_load_data(self):\n",
    "        docs = []\n",
    "        for dirpath, dirnames, filenames in os.walk(self.input_dir):\n",
    "            # Skip certain directories\n",
    "            if \"Images_Media\" in dirnames:\n",
    "                dirnames.remove(\"Images_Media\")\n",
    "            dirnames[:] = [d for d in dirnames if not d.startswith(\".\")]\n",
    "\n",
    "            for filename in filenames:\n",
    "                if filename.endswith(\".md\"):\n",
    "                    filepath = os.path.join(dirpath, filename)\n",
    "                    content = MarkdownReader().load_data(Path(filepath))\n",
    "                    docs.extend(content)\n",
    "        return docs\n",
    "\n",
    "\n",
    "class PersonalObsidianChat:\n",
    "    def __init__(self, index):\n",
    "        self.index = index\n",
    "        self.retriever = self.index.as_retriever(\n",
    "            similarity_top_k=3\n",
    "        )\n",
    "\n",
    "        # Create a more focused query engine\n",
    "        self.query_engine = self.index.as_query_engine(\n",
    "            similarity_top_k=3,\n",
    "            response_mode=\"tree_summarize\",  # Changed response mode\n",
    "            streaming=False\n",
    "        )\n",
    "\n",
    "    def chat(self, query: str) -> str:\n",
    "        try:\n",
    "            # First check if the query is empty or just whitespace\n",
    "            if not query or query.isspace():\n",
    "                return \"Please provide a valid query.\"\n",
    "\n",
    "            print(f\"Processing chat query: {query}\")  # Debug print\n",
    "\n",
    "            # Get relevant context\n",
    "            nodes = self.retriever.retrieve(query)\n",
    "            if not nodes:\n",
    "                return \"No relevant information found in your notes.\"\n",
    "\n",
    "            # Build context string\n",
    "            context = \"\\n\".join([\n",
    "                f\"Context {i+1}: {node.node.text if hasattr(node, 'node') else node.text}\"\n",
    "                for i, node in enumerate(nodes)\n",
    "            ])\n",
    "\n",
    "            # Form complete query with context\n",
    "            complete_query = f\"\"\"\n",
    "            Question: {query}\n",
    "            Based on the following context from notes:\n",
    "            {context}\n",
    "\n",
    "            Please provide a focused answer specifically addressing the question.\n",
    "            \"\"\"\n",
    "\n",
    "            response = self.query_engine.query(complete_query)\n",
    "            return response.response\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error in chat: {str(e)}\"\n",
    "\n",
    "    def query(self, query: str) -> str:\n",
    "        try:\n",
    "            if not query or query.isspace():\n",
    "                return \"Please provide a valid query.\"\n",
    "\n",
    "            print(f\"Processing direct query: {query}\")  # Debug print\n",
    "\n",
    "            response = self.query_engine.query(query)\n",
    "            if not response or not response.response:\n",
    "                return \"No relevant information found.\"\n",
    "\n",
    "            return response.response\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error in query: {str(e)}\"\n",
    "\n",
    "    def search_notes(self, query: str) -> list:\n",
    "        print(\"=== Debug: Starting search_notes method ===\")  # Debug print\n",
    "        try:\n",
    "            print(f\"Debug: Query received: '{query}'\")\n",
    "\n",
    "            if not self.retriever:\n",
    "                print(\"Debug: Retriever is not initialized\")\n",
    "                return []\n",
    "\n",
    "            print(\"Debug: Calling retriever.retrieve()\")\n",
    "            results = self.retriever.retrieve(query)\n",
    "\n",
    "            print(f\"Debug: Retriever returned {len(results) if results else 0} results\")\n",
    "\n",
    "            if not results:\n",
    "                print(\"Debug: No results found\")\n",
    "                return []\n",
    "\n",
    "            print(\"Debug: Processing results\")\n",
    "            processed_results = []\n",
    "            for result in results:\n",
    "                if hasattr(result, 'node'):\n",
    "                    print(f\"Debug: Found NodeWithScore object, score: {result.score}\")\n",
    "                    processed_results.append(result)\n",
    "                else:\n",
    "                    print(f\"Debug: Found regular Node object\")\n",
    "                    processed_results.append(result)\n",
    "\n",
    "            print(f\"Debug: Returning {len(processed_results)} processed results\")\n",
    "            return processed_results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Debug: Error occurred in search_notes: {str(e)}\")\n",
    "            import traceback\n",
    "            print(\"Debug: Full traceback:\")\n",
    "            print(traceback.format_exc())\n",
    "            return []\n",
    "\n",
    "def init_llm():\n",
    "    try:\n",
    "        llm = Ollama(\n",
    "            model=\"tinydolphin\",\n",
    "            request_timeout=120.0,\n",
    "            temperature=0.7,  # Add temperature control\n",
    "            context_window=2048  # Set context window\n",
    "        )\n",
    "        return llm\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing LLM: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_enhanced_index(documents: List[Document]):\n",
    "    try:\n",
    "        processor = ObsidianProcessor(chunk_size=512, chunk_overlap=50)\n",
    "        processed_docs = processor.process_documents(documents)\n",
    "\n",
    "        if not processed_docs:\n",
    "            print(\"Warning: No documents to index\")\n",
    "            return None\n",
    "\n",
    "        return VectorStoreIndex.from_documents(\n",
    "            processed_docs,\n",
    "            show_progress=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating index: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import ChatMessage, MessageRole\n",
    "\n",
    "chat_prompt = PromptTemplate(\n",
    "    template=(\n",
    "        \"You are an AI assistant helping to search and analyze personal notes from an Obsidian vault. \"\n",
    "        \"Use the following context from the notes to answer the question. \"\n",
    "        \"If you cannot find relevant information in the context, say 'I cannot find relevant information about that in your notes.'\\n\\n\"\n",
    "        \"Context: {context}\\n\\n\"\n",
    "        \"Human: {query}\\n\\n\"\n",
    "        \"Assistant: \"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/py39/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a2dc1786eb4a029bc216c3803fb33f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/4872 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed89f12b18041a481183cbdb65396d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c84c9da7e347e08b7b95a894e3ed34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea778c649b434da7ad8ad5194b003e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/784 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Main execution\n",
    "\n",
    "# Initialize LLM\n",
    "llm = init_llm()\n",
    "\n",
    "# Set up global settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "# Load and process documents\n",
    "reader = MyObsidianReader(input_dir=\"/Users/cairo/Library/Mobile Documents/iCloud~md~obsidian/Documents\")\n",
    "\n",
    "\n",
    "documents = reader.my_load_data()\n",
    "\n",
    "# Create index\n",
    "index = create_enhanced_index(documents)\n",
    "\n",
    "# Create chat interface\n",
    "\n",
    "# obsidian_chat = PersonalObsidianChat(index)\n",
    "# response = obsidian_chat.chat(\"What are my notes about social perception?\")\n",
    "# print(response)\n",
    "\n",
    "\n",
    "# # Interactive chat loop\n",
    "# while True:\n",
    "#     query = input(\"You: \")\n",
    "#     if query.lower() in ['quit', 'exit']:\n",
    "#         break\n",
    "#     response = obsidian_chat.chat(query)\n",
    "#     print(f\"Bot: {response}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_interface():\n",
    "    print(\"Debug: Creating PersonalObsidianChat instance\")\n",
    "    obsidian_chat = PersonalObsidianChat(index)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\nOptions:\")\n",
    "        print(\"1. Chat\")\n",
    "        print(\"2. Query\")\n",
    "        print(\"3. Search notes\")\n",
    "        print(\"4. Exit\")\n",
    "\n",
    "        # Menu choice with validation loop\n",
    "        valid_choice = False\n",
    "        while not valid_choice:\n",
    "            choice = input(\"Choose an option (1-4): \").strip()\n",
    "            print(f\"Debug: User selected option: '{choice}'\")\n",
    "\n",
    "            if choice in [\"1\", \"2\", \"3\", \"4\"]:\n",
    "                valid_choice = True\n",
    "            elif choice:  # Only show error for non-empty invalid input\n",
    "                print(\"Invalid option. Please enter 1, 2, 3, or 4.\")\n",
    "\n",
    "        if choice == \"3\":\n",
    "            # Separate prompt for search\n",
    "            print(\"\\nSearch Mode:\")\n",
    "            while True:\n",
    "                search_term = input(\"Enter your search term (or 'back' to return to menu): \").strip()\n",
    "                if search_term.lower() == 'back':\n",
    "                    break\n",
    "\n",
    "                if not search_term:\n",
    "                    print(\"Search term cannot be empty. Please try again.\")\n",
    "                    continue\n",
    "\n",
    "                if search_term == \"3\":\n",
    "                    print(\"Invalid search term. Please enter your actual search keywords.\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"Debug: Searching for: '{search_term}'\")\n",
    "                results = obsidian_chat.search_notes(search_term)\n",
    "\n",
    "                if results:\n",
    "                    for i, result in enumerate(results, 1):\n",
    "                        print(f\"\\nResult {i}:\")\n",
    "                        if hasattr(result, 'node'):\n",
    "                            text = result.node.text\n",
    "                            score = result.score\n",
    "                        else:\n",
    "                            text = result.text\n",
    "                            score = getattr(result, 'score', 'N/A')\n",
    "\n",
    "                        print(f\"Relevance Score: {score}\")\n",
    "                        if text:\n",
    "                            print(f\"Content: {text[:300]}...\")\n",
    "                        else:\n",
    "                            print(\"No content available\")\n",
    "                        print(\"-\" * 50)\n",
    "                else:\n",
    "                    print(\"No results found for your search term.\")\n",
    "                break\n",
    "\n",
    "        elif choice == \"1\":\n",
    "            chat_input = input(\"Enter your question: \").strip()\n",
    "            if chat_input:\n",
    "                response = obsidian_chat.chat(chat_input)\n",
    "                print(f\"\\nResponse: {response}\")\n",
    "\n",
    "        elif choice == \"2\":\n",
    "            query = input(\"Enter your query: \").strip()\n",
    "            if query:\n",
    "                response = obsidian_chat.query(query)\n",
    "                print(f\"\\nResponse: {response}\")\n",
    "\n",
    "        elif choice == \"4\":\n",
    "            print(\"Goodbye!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents loaded: 2306\n",
      "Index created successfully: True\n",
      "LLM initialized: True\n",
      "Debug: Creating PersonalObsidianChat instance\n",
      "\n",
      "Options:\n",
      "1. Chat\n",
      "2. Query\n",
      "3. Search notes\n",
      "4. Exit\n",
      "Debug: User selected option: '3'\n",
      "\n",
      "Search Mode:\n",
      "Debug: Searching for: 'social'\n",
      "=== Debug: Starting search_notes method ===\n",
      "Debug: Query received: 'social'\n",
      "Debug: Calling retriever.retrieve()\n",
      "Debug: Retriever returned 3 results\n",
      "Debug: Processing results\n",
      "Debug: Found NodeWithScore object, score: 0.7557319065455824\n",
      "Debug: Found NodeWithScore object, score: 0.7555729386853011\n",
      "Debug: Found NodeWithScore object, score: 0.7441187710480391\n",
      "Debug: Returning 3 processed results\n",
      "\n",
      "Result 1:\n",
      "Relevance Score: 0.7557319065455824\n",
      "Content: Social media项目\n",
      "background/problem: Mkt这块，研究人们如何表达自己的人设的东西很多，比如我最近买了什么，人们会不会说，用什么方式来说，叫word of mouth。但研究别人如何来看待这些人设的很少。所以我这个项目是研究audience如何看待人们在socail media上发的东西。\n",
      "我觉得线上和线下的一个主要的区别是，social media上一个人的身份是很多元的，比如一个人可以展现他是父亲，是球迷，是摇滚乐粉丝，但线下其实其他人很多时候看不到这种多元化的人设，比如我一个经常和我一起上课的同学，可能我只能看到他是学霸这一点，并不太清楚他喜欢什么音乐这些...\n",
      "--------------------------------------------------\n",
      "\n",
      "Result 2:\n",
      "Relevance Score: 0.7555729386853011\n",
      "Content: other to-do\n",
      "- Does SNS usage make people feel more (in)authentic? 之前测的是，人们用SNS越多，他越觉得SNS上的identity和自己的trueself overlap；没有测他的general authentiticity feeling\n",
      "- environmentalist vs. high emission muschle cars\n",
      "- less correlated labels on instagram means they are more conflicted\n",
      "- mediator: to what extent...\n",
      "--------------------------------------------------\n",
      "\n",
      "Result 3:\n",
      "Relevance Score: 0.7441187710480391\n",
      "Content: **Describe the Product**:\n",
      "**Facebook** is a leading **social media** platform that offers users a platform to share and express different aspects of their life. With its multiple features such as Messenger, Marketplace, Groups, Games, and Jobs, Facebook provides a comprehensive experience to users.\n",
      "...\n",
      "--------------------------------------------------\n",
      "\n",
      "Options:\n",
      "1. Chat\n",
      "2. Query\n",
      "3. Search notes\n",
      "4. Exit\n",
      "Debug: User selected option: ''\n",
      "Debug: User selected option: '1'\n",
      "Processing chat query: whats my identity based on my notes\n",
      "\n",
      "Response:  Based on my notes, I am an individual whose identity is documented in the context provided. The context includes information about me as the person referenced within the given note. In this case, it refers to user. \n",
      "\n",
      "To provide a more detailed answer: Based on my notes, I'm an individual who appears to be referred to by the term \"user\" in a context that mentions being documented or written down in some manner. This is consistent with the information provided in the context from multiple sources and not prior knowledge.\n",
      "\n",
      "Options:\n",
      "1. Chat\n",
      "2. Query\n",
      "3. Search notes\n",
      "4. Exit\n",
      "Debug: User selected option: ''\n",
      "Debug: User selected option: '1'\n",
      "Processing chat query: what's my personality based on your analysis of my notes\n",
      "\n",
      "Response:  Based on your notes, you are someone who has written or collected information on various topics. It's difficult to determine specific details about you, such as your age, gender, or occupation, from the provided notes alone. However, you seem to have an interest in a wide range of subjects, including self-help, social perception, motives, and metacognition. You may be a student, researcher, or someone who enjoys learning new things.\n",
      "\n",
      "Options:\n",
      "1. Chat\n",
      "2. Query\n",
      "3. Search notes\n",
      "4. Exit\n",
      "Debug: User selected option: '1'\n",
      "Processing chat query: can you run deeper analysis of my personality\n",
      "\n",
      "Response:  Based on the context from notes, it seems that you are exploring your personality and self-worth. This process often involves introspection and evaluation, which may involve confrontation and change in certain areas of their life. However, this process may also involve understanding their own identity.\n",
      "\n",
      "In conclusion, while the person's reflection and self-examination process involves introspection and evaluation, it seems that the focus is more on personal growth and development than reflecting on their notes or understanding their own identity.\n",
      "\n",
      "Based on the context from notes, it seems that you are exploring your personality and self-worth. This process often involves introspection and evaluation, which may involve confrontation and change in certain areas of their life. However, this process may also involve understanding their own identity.\n",
      "\n",
      "In conclusion, while the person's reflection and self-examination process involves introspection and evaluation, it seems that the focus is more on personal growth and development than reflecting on their notes or understanding their own identity.\n",
      "\n",
      "Options:\n",
      "1. Chat\n",
      "2. Query\n",
      "3. Search notes\n",
      "4. Exit\n",
      "Debug: User selected option: '1'\n",
      "Processing chat query: analyze my career based on my notes\n",
      "\n",
      "Response:  Based on your notes, you are someone who has written or read notes on various topics, including self-help books, social perception theories, motives, leaving a job, and metacognition. You have expressed thoughts on the importance of personal growth, the role of social relationships, and the challenges of making decisions and judgments. You have also shown an interest in psychology and philosophy. The context information from multiple sources is below.\n",
      "\n",
      "Assistant to the User\n",
      "\n",
      "Options:\n",
      "1. Chat\n",
      "2. Query\n",
      "3. Search notes\n",
      "4. Exit\n",
      "Debug: User selected option: ''\n",
      "Debug: User selected option: '1'\n",
      "Processing chat query: summarize my notes about data science\n",
      "\n",
      "Response:  To visualize more than three dimensions of data in a single chart, we can use color, size, and shape cues. By combining these visual cues, we can create an intuitive representation of our dataset that is easy to understand and visually appealing at the same time.\n",
      "\n",
      "For example, if you are looking at sales volume for each location over the past three months, you could create a visual representation that shows each location with its own area under the curve. This would allow users to quickly understand which regions are performing well and which areas might be experiencing slow growth. Similarly, if you wanted to visually represent the relationship between user engagement metrics and specific demographics, you could use color to indicate the presence of a positive trend in one variable while using size to represent the magnitude of that trend.\n",
      "\n",
      "By utilizing color, size, and shape cues, we can effectively convey the relationships between different variables while also allowing users to interpret the data in a clear, understandable format. By combining these visual cues, we can create an intuitive representation of our dataset that is easy to understand and visually appealing at the same time.\n",
      "\n",
      "Options:\n",
      "1. Chat\n",
      "2. Query\n",
      "3. Search notes\n",
      "4. Exit\n",
      "Debug: User selected option: '3'\n",
      "\n",
      "Search Mode:\n",
      "Debug: Searching for: 'transformer'\n",
      "=== Debug: Starting search_notes method ===\n",
      "Debug: Query received: 'transformer'\n",
      "Debug: Calling retriever.retrieve()\n",
      "Debug: Retriever returned 3 results\n",
      "Debug: Processing results\n",
      "Debug: Found NodeWithScore object, score: 0.891647454995049\n",
      "Debug: Found NodeWithScore object, score: 0.827481723282233\n",
      "Debug: Found NodeWithScore object, score: 0.804409782217907\n",
      "Debug: Returning 3 processed results\n",
      "\n",
      "Result 1:\n",
      "Relevance Score: 0.891647454995049\n",
      "Content: Transformer...\n",
      "--------------------------------------------------\n",
      "\n",
      "Result 2:\n",
      "Relevance Score: 0.827481723282233\n",
      "Content: sentence transformer\n",
      "!Untitled...\n",
      "--------------------------------------------------\n",
      "\n",
      "Result 3:\n",
      "Relevance Score: 0.804409782217907\n",
      "Content: Transformers\n",
      "The transformer is a new encoder-decoder architecture that uses only the attention mechanism instead of RNN to encode each position, to relate two distant words of both the inputs and outputs w.r.t. itself, which then can be parallelized, thus accelerating the training. As RNN is sequen...\n",
      "--------------------------------------------------\n",
      "\n",
      "Options:\n",
      "1. Chat\n",
      "2. Query\n",
      "3. Search notes\n",
      "4. Exit\n",
      "Debug: User selected option: '1'\n",
      "Processing chat query: summarize my notes about transformers\n",
      "\n",
      "Response:  The context provided in your notes revolves around transformers which are a family of neural network architectures and models that have been widely adopted for natural language processing tasks such as machine translation, summarization, and sentiment analysis. Transformers are composed of multiple layers, each with its own set of learnable parameters or 'heads'. In this context, we are discussing the transformer architecture and how it is used to perform language translation tasks.\n",
      "\n",
      "The question you posed is about a transformer's ability to perform well on complex natural language understanding tasks, such as summarization and sentiment analysis. Transformers have shown to outperform traditional machine learning approaches in these domains by being able to learn complex relationships between words, sentences, and even entire phrases, which can then be used for prediction, generation, or classification tasks.\n",
      "\n",
      "Transformers are known for their ability to learn long-term dependencies and can generate new information based on the knowledge already stored within them. They are widely used in natural language processing due to their ability to capture complex relationships between words and their meaning. Transformers also have the advantage of being able to handle both short and long sequences, which is beneficial for tasks such as natural language understanding and generation.\n",
      "\n",
      "In summary, transformers have the potential to perform well on a variety of natural language processing tasks due to their ability to learn complex relationships between words and their meanings, making them more powerful than traditional machine learning approaches in many domains.\n",
      "\n",
      "Options:\n",
      "1. Chat\n",
      "2. Query\n",
      "3. Search notes\n",
      "4. Exit\n",
      "Debug: User selected option: '1'\n",
      "Processing chat query: show me top docs containing transformer related content\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Number of documents loaded:\", len(documents))\n",
    "print(\"Index created successfully:\", index is not None)\n",
    "print(\"LLM initialized:\", llm is not None)\n",
    "\n",
    "chat_interface()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
