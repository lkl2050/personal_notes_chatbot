{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import qdrant_client\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    StorageContext\n",
    ")\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import Settings\n",
    "\n",
    "from llama_index.readers.obsidian import ObsidianReader\n",
    "from llama_index.core.readers.base import BaseReader\n",
    "from llama_index.readers.file.markdown import MarkdownReader\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.document_loaders import ObsidianLoader\n",
    "# loader = ObsidianLoader(\"/Users/cairo/Library/Mobile Documents/iCloud~md~obsidian/Documents\")\n",
    "# docs = loader.load()\n",
    "# print(docs[4].page_content[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyObsidianReader(BaseReader):\n",
    "    \"\"\"Utilities for loading data from an Obsidian Vault.\n",
    "\n",
    "    Args:\n",
    "        input_dir (str): Path to the vault.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dir: str):\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self.input_dir = Path(input_dir)\n",
    "\n",
    "    def my_load_data(self, *args, **load_kwargs):\n",
    "        \"\"\"Load data from the input directory.\"\"\"\n",
    "        docs = []\n",
    "        for dirpath, dirnames, filenames in os.walk(self.input_dir):\n",
    "            # Exclude 'image_media' from directory traversal\n",
    "            if \"Images_Media\" in dirnames:\n",
    "                dirnames.remove(\"Images_Media\")\n",
    "            dirnames[:] = [d for d in dirnames if not d.startswith(\".\")]\n",
    "            for filename in filenames:\n",
    "                if filename.endswith(\".md\"):\n",
    "                    filepath = os.path.join(dirpath, filename)\n",
    "                    content = MarkdownReader().load_data(Path(filepath))\n",
    "                    docs.extend(content)\n",
    "        return docs\n",
    "\n",
    "    def load_langchain_documents(self, **load_kwargs):\n",
    "        \"\"\"Load data in LangChain document format.\"\"\"\n",
    "        docs = self.load_data(**load_kwargs)\n",
    "        return [d.to_langchain_format() for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/py39/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize ObsidianReader with the path to the Obsidian vault\n",
    "reader = MyObsidianReader(input_dir=\"/Users/cairo/Library/Mobile Documents/iCloud~md~obsidian/Documents\")\n",
    "\n",
    "# Load data from the Obsidian vault\n",
    "documents1 = reader.my_load_data()\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='ae4ef000-56c8-4d53-8312-bfa6717f144c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6fd348ac-515e-4635-bc0c-8497cef8d7bb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n海带黄豆排骨汤\\n\\n海带除了出名的利尿消肿、补充碘元素、抗辐射，还能补脑 ~ 而且日本人说的味素（也就是味精）之所以能够被发明也是因为海带！为啥？因为鲜啊！而且价钱便宜，是我们普通人家日常都能经常喝的美味~\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='99c06f2d-037e-4558-b6dd-cf163e0a64cf', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n食材：\\n\\n猪骨大概1磅，海带干一大片，黄豆1把，姜一块\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='60942faa-8ba3-415d-b356-63ddce9b9948', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n步骤：\\n\\n1. 黄豆提前至少2小时洗净泡发；\\n\\n2. 洗净的猪骨放入instant pot，加水没过猪骨，再放一小块姜，保持开盖，按下saute，水开后加料酒，用勺子舀去血沫，如果不介意血沫的宝宝可以跳过这一步直接把黄豆和猪骨一起煮；\\n\\n3. 把泡好的黄豆倒入锅中，按soup/broth煮半小时；\\n\\n4. 放气后加入冲过一遍的干海带，按soup/broth再煮15-20分钟。\\n\\n**如果海带是已经泡发好的，加入海带后煮汤的时间要缩短\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b86820dc-936a-4c36-afe0-8d99fd554f93', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n空气炸锅炸蛋\\n纸上涂油，360F炸 6min\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fd0a6919-6734-45ed-bd27-5831e289ecb4', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n空气炸锅鸡腿350F；鸡翅时间减少\\n上层15 分钟之后翻面再 14min\\n下层把上层取出来后，再单独7-10min\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fa3bf0a2-7570-481c-9078-e60e7fac6190', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\ninstapot 蒸蛋\\nno pressure 10min\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0fc06a63-5348-4ad0-84dc-9472aaf5b76e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n蛋花汤\\n**教大家真的懒人蛋花汤：** 直接在碗里加盐加味精加油加蛋液加紫菜加葱花，然后一壶滚烫的开水下去\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5682bdab-aaa7-4474-aa50-bfbf349520c5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n牛排\\n\\n1.5cm的\\n\\n一定要擦干\\n\\n牛排放外面回温一小时，两面拿纸吸干水分\\n\\n  \\n\\n油盐黑胡椒一起裹上，马上下锅；或者盐+黑胡椒;两面磨粗盐\\n\\n  \\n\\n用烟点高的油比较好，比如芥花籽油，黄油不好，可以在牛排两面初步上色煎黄过后加入一块，煎的时候不停把黄油浇到肉上，可以加入大蒜和迷迭香等香料\\n\\n  \\n\\n下锅前加油，油足够热冒烟\\n\\n1.5cm的每面大概1分钟，可以翻面，要看锅的厚度调整; 看牛排厚度，不过一面四五十秒左右，每面煎两次就行\\n\\n  \\n\\n在每面煎完第一次翻过来的时候在上面磨黑胡椒; 这样确保胡椒不会焦得太厉害\\n\\n  \\n\\n可以不停地翻面，厚的需要每面两分钟or以上\\n\\n铸铁锅最好；不粘锅不推荐\\n\\n  \\n\\n醒肉两分钟以上，等于煎牛排的时间，可以盖上锡纸防止放凉\\n\\n  \\n\\n横着放，45度切，撒海盐\\n\\n  \\n\\n酱汁：\\n\\n3颗蒜压碎\\n\\n20克左右的姜切小块2颗红葱头切小块留锅底底油爆香料头沙茶酱汁\\n\\n竹生抽1勺耗油2勺白胡椒2勺沙茶酱 适量清水和开将锅底的焦化层冲开\\n\\n中火收汁到浓稠\\n\\n浇到牛排上\\n\\n\\n----\\n\\n\\n\\n\\nOrganize everything in your life in one place.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='29441aa7-e969-4de3-9624-0e3914208848', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nShopping list\\n\\nMEC T2 Warmer Wind Block Gloves - Unisex | MEC\\n\\n\\nIgnition Model 1/64 Mazda RX-7 (FC3S) RE Amemiya Green IG2496 – Tokyo Station\\n\\nIG2463 RWB 964 Matte Black – ignition model\\n\\n---\\n\\n牛肉菜谱：墨西哥辣椒，橙皮，八角，小茴香，孜然，海南辣椒，zy家的辣椒面，干辣椒，花椒面；肉和辣椒要分开弄熟，下肉的时候下料酒；肉要先和料版一起\\n\\nhttps://post.smzdm.com/p/a83gl87n/\\n\\nSymfonisk 音箱\\n\\n床单135 cm 189 cm 53 1/8 \" 74 3/8 \"\\n\\nIK iloud Micro Monitor\\n\\nOSPREY 小鹰日闪6升运动跑步斜挎包多功能防水登山跑步防盗隐形贴身手机包袋休闲户外DAYLITE SLING\\n\\n乳胶枕\\n\\n跑鞋\\nhttps://post.smzdm.com/p/670798/?promotion=WEBHPPBBNL01\\n\\nporter tank 包日本\\n\\n迪卡侬 SH500 袜子\\n\\nNIKE ZOOM PEGASUS 35 TURBO 灰色\\n\\nTravel pack\\nhttp://www.tortugabackpacks.com/\\n\\nPatagonia Torrentshell 冲锋衣\\n\\nTom bihm TRAVEL TRAY\\n\\nGamma MX hoody Jacket\\n\\nhttps://bellroy.com/products/slim-sleeve-wallet/black\\n\\nhttps://bellroy.com/products/travel-wallet/leather_rfid/black/#image-8\\n\\nhttps://www.tombihn.com/products/the-guides-pack?variant=16409789831\\n\\nhttp://post.smzdm.com/p/561761/p2/#comments\\n\\nhttps://www.zhihu.com/question/30937592/answer/158666462', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='22de7f84-295f-402f-8bd2-09d5c697a440', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='921a2364-988b-429a-8315-37b983b9cdda', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0d828b31-1c22-4aaa-9812-74cee5ce6dd2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='28d7e551-4d5b-42e9-9dd1-b8cd2fe5b900', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1f3aa486-715c-4a3e-a0ca-2fda3f5c2a46', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b32f5c32-db1e-4616-affa-c76cc8ffc0b7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e9fa00e4-a7b5-43b2-ae48-fa0bd8e0a5e3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n\\n#230502\\n\\n差点破戒\\n\\n意识到了，我破戒的原因是因为脑雾让自己开会听不进去，心情烦躁；但脑雾的原因就是破戒；如果能一周以上不破，脑雾就会改善很多，就不存在破戒的直接trigger了\\n\\n\\n#230503\\n\\n做出了超好吃的炸鸡腿！\\n\\n  \\n\\n#230507\\n\\nhiking之后身体过于疲累，干不了正事，就导致破戒；\\n\\n如何解决：疲劳就睡，或者看剧，不要强撑\\n\\n今天hiking 很疲累，后半段完全不想社交\\n\\n\\n#230508\\xa0\\n\\n运动之后是会累的干不了正事。要接受这一点\\n\\n  \\n\\n#230509\\n\\n疲劳和jenn不回我两个因素叠加导致破戒\\n\\n  \\n\\n#230510\\n\\npotential triggers: sukru约了明天的meeting让我紧张，虽然仔细想来没必要紧张\\n\\n发现我的credit score比预想的差；不是我现在能决定的\\n\\n  \\n\\nachievement: 去了银行；不打算浪费时间在昨天date的女身上\\n\\n  \\n\\n  \\n\\n  \\n\\n#230513\\n\\n我现在在尝试的一个做法，是记录自己工作中的「尖峰时刻」，也就是那些做得很好、值得翻阅和回味的经历。可以是你费劲终于解决了一个问题，也可以是你取得了一个成就、得到客户、老板或同事的赏识，还可以是你做出了一个非常棒的成果、让自己获得充实的成就感……\\n\\n  \\n\\n不妨都把它们记录下来。做一个表格，第一栏写上你的「尖峰时刻」，第二栏写上日期，第三栏简要概述为什么它值得记录，再把相关的截图粘贴进去，做成一本独属于你的「尖峰记录簿」。在闲暇时，可以拿出来翻阅、回味、思考、复盘。\\n\\n  \\n\\n看了bs总决赛很开心\\n\\n找到了喜欢的房子\\n\\n  \\n\\n  \\n\\n#230514\\n\\n因为刺激破戒\\n\\n  \\n\\n  \\n\\n#230516\\n\\n对avani很不爽，沟通中也比较不耐烦\\n\\n  \\n\\n找到真爱\\n\\n  \\n\\n可以分享生活中有意思的事；可以面对挫折时互相扶持；\\n\\n  \\n\\n早起，吃水果和坚果早餐；出去晒太阳锻炼\\n\\n在工作中承担责任，创造产品\\n\\n晚上能和爱的人一起做饭，主动学习，自在娱乐和游戏\\n\\n  \\n\\n  \\n\\n首先，想象一下 10 年后的自己，这将是一个非常理想的画面。\\n\\n让我们假设，在这期间，你生活中的一切都进展得非常顺利，你通过努力工作实现了自己所有的人生目标。你想做的任何事情都可以放心的去追求和体验,不用考虑时间、金钱和别人的看法。这个时候你的生活会是什么样的?你的一天会如何度过？你会做些什么事？你会和什么样的人在一起？\\n\\n然后，如果你的脑子里已经有了这样的画面，把它记录下来。推荐你拿起纸笔，或者挑一个喜欢的本子亲自写下来，在手写时，你可以更自由地去展开丰富的联想，思维的体验会比手机输入更加流畅。\\n\\n也许你想象中的画面非常丰富，你可以把这些细节都写下来，也可以让这个画面停留在你的脑海里，只把其中印象最深的，或者让你感觉最舒服、最喜欢的部分记录下来。\\n\\n最后,需要给自己的作品起一个简洁有力的名字,能够把这个画面中最重要的主题包含进去，例如：找到真爱，全家团圆，挑战极限,全球旅行。请不要使用「无题」这样的标题，任何的画面背后一定都包含着某种美好的愿景和预期，把它找到，然后写下来。\\n\\n  \\n\\n  \\n\\n  \\n\\n和理想的生活相比,现实的生活会有略显残酷的一面,会发生一些我们无法改变的,又不尽如人意的事情。当理想和现实被对立起来时,这种差距可能是我们感受到的压力、痛苦和挫折的来源。\\n\\n但理想和现实从来不是割裂的两个时空，而是连接在一起的一条完整的时间线。把它们串在一起的，就是被我们称之为「自我」的东西。\\n\\n「自我」是心理学永恒的研究主题,积极心理学的目标,其实是让自我变得更加强壮和灵活，强壮到有足够的力量去面对现实中的不完美，灵活到能够摆脱人为的束缚，自由的去追求理想生活。\\n\\n这可能是一个非常漫长的过程，需要你自己在实践中慢慢领会，但我希望这个目标能够在你心中埋下一颗种子。在这21天里,你需要做的,就是静静地来观察这个「自我」,去观察它的行动,去感受它的情绪,去琢磨它的想法，去接近它，了解它，和它成为朋友。这样你才能在未来的生活中帮助它变得更加强壮和灵活。\\n\\n  \\n\\n  \\n\\n#230517\\n\\n对重要的决策比如买房，一定要做好万全的准备\\n\\n  \\n\\n不管发生什么打击，一天中最重要的事都是关照自己；就算买到了理想的房子，没有健康的日常情绪也不可能幸福\\n\\n  \\n\\n  \\n\\n#230518\\n\\n首先，请你想象一下，从起床开始，你今天的经历就像是一个个画面或场景串联起来，也就是人们平常会说的「过电影」。用这样的方式来回顾你的一\\n\\n天。\\n\\n然后，按照时间顺序把每一个场景记录下来，给每个场景起一个恰当的名字(例如「工作汇报」或者「和家人一起吃早餐」),回忆一下这个场景开始\\n\\n和结束的具体时间。通常一个场景的时间在 15~120 分钟左右，一天大概能\\n\\n够分成 8-15 个片段。\\n\\n在想象场景时，请尽可能详细的回忆一下当时的心情，可以用语言把你的心情记录下来，再用 0-10 分的数字来打个分。\\n\\n在这个过程中，如果想到了今天发生的一些让你感到不舒服的事情，先把它们记录下来。就像是发生在别人身上的，或者电影里的情节，你只需要做到客观记录，不用在某件事情上纠结过长的时间。\\n\\n  \\n\\n我的一天经历了这些场景：\\n\\n  \\n\\n场景1：起始时间，起床，我当时的心情\\n\\n  \\n\\n场景2：起始时间，场景名称，我当时的心情\\n\\n  \\n\\n场景3：起始时间，场景名称，我当时的心情\\n\\n  \\n\\n……\\n\\n  \\n\\n场景8：起始时间，场景名称，我当时的心情\\n\\n  \\n\\n  \\n\\n  \\n\\n#230519\\n\\n半夜噩梦醒来，因为失去了想要的房子；但隐约还在安慰自己说，照顾自己最重要，不要影响了自己\\n\\n上午的工作卓有成效，有成就感；主动和sukru说了avani的事得到了支持\\n\\n下午发现了有意思的量化交易git repo\\n\\n晚上又尝试了下ps5的几款竞速游戏，感觉没啥意思\\n\\n晚上恢复了锻炼\\n\\n  \\n\\n其中我最期待的事情是：\\n\\njs第二天完成\\n\\n  \\n\\n可能会影响它发生的阻碍有：\\n\\n网上的一些刺激\\n\\n不用煎蛋xhs和b站等\\n\\n  \\n\\n我可以提前做好这些准备：\\n\\nself-control app\\n\\n删掉app\\n\\n  \\n\\n  \\n\\n  \\n\\n#230520\\n\\n我的优势是什么？\\n\\n  \\n\\n今天开心的事：下定决心卸载了游戏；\\n\\n小睡了一会；\\n\\n牙医还算成功\\n\\n和几个妹纸聊了天\\n\\n  \\n\\n  \\n\\n#230521\\n\\n和妹纸们聊天略有不顺，但问题不大\\n\\n玩到了很优质的极品飞车\\n\\n明天尝试，吃饭不看视频\\n\\n  \\n\\n#230522\\n\\n这个想法能推动我的行动吗\\n\\n  \\n\\n  \\n\\n#230523\\n\\n快破戒的时候，成功和妹纸约了聊天\\n\\n感谢gap批准了我的假\\n\\n  \\n\\n  \\n\\n#230524\\n\\n昨天重度失眠，导致今天精神萎靡，下午破戒\\n\\n失眠过后第二天不要给自己有太大压力，可以效率低一些没问题\\n\\n  \\n\\n  \\n\\n#230525\\n\\n今天主动冥想了\\n\\n居然买房的事出现了一丝曙光；更显得之前的自我惩罚毫无意义\\n\\n  \\n\\n  \\n\\n::记录每天做的事。哪些是符合我的价值的哪些不是::\\n\\n和bro聊天；努力工作，晚上看价值观，锻炼身体都算是符合价值的；\\n\\n看手机过多是不符合价值的\\n\\n  \\n\\n  \\n\\n#230526\\n\\n居然和日本妹纸聊得有来有回\\n\\n和朋友们聚餐很开心\\n\\n  \\n\\n  \\n\\n#230527\\n\\n险些破戒\\n\\n自己每天的情绪波动是有规律的；早中晚情绪如何，每天都会比较一致\\n\\n  \\n\\n  \\n\\n#230529\\n\\n完成了batch norm的视频\\n\\n晚上尽管身体很疲累，但还是坚持了锻炼\\n\\n  \\n\\ntracking my energy and mood\\n\\nmorning: 7E, 9M\\n\\nafternoon: 3E, 9M\\n\\nevening: 7E, 9M\\xa0\\n\\n  \\n\\n  \\n\\n#230530\\n\\n早上工作状态不错\\n\\n坚持了五天多有进步\\n\\n纯粹因为无聊破戒；下午完成了一些工作，瞬间觉得没有目标，感到彻底的无聊\\n\\n晚上恢复了状态工作了几小时，解决了一些难题\\n\\n坚持了锻炼\\n\\n  \\n\\n#230531\\n\\n  \\n\\n今天出门在路上看到了很多之前没注意过的涂鸦\\n\\n买到了鸡架\\n\\n  \\n\\n感受到了夏日的第一缕微风', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b75c06f3-8c38-467b-8ac5-08f554e1debc', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0cb4d208-09b8-49cd-a305-be675afa2a18', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d078ec06-f0cd-4dd9-b2fd-e6a9596f854c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b40be8b5-1560-4281-9544-f10afaa9e2d8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5f0abf48-37fc-41f7-b125-86b9209886d6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8ad2baec-85da-4587-b623-0a449b16df74', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5d27c9ff-eebe-436c-b380-596f00745524', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='009fbe4a-dd36-49d1-ae96-522773c24941', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n开始了新一轮的冥想练习。关怀自己的初心不能丢\\n找到了一本好书 by jan morris\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='79dd0f62-f340-4a58-b08f-aa2ff38b7376', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n完成了人生一大步，买房\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='59b7c7fa-0a1f-493e-b2b4-6c54d0c4d0f3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n谢谢tom给我把贷款搞定了；感谢买房途中的人\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bf578b86-be6e-4291-b13a-5e8cbb576bcb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n因为工作上的压力导致去看h破戒\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5a50b909-2746-4bea-bab8-47238c3a2b8f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄 done\\n- 不用手机 done\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='44913abf-b2ec-4aa6-af6a-35722bd9f3f0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n充实的一天\\n解决了所有家具的问题\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='57cc7518-9a9c-4c66-ad0f-9d0b1c909980', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c0bf3faf-b8ab-4d08-a802-9620859a908a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='efd8371b-fecc-4888-bb04-d7403949dfc3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c653a18a-2da2-4624-8634-2fc45b5b5539', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cf68e0d9-138f-4334-be1c-e52ca074863c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='773eb6fc-de32-4671-9b52-551acbd42404', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nEBP 课程记录\\n\\n!Untitled\\n\\n!Untitled\\n\\n1.我参加行动营的初心是：\\n有人陪伴和监督我的心理改变\\n\\n2.参加行动过程中，我积累了这些成就：\\n每天按时完成了行动作业；\\n学习了很多细节，了解到自己之前的改变为何总不了了之\\n\\n3.这些方法能帮我更好的坚持行动：\\n回到当下；\\n书写情绪；\\n诚实地面对自己；\\n关怀自己，和自己合作\\n\\n未来，我将继续实践这些技能，在人生的每一个时刻，照顾好自己。\\n\\n1.近期生活中，我想要达成的目标是：\\n去大厂！！！\\n\\n2.这个目标背后的价值是：\\n一个智力和意志力上的挑战；\\n创造；为以后创业做准备\\n\\n3.我可以把这个目标，拆解成以下几步\\n\\n第一步：\\n每天至少刷一道题，周末解决难题；坚持至少三个月\\n\\n第二步：\\n工作时间效率要高，晚上不加班\\n\\n第三步：\\n准备面试\\n…………\\n\\n4.马上就能开始做的、最容易达成的一个步骤是：\\n复习一道题\\n\\n5.你打算今天什么时候，开始做这一步：\\n明早上班前\\n\\n6.行动过程中，我可能会遇到这些阻碍：\\n畏难，自卑，怀疑自己；看不懂答案；看懂了自己也写不出\\n\\n（情绪、想法、感觉、欲望）\\n\\n7.回顾学到的技巧，当遇到困难、挑战的时候，我可以这样做：\\n让自己慢下来，回到题目上来，让思绪不要逃避\\n进一步拆解题目，或者去搜不同类型的答案和视频解答\\n\\n——————————————————————\\n\\n你可以把今天的书写，当作日常练习\\n\\n可以用它去制定一个新目标，养成一个新习惯\\n\\n1.我想到的事情是：\\n刷题遇到的难题解决不了\\n\\n2.我注意到，当时 _________ （你的名字）有 _________ 的情绪；\\n畏难，自卑，烦躁\\n\\n3.我注意到，当时 _________ （你的名字）有一些这样的想法：\\n自信心崩塌，怀疑自己的职业前景\\n\\n4.我注意到，当时 _________ （你的名字）的 _________ （身体部位），\\n有一些这样的感受：\\nNA\\n\\n5.当时，我采取了这样的行动：\\n看视频逃避；\\n\\n6.回顾学到的技巧，我现在可以采取这些新行动：\\n出去走一圈放松下大脑；稍微锻炼一下\\n把问题拆分，仔细想想自己到底是哪里没理解，而不是简单的感觉不会\\n\\n7.新行动带来的短期结果是：\\n不被情绪影响我的行动；不会丧失信心\\n\\n8.新行动带来的长期结果是：\\n职业上的成功\\n——————————————————————\\n\\n被情绪感受套住时，你可以在内心默念：\\n\\n「我注意到，我此刻有一个 **如何如何**_ 的想法 / 情绪 / 躯体感受」来解套\\n\\n/拿起即用的「应急工具」\\n如果情绪强烈,想快速降温\\n尝试「情绪小锦囊」\\n离开压力环境,去户外走十分钟(day5正念)\\n命名当下,着陆当下(day8陪伴者分享)\\n如果情绪中度及以下,无法聚焦手头事情\\n情绪/想法命名(day14 书写)\\n02\\n/随时进行的「日常练习」\\n积累积极资源,持续提升幸福觉察力\\n三件好事(day1-4书写)\\n·开心清单(day7书写)\\n成就清单(day8 书写)\\n重新认识消极情绪,以「接纳」的态度与之相处\\n读懂消极情绪传达的信号(day10书写)\\n梳理情绪产生过程,了解自己的反应模式(day11书写)\\n情绪气象图,了解情绪变化特点(day12书写)\\n澄清价值,探索新行动\\n分析行为的长期/短期结果,决定是否保持(day15书写)\\n澄清行为是否满足期待,发现其他新行动(day16书写)\\n探索重视的价值,找到朝它前进的第一步(day17-18心理测试、书写)\\n\\n我重视的价值是：\\n创造\\n\\n它对我来说，意味着：\\n和别人区别出来，我的存在才有意义\\n\\n为了实现它，从现在开始，我可以做这些小事：\\n坚持写段子\\n坚持学习data science，让自己找到新的知识上的链接\\n\\n1.我的情绪关键词是：\\n自卑，逃避\\n\\n2.当时，我所处的场景是：\\n喜欢的女生不回信息\\n\\n3.当时，我有这样的行动：\\n胡思乱想，否定自我\\n发别的东西想办法引起女生注意\\n\\n4.我这么做，原本的期待是：\\n能得到回复；关系有进展\\n\\n5.从结果看，这个行为是否满足期待：\\n无法\\n只会让女生更烦我\\n\\n6.下一次，我也许能试试这些行动：\\n周末再去联系\\n平时忙自己的事continue boost confidence\\n——————————————————————\\n\\n1.我的情绪关键词是：\\n烦躁，畏难情绪\\n\\n2.当时，我所处的场景是：\\n看不懂书上的东西，没法解决技术上的问题\\n\\n3.当时，我有这样的行动：\\n去看油管，去看闲书\\n\\n4.在短期内，行为带来这样的效果：\\n浪费了很多时间。虽然问题还是要解决的\\n\\n5.从长期看，行为带来这样的效果：\\n没法进步，一直被难题困扰\\n\\n6.根据上述分析，我认为这是一个值得保持的行为，或是要减少的行为（例如回避、拖延等）：\\n减少。因为难题总是要找到出口的，不管是继续死磕，还是和老板说放弃，都需要出口。逃避不能帮我找到任何出口。\\n\\n——————————————————————\\n\\n如果这是一个回避行为，当未来它出现时，\\n\\n我们可以带着觉察按下暂停键，知道自己有其他选择。\\n\\n1.我想到的事情是：\\n工作上的难题解决不了\\n\\n2.我注意到，当时我有这些情绪：\\n焦虑，无助，自我否定的低自尊，\\n\\n3.我注意到，当时头脑里有这些想法：\\n觉得缺少support，孤独\\n觉得成长太慢\\n本能的玩手机和油管想逃避\\n\\n4.我注意到，当时身体还出现这些反应：\\n发热\\n坐不住\\n\\n5.这些来自头脑和身体的体验感受，我给它取名为：\\ntiny career hit\\n\\n这样，下次遇到它时，我会更快地认出它。\\n\\n6.请记住，这些内心体验是大脑、身体遭遇事件的正常反应。\\n\\n它们会自然地升起、维持和消失。\\n\\n现在，让我们和大脑、身体说一声「谢谢你，给我提供了这些信息与体验」，\\n\\n以接纳的态度容许它们停留多一会儿。\\n\\n然后，继续投入当下的生活。\\n\\n我今天经历了这些情绪：\\n\\n【情绪1】\\n\\n时间：\\n上午\\n\\n事件：\\n等车的时候有点焦虑怕朋友接不到我\\n\\n情绪关键词：\\n焦虑\\n\\n情绪强度分数：\\n5\\n\\n情绪持续时间：\\n10 min\\n\\n【情绪 2】\\n\\n时间：\\n下午聚会\\n\\n事件：\\n聚会时没话说有点紧张，但其实大家都很nerd就感觉无所谓了\\n\\n情绪关键词：\\n无聊、焦虑\\n\\n情绪强度分数：\\n\\n4\\n情绪持续时间：\\n两小时\\n························\\n【情绪 3】\\n\\n时间：\\n晚上聊天\\n\\n事件：\\n主动向朋友提供了帮助\\n\\n情绪关键词：\\n自信，温暖\\n\\n情绪强度分数：\\n6\\n\\n情绪持续时间：\\n5min\\n\\n1.我想到的情绪关键词：\\n不安，自卑\\n\\n2.我遇到的事情是：\\n女生不回我信息，或者回得很敷衍\\n\\n3.我注意到，当时头脑里有一个「想法小人」，它说：\\n你没用\\n你不值得被爱\\n\\n4.我给这个「想法小人」取名为：\\n求偶日常小人\\n\\n这样，下次遇到它时，我会更快地认出它。\\n\\n5.请记住，想法小人是大脑遭遇事件时的自然产物。\\n\\n它会根据以往的经验，飞快地给出评论。这些评论可能对我们有帮助，也可能没有。\\n\\n现在，让我们闭上眼睛和大脑说一声「谢谢你，为我提供了一种思考的角度」，\\n\\n以接纳的态度容许它停留在脑海。\\n\\n然后，继续投入当下的生活。\\n\\n——————————————\\n\\n被消极想法套住时，你可以在内心默念：\\n\\n「我注意到，此刻我有一个名字叫 ________ 的想法小人」来快速解套\\n\\n1.影响我心情的诱因是：\\n\\n一个bug解决不了。原来以为20分钟能搞定的事，花了一上午都没搞定\\n\\n2.我注意到，当时头脑里有这些想法：\\n不记得了\\n\\n3.我注意到，当时头脑里有这些情绪：\\n沮丧，烦躁\\n\\n4.我注意到，当时身体有这些感受：\\n发热，抖动，坐不住\\n\\n5.我注意到，我做了这些事：\\n开油管逃避问题浪费了一两个钟头的时间\\n\\n1.我们邀请来访者注意他们的想法：\\n\\n·“现在你的头脑正在对你说什么？”\\n\\n·“关于这一点，你的思考自我必须说些什么？”\\n\\n·“你是否注意到你正在想什么？”\\n\\n·“注意你的思维正在做什么。”\\n\\n2.我们邀请来访者看看他们想法的有效性：\\n\\n·“那这是一个有帮助的想法吗？如果你紧紧抓住这个想法，它是否可以帮助你有效地应对你的处境？”\\n\\n·“如果你让你的想法来告诉你应该做什么，它会将你引向富足、充实和有意义的人生，还是引向痛苦和困境？”\\n\\n·“如果只是因为你的思维说‘这不可能有用’或‘我做不到’就结束本次咨询，这是否会帮助你改变你的人生，或者这只会让你继续陷在困境中？”\\n\\n3.我们邀请来访者觉察他们是处于与想法融合还是解离的状态：\\n\\n·“那么现在，你与你的想法之间有多紧密？”\\n\\n·“你是否注意到当时你的想法是如何把你钩住的？”\\n\\n当我们进行回顾时，我们在六个关键点上关注融合：规则、理由、评价、过去、将来和自我。现在让我们快速地了解一下这六个关键点。\\n\\n规则\\xa0（rules）。关于生活、工作、人际关系等方面，来访者具有什么样的死板规则呢？具体而言，找到那些关于人们在采取行动之前需要如何感受的规则。关注这些关键词，如：应该、必须、不得不、一定、对的、错的、不能、不可以等。还有这些关键短语，如：我本来没必要；如果我感到X，那我就不能做Y；如果我做了A，那么你就应当做B。这些词汇和短语通常在向你预警关于生活应当如何被操纵，或者在改变之前必须要满足什么条件的僵化思维。如果来访者与这些规则完全融合，那么规则将会给其带来大量的痛苦。这里有一些例子：“我不应该有这种感受”“当我感到焦虑时我不能去参加聚会”“如果我不能完美地做完它，那么就不值得去尝试”“这不应该如此之难”“我孩子应该按我说的去做”“正常人都不会有这样的感受”。\\n\\n理由\\xa0（reasons）。当来访者说改变是不可能的、不需要的或无法操作时，他给你的理由是什么？人类很擅长为他们为什么不能或不应该改变找寻理由：“我很忙/累/焦虑/抑郁”“我可能会失败”“我本来就没必要做”“这太难了”“这是基因决定的”“在我们家，大家都酗酒”“这是内分泌失调”“我会受到伤害”“我总是像这样”“我不能面对孤独”“一切都会乱套”“只要我有更多的时间/能量/金钱，我会做的”，诸如此类。如果我们或者我们的来访者与这些想法融合，它们常常会阻碍我们做出改变。\\n\\n评价\\xa0（judgments）。人类总是在评价。我们所做的大部分评价是有用且重要的：这个人是值得信任的还是不值得信任的？这辆车是否物有所值？这个水果熟了没有？但不幸的是，还有很多评价是没有帮助的。当然，如果我们不在意我们的评价，那么就没什么问题。但是，如果我们和以下这些评价相融合——“我很糟糕”“你很卑鄙”“焦虑是不好的”“我太胖”“被拒绝是不可接受的”“他是如此的自私”“活着太痛苦了”“男人都是骗子”，那我们就很容易走向挣扎和痛苦。你的来访者融合了什么样的审判性或评价性的想法呢？\\n\\n过去\\xa0（past）。来访者是如何与过去相融合的？沉浸在过去的伤害、失败、错误、失去的机会中？总想再回到生活变糟之前的“美好往日”中？有闪回体验（闪回体验是与记忆融合的极端情况）吗？\\n\\n将来\\xa0（future）。来访者是如何与未来融合的？总是担忧？幻想着更好的生活？不断思考今后不得不做的事情？\\n\\n自我\\xa0（self）。来访者融合了什么样的自我描述呢？这里有一些常见的描述：“我很弱/无用/不可爱”“我不需要帮助”“离开我的工作我就什么都不是”“我处理不了”“我不愿容忍笨蛋”“我是对的，他们错了”，等等。他是否与他的诊断或身体意象相融合了？例如“我是双相情感障碍患者”“我很胖”。他也许与他的工作职称或他在家庭中的角色相融合了？\\n\\n自己看了看窗台上的多肉。上个月拿回家之后，我都没真正仔细地观察过他们的形态。大盆的紫里透蓝的很好看，而且叶片上的褶皱也很美，其中一片叶上还有一道划痕，有点残缺美的意思。熊掌出现了很多黄色。之前快挂掉的lavendar也在继续挣扎没有躺平认输\\n\\n我的感受\\n\\n确实有打开了视角的感觉。自认为还是挺喜欢多肉的人，但自己窗台上的多肉我都没这么仔细地观察过\\n\\n吃了个橙子。感觉好甜啊。确实比平时匆匆地补充维生素多了很多丰富细微的感受。感觉以前自己浪费了好多食物\\n\\n黄色很不均匀，并不好看的一个橙子。嘴唇触碰的时候能感到很充盈的汁水。舌头碰到的时候唯一的感觉是好甜。开始咀嚼后，纤维感很重，橙子的味道才会升起来，一段时间后汁水都流出来了，感觉果肉都没有味道了，再咽下\\n\\n1.我今天体验到的一个积极情绪是：\\n自信\\n\\n当时的场景：\\n下午主动的面对了工作上的几个任务，并成功完成了\\n\\n情绪强度分数：\\n7\\n\\n我感受到这个情绪，可能因为我希望：\\n自己今后能延续这种工作状态，不回避，才能有成长\\n\\n2.我今天体验到的一个消极情绪是：\\n恐惧和逃避\\n\\n当时的场景：\\n不想刷题。二分法真是太难了。都见过几次的题了还是没想法\\n\\n情绪强度分数：\\n6\\n\\n我感受到这个情绪，可能因为我不想要：\\n感到自己笨和愚蠢\\n\\n前几天不会有变化。只需要关注自己的完成\\n\\n卡巴金：尽最大努力；效果源自投入的程度\\n\\n正念需要保持全程清醒\\n\\n正念：当下，非判断，有益\\n\\n注意到情绪or想法，不跟着想法跑，接纳这个情绪，不反应不对他进行注意力加工\\n\\n通过身体姿势主动创造不适感，学会接纳这种不适感\\n\\n觉察：扩大视角，觉察到被忽略的视角；不要牢牢盯在威胁上\\n\\n书写是重新消费自己幸福体验的过程\\n\\n接纳意味着冒着大雨去到自己想去的地方，不是躲雨，也不是喜欢雨天\\n\\n写情绪日记的时候无话可说：看其他人怎么写的，陪伴者语音分享\\n\\n正念是关于觉察，专注，接纳的平衡\\n\\n如果你的情绪是拖延引起的，那你行动开启后情绪自然就会消失\\n\\ncbt要求人理性的审视自己的情绪和想法，不对他进行评判\\n\\n更高端的技术，可以看到情绪变化的过程，从升起到衰减\\n\\n很多情绪，我不需要明白他背后的意义之类的，我需要的只是行为\\n\\n目标不是成为一个积极的人，而是成为一个自由有选择的人。在任何时候，积极都是我的一种选项\\n\\n积极包括了：积极的思维模式，和行动力\\n\\n内在动机不是等来的，是在我们体验生活的过程中慢慢生长出来的\\n\\n自我拉扯理应不让你产生失落感，而是让你努力和自我握手言和，意识到自己行动的价值，更谨慎的生活\\n\\n及时识别出来自己现在的行为，比如玩手机，是不是一种逃避，还是主动的娱乐行为\\n\\n你每次都逃避那种情绪，就永远没机会和他相处，你的舒适区就会越来越窄\\n\\n暂停：和情绪和想法保持距离，不把注意力用在加工情绪，而是用在做事上\\n\\n行动：做当下符合你自己价值的事情\\n\\n正念不是去解决情绪，而是带着情绪去做事情，更能理解拆解自己的问题，更有能力去解决；一旦你愿意开始解决了，就会找到方法\\n\\n观察自己的冲动和不愉快状态的变化，他是固定的还是放大缩小的，他的来源是什么；他一般会越来越强烈，会达到一个峰值，然后会衰减。你在他增长的时候是不会意识到他会下降的，所以你才会顺从他。但你如果观察他就这么流过去之后，会意识到你不顺从他也没事，啥都不会发生。这么练习过几次之后，你就更能自如的对待冲动了。\\n\\n有些冲动比较强，需要几个h，甚至几天才能达到峰值开始衰减。你必须一直保持警惕。练习之后，你对冲动的控制会越来越快会更容易到峰值\\n\\n回避与接纳的差别：\\n\\n回避说明你在态度上不承认这个情绪，希望他消失\\n\\n行为上，逃避和接纳可能是没差别的，但逃避之后你内耗会很大，因为你大脑的后头需要一直监控这个情绪确认自己是真的转移了，但正念选择面对这种情绪，就不会有内耗，你让这个情绪爱在哪在哪就行\\n\\n正念让你还原压力的本来面目。让你不用把70分的工作压力，因为担忧自己有压力而达到90分。降到70分和他共处\\n\\n接纳不会把坏事变成好事。他是让情绪在该来的时候来该过去的时候过去。不接纳是指和情绪纠缠达到不健康的程度\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bf88be6f-4695-45cd-bd6f-86e8718ec636', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='by reimagining an uncomfortable internal trigger, we can disarm it.\\xa0\\n\\nStep 1: Look for the emotion preceding distraction. Step 2: Write down the internal trigger. (Use the Distraction Tracker included in the book and bonus bundle)\\xa0\\n\\nStep 3: Explore the negative sensation with curiosity instead of contempt.\\xa0\\n\\nStep 4: Be extra cautious during liminal moments. (Liminal moments are “distraction traps” that transition us from one thing to another, like picking up our phone while waiting for a traffic light to change).\\n\\n  \\n\\n  \\n\\n   Craft your dream \"ideal day” on a typical day of the week. What does it look like?\\n How are you spending your time to live up to your values?\\n Compare your dream \"ideal day\" to your current daily schedule. What could be\\n improved? What are you not making enough time for?\\n\\n What discomfort(s) or internal trigger(s) did you feel immediately prior to the\\n distraction (check all that apply)\\n Afraid\\n Frustrated\\n Bored\\n Excited\\n Worried\\n Overwhelmed\\n Angry\\n Hungry\\n Nervous\\n Sad\\n Guilty\\n Insecure\\n Anxious\\n Lonely\\n Embarrassed\\n Pressured\\n Jealous\\n Tired\\n Confused\\n Resentful\\n\\n\\n  \\n\\n\\n  \\n\\nRemember this:\\n\\nWe can master internal triggers by reimagining an otherwise dreary task.\\n\\nFun and play can be used as tools to keep us focused.\\n\\nPlay doesn\\'t have to be pleasurable.\\n\\nIt just has to hold our attention.\\n\\nDeliberateness and novelty can be added to any task to make it fun.\\n\\n  \\n\\n  \\n\\nRemember this:\\n\\n**You can\\'t call something a distraction unless you know what it is distracting**\\n\\n**you from.**\\n\\n**Planning ahead is the only way to know the difference between traction and**\\n\\n**distraction.**\\n\\n**Does your calendar reflect your values?**\\n\\nTo be the person you want to be, you have to make time to live your values.\\n\\n  \\n\\nTimebox your day.\\n\\nThe three life domains of you, relationships, and work provide a framework for\\n\\nplanning how to spend your time.\\n\\nBeflect and refine.\\n\\nRevise your schedule regularly, but you must commit to it once it\\'s set.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ce9dfa84-1bfd-409d-bc20-6bc30e1bf587', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='72f0266d-5829-40c1-83ce-7e8ecf8a0f56', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='66741e87-95f1-42fd-9df5-c909e3efdd44', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1c8409e8-a34a-4c35-854d-d30a500e64a5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e87c3f21-0ae6-4057-8f3e-419682f72752', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n上午和team一起发现了主要的问题，感觉不错\\n下午才开始了第一个番茄\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1a4f2abf-1eaa-4612-ad92-b85decb21140', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄 \\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6aa8b3f3-d2e7-443a-a56b-65b8cea3d3f9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5a104515-1229-48dc-b859-15831cc90c58', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dd0f4203-9865-41be-bfe8-722233ffd1a2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cf47d9dc-7280-49e8-a0cd-5eec5074046f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f025f498-a47e-47da-8fe4-b06f088e150d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8c826ca1-7e69-4599-ac8f-2eb345b2f6fd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5ec9f744-5654-4c87-b2fe-c6996c6629ec', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='024a7f2c-1273-4ef9-b8b4-996adcbe85b4', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='47fd07d4-ed75-44e1-b76c-5412b3941fac', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6bbdc30e-1e70-48ee-85b7-05ca44e29204', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e69709ed-85f4-4b59-b1c2-872e9c5e6720', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fb2c2bb2-f3ec-4701-ae41-8bd82e392500', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='80d6e7bc-6761-43df-9671-37e425fc7279', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d28c3cf5-d074-499f-b297-e5a2ef742f81', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='59d024d3-f7a2-4301-9428-5c3c14b83c50', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='726012df-0d81-4de0-9507-371e0f1e4b36', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='97fc92ee-ea9d-4a78-874e-a2d407be50a7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='86c372c5-3423-41fa-a649-d89e776d4f3c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1fc61409-3e15-47bf-a4a6-4849efc66ec2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n我现在的状态是越孤独就越浪费时间玩手机，越玩手机就没时间发展自我，导致自己缺乏吸引力，就越孤独。恶性循环\\n\\n如何破局：发展日常友谊降低孤独，省出时间来提升自己做有意思的事\\n\\n  \\nFor me, this started when I was going thru a break up and I decided “I can only receive what I give to myself”. So I decided to learn how to accept compliments and gifts and time and the like.\\n\\nI started doing things for myself I wished others would do. I often buy myself flowers. I allow myself little treats as if I’m romancing myself. I take myself out on a date, to dinner, for a walk in a park, to the zoo, to a museum, etc. I like exploring so I take myself out on drives down roads I’ve never traveled.\\n\\nWhen I feel lonely, I ask myself “what would I want to talk to someone about?” And then I often reach out to other friends and set up a coffee or lunch date.\\n\\nI started noticing my internal dialogue, and worked with the negative voices, examining the validity of their perspectives. Now I talk to myself in the mirror, and I say nice things. This was not easy at first, but I just started with 1 minute of smiling and beaming at myself and telling myself how I’m doing far better than I think i am.\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='67bd0c34-cb62-47b0-b46d-1cd489fa4126', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n230403\\n\\n今天状态很差，但没有破戒，也没有完全放纵自己，晚上依然做了一些工作\\n\\n  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='67f4d22a-8af6-4bcc-b90c-a091f41ab94a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n230404\\n\\n状态很差差点破戒\\n\\n  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='19fe2f9e-84d2-47fa-803a-e74ad49b56e5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n230409\\n\\n昨天因为太闲了破戒；\\n\\n今天出去hiking看到了很多有意思的东西，觉得自己的生命力得到了增长\\n\\n  \\n\\n  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='541036f1-55cc-47c6-a988-5b3404955e9c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n230410\\n\\n状态很不错；完成了一些工作，下班后看了一个ds video，晚上写了文章，复习了一些notes\\n\\n  \\n\\n  \\n\\n  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f0ccbeb3-1bb3-455f-a776-b7f1ad0ea019', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n230416\\n\\n本来周末状态不错的，但纯因为无聊破戒了；坚持了一周有进步\\n\\n学到了disciplinary perception；一定要控制自己的信息摄入，因为他对我的影响是潜移默化的\\n\\nherculus 打仗的时候碰到日食，士兵们都很害怕；herculus说如果我蒙住你的眼睛你还会害怕么，答不会。so the darkness itself is not scary, you are afraid of what causes it; so just focus on darkness u should be fine\\xa0\\n\\n上周的screen time降了10%！！！\\n\\n  \\n\\n  \\n\\n#230417\\n\\n工作上有挫败，但主动组织了meeting 来提问和学习\\n\\n  \\n\\n#230420\\n\\n因为b站评论看到了违禁内容，没控制住自己的念头主动去搜了；\\n\\n工作还是很有压力，但还是主动完成了，没有逃避别人的负面评价\\n\\n主动接触了房屋经纪\\n\\n  \\n\\n#230421\\n\\n**关键是要体验自己做每件事的体验是怎样的**\\n\\n这周走神和玩手机更严重了\\n\\n  \\n\\n#230423\\n\\n不知不觉度过了还算productive的一天; 但晚上因为b站评论看到了违禁内容破戒了\\n\\n  \\n\\n#230424\\n\\n还不错的一天；凯鲁亚克牛逼\\n\\n生活中遇到很多blocker比如连不上网，但都主动打电话说明情况克服了\\n\\n  \\n\\n  \\n\\n#230427\\n\\n问题依然是工作期间玩手机，看到刺激破戒\\n\\n早上冥想\\n\\n有条不紊的完成了工作和看房的交流\\n\\n  \\n\\n#230429\\n\\n看房学到了不少\\n\\n没有被dating app上打枪影响\\n\\n看到了非常优质的deep learning教程\\n\\n  \\n\\n#230430\\n\\n早上因为煎蛋上的trigger导致主动搜索破戒；明天一天不看煎蛋xhs和dating app之类\\n\\n成就：勇敢地找pizza老板要到了之前的too good to go，回家烤了好香；剪了头；看完了一个牛逼的ytb backpropog视频；读了书', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aa82d066-cdfd-4787-980c-f53d69693035', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='79cf2fa8-1ad2-40e5-9b4a-2a572969f27d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n和可颂良性互动\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c4189f6c-09ad-4039-8503-6bda6cfc7ad0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a7c14097-8c86-47ec-a166-23d8a5e2e1df', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8dc4c1f8-65d2-47c1-8385-4751615175ee', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a5d73c0a-705c-4ab0-971d-bd2968e18b85', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a2450e39-86b6-46b3-8394-49065207a1ca', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dda8d593-27e6-4161-8050-418cb99950fa', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n和bro聊了下买吸尘器的事\\n开始恢复了一些锻炼\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='289c200e-0811-4c00-bf76-537782622a7b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n房子大体收拾完了，也解决了一些后续的生活杂事\\n下楼买了grocery；下载了其他外卖app\\n晚上状态差的时候强迫自己看了回血清单\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f01b460c-12f9-4b57-9cc2-9d85571ad677', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8fb9d9db-5294-4ab8-86be-05e212ffc84e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n今天基本没咋玩手机；主要因为有刺激的app比如煎蛋都删掉了\\n有破戒的心思但没有任其发展\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aa30c215-0d8c-43d0-98e0-f017dbe72e45', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='30f8c719-7e56-4327-bc78-8b6433a5b0e4', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f023436e-c8e5-45fe-83eb-60495291a3e2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bbe75d8f-0d33-425e-bae9-65c52b07d4a5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f9b00b10-531a-471f-8350-2c7b5c7aea0d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='54125172-a5ec-4eae-8bc3-b9a70ced4598', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b18a0695-75e6-4cd2-aef7-9b3a7535cc13', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='971b7f9c-05b6-4587-a8f1-66e0b5d731c6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n****我不知道如何实现幸福，因为我都不知道什么是幸福的生活；当我把幸福生活具体化，其实是能实现的，你就能注意到这些小事了；****\\n\\n****感受是要刻意练习的****', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f8f8335f-c6be-4dce-9c7e-0336989740cb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n暂停实验室学到了很牛逼的insight\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0bd8bbe8-a6ff-4d76-a5b8-6a785bcf456a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='da2583ac-9c1e-4f66-9484-f32e446999a2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e36c3f22-b947-4da6-bf57-83c00a9d6df0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='887a199d-5b64-437a-a269-1425336aed36', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7080fff8-7027-4119-b579-0b797a15f930', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='683edf6a-38cc-41ca-a676-272432997099', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bbf0856e-6221-45d1-b763-8e8837352409', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='eb819747-1cc0-4916-877b-7b5624e04ea5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f5049ca2-3ab4-446f-9957-18d0a0d1fafc', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3c2c438e-ce7a-46dd-bc04-fd03645157da', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3267d44c-b064-4cef-8fa3-6a6dfcb90b18', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n和bbb聊得不错\\n开始了新一轮的关怀自我培训营\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='49342759-d43a-4afd-8954-0a8cff27ef62', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n感谢新的职业机会\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='36aad0c7-2f53-4c8f-a500-a3fae3d2f3f3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n做到了早睡早起！\\n天气太热不太想运动\\n\\n目标：每天在ds学习上至少花一个番茄！！！\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='91412960-8534-4726-ac5d-918d6a05d445', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='052db13b-481e-4b37-8565-f5e30f4ea621', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fa774c32-1ece-408a-9b9f-2e20d5cfb84c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='16a31312-4fab-41fa-889f-ef27b2fcafa4', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='31172486-b09d-4cee-a19d-051992451a1a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n纯因为无聊破戒，深入原因是，模型在跑，我找不到事情做\\n头一次感到有效：愿我有力量，愿我勇敢，愿我能做想做的事情\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a768890f-5c2b-4883-b3d8-9480aaa02ba6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f56547a5-db85-454d-9b68-499b5065bbd5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='74924d7b-fc81-4463-8042-0cfc4acad7ec', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\nfy要当妈了我突然很开心！\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='006842d8-024f-416f-bf2d-d280ad62da38', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n职业上的progress完成了一个面试\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ce8fda7a-a7fc-42c6-8e46-ece5901c9265', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n楼内碰到美女是很幸运了\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='71e89866-43cf-432d-93c3-29478dd3aa85', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n前天强力健身了一阵\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='758bff6f-8231-4375-bd3b-d4ed2eadde46', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄 ck\\n- 不用手机 ck\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e7a0afd7-1573-42b5-840c-a31586399732', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2750600e-c0df-4e05-aaef-a17df783bb04', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b6748e21-c340-40cf-b780-6d36dfa34ddb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a7c13dfc-0800-44a2-beb3-a7a1e4ccc00b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c22ff512-7876-43ce-bf49-c245493e52ed', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c4d4641e-8a7f-40f7-8cc7-a3ec5d797864', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4c80bb0f-fda5-4aca-acfd-13e1e5c50c29', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n#230601\\n\\n\\n完成了工作任务\\n\\n没有因为房子的事情破戒\\n\\n行动激活特别强调，需要先识别求助者日常的行为模式，以及帮助求助者追踪自己的情绪,了解它们随活动和环境发生变化的规律。然后,尽可能多的参与那些,能够让他感觉到愉悦和掌控的活动。\\n\\n想要让人行动起来,最重要的有两个原则,一个是这件事本身的愉悦感足够强,做事情的过程是享受的。这时我们更容易主动的把时间投入这项活动。\\n\\n另一个重要原则是，对事情结果的掌控感。如果我预期自己能够把这件事做的很好，能够获得好的结果，从中收获成就的体验，那我就更愿意主动去完成它。反之，如果做一件事的结果不确定，或者不能马上得到反馈，我们行动的动力就会大打折扣。\\n\\n在规划行动时，不妨先选择最容易完成的，能够给人带来愉悦和掌控感的事情,把它们放在合适的时间，然后再补充上其它的日程安排。对于那些困难的，不好启动的任务，可以事先许诺自己，在完成后，给予自己一定的自我奖励。\\n\\n如果你规划好了自己的行动，为了鼓励自己按照计划执行，可以把这个计划己分享给信任的朋友或者家人,和他们订立一个契约,请他们监督你完成。\\n\\n在执行的过程中，如果你发现这个计划遗漏了一些重要的事情,也可以进行修改和调整，只要它和你的目标价值保持一致就好。\\n\\n  \\n\\n  \\n\\n  \\n\\n#230603\\n\\n完成了一些工作；有意义的推进但没法彻底搞定\\n\\n尝试了做牛筋，虽然不好吃\\n\\n睡前坚持了锻炼\\n\\n  \\n\\n  \\n\\n#230604\\n\\n坚持了一天就又破戒；因为hiking完之后太累了，回家干不了啥，且self depletion严重\\n\\n工作得到了进展\\n\\n晚上记了日记\\n\\n  \\n\\n从很多角度上来说，逆境都是人生中的一段宝贵体验。\\n\\n首先，它能够让我们正视自己的生活，认真的思考并解决问题。每个人都拥\\n\\n有自我完善的潜能,但是如果没有一个外在的事件,把这种潜能激发出来,\\n\\n大多数时候我们更有可能顺着行为的惯性，重复日复一日的生活。\\n\\n  \\n\\n关于dopamine detox的总结：\\n\\n失败经验：不能完全不娱乐，这样的话我会报复性的刷视频啥的\\n\\n替代性娱乐的搜寻一直不顺利：读书有时读不进，玩游戏觉得很累且产生空虚感；副业探索不顺，每次看到高难度的code就被迫放弃\\n\\n如何解决：继续低难度的副业探索\\n\\n  \\n\\n  \\n\\n#230605\\n\\n做到一件事：睡前不玩手机\\n\\n  \\n\\n#230606\\n\\n昨天基本做到了\\n\\n今天玩nfs还蛮开心的，但时间太长，玩得头晕\\n\\n**强化未来目标对自己的吸引力很关键**\\n\\n  \\n\\n  \\n\\n#230608\\n\\n**为每天创造一件值得回忆的印记**\\n\\n  \\n\\n这是我践行了许多年的习惯：**为每一天打上一个「印记」。**\\n\\n  \\n\\n这个印记是什么呢？简而言之，它就是在我们平凡而按部就班的一天里面，那一抹与众不同的、能够把这一天跟其他日子区分出来的「色彩」。可以是一次经历，一次冒险，一次尝试，一次见面，一次成就……\\n\\n  \\n\\n举几个简单的例子：\\n\\n- 今天遇到一个百思不得其解的问题，经过一番艰苦的思索和学习后终于弄懂。这可以是一个印记。\\n- 今天联系了许久没联系的朋友，聊了彼此的近况，互相打气加油，这可以是一个印记。\\n- 今天交付了一个小项目，得到客户的肯定和老板的赞赏，这可以是一个印记。\\n- 今天下班后有点时间，绕了点远路去试试一家一直想尝试的餐馆，这同样可以是一个印记……\\n\\n  \\n\\n简单来说：如果让你写日记，用一句话来概括你刚度过的这一天，你会怎么写？\\n\\n  \\n\\n你是会立刻想到一两件有趣的事情，迫不及待地把它写下来，生怕自己忘记；还是搜索枯肠之后来一句「今日无事」？\\n\\n  \\n\\n如果你是后者，那么我建议：不妨试一试，每一天有意识地给自己一个挑战：**我能不能去主动地做一些事情，让今天变得跟平时不一样？**给我的生命和回忆留下一些更加印象深刻的痕迹？\\n\\n  \\n\\n当然，你未必要写日记，也不一定非要把它写下来 —— 你只需要去尝试，去创造出这么一个「印记」，让自己把这一天跟这个印记绑定起来，让这一天在你的生命中获得某种意义，从平淡无奇变得熠熠发光……\\n\\n  \\n\\n久而久之，这可以使你的回忆变得更充盈、更丰满。无论是遭遇失落、沮丧，抑或是无聊、慵常……都可以把它们翻出来，让自己去回味。从这种沉浸之中，获得休憩和力量。\\n\\n  \\n\\n它们会是你对抗不确定的未来的最好倚仗。\\n\\n  \\n\\n  \\n\\n#230609\\n\\n最有成就感的事：听了ritayu的建议直接fixed the code没有拖延\\n\\n情绪纪录：下午的时候非常混乱，avani不回我，bug无法解决，导致心情很难平复，导致破戒\\n\\n  \\n\\n#230610\\n\\n我觉得训练太辛苦了\\n\\n其实这些自信都来源于训练\\n\\n训练到位了都很自信\\n\\n你平时做的小事\\n\\n每一件事都做好了\\n\\n都会自信\\n\\n所以我也就非常自信\\n\\n— 张伟丽\\n\\n  \\n\\n#230612\\n\\n周末的辛苦没有白费。今天上午就独立解决了bug\\n\\n今晚完成了不错的锻炼量\\n\\n上午紧张；下午很放松但过于放松又开始玩游戏啥的了\\n\\n  \\n\\n#230613\\n\\n因为td的贷款问题冲击情绪破戒\\n\\n当时感觉到了心跳很快，\\n\\n  \\n\\n#230614\\n\\n去了career fair；吃了宜家肉丸\\n\\n听到了healthygamer的个人历程，很感动\\n\\nwhat is one thing i can do to move myself forward a single step\\n\\n  \\n\\n  \\n\\n买房最大的收获是认识了sunny，一个十分competent的人，competence改造周围的环境，带来自信，会自然地吸引来更多的资源；相反incompetent的人，比如别的agent，律师和mortgage specialist，会陷入负向的循环\\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\n\\xa0#230615\\n\\n因为贷款问题冲击情绪破戒；不止是坏消息，更多的是对各种人的失望；avani不回我，各个贷款的人也不回我，让我觉得自己对生活没有任何掌控感；\\n\\n晚上还是坚持了跑步\\n\\n设定了早起的闹钟\\n\\n  \\n\\n  \\n\\n\\xa0#230616\\n\\n成功早起！\\n\\n意识到j的另一个目标是，对正常女性重拾兴趣\\n\\n被sindo打枪但没太受影响。毕竟就几百块的事，没必要操心\\n\\n  \\n\\n  \\n\\n  \\n\\n\\xa0#230617\\n\\n成功早起！\\n\\n和karel主动联系上了\\n\\n今天发现，哪怕一个电线杆上，都有很多有意思的细节\\n\\n感谢父母在经济上的支持\\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\n\\xa0#230618\\n\\n习惯: 屏幕时间2h, 完成了冥想\\n\\n成就清单：坚持了coding学习；尽管和危险，但没有破戒；买完了所有该买的东西\\n\\n感恩清单：谢谢zy一直陪我说话；谢谢B站上那么好的免费课程\\n\\n  \\n\\n  \\n\\n#230619\\n\\n因为工作上混乱的破事，包括阿迪的鞋，垃圾的网速等破戒了；浪费了3h在p site; 下班后认真做工作其实并不难一小时就完成了\\n\\n习惯: 屏幕时间1.44h, 完成了冥想\\n\\n成就清单：坚持了coding学习；主动打给了td 推进了一点事情；看了卡拉马佐夫兄弟的一个视频，体会到了经典的快感\\n\\n感恩清单：谢谢soul app帮我认识了一些朋友\\n\\n  \\n\\nAshley Peedikaparambil, 7 min\\n\\nHey Cairo, sorry did not mean to interject you there\\xa0on the call with Sukru but i think we can use this opportunity to do some housekeeping chores (which in your case might not be much i understand). But it would be productive in the long term for sure :)\\n\\n  \\n\\nCairo Liu, 6 min\\n\\nthanks. I will do my best\\n\\n  \\n\\nAshley Peedikaparambil, 4 min\\n\\nNo problem! Let me know if theres anything i can help with. Also did not want Sukru to presume theres absolutely nothing to work on because as Tiger team we're solution providers to Gap so theres that dynamic at play here\\n\\n  \\n\\nCairo Liu, 1 min\\n\\nUnderstood. Felt that I was a bit annoyed by the code blockage and got emotional\\n\\nWill try to bring a more productive view to the problems in the future\\n\\n  \\n\\n  \\n\\n#230620\\n\\n习惯: 屏幕时间3h, 完成了冥想\\n\\n成就清单：坚持了coding学习；完成了足量的工作；没有逃避sukru六点多的消息\\n\\n感恩清单：阿迪的退货搞定了\\n\\n  \\n\\n复习了岁月时间管理法，很有用\\n\\n没有被美女的ghost困扰\\n\\n  \\n\\n#230621\\n\\n习惯: 屏幕时间2h, 完成了冥想\\n\\n成就清单：坚持了coding学习；完成了少量的工作；稳步推进了买房\\n\\n感恩清单：谢谢soul上今天有人回我\\n\\n李然在冥想的课真的有用！\\n\\n  \\n\\n  \\n\\n#230622\\n\\n习惯: 屏幕时间2h, 完成了冥想但效果很差\\n\\n成就清单：和sukru的meeting比较成功\\n\\n感恩清单：\\n\\n工作太难了，bug根本没法解决。虽然很危险但没破戒。希望明天能搞定bug\\n\\n  \\n\\n  \\n\\n#230623\\n\\n习惯: 屏幕时间4h,\\xa0\\n\\n成就清单：休息日；勉强看了1h coding video; 没有干不好的事就是成功\\n\\n感恩清单：\\n\\n  \\n\\n  \\n\\n#230625\\n\\n习惯: 屏幕时间3h24,\\xa0\\n\\n成就清单：险些破解。但在一瞬间有了change of mind，主动删掉了下载任务；抓住那清醒的一瞬间就可以改变当下的行为\\n\\n感恩清单：谢谢bro每天和我聊天\\n\\n  \\n\\n  \\n\\n#230626\\n\\n习惯: 屏幕时间3h13,\\xa0\\n\\n成就清单：险些破解。完成了银行的不少工作\\n\\n感恩清单：谢谢bro每天和我聊天\\n\\n  \\n\\n  \\n\\n#230627\\n\\n习惯: 屏幕时间3h5, 完成了冥想和锻炼; 昨晚重度失眠导致破戒\\n\\n成就清单：虽然不顺但还是完成了工作上的几个任务\\n\\n感恩清单：谢谢HJ把我招进来且愿意带我\\n\\n价值总结：今天的行为哪些符合我的价值哪些不符合\\n\\n梦想总结：做了冥想\\n\\n明早计划：开始用番茄\\n\\n冥想的时候感受到微风非常舒服\\n\\n  \\n\\n  \\n\\n  \\n\\n#230627\\n\\n习惯: 屏幕时间3h,\\xa0\\n\\n成就清单：较好完成了工作上的几个任务\\n\\n感恩清单：很幸运！rrsp的事情基本搞定了！\\n\\n价值总结：\\n\\n梦想总结：\\n\\n明早计划：用番茄\\n\\n  \\n\\n  \\n\\n这样当然是没意义的。复盘的目的是什么？不是为了记录本身，而是能够为我们的行动指引方向，为我们未来的项目提供指导和参考。如果你连「记录」这一步都感到无聊，又怎么可能真的把它用起来呢？\\n\\n因此，我自己的复盘方式很简单：我不会去做那些复杂的表格和报告，而是在项目执行过程中，随时随地记录下自己的思考，包括：这项工作遇到了什么问题？有什么方式是可以尝试的？我采取了什么举措、效果如何？等等。\\n\\n这些记录看似简单而琐碎，但当我下一次做类似的项目时，它们就能起到强大的作用。它们可以帮我预先判断可能出现问题的环节，让我试验新的、有趣的想法和举措，不断完善和优化已经证实可行的做法，缩短时间、提高速度，等等。极大地提升效率和手感。\\n\\n我这个做法是怎么来的呢？是我先有了这个做法，再去发掘它的作用吗？不是的。是我先在工作过程中去思考「我需要什么资料来帮我更好地完成它?」再去有意识地积累这些资料。能够满足我需求的，那就是有用的，记下来；反之，对我的需求帮助不大的，那就不适合我，没有必要强求自己去适应。\\n\\n  \\n\\n  \\n\\n#230630\\n\\n习惯: 屏幕时间2.5h, 做了冥想和锻炼\\xa0\\n\\n成就清单：因为昨天工作有些进展，今天特别浮躁没完成什么\\n\\n感恩清单：谢谢家人经济上的主动支持\\n\\n价值总结：做到了乐观；昨天还是挺勇敢了，意识到了code的timestep老问题，没有放弃，立即就投入时间解决了问题\\n\\n梦想总结：主动求职了GS虽然失败了\\n\\n明早计划：继续早起！用番茄给自己规定时间\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='23d4b1d9-9c02-430d-8be2-22111e28cf98', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n#230702\\n\\n连续两天破戒，因为面对自己的梦想task遇到挫折后，没有主动分析问题拆解问题\\n\\n破戒之后下腹肿胀不适🤢\\n\\n  \\n\\n  \\n\\n#230703\\n\\n卖桌子少卖了几十块。当我的生命中有了更大的课题，就不会为这种鸡苗蒜皮的小事操心了。那个老人因为优惠券不能用大发脾气的例子。因为他的生命中最重要的事就是省钱\\n\\n  \\n\\n  \\n\\n#230704\\n\\n记录今天的两个场景：\\n\\n看到云的分布非常的多变\\n\\n拿到了一个有趣的卡带\\n\\n  \\n\\n因为高盛的原因重新开始lc；虽然很难，但我还是成功复习了一些内容，比如dict.get(), collections.deque; [ [0]*size1 for _ in range(size2) ]\\n\\n  \\n\\n  \\n\\n还记得第一天我们练习的「理想的一天」吗？今天的练习活动和它有关。\\n\\n现在需要你查看一下当时的记录，并把在第一天写下的标题抄写下来，作为自己理想生活状态的一个目标。\\n\\n然后，需要你开动脑筋想一想，在自己每天的生活中，有哪些经常参与的日常活动是和这个目标有关的，请列出至少三个和目标相关的活动。\\n\\n接下来你可以告诉自己,当参与这些活动时,自己应该如何做,或者拥有着怎样的状态，能够有利于理想目标的推进，让自己每天都能离它更近一步。\\n\\n  \\n\\n理想的一天该是怎样的：\\n\\n1. 早起，不玩手机；直接刷牙早餐不拖延\\n2. 早上第一件事是最难的工作\\n3. 少玩手机；遇到工作上的困难不做回避\\n4. 晚餐吃好吃的；晚餐后have some fun\\n5. 不做Mindless browsing，做原创性的，有intellectual刺激的娱乐；和朋友聊天\\n6. 锻炼身体；总结当日\\n\\n  \\n\\n  \\n\\n  \\n\\n#230705\\n\\n快乐的事：搞定了该搞定的工作；开出的椰子十分优质水都满出来了；好消息房主愿意把家具都给我\\n\\n  \\n\\n身体上的病痛是不舒服。但如果专注做事不去理他，也对我影响没那么大\\n\\n  \\n\\n  \\n\\n#230706\\n\\n下午纯粹因为太过无聊而破戒；另个原因是游戏里的setback让我心情很差\\n\\n  \\n\\n快乐的事：晚上主动完成了签证的所有准备工作；身体上的疼痛好了很多\\n\\n  \\n\\n  \\n\\n#230709\\n\\n这两天一直被各种妹纸拒绝。险些破戒。有一个insight是，妹纸是不缺的，缺的是我发展和维系关系的能力。就像idea是不缺的，缺的是执行力\\n\\n  \\n\\n完成了冥想；坚持了刷题\\n\\n  \\n\\n  \\n\\n  \\n\\n#230714\\n\\n3 things I can do today so tmr would be a better day than today\\xa0\\n\\n  \\n\\n1. finished onboarding of new team members and assigned tasks to them to work on\\xa0\\n2. evaluated results against SAS results instead and found that tft is better with 6wks store level results, but worse with 12 and 24 wks chain level results\\n3. Found the problem of low cc overage, running new experiments during the weekend that drops less data\\n\\n  \\n\\n  \\n\\n和老板汇报要体现1,2,3 点，what I observed, what I plan to work on next week: 1. check naive model; 2. experiments on chain level 3. current res dig deeper\\xa0\\n\\n  \\n\\n  \\n\\n#230718\\n\\n下午因为压力大破戒；压力大的原因是感觉手上有很多活，干不完；但其实如果能坐下来安静拆解，其实是能干完的\\n\\n  \\n\\n好事：主动和jenny聊天了\\n\\n  \\n\\n  \\n\\n  \\n\\n#230719\\n\\n完成了不少生活上的事；主动收拾了东西；工作上任务也还ok；背上的皮肤问题略有好转\\n\\n不要用别人性格上的问题来惩罚自己\\n\\n幸福小事：东西挂出去马上就有了买家\\n\\n  \\n\\n  \\n\\n  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3276258b-a7fa-43ff-b073-849125545af0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n\\n\\n#230722\\n\\ngood:\\xa0\\n\\nbad: 因为看到色图，引发连锁反应破戒；删掉了煎蛋', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='10ea2f23-e1de-4578-8426-8b4b26367577', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n今晚看了一些自己的笔记和习惯，感受到了和自身的链接，不需要那么外求了\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0344e80f-3e69-4692-ad42-b03b261b8070', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='86e569e4-9bf6-46ca-adea-c99c8f33b2d1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d0a2c997-6f78-47f8-95ec-116fd96ce2d5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6e8697e7-5b77-40c6-bd9a-e4ef61122156', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e4041c92-59cf-4db8-bfb8-d176bf3e1253', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='230608ca-2b44-4224-afc2-5dfa13a87391', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5c569866-4a23-432e-9448-c19c0b07373c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ffc74dc4-e235-4b76-a94f-780f0c56aefb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='71493bcd-6c40-456b-85a7-c2056b87b96e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='399bfed0-edbb-4568-a413-22caaa488eb6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f47b5620-9cad-4138-ae38-070a48d674d4', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e880f737-f519-430c-bf19-30c5190f09ca', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n创造力\\n\\n突破常规，善于想出更好的方法做事。\\n\\n好奇心\\n\\n喜欢探索事物,问问题,乐于体验不同的经历和活动。\\n\\n判断力\\n\\n仔细考虑不同角度和观点的信息，综合做出判断。\\n\\n好学\\n\\n喜欢学习新的知识、观念，主动发展个人技能。\\n\\n洞察力\\n\\n视野宽广，跳出自身的局限看待问题。\\n\\n勇敢\\n\\n不怕面对困难和挑战，不怕外界的质疑，相信自己。\\n\\n毅力\\n\\n坚持不懈，持之以恒\\n\\n真诚\\n\\n实话实说，言行一致，不掩饰自己。\\n\\n热情\\n\\n有所热爱，全情投入，充满活力。\\n\\n爱与被爱\\n\\n能够自然的表达对他人的关心，建立一段平等尊重的关系。\\n\\n善良\\n\\n慷慨大方，乐于助人。\\n\\n社交智慧\\n\\n擅长和人打交道，能理解人们做事情的原因。\\n\\n团队合作\\n\\n主动参与团队活动，并对团队目标负责。\\n\\n公平\\n\\n平等的对待自己和他人，给予每个人同等的机会。\\n\\n领导力\\n\\n有能力带领团队朝向有意义的目标前进。\\n\\n宽容\\n\\n平和的看待不好的事情，以及原谅伤害过你的人。\\n\\n谦逊\\n\\n不出风头，不慕虚荣，保持平常心。\\n\\n谨慎\\n\\n小心谨慎，三思后行。\\n\\n自制力\\n\\n为了长远的目标，抵制诱惑，约束自己的行为。\\n\\n审美\\n\\n能够发现隐藏在生活细节中的美好的事物。\\n\\n感恩\\n\\n心存感激，为自己的幸运而满足。\\n\\n希望\\n\\n对未来充满期待，并为之做好充分的准备。\\n\\n幽默\\n\\n保持一颗有趣的心态笑对生活。\\n\\n精神力量\\n\\n努力在让自己的人生变得更有深度，思考生命的意义。', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c69eeade-346e-47b0-b0b3-976cbb4e0806', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n昨天纯粹因为女生都不回我破戒，今天因为眼镜的事有点波折，但问题不大，直面了困难马上开始了面试准备\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='efb3b690-51f6-42ac-ae58-be9c6ca5afa1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e51b3c27-20e8-4f41-a543-4807c16608fb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3a0b7808-0f09-4c85-b16a-a0b300d711dd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5d6fde84-a392-42c5-95d0-87f2f0b33ba9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='647e5d9b-7771-4ca3-9d0a-22a17a0c8224', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d7bf6b98-1aac-45b4-88cf-01e778b358d5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='顺利讲价买到了牛逼好用的落地灯\\n没有破戒没有破戒没有破戒\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f3a509ec-388e-4ec6-8d58-55810c76902f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='32a70b16-4fa9-4b14-bb40-71b43d199a89', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5376610a-e6a1-4288-a0c7-3a1b25a0fc41', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ab8e91f3-1142-4a69-b1ed-e2da331f04d2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8220cb43-2417-41aa-81d1-883c6d681038', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d2a007fd-cdd3-47b5-9d98-f1a1e63302bc', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='22b0d6cd-d68e-4bf4-a5c1-8a54af6a602e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7bc1eb3b-9940-4d84-99ba-0d538cfadb1d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1cbdfa43-d869-49f8-89e0-4bdfb9b5c1f5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='79024294-2e1c-42bd-9a24-c70f553ed659', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bebf3f29-59cb-4d81-8b47-f89a47a82c89', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8b01db2a-5fa5-4c5b-adc5-6837a637c2fa', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d81695d2-6163-4430-8cb4-a8178586ec27', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='42340c29-17a3-4d36-89f9-92a050dc4318', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='97ffd5ec-b568-408a-bea0-b91d10de62fa', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6ae7bd79-bd5d-46e9-aa17-4fdd73ee286e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ab2f6a84-5036-46a4-a8a2-50ebc060c07a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='626266cb-a27b-40d3-863c-241ca83e573f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c62f8e32-b6fd-4a08-aa1f-481a2bf05dc0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='705977e3-547f-4f48-b221-38f69141cef5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e2379fc5-d6e3-4f0d-8dc2-85bc2edd8edc', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5322eaf1-ae02-4efa-a6ba-27bcf3b6c487', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5d903d48-c3a4-46cb-9915-3b29ca88c106', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a46b00e6-9c6a-4501-b90a-07a1b05e4827', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='276e01be-9349-4e26-8f30-9d2de16908f7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d4585489-a10a-4320-88d7-cbe32248850a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2fa7bb81-7a82-4245-9d02-6e951de8506a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='db50280d-9c6d-46c3-b83f-a24b85e13cfd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c9bcd523-a4a5-44ec-b822-c88b34928ac7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ea259927-f4c0-43b1-9c09-5427e2c4d7f5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='12aebd60-495c-4820-90d6-2c3df2a0615e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2fb2c83e-64b9-4d15-ac09-85d8f9b9df1e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='053d80f3-d5e4-4bda-bb6e-d33e95bc707f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='35a5213c-b0ca-4865-9c0b-b94bcb875bda', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1cf0ea7b-b9be-46cd-8273-1c5aeedcfbaf', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0b45dacc-50e5-47f0-a2b7-c5d729dee670', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='eb543a60-a3e0-4792-ad3b-8c130456ba4f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n直面了困难，晚上加班了3h\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cbe349bc-e006-4aa8-bc79-f575aae15260', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='43b99ee4-094e-4a21-83eb-b24e86947b09', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n新的同事勉强还算ok\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f34f984b-fe7e-44bf-a3e3-8bc1ecc25f49', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n下午因为压力太大破戒了，没有及时意识到问题转换心态\\n但是坚持了5day+ 算是有进步了\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='54b566ea-d9b4-40c5-a237-c964817a4084', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a02eef77-442d-4a32-a225-03c4d478cc4b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n**previous\\xa0 dairy notes**\\n\\n  \\n\\nHow to journal:\\n\\n1. write the most emotional engaging event today; the goal is to remind urself of ur exp; give ur mind time and space to process emotions\\xa0\\n\\n2. write tiny achievement\\n\\n3. write about ur day\\n\\n4. write about ideas\\n\\n\\n\\n::记录每天做的事。哪些是符合我的价值的哪些不是::\\n\\n  \\n\\n\\n【情绪1】\\n\\n时间:早上8点\\n\\n事件:睁眼,看到窗外阳光明媚\\n\\n情绪关键词:温暖舒适\\n\\n情绪强度分数: 2\\n\\n情绪持续时间:3分钟\\n\\n  \\n\\n  \\n\\n220817\\n\\n这两天是倒退了一些，连续破戒，但身体和精神上，还是ok的，工作效率还算ok没有白天摆烂，晚上依然刷了题。自信心也没有崩塌。\\n\\n破戒的原因：好几个人都没回我信息，孤独感；lc的题目太难，之前刷过几次的题依然没思路\\n\\n  \\n\\n孤独感如何破除？正念跳出来；玩游戏；\\n\\n题做不出咋办：慢下来，过会再看那道题\\n\\n  \\n\\n  \\n\\n220817\\n\\n本来一个比较负面的遭拒绝，我产生了负面的思维比如：我不打扰你了。但我扭转了这种思维回复了个，以后有机会可以叫我\\n\\n  \\n\\n今天晚上被网友怼说我不尊重他。他是对的，我错了。解决这种负面情绪的办法就是自己认识到错误，并想到改进方法。以后不要一开始对女性用轻浮的话\\n\\n  \\n\\n220819\\n\\n破戒原因：技术上无法解决的问题。和预期不符的难题。本以为20分钟能搞定的东西弄了一上午，烦躁。\\n\\n如果我能用effort的角度来衡量，其实我高效率地工作了一上午，只是结果不佳而已，没必要否定自己的努力和价值\\n\\n  \\n\\n220820\\n\\n刚意识到了摸手机的冲动克制住了\\n\\n  \\n\\n220822\\n\\n洗澡，和工作的时候，意识到了两次，我所拥有的只有当下\\n\\n  \\n\\n220825\\n\\n今天破戒纯因为在知乎上看到了刺激，然后主动去搜索了\\n\\n  \\n\\n晚上九点多收到同事的回复，拒绝了我的请求觉得不爽。但想想这也不算是我的事。product的人觉得这个项目不重要我和CTO说就行了，我该做的已经做到了。\\n\\n  \\n\\n主动强调了自己的权力。买背包很想买，但价高有点不爽，主动和他讲了价，赚了一些；主动和公司的人要求了自己的东西\\n\\n  \\n\\n情绪不管你看不看他，他都会消失。正念是让你意识到这个过程，让你不被情绪影响，自然消失\\n\\n  \\n\\n220829\\n\\n两个原因：因为在煎蛋上看到女优受了刺激 + 午睡导致的勃起\\n\\n  \\n\\nEBP行动营结束了。对我有帮助，但他并不能奇迹般地替我解决问题\\n\\n  \\n\\n220830\\n\\n止住了自己的冲动\\n\\n220831\\n\\n破戒的原因是，CT那篇paper的恐惧传染了我；以后不要考虑他了，切断就好\\n\\n  \\n\\n220908\\n\\n和omer开会的时候有点被他带着走。我需要主动把握谈话的节奏把握主动权\\n\\n  \\n\\n220909\\n\\n尝试一天情绪记录：早上9点，中午12，晚上6，睡前\\n\\n9：被泡妞失败的情绪裹挟；自卑，迷茫，纠结不知道该删掉还是留着；强度5\\n\\n12:\\n\\n  \\n\\n  \\n\\n220919\\n\\n破戒之前尝试了一下停在当下，做手头的事情看了一些code。虽然做到了但过了几个小时，到下午还是没坚持住，去看了漫画\\n\\n  \\n\\n220924\\n\\n半夜因为网易蜗牛读书的bug很愤怒焦躁，差点破戒，但还好情绪过了几分钟就开始消退了\\n\\n  \\n\\n220928\\n\\n下午因为python plot的一个小问题无法解决十分焦躁差点破戒，还好感知到了当下\\n\\n  \\n\\n221001\\n\\n感觉每天的身体拉伸运动，都可以用正念的方式进行。其他一些工作也可以慢慢正念化\\n\\n  \\n\\n221003\\n\\n想到了各种人际关系和如何和其他人沟通，如何回复其他人\\n\\n  \\n\\n221021\\n\\n差点破戒但感觉并没有什么吸引我的\\n\\n  \\n\\n221022\\n\\n意识到自己为了泡妞假装说自己出去跑步啥的，not honest\\xa0\\n\\n  \\n\\n  \\n\\n221026\\n\\n意识到自己没法一直和妹纸聊，主动决定缓一缓，说明白\\n\\n我隔几天积累了些素材再去聊\\n\\n  \\n\\n221118\\n\\n意识到自己还没有彻底放下博士这些标签；要放下才能专注当下\\n\\n  \\n\\n  \\n\\n221205\\n\\n身体其实已经给我信号说你需要休息了。不应该强迫自己努力，然后努力不成就破戒\\n\\n  \\n\\n221207\\n\\n身体不适，没精神觉得无力改变当下的状态，烦躁；这时候不要试图改变。接受它继续该干嘛干嘛就行。刷题就刷比较简单的题，不要逃避去做别的事\\n\\n  \\n\\n221211\\n\\n头一次意识到，我这两天的行为是大部分符合自己的价值的，比如勇敢和专注；虽然没有结果，但也是值得鼓励的\\n\\n  \\n\\n221214\\n\\n  \\n\\n230101\\n\\n今日情绪：有些茫然无措；不知道该忙什么；导致焦躁不安，也不敢玩游戏\\n\\n强度：4\\n\\n  \\n\\n230104\\n\\n我为何会一直回忆自己面试的失败经历？太看重了，没有把重点放到自己的成长和系统建立上，完全去看重了结果\\n\\n  \\n\\n在练习的时候，不敢引导自己陷入负面情绪中。。。。。。。这可能是我问题的根源之一，不敢直面情绪允许他存在\\n\\n  \\n\\n230106\\n\\n父母并没有责怪我\\n\\n  \\n\\n230109\\n\\n感谢自己的身体，这种感觉很奇怪。意味着身体不是我的工具，是和我平等对话的一部分\\n\\n  \\n\\n230114\\n\\n胸腔很痛，感受到了平时会忽略的身体感受\\n\\n  \\n\\n230117\\n\\n以前行动力强的时候，是完全忽略了自己的感受，也不见得对；理想的状态是明确自己的感受然后随着感受去行动\\n\\n  \\n\\n  \\n\\n230130\\n\\n今天和传销的人扯了半天。我用打哈哈的方式逃离了，而没有say things that are as true as I can say them\\n\\n  \\n\\n230131\\n\\n放低对「幸福」的标准，从生活小事入手，选择一些感觉「还\\n\\n可以」的人事物。\\n\\n当情绪比较消极的时候，一下子转到积极状态跨度太大了。你可以先从关注中性的体验开始。例如，选择一些令人感到「平静」「安全」「放松」的人事物。\\n\\n积极体验是可以主动创造的，如果你在回忆中找到一些让自己开心的方法，你现在就可以试试去实践它。\\n\\n接受自己暂时写不出来，如实写下：「今天暂未发现积极体验，明天我会继续试试看，等明天再说吧~」\\n\\n  \\n\\n  \\n\\n230204\\n\\n持戒足够久之后受到刺激也不那么容易破戒了\\n\\n  \\n\\n230207\\n\\n如果身体没有不适，脑内的想法基本就是两类：对工作能力的担忧，和对社交关系的担忧。前者只能用行动解决，后者不是我能决定的\\n\\n  \\n\\n  \\n\\n230209\\n\\n早上效率最高！早上不要把杂乱的东西塞入脑子形成堵塞\\n\\n  \\n\\n230218\\n\\n对异性的幻想是错误的，尤其想象自己一些舔的行为能有一天感动到别人。这是在act in my imagined world, not how the world works;\\xa0\\n\\n关于sena: 过一个月以上诚恳道歉; 找某二次元音乐会的时机\\n\\n关于j: 不再联系；下次旅行or digital normand之后分享\\n\\n  \\n\\n  \\n\\n230228\\n\\n被无聊的cbc鸽。但突然意识到我是被爱过的。谢谢daisy\\n\\n  \\n\\n230304\\n\\n今天发现一个B站冥想教程，感觉练习很有效果\\n\\n  \\n\\n230305\\xa0\\n\\nemotional event: feel sorry/uncomfortable for rejecting yujin\\n\\nfeel happy that I identified my key improvement area: have a schedule and avoid using phone all day\\n\\n  \\n\\nachievement: 没有破戒，用手机的情况比昨天略有好转\\n\\n  \\n\\n230306\\n\\n意识到我是班上中上的；training session我理应都能听懂的；但做出成绩就是另一回事了，不止需要懂，还需要push ppl;\\xa0\\n\\n  \\n\\n230307\\n\\n痛快删了一个聊不来的人。在考虑断掉韩国人的关系，恢复j的关系\\n\\n早起认真工作的一天；\\n\\n  \\n\\n230312\\n\\n和Nicole很开心的交流；盲盒给了我一大堆好吃的淀粉食物\\n\\n  \\n\\n230312\\n\\n回家的时候担心肉会坏，就直接煮了明天吃；也是通过行动消解不确定性，让自己重获安全感的例子\\n\\n  \\n\\n230313\\n\\n因为少了一小时，白天工作很困很不适，工作压力又很大，导致破戒了；各方面状态都很差的时候，就很难熬过去；以后不要熬，要主动换环境换行为，比如背单词，打扫\\n\\n和nik在看同一个冥想课好巧\\n\\n  \\n\\n230320\\n\\n拿到了PR是值得庆贺的一件事，虽然我的心情毫无变化\\n\\n想到了当bartender的兼职方式；\\n\\n玩手机还是太多，但还是强迫自己完成了两个番茄\\n\\n晚上强行做了一下成功学学习，为梦想前进\\n\\n  \\n\\n230321\\n\\n工作中规中矩，还是手机用的太多了，走神过多\\n\\n昨天读到了一段很感人的英文；今天读到海子的一句诗：“当我痛苦地站在你的面前/你不能说我一无所有/你不能说我两手空空；\\n\\n今天感觉从内心里，真正开始接受了自己会孤独的事实，但并不悲观\\n\\n  \\n\\n230326\\n\\nwhat I learned today: multi head structure: different MLP model for two separate time series, then concatenate at the end;\\xa0\\n\\n我之所以没有leadership，一方面是以为我从小就厌恶被权力规训，也讨厌自己使用权力\\n\\n我在亲密的人前社恐陌生人前社牛\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bd4a143d-4e89-4665-bc0f-b6179522f4ee', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n2022/06/27\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fa261afd-c5c2-416b-a67a-e61a6267c094', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**昨日**\\n\\n理性\\n\\n几个需要锻炼的操作手法：\\n\\n和自己对话，像一个智慧的朋友一样关心自己\\n\\n坚持日记和冥想\\n\\n情绪\\n\\n会被我自己无法左右事情hauting，比如不确定室友会不会搬走\\n\\n被公司的设计师傻逼气到\\n\\n人际\\n\\n尝试理解关心我的人，而不是去理解我感兴趣但对我没兴趣的人\\n\\n梦想\\n\\n重新开始了coursera的课提升基础ds skills\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4ff21944-c0d5-40fb-a8e4-b6c59e45dd5c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**今日**\\n\\n理性\\n\\n情绪\\n\\n人际\\n\\n梦想\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7920dd02-26b3-4981-8c06-f39b6ed7d907', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n2022/06/30\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='34574140-e1ba-46fb-844a-6eac820eb700', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n\\n\\n理性\\n\\n几个需要锻炼的操作手法：\\n\\n和自己对话，像一个智慧的朋友一样关心自己\\n\\n坚持日记和冥想\\n\\n情绪\\n\\n昨天的破戒是因为五点多才睡着，9点起来人都是崩溃的\\n\\n今天依然睡眠严重紊乱。但要坚持住。多坚持几次身体才会习惯这种不适，才不会去强行找出口。事实证明破戒了也没用还是得靠补交解决问题\\n\\n人际\\n\\n做到了主动回应关心我的jenny，主动和椰子皮开玩笑\\n\\n梦想\\n\\n比较努力的完成了白天的工作\\n\\n晚上主动拾回了lc\\n\\n智慧\\n\\n体会到了活在表层的感觉：不开心的人就删掉就好，不要去考虑说我要努力做一个成熟的男人，对自己要求越多越难受\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='22adb72a-da18-4ddf-a500-4b6362541c7d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n2022/07/3\\n\\nself-care\\n\\n有几次成功压抑住了想V的念头，也没有去主动加他\\n\\n理性\\n\\n读mr nice guy学到了很多知识，比如通过别人认可自己获得validation是impossible的；比如我对V的投射然后期望他能对我好，这其实也是一种对别人的manipulation；\\n\\n我之前在关系中一直没有明确表达自己的需求，只是希望通过自己做一些对别人好的事，就期待别人对我好\\n\\n情绪\\n\\n有破戒的倾向，但我及时通过打扫和俯卧撑过去了；多来几次就会越来越容易\\n\\n人际\\n\\npenny不回我有点不爽，但还好我熬过去了没再打扰\\n\\n梦想\\n\\n坚持了刷题没玩游戏\\n\\n智慧\\n\\n极大地提升了对J的认知也算是为梦想的进步\\n\\nfind guy buddies \\n\\n明天的改变\\n\\n早餐之后手机放抽屉，晚餐之后才能拿出\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2b09b698-653b-4f56-8b45-72ea5e92a799', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n2022/07/4\\n\\n自我疗愈\\n\\n理性\\n\\n情绪\\n\\n没被几次房租等小问题弄生气\\n\\n进电梯的时候看到那人没害怕直接站进去了\\n\\n人际\\n\\n梦想\\n\\n智慧\\n\\n明天的改变\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5e284c5d-94ec-4d53-abdc-0cffc8c6318b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n2022/07/9\\n\\n自我疗愈\\n\\n理性\\n\\n情绪\\n\\n人际\\n\\n和zy吃饭她很明显的体会到我这两次之间能量的差异。所以保持一个积极的能量对人际关系极为重要，别人是能很明显体会到的\\n\\n梦想\\n\\n短期目标是进大厂\\n\\n长期目标是拿投资做一个用nlp帮助人们理解自己的产品\\n\\n智慧\\n\\n明天的改变\\n\\n手机放抽屉\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='90ef22ea-690e-48fa-be24-f0c455aecac6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n2022/07/14\\n\\n自我疗愈\\n\\n没必要因为今天玩太多手机就埋怨自己。这只是我开始这个习惯的前一两天\\n\\n理性\\n\\n学习了三种自由\\n\\n情绪\\n\\n人际\\n\\npenny没回我信息；虽然情绪影响不大，但我还是不知道如何handle这种负面情绪维持高能量状态。可能最近对求偶太看重了些。其实应该把重心放在别的方面，因为目前其他方面都挺顺的\\n\\n梦想\\n\\n开始看色图了居然还没lu。极度危险\\n\\n智慧\\n\\n身体\\n\\n早上不能吹电扇\\n\\n明天的改变\\n\\n手机放抽屉！！！！！晚饭之前不能拿出来\\n\\n坚持做到下午四个番茄\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5dc041bc-e7df-4ee0-9415-ee7185340749', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n2022/07/16\\n\\n自我疗愈\\n\\n为何玩手机这个事情戒不掉？是不是纯粹因为自己的身体不适应低多巴胺水平。强行戒除一天试试\\n\\n理性/思考\\n\\n情绪\\n\\n人际\\n\\n和lgr他们去岛上玩了玩，感觉有点无聊。主要那俩人本身太无聊了\\n\\n梦想\\n\\n早起就做了两题LC，虽然起床就玩了挺久的手机\\n\\n智慧\\n\\n身体\\n\\n走了大几公里\\n\\n明天的改变\\n\\nwish: 多巴胺斋戒；早上不看手机\\noutcome: 第一次成功\\nobstacle: 拿起手机的习惯性行为；打开YouTube的习惯性行为\\nplan: 早上喝茶，刷牙；找本杂志来读；晚上要是做到了就主动找人或者网上分享成功喜悦\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c2a0f2d7-7d6a-4371-bedc-ca6a39fd2fce', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n2022/07/20\\n\\n自我疗愈\\n\\n破了也没关系。是今天太热了，这是一个不在我预期中的trigger所以自己没有处理成功\\n\\n理性/思考\\n\\n情绪\\n\\n人际\\n\\n和bb聊得深入了些\\n\\n梦想\\n\\n智慧\\n\\n帮助身边的人破除枷锁获得自由的最好办法就是自己去获得自由，你让他们看到新的可能性\\n\\n身体\\n\\n虽然破戒了但没完全摆烂。强行逼自己做了运动和冥想\\n\\n明天的改变\\n\\n继续戒除多巴胺，不要戒音乐就行\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a963f77a-4586-4b68-b9ed-d1e7a676f26b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n2022/07/21\\n\\n自我疗愈\\n\\n理性/思考\\n\\n情绪\\n\\n下午有一阵很烦躁，但还是坚持把工作做完了\\n\\n人际\\n\\n和咨询师聊了会感觉一般，但功课还是要做\\n\\n在soul上又有几个人回复我了\\n\\n梦想\\n\\n坚持了写日记这件事\\n\\n智慧\\n\\n身体\\n\\n感受到了这段时间俯卧撑的效果\\n\\n明天的改变\\n\\n起床就把手机放抽屉\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2954a3a6-2491-440a-8fd8-fa2b713977d0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n2022/07/24\\n\\n自我疗愈\\n\\n改为争取做到两周一次fap。一步步来\\n\\n理性/思考\\n\\n情绪\\n\\n没有控制住自己的无聊情绪破戒了。还是原来的问题，欲望升起之后的第一反应并不是去观察他而是跟随本能\\n\\n人际\\n\\n梦想\\n\\n智慧\\n\\n要去建立新的关系，哪怕是和旧的人。逃避不能解决我自己的心理问题。创造才是\\n\\n身体\\n\\n手肘很痛今天就没锻炼了\\n\\n明天的改变\\n\\n上午不碰手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a45e236b-3e8a-4fe6-9715-96bfe3deea57', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n2022/07/27\\n\\n自我疗愈\\n\\n理性/思考\\n\\n情绪\\n\\n人际\\n\\n梦想\\n\\n智慧\\n\\n身体\\n\\n明天的改变\\n\\n上午不碰手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f6087aff-1ec3-4816-a761-1424ff61df20', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n2022/07/28\\n\\n自我疗愈\\n\\n理性/思考\\n\\n情绪\\n\\n尝试忽略自己的萎靡状态，有效！！！\\n\\n人际\\n\\n不要被人manipulate，尤其网上认识的人\\n\\n梦想\\n\\n开启脱口秀之旅！\\n\\n从写段子中找到了快乐\\n\\n智慧\\n\\n学会忽略自己寻求舒适感的本能。只有抛弃对舒适感的幻想，才能真正满足\\n\\n身体\\n\\n明天的改变\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9e1a0563-b2a1-4d73-98cd-4fc32e880863', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n2022/07/30\\n\\n自我疗愈\\n\\n自信的时候，就完全无所谓V怎么回我！！！\\n\\n理性/思考\\n\\n情绪\\n\\n尝试了忽略自己的萎靡状态。需要继续坚持。这就是佛教里说的三马地的培养\\n\\n人际\\n\\n只有具备充分自信，毫不心虚，才能接受责备\\n\\n梦想\\n\\n智慧\\n\\n身体\\n\\n明天的改变\\n\\n手机！quit! 早上起来就扔抽屉; 午饭前不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='094e8399-89e8-4e13-a729-b99afaa59577', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n2022/08/4\\n\\n自我疗愈\\n\\n坚持干活，没有破戒\\n\\n理性/思考\\n\\n情绪\\n\\n尝试了忽略自己的萎靡状态。需要继续坚持。这就是佛教里说的三马地的培养\\n\\n人际\\n\\n当我说出自己相信的真相的时候，才会在人际关系里有足够的自信，来抵御其他人的反对，诋毁，和对我的伤害，包括说出真相之后让人不爽然后不回我，我的负面情绪也无法掩盖说出真相带来的力量感\\n\\n梦想\\n\\n大体了解了脱口秀的主题，意识到了自己可能的定位\\n\\n智慧\\n\\n身体\\n\\n明天的改变\\n\\n不看dating app\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='54a0f253-c15e-453f-8af9-0ab935e3a281', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n2022/08/10\\n\\n自我疗愈\\n\\n完成了微信营的功课，确实尝到了很多食物的滋味\\n\\n理性/思考\\n\\n情绪\\n\\n需要更多地用番茄来保持专注；专注过程中不要想人际关系和撩妹的事\\n\\n人际\\n\\n梦想\\n\\n智慧\\n\\n身体\\n\\n跑步了\\n\\n明天的改变\\n\\n不要午睡，要和困倦感共处\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='80a70762-a21c-4459-98ce-b030a20e5a1f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n2022/08/12\\n\\n自我疗愈\\n\\n没有破戒。一方面可能是因为，坦诚正面面对了女生的拒绝，女生也给予了正面的回应，虽然回应并不ideal\\n\\n理性/思考\\n\\n情绪\\n\\n人际\\n\\n梦想\\n\\n智慧\\n\\n身体\\n\\n家中锻炼\\n\\n明天的改变\\n\\n早上开始刷题\\n\\n手机手机锁抽屉！！！', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a39ef7d2-6df6-4b51-bcb0-d8cadbc3e41a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e959f46a-8a79-4b02-8086-e96daa195a42', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b0d20b16-e737-4d63-8a9e-b6f54ba10efb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0acb0266-85aa-4e11-8d07-83bb55ab3cc7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e7fe68e1-643b-439d-897d-ef352957a43a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='095fbcfd-5833-4dd6-9fc4-8e197d2d4215', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='40d302a5-59db-41bd-9fa7-a0d68c6ad749', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n又破戒了，因为时间太长了8天了，很轻易的刺激就破戒了; 主要是前几天就开始接触porn；所以关键还是完全不能接触\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='104be6ff-b7b3-40dc-b7af-f5214cc1c418', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6b56269e-f020-488e-a98a-90536ee0dc5e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='73b48aa4-a31d-4fdd-b420-f9d8ffa0bbff', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0bdb4f27-d390-4240-b793-202de85840d8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b7ac3d4b-8e9e-468e-b021-3b290f8a97ea', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e9fff641-9bf6-4a49-8cae-8349b541da14', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5ab12e1e-0f16-405c-9b7e-40fbb3ecf926', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7f6e03d9-fffb-491f-9eba-ad0cf984642d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fd950278-c243-4687-b1ed-3f13581696be', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='786bc84c-06bc-4974-af12-d0ae28a52e19', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5e859d75-1c28-4491-862d-8db3782522d3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='13c119e0-dd42-4a56-8f4e-fef506c31f50', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4c29174b-4e26-48f1-8b1d-c98afaf572cc', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ee08bbcd-d2df-4a77-a4af-3ac31d12330c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5afffc50-642a-4104-b7ae-06ef35a15d50', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dae429b4-f47d-4757-82e4-ef72838c60d5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='42a4ad41-0bce-4f25-b483-d07a7180b54c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='30e4f079-14b7-4b4f-91f6-b31eb44b79e9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bd1842bb-549a-4a86-b51d-f5690fa7788a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='acba8dd9-8890-435f-9c51-b7af97012008', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='febfae2a-7ddf-4cfb-8497-a1fe1c698999', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='38d29bc8-7f3c-4356-a03b-e0893ea8168a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='90303bbe-827b-4f22-a53d-8a92ab115768', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9b729c19-526a-46a3-ae45-6125051f656e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='--- \\ncssclass: dailynote \\nfc-category: DNP \\nfc-calendar: Unified \\nfc-date: 2023-07-23 \\n\\n---\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1b06edb4-db00-4814-bb9a-4cfe69706310', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n<< [[<% tp.date.now(\"YYYY-MM-DD\", -1)%>]] | [[<% tp.date.now(\"YYYY-MM-DD\", 1) %>]] >>\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='933be22c-d25e-4595-acf1-638dde5775ef', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n<% tp.date.now(\"dddd, MMMM DD, YYYY\") %>\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e409d715-da78-4d46-9ca0-9c41c5b88b0a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n卖出去一样东西\\n昨天bbbbb主动和我聊天了\\n看了一些huggingface的视频\\n帮助了bro的失眠问题\\n重新开始了一个番茄\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6eba3cab-775a-41f8-ba05-8048318ec169', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\nzy对我不错\\n半佛每天写一千多字写了十几年。坚持，熬住就是一切；\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3f79b538-4cad-4c30-b57a-d26e155cd21a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n纯因为无聊破戒；还是因为没有明确的目标，因为过几天就搬家了，这几天就觉得，没啥可以忙的\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b522bffc-c719-4b74-adb5-167369060b8e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0548c539-6dc0-479a-89e7-28e7886dced2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='56d7aeef-ffa6-4784-a78b-1c0d57b7b686', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b3848902-f6e5-4f93-991a-dee5c1e59dde', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='342858d7-883c-4da1-bebe-ed9c8b24c156', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dacece98-5bc6-461e-bf72-cf7d08c37332', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7329ff42-e36c-4b40-a87c-1ce0311520d2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n因为陌森不洗澡让我很不爽，所以破戒\\n这件不爽的事我能做什么？暂时什么都做不了\\n\\n️早上：不太记得干了啥；情绪分5\\n中午：出去晃了一圈；情绪分5\\n下午：做了一些coding，longchain觉得没啥意思，但还是完结了那门课上传到github了；；情绪分5\\n晚上：基本就在破戒+玩手机 + 为了陌森的事感到后悔️懊️恼\\n总体来说很脱节，今天一整天都没有什么感知，玩玩手机看看视频就过去了\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aedad336-8690-456e-ae98-db2ee4863fb1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\npersonal chatbot总算开始了\\nholiday 的休闲放松氛围 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e8187c5a-3c0b-479d-840f-5f13612f8aef', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='422540ae-4200-49e4-9f62-2b9f6b3433fa', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e5802d3e-3a24-4bf8-ac52-567b6f919aad', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n这几天坚持了去gym \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5f03867e-016e-4a8e-92e4-b409b4e5a727', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3173262f-6d40-4c0c-a9f6-f10c608673a3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b0131fc2-b899-4d04-80cc-90aacd4722dd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ea632a7b-dfc8-4bd9-929b-90d6a97b5ac0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2804b7a1-e523-472f-9bb2-1e8e7288a684', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e6e2c997-6130-4d54-8af5-572cc38ccdbb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e10e308c-d752-42b0-97c8-f918840ec6e5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6bc46bcc-23da-492a-a4bb-3d8d64a624f8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3b3bb332-5bc3-4a92-8ee4-aa0b77c91b38', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='37223296-c93d-4702-9ba4-6250fd54736a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5b89bb23-a1ff-46fe-b104-5b7d6d6d03aa', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3d6e9a69-2c06-4d2b-bbf4-f73bb59b244a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a1144ae5-2542-4909-b8d5-4149a38c9108', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d5136cb8-8603-4592-b439-bcce69208584', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9ec87910-2c47-4709-bacc-5f249df7b7b7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8e6d5cca-b70c-48a7-adc6-959b7178b60b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c58a9b8c-3dbd-46a0-b6f5-641a60f1f2e8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d5db2123-0581-4103-be98-560a5e4a948d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f6a40f7a-d64e-491e-88e7-e3281145c619', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='60b6effb-1723-4cbb-b6d6-9f6cb09167d7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d34237da-9a78-4882-8234-b0525bd352d1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n[[<% tp.date.now(\"YYYY-MM-DD\", -1) %>|←]] 2023-07-22 [[<% tp.date.now(\"YYYY-MM-DD\", +1) %>|→]]\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d8fb4a3e-5f61-40ab-86b4-c36dccb39279', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n卖东西产生很多破事，但还好我没太往心里去\\n迁移到了obsidian; huge achievement!!!!\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1a6e068b-e133-42dc-a063-625dd7dcd4be', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n感谢zy难得的朋友\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0cea8fb4-c1af-4127-93cf-20ee2ea3abf3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n早起了；晚上没玩手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a0c49204-3823-404d-a9a9-75be5cd596e9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nAccomplishments\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d493a971-97fc-4fe2-bf74-193515da8829', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b06d7221-c153-46b1-8b03-a66f93dc9dc4', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHappy things\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dee3b349-2084-439d-b3d4-5c02fde73214', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProgress\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='82c44105-f35b-4ae7-bf38-2f717aeaf4f8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGratitude\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2e6ae84d-c6a7-4938-aa1b-adfabc840145', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHealth\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='86a82000-843a-4aaf-af44-89ad73950271', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n习惯养成：\\n- 每天在ds学习上至少花一个番茄\\n- 不用手机\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dab55d69-b523-4d0e-b69b-5e90b7e91af8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n聊天获得的信息\\n\\ndiaplad 面试：\\n\\nimprove new features; experiments; deploy the model in optimized way; \\n\\nhow to define the questions; \\n\\nproject exp; ml questions; coding LC easy and medium; \\n\\nmanulife：比较简单的lc，工作要求技术水平不高主要看项目\\n\\nKenix\\n\\nconsulting co with clients; \\n\\nbusiness data scientist\\n\\nsupply chain issues and challenges \\n\\nbusiness DS role belong to professional services team;', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3e9ac2a8-5372-4b81-beb1-6cef3c15acb5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nmachine learning questions 4\\n\\n是区分一个算法工程师是否真的有算法能力，而不是会几个模型会调些参数。会模型和调参，放之四海之内，只要看点论文看点代码，都不是问题。而真正有能力的算法工程师，是知道为什么会用这个模型，以及在这个模型里为什么要做这种改动，为什么在具体业务场景里，模型会有预期或者非预期的表现。\\n\\n作者：婷播播\\n\\n链接：https://www.zhihu.com/question/527696166/answer/2445990647\\n\\n来源：知乎\\n\\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\\n\\n举个例子，假设我是一个NLP（自然语言处理）的算法工程师，现在业务的主线是提升用户的文章阅读体验。如果直接将用户的阅读时长或者日均活跃度作为模型优化目标，想必是非常困难的，甚至是一个不可解的问题。\\n\\n但是如果将“提升用户文章阅读体验”这个问题，转换成“提升文章的质量”，那就可以用模型建模一篇文章的优质度打分，甚至可以用模型识别低质无营养的文章，那么这里面就有很多方法可以来实现了。这其实就是对业务场景的理解。\\n\\n因此，**如何挖掘和构造优质的业务模型数据，便成为了你与别人打出差异化的关键所在**。\\n\\n在公司里，模型的数据肯定不能完全依赖人工标注，这样的做法耗时耗财。而从业务场景下挖掘的数据，必然会存在标签错误的情况。\\n\\n比如，还是以文章的优质度打分模型为例，那应该如何挖掘优质和低质的文章呢？有的人会说可以用点击和阅读时长数据来挖掘，那如何确保用户看完了这篇文章，这就一定是优质文章呢？又或者，如何能证明这篇文章不属于低俗、口水文、软文等这种类型呢（有的人就喜欢看这种类型的文章，但这种属于低质文章）？此时，我们发现仅仅靠这两个数据是无法满足挖掘数据的质量的，我们还需要引入更多的特征，甚至是数据清洗方法，来辅助我们构造出一份优质的训练数据。\\n\\n搭建和实现模型，讲究一个原则：**奥卡姆剃刀原则**\\n。就是最简单的模型结构，往往是最有效的，且能最直接地反映问题 \\n\\n后续模型结构的优化，也应该是基于现有模型结构上，针对业务存在的特定问题来进行优化，优化的方法有很多，比如引入更多的基础特征甚至高级特征，或者额外加入一个简单有效的模型结构。\\n\\n因为除了你自己，没有人真正关心模型的结构具体是怎么样的，他们只关心最终在业务场景下的效果，所以快速实现和快速验证模型也算是算法工程师的另一基本功\\n\\n作者：William\\n\\n链接：https://www.zhihu.com/question/527696166/answer/2500135192\\n\\n来源：知乎\\n\\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\\n\\n“天下熙熙，皆为利来，天下攘攘，皆为利往。”古人早就把人性总结得透彻精辟。既然用户对推荐有需要，那么就推荐就有市场，有市场自然就会有资本介入，通过技术或者非技术方法，满足用户的需要。\\n\\n用户的需要是一个“懂”自己的东西，那平台就尽量让自己的产品“懂”用户，给用户提供情绪价值，这就是推荐做的事情，理解用户“懂”用户，从而高效地为用户从海量信息中筛选信息。\\n\\n推荐越“懂”用户，用户越能收获情绪价值，平台就可以利用用户的情绪价值，实现平台的需求。\\n\\n这是一个双方互相满足的双方人性需要的过程。\\n\\n这就是因果必然性的“合理”下，推荐的存在。\\n\\n但如果从另一种角度理解“合理”，即应然性，是否应该，那会引发更多的讨论。\\n\\n比如推荐一方面提高了效率，另一方面又容易让人陷入信息房茧；一方面给用户提供了人类需要的情绪价值，另一方面又有人思考这种情绪价值是有利的吗；一方面让用户舒服地做自己，另一方面又被用户怀疑让自己舒服地做自己真的有利于自己吗……\\n\\n一个东西的存在，肯定是有利有弊的，我们不能要求一个东西只提供好处而拒绝它的坏处。所以对于用户来说，如何让自己更好地利用推荐带来的情绪价值，是需要用户自己思考的，而不能一味地让推荐背锅。对于从业者来说，如何让自己所参与的推荐系统更好地理解用户，是需要漫漫求索的，这个过程当然也可以有对推荐和人性之间的思考，但基石始终都是理解用户。\\n\\n在不同阶段,算法工程师的竞争力要求不一样。大致如下:\\n1.实现模型,调参能力\\n给一个idea或论文,能够快速实现(可以基于开源),并复现结果。\\n2.实际问题的抽象能力\\n将实际的繁琐问题抽象为某个典型的NLP任务(比如分类或序列标注) ,并用成熟的模型来解决。\\n3.新问题的解决能力\\n有些问题无法用现有模型解决,需要改进模型或别的方法来解决。\\n4.经验累积的能力\\n经过长时间的打磨,不管是使用了高大上的技术还是人工规则,成功开发了一种核心技术,具有不可替代性。这种经历和经验就是你最核心的竞争力。\\n5.技术路线的把控能力\\n结合公司的业务,确定团队的整个技术路线。要有适当的前瞻性,不走弯路。正确衡量某个任务的工作量,合理进行团队分工,制定工作计划。\\n\\n这三个数据集的严谨用法是在训练集上训练，在验证集上比较，在测试集上评估。\\n\\n训练集一般用于对模型进行训练。在训练阶段，和训练集的ground truth对比得到训练效果。训练效果好的，不一定表示上线效果好，因为可能存在过拟合、训练数据和线上数据分布不一致等问题。因此需要验证集和测试集。\\n\\n验证集用来比较模型。在训练阶段，通常会用同一个方法训练多个模型，这些模型大体相同，部分超参有差异，目的是找到合适的超参。对于训练阶段训练出的多个模型，需要筛选出效果最好的，通过比较这些模型在验证集上的效果，完成筛选。\\n\\n测试集用来评估模型的效果。在验证集上筛选出效果最好的模型后，利用测试集最终进行模型效果评估。\\n\\n在实际操作过程中，一般会省略验证阶段，直接用测试集来评估模型，如果需要模型筛选，也直接利用测试集进行效果比较\\n\\n真阳性率(TPR) = TP/(TP+FN)，即所有正类样本中被正确分为正类的比例，计算方式和召回率相同。假阳性率(FPR) = FP/(FP+TN)，即所有负类样本中被错误分为正类的比例。\\n\\n随着预测为正类的阈值变化，TPR和FPR相应地变化，因此可以得到以TPR为纵坐标和FPR为横坐标的曲线，即ROC曲线，因此可以得到AUC。\\n\\n**2.2.1 各指标特点**\\n\\n单独使用准确率作为评估指标，样本分布对模型影响大。例如，当样本分布极度不均衡时，如负样本占99%，正样本占1%，若模型将全部样本预测为负样本，准确率可以达到99%，但此时模型对正负样本没有区分能力，对正样本的预测能力几乎为0。一种缓解方法是，根据正负样本分布，对正负样本的准确率进行加权求和。\\n\\n单独使用精确率作为评估指标，模型将提高预测为正类的门槛，使得预测为正类的置信度非常高，同时导致把实际为正类的样本预测为负类的数量增多，也就是FN增多，因此模型只能识别非常明显的正类，此时模型对弱正类和负类的区分能力很差。\\n\\n单独使用召回率或者真阳性率作为评估指标，容易导致模型将样本都预测为正类。当模型对所有样本均预测为正类时，FN=0，召回率/TPR为1，即所有正样本都被找到了，但代价是被预测为正类的样本中有大量真实负类(FP)，此时模型对负样本的预测能力几乎为0。\\n\\n单独使用假阳性率作为评估指标，效果和单独使用真阳性率类似，当模型将样本全部预测为正类，此时TN=0，FPR=1，模型对负样本的预测能力几乎为0。\\n\\nVIPVIP 关键是这个门槛\\n\\n精确率和召回率之间存在一定的“此消彼长”。当精确率越高，通常预测为正类的门槛越高，则越多的正类容易被预测为负类，导致召回率低。当召回率高，通常预测为正类的门槛低，导致预测为正类的样本里真实为负类的数量增多，使得精确率低。当一个模型能同时做到精确率和召回率都高的时候，说明模型的分类能力比较好。\\n\\nTPR和FPR之间存在正相关性。从混淆矩阵和TPR以及FPR的定义可以看出，当TPR越高，说明FN越少，说明预测为正类的越多，则TP和FP越多，而样本总量不变，因此FPR越多。极端情况下，当没有样本被预测为正类，则TPR和FPR均为0，当所有样本均被预测为正类，TPR和FPR均为1。\\n\\n**2.2.2 AUC的优势**\\n\\n单个指标对模型评估存在多种问题，从而无法真实地评估模型的性能，因此通常综合考虑多个指标。比如F值，综合考虑精确率和召回率，比如AUC，综合考虑TPR和FPR。\\n\\nAUC作为指标衡量模型时，不依赖于分类阈值的选取，而准确率、精确率、召回率、F1值对阈值的选取依赖大，不同的阈值会带来不同的结果，而从AUC的定义(ROC的生成)知道，AUC是根据所有分类阈值得到的，因此比单独依赖一个分类阈值的指标更有优势。AUC体现的是对样本的排序能力，与具体的分值无关，和推荐系统中的大多数业务场景更贴合，因为大多数业务场景关心item之间的相对序而不关心item的预测分。\\n\\nAUC对正负样本比例不敏感，也是它在业界被广泛使用的原因。2.4节中AUC的理解会证明AUC对正负样本比例不敏感。\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5adfc357-55b0-494e-8e05-6d11c878f3ad', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**2.4 AUC的理解**\\n\\n**AUC对正负样本的区分能力**\\n\\n根据AUC的计算公式，可以将AUC理解为，随机选择一个正负样本对，这个正负样本对正样本预测值大于负样本预测值的概率。因此可以把AUC理解为区分正类和负类的能力，当AUC越大，表示区分正负类的能力越强。\\n\\n当模型对正负样本没有区分能力时，即对一个样本随机预测为正类或负类，此时AUC=0.5。\\n\\n从AUC的定义角度，因为模型对正负样本的预测为随机的，所以TPR=FPR，故ROC曲线下方面积为0.5。\\n\\n模型对正负样本随机预测时TPR=FPR的证明：\\n\\n- 假设有a个正样本，b个负样本，有n个样本被预测为正样本；\\n- 由于随机，所以预测为正样本中真正为正样本的概率和样本中正样本的概率相同=a/(a+b)，故TP=n*a/(a+b)，TPR=TP/a=n/(a+b)\\n- 则FP=n*(1-a/(a+b))，FPR=n*(1-a/(a+b))/b=n/(a+b)\\n- 因此有TPR=FPR\\n\\n从AUC的计算公式角度，因为模型对正负样本的预测时随机的，所以正样本预测值大于负样本预测值的概率为0.5，因此分子是分母的1/2，故AUC=0.5。\\n\\n当ROC和X轴围成的面积越大，AUC越大，模型的性能越好。理论上，AUC的上限值为1，即当模型对所有样本都可以预测准确，而实际中由于样本数据的复杂性，几乎不可能实现AUC为1。\\n\\n**对正负样本分布的敏感性**\\n\\n从AUC的计算公式也可以得出，AUC对正负样本分布不敏感，前提是正样本或负样本进行随机采样，导致正负样本比例发生变化。因为当正或负样本随机采样时，分子和分母发生等比例的缩放，其值不变。当采样时不随机的，则分子分母的变化将不一定等比例，因此AUC的值可能受影响。\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='084c1b88-c7c3-4ca1-b46c-d76de74f3a32', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**2.5 AUC的缺点**\\n\\n虽然AUC有对分类阈值、正负样本分布不敏感等优点，但也存在一定的问题。\\n\\n优点和缺点都是相对的，某些特点在一些场景可能是有点，但在另一些场景可能会成为缺点，人也如此。\\n\\n**2.5.1 AUC的全局性**\\n\\n在推荐场景，样本来自所有用户，根据这些样本计算得到的AUC，是全局的AUC，而推荐发生时刻是针对某个用户，因此全局的AUC对某个用户而言，并不准确，因为AUC的计算使用了其它用户的正负样本，而其它用户的正负样本和当前用户关系不大，导致AUC并不能真正表示对当前用户的正负样本的区分能力。\\n\\n这个问题的一种改进方法是使用更细粒度的AUC，如UAUC或GUAC。UAUC，即针对每个用户，仅使用该用户的正负样本，计算AUC。若在某些场景以user为维度计算AUC会带来一些问题，比如用户消费深度浅，每个用户的正负样本数量有限，导致计算出的UAUC置信度低，则可以使用GAUC，即对用户按照一些标准进行group划分，对每个group计算AUC.\\n\\n**2.5.2 忽略模型拟合度**\\n\\nAUC可以衡量正负样本的区分能力，但难以根据AUC确定模型对正负样本的拟合能力。举个例子，当模型对正样本预测概率均为0.55，对负样本预测概率均为0.45，此时AUC为1，但模型对正样本或负样本的拟合能力差。\\n\\n**2.5.3 不适用对特定指标有要求的场景**\\n\\nAUC无法反映精确率、召回率等指标，这些指标可能在某些场景非常重要，比如对判断用户是否为新冠阳性这个场景，要求召回率为1，而精确率可以低一些，此时AUC将不适用。\\n\\n**2.5.4 TPR和FPR对AUC贡献等价**\\n\\n当对正负类的预测准确率有不同要求时，有些类似2.5.3，此时AUC将无法满足区别对待TPR和FPR。\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8529446e-0ca4-4f8d-8ad1-2338654c0403', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**2.6 面试中AUC的相关问题**\\n\\n- AUC对正负样本分布是否敏感，不敏感的原因\\n- 给定数据集，已知正负样本，模型对所有样本的预测，计算模型的AUC\\n    - 可根据2.3中AUC的计算公式，利用简单代码实现(后续有空更新待补充)\\n- 训练AUC高，线上AUC低，可能的原因(后续会针对这个问题细聊)。\\n\\n!Untitled\\n\\n如何计算AUC - 知乎 (zhihu.com)\\n\\n!Untitled\\n\\n从以上推导结果可以看出，均方误差由偏差、方差和随机误差三者共同构成', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dfc101e4-9ea2-4228-b7eb-082b5ab9f550', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nwhat’s the career path look like at ur company?\\n\\nbe a expert of a more generalistic or expert?\\n\\n• What’s ur exp in this company? Has your role changed since you’ve been here?\\n\\n• How has the company changed since you joined?\\n\\n• What’s one challenge you occasionally or regularly face in your job?\\n\\n1. What are the team’s plans for growth and development? in terms of tech stack; future projects, team growth \\n\\n• How does the company ensure it’s upholding its values?\\n\\n**“How would you describe the culture here? What type of people tend to really thrive here, and what type don’t do as well?”**\\n\\n\\nBased on the research you\\'ve done about the company,\\xa0**try to guess what a few of their biggest data science problems are**. It\\'s often not that hard to do, especially if you\\'ve tried their product. It\\'s okay to guess wrong. What matters is that your guess is logical.\\n\\nIf you expect an interview about A/B testing, you might get asked questions like:\\n\\n- How many users should we look at before we can confirm that Feature X works better?\\n- How can we tell if a feature change will have downstream effects that we don\\'t anticipate?\\n- How could you resolve the effects of two different A/B tests happening at the same time?\\n\\nIf you\\'re told that an interview is going to focus on business acumen and product intuition, you might get questions like:\\n\\n- You work for a company that builds product X for user base Y. How would you know if the product was working well? What data would you want to collect? What kind of dashboard would you want to have built?\\n- What would be some easy mistakes to make? i.e., what numbers could a company decide to measure that would give a misleading sense of how their product is performing?\\n- What kinds of modeling tools would be helpful here? What sorts of predictions would be valuable?\\n- How would you measure the performance of those predictive models? Accuracy? F1 score? Precision? Recall? AUC? Which is most relevant to the user experience? Which is most relevant to the company\\'s bottom line?\\n- What problems could arise if the metric for user experience differs from the metric for the company\\'s bottom line? i.e., in what ways are the company\\'s incentives misaligned from the user\\'s?\\n\\n**The company:**\\n\\n what is the biggest challenge u guys are facing; the industry is facing?\\n\\n- *What does the average day look like for your team?*\\xa0- Here you’re trying to understand communication patterns, reporting structures, etc.\\n- *How do you guys decide what to focus on?*\\xa0- This will give you another glimpse into how things are structured and the relative autonomy you can expect.\\n- *How would you describe the culture for more junior members? Are they encouraged to ask questions and speak up when they’re unsure how to execute something?*\\xa0- Another foray into the actual culture, as opposed to what’s written on the website’s mission statement.\\n\\n**The person:**\\n\\n- *Is there anything you now know that you wish you would’ve known when first taking on your current role?*\\n- *What do you like most about your role? What, if anything, do you dislike the most?*\\n- *What project would you like to be working on a year from now?*\\n\\n**The role:**\\xa0(Not every person you speak to will be intimately familiar with the role you’re applying to so use these questions when appropriate.)\\n\\n- *What’s the most pressing project that you hope this role will help address?*\\n- *What would you say is the most important attribute that correlates with excelling in this role?*\\n- *What are the biggest challenges someone in this position would face?*\\n\\nNOTE: One of the best ways to communicate with people is to repeat back what they said to you. This signals clearly that you’re listening and engaging with them. E.g. \"*I think I’m hearing you say {what they said}*\" or \"*To paraphrase for my own understanding, {what they said}*\".', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4d499dee-e897-4668-ab58-80133eda7f1d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nmachine learning question 2\\n\\nHow would you approach a categorical feature with high-cardinality?\\n\\n如何处理具有高基数（high-cardinality）的类属特征？\\n\\nOne-Hot encoding\\n\\nDummy encoding\\n\\n前根据业务理解，分类，变成 ordinal data 然后encoding\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\nAbove, you can see that variable “Age” has bins (0-17, 17-25, 26-35 …). We can convert these bins into definite numbers. 就是取各个区间里的均值\\n\\n**Combine levels:**\\xa0To avoid redundant\\xa0levels in a\\xa0categorical variable and to deal with rare levels, we can simply\\xa0combine the different levels。比如合并临近的post code为一个Bin\\n\\nTo combine levels using their frequency, we first look at the frequency distribution of of each level and combine levels having frequency less than 5% of total observation (5% is standard but you can change it based on distribution).\\xa0This is an effective method to deal with rare levels.\\n\\n**How to deal with unbalanced binary classification?**\\n\\nWhile doing binary classification, if the data set is imbalanced, the accuracy of the model can’t be predicted correctly using only the R2 score. For example, if the data belonging to one of the two classes is very less in quantity as compared to the other class, the traditional accuracy will take a very small percentage of the smaller class. If only 5% of the examples are belonging to the smaller class, and the model classifies all outputs belonging to the other class, the accuracy would still be around 95%. But this will be wrong. To deal with this, we can do the following-\\n\\n1. Use other methods for calculating the model performance like precision/recall, F1 score, etc.\\n2. Resample the data with techniques like undersampling(reducing the sample size of the larger class), oversampling(increasing the sample size of smaller class using repetition, SMOTE, and other such techniques.\\n3. Using K-fold cross-validation\\n4. Using ensemble learning such that each decision tree considers the entire sample of the smaller class and only a subset of the larger class.\\n\\nWhy use feature selection? If two predictors are highly correlated, what is the effect on the coefficients in the logistic regression? What are the confidence intervals of the coefficients?\\n\\nMulticollinearity is a statistical phenomenon in which two or more predictor variables in a multiple logistic regression model are highly correlated or associated. Multicollinearity does not reduce the predictive power or reliability of the model as a whole; it only affects calculations regarding individual predictors. The existence of collinearity inflates the variances of the parameter estimates, and consequently incorrect inferences about relationships between explanatory and response variables. Much better diagnostics are produced by linear regression with VIF. If VIF is over 5, we should exclude that feature.\\n\\nYou want to run a regression to predict the probability of a flight delay, but there are flights with delays of up to 12 hours that are really messing up your model. How can you address this?\\n\\nFirst thing that comes to my mind is to create groups like, delays less than 40 mins, delays between 40 mins and 2 hours, between 2 to 10 hours and 10+ hours. But that means using a classification model, like logistic regression. When we think about delay, we tend to think like \"will there be a delay or not\", rather than how long of a delay we can come across. If we are asked to use linear regression, we may either transform the delay column (i.e. log transform) or ignore these outliers and simply drop them.\\n\\n**How would you tune a random forest?**\\n\\nThere are primarily 3 features which can be tuned to improve the predictive power of the model :\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='815f9b22-7e2d-484d-9ee1-b024dd36c5a4', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**1.a. max_features:**\\n\\nThese are the maximum number of features Random Forest is allowed to try in individual tree. There are multiple options available in Python to assign maximum features. Here are a few of them :\\n\\n1. *Auto/None*\\xa0: This will simply take all the features which make sense in every tree.Here we simply do not put any restrictions on the individual tree.\\n2. *sqrt*\\xa0: This option will take square root of the total number of features in individual run. For instance, if the total number of variables are 100, we can only take 10 of them in individual tree.”log2″ is another similar type of option for max_features.\\n3. *0.2*\\xa0: This option allows the random forest to take 20% of variables in individual run. We can assign and value in a format “0.x” where we want x% of features to be considered.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='173480a9-53b0-4c08-a764-a5ff74397bf7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**1.b. n_estimators :**\\n\\nThis is the number of trees you want to build before taking the maximum voting or averages of predictions. Higher number of trees give you better performance but makes your code slower. You should choose as high value as your processor can handle because this makes your predictions stronger and more stable.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='915816c7-14e0-4b1e-8545-24fa4095a77d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**1.c. min_sample_leaf :**\\n\\nIf you have built a decision tree before, you can appreciate the importance of minimum sample leaf size. Leaf is the end node of a decision tree. A smaller leaf makes the model more prone to capturing noise in train data. Generally I prefer a minimum leaf size of more than 50. However, you should try multiple leaf sizes to find the most optimum for your use case.\\n\\n!Untitled\\n\\n**How do you explain Random Forrest to a non-technical person?**\\n\\n**Do you think an additional feature improves GBM or Logistic Regression more?**\\n\\n**How do you optimize model parameters during model building?**\\n\\n**When you are doing logistic regression, how do you assess your model? What is the different compared to simple linear regression?**\\n\\nLogistic regression is a classification model, it predicts probabilities of classes. So, the model should be assessed using classification metrics. For example, ROC-AUC or confusion matrix. Simple linear regression used only for regression problems. To assess linear regression we use MSE, MAE, RMSE - regression metrics.\\n\\n**What is the relationship between PCA and LDA/QDA?**\\n\\nAll of these are techniques for dimensionality reduction. PCA is unsupervised, LDA/QDA is supervised.\\n\\n**What are the main advantages of Principal Component Analysis (PCA)?**\\n\\nDimensionality reduction.\\n\\nUseful to reduce dimensionality of data prior to clustering.\\n\\n**How does the K-means algorithm work?**\\n\\nA priori assume number of clusters\\n\\nInitialize by randomly picking n data points as centroids.\\n\\nCompute euclidean distance between centroid and other points. Points assigned to nearest centroid cluster.\\n\\nNew centroid of cluster computed as geometric center of all clustered data points.\\n\\nRecalculate euclidean distance and change cluster of datapoints to nearest centroid.\\n\\nRepeat until converges.\\n\\nMetrics are: (between class scatter) / (within class scatter)\\n\\n**How does K nearest neighbors work?**\\n\\nK-means clustering is an unsupervised clustering algorithm.\\n\\nK-NN is a supervised classification model.\\n\\nYou have some labelled data points.\\n\\nEuclidean distance between new data point and labelled points is computed. All k-nearest points vote for which label new point will be.\\n\\n**What is Bayesian Logistic Regression?**\\n\\nIt is a kind of logistic regression when you have some initial belief about the distribution. In logistic regression, you maximize the likelihood function p(y|β0,β1,x). That is, you find the weights β0,β1 that maximizes how likely your observed data is. In bayesian logistic regression, you start with an initial belief (prior) about the distribution of p(β0,β1) and use it to find the weights.\\n\\n**I built a machine learning model to solve a binary classification problem. The accuracy of my classifier is 99%, and the AUC is 0.51. Is my model doing well? Why or why not?**\\n\\nThis means that there is tremendous class imbalance. Your model is not predicting the majority class with precision, or the minority class with good recall.\\n\\n**What is \"naive\" about a Naive Bayes classifier?**\\n\\nNaive Bayes classifier is called naive because it assumes that each input variable is independent. This is a strong assumption and unrealistic for real data. However, the technique is very effective on a large range of complex problems.\\n\\n**What are the assumptions of a Linear Regression?**\\n\\nThe number of observations must be greater than number of Xs • \\n\\nNo perfect multicollinearity of features • \\n\\nNormality of residuals\\n\\n**Explain the difference between bagged and boosting models**\\n\\nBagging is the simplest way of combining predictions that belong to the same type. On the other hand Boosting is a way of combining predictions that belong to the different types.\\n\\nThe aim of Bagging is to decrease variance as well as solve over-fitting problem, But Boosting tries to reduce bias not variance.\\n\\nIn Bagging each model is built independently and receives equal weight. In Boosting new models are influenced by performance of previously built models and models are weighted according to their performance.\\n\\nIn case of unstable (high variance) classifier, we apply Bagging. And Boosting is applicable for stable classifier.\\n\\nBagging method is used in Random Forest algorithm and, Boosting method is used in Gradient boosting algorithm.\\n\\n**What is difference between R squared and \\'adjusted R squared\\'**\\n\\nAdjusted R squared is adjusted to penalize the number of independent vars (features) you have in your model.\\n\\nIf you go purely by R squared, you can keep improving it by adding extra features, even if you are causing overfitting.\\n\\n**How do you address class imbalance?**\\n\\nTo address some class imbalance, you can randomly interpolate data points of minority class using SMOTE.\\n\\nAlternatively, you can take a random bootstraped sample from majority class so that you are training with 50/50 class balance.\\n\\nAlso keep a close eye on model evaluation. Pay attention to precision and recall of model, not just accuracy.\\n\\n**What is the difference between Gradient Descent, Stochastic Gradient Descent, Gradient Descent with Momentum?**\\n\\nStandard gradient descent always takes the steepest downward route.\\n\\nStochastic GD picks a random point downhill and takes steepest route toward point. This adds an element of randomness that can be useful to get past local minima and toward global minimum.\\n\\nGD with momentum means that the learning rate increases as the GD build momentum continuously on downhill descents. This momentum helps to model move past local minima and reach global minima.\\n\\n**How does a neural network with one layer and one input and output compare to logistic regression?**\\n\\nIt would be extremely similar. You have single coefficients for each input, then you take the sum of the linear combination as your output.\\n\\nThe application of a non-linear transform at the end (activation function) makes it more similar to logistic regression.\\n\\n**If you can build a perfect (100% accuracy) classification model to predict some customer behavior, what will be the problem in the application?**\\n\\nThe model will become increasingly overfit to the sample of customers in your data set. This could make the model extremely inflexible to adapt to new customer data in the future.\\n\\n**What is regularization? Which problem does regularization try to solve?**\\n\\nRegularization adds bias to your model in order to reduce variance. It prevents models from overfitting the training data by limiting the coefficients of features so that no feature can influence the output of the model too much.\\n\\nLASSO L1 regularization is also useful for feature selection as it can reduce coefficients of uninformative features to zero and remove them from the model.\\n\\n**What is the lifetime value of a driver? (From Lyft)**\\n\\nSimply, the lifetime value is the total revenue generated over the duration the driver is working. $1000 a month * drives for 8 months = $8000\\n\\n**What metrics do decision trees use to separate data at nodes?**\\n\\nGini impurity:\\n\\ncomputed for each child node after decision node:\\n\\n1 - (% target==1)^2 - (% target==0)^2\\n\\nIf a decision node perfectly splits observations of opposite class, gini impurity = 0.\\n\\nWe pick the decision nodes (features) that have the lowest gini impurity and put them at the root of the tree.\\n\\n**What are Recurrent Neural Networks useful for?**\\n\\nThey are powerful at making classifications given sequential data (ex: given the previous 9 words, predict the 10th word)\\n\\nThis is because each RNN node integrates the current input with enduring \\'internal state\\' information that informs its classification of the current input.\\n\\nRNN nodes have 2 inputs: current input + internal state info passed from previous node.\\n\\nThese are state-of-the-art RNN nodes LSTM = Long short term memory GRU = gated recurrent unit\\n\\n**What is the purpose of using LSTM and GRU?**\\n\\nSimple RNN nodes have a major issue with the vanishing gradient problem. They tend to always give much more consideration to the immediate previous nodes and not distant nodes.\\n\\nSay we have the sentence: \"Harry Potter and the ... ... ... ... ...\"\\n\\nObviously, the oldest words in the sequence are hugely important in predicting the last word. Basic RNN nodes would not consider these crucial words.\\n\\nLSTM and GRU are powerful in that they can selectively give more weight to older, yet important steps in a sequence. They can also selectively forget unimportant previous steps, which helps a lot with computational efficiency and minimizes memory usage.\\n\\n**What is the \\'vanishing gradient\\' problem in deep learning?**\\n\\nIf you have a multi-layer neural network, error has a much larger effect on learning for the last layers than for the early layers. In fact, the early layers often don\\'t learn much at all because the magnitude of gradient descent is divided dramatically with every back propagation step.\\n\\nNote: back propagation through a multi-layer NN occurs via the chain rule from calculus.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='873ed981-4d64-4ace-beab-498af0fc4229', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nWhat are dense layers in a CNN? What are they useful for?\\n\\nWhat are 2D convolutional layers in CNN? What are they useful for?\\n\\nWhat are pooling layers in CNN? What are they useful for?\\n\\n**What are the common activation functions in NN?**\\n\\nrelu; softmax; \\n\\n**What is RFM analysis?**\\n\\nRecency, frequency, monetary Used in Business intel. to model value of customers\\n\\n**What does it mean for a signal in time to be stationary?**\\n\\nStationary signals are signals where their frequency content is the same at every point in time point.\\n\\nEx. linear combination of sinusoids will always have same fourier spectrum. Meanwhile many real world signals have different frequency spectra at different time epochs\\n\\n**How do decision tree nodes separate observations for binary features? categorical features? numeric features?**\\n\\nbinary features: one side yes, other side no\\n\\ncategorical features: every category is a different leaf\\n\\nnumeric features: pick a threshold and separate by what is above and below the threshold. Threshold is chosen based on the lowest possible gini impurity\\n\\n**What is the difference between bagging, boosting and stacking?**\\n\\nbagging (bootstraped aggregation): random resample of\\n\\nboosting:\\n\\nstacking: using ensemble models, where many different ML models are trained and they each vote for what the predicted class is for each observation.\\n\\n**How do random forests work?**\\n\\nMany decision trees are randomly generated. Each decision tree labels an observation and all the votes from the many trees are used to find the average prediction.\\n\\n**Define these NLP terms:**\\n\\nVectorization\\n\\nStopwords\\n\\nStemming\\n\\nTF-IDF\\n\\nVectorization: taking words from a document at putting them in an encoded sparse matrix with docs as rows and each possible word as a column, with the value of each element being the number of times a word is in a document. This sparse matrix is called a Bag of Words\\n\\nStopwords: known words that we want to exclude from our Bag of Words (ex: the, a, that, of, punctuation, special characters)\\n\\nStemming: converting all forms/conjugations of root word to the root word (ex: running, runs, ran = run)\\n\\nTF-IDF: term frequency - inverse document frequency. normalization of word counts in the BoW to be in units of how rare/significant that word is in the sample of documents. (rarer words = more important info)\\n\\n**Which ML models are used for NLP text prediction (guess 10th word, given first 9 words) ?**\\n\\nRNN using LSTM nodes', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b9e2e143-8ce5-41cf-a4cf-d32cc2eb360b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\nWhen you are ramping up the change, you may see the effect flatten out. Thus making the tested effect not repeatable. There are many reasons for this phenomenon.\\n\\n1. **Seasonality effect**: Social network platform user behavior changes a lot when students start summer vacation or going back to school. Holidays affect users’ shopping behavior a lot. Solution: use hold-back method , launch the change to everyone except for one small hold-back group of users, and continue comparing their behavior to the control group.\\n2. **Novelty effect**\\xa0or\\xa0**change aversion**: cohort analysis may be helpful.\\n3. \\n\\nTwo issues to consider when it comes to new experience: (1) what is the base of your comparison? (2) how much time you need in order for your users to adapt to the new experience, so that you can actually say what is the plateaued experience and make a robust decision? Except for new experience, long term effect is hard to test too. For example, a home rental website test its referral effect, but a customer may not return even in six months, it’s very hard to measure through A/B testing. If this is the case, what shall we do?\\n\\nWhen A/B testing is not useful, we can:\\n\\n- Analyze the user activity logs\\n- Conduct retrospective analysis\\n- Conduct user experience research\\n- Focus groups and surveys\\n- Human evaluation\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='13fe7d5c-ae2d-4d55-8de8-a8974dc40fae', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nHow to Answer a Product A/B Testing question?\\nAnswering Product A/B Testing interview questions can be challenging, but with the right approach, you can showcase your skills and knowledge effectively. Here is a comprehensive framework that you can follow to answer product A/B testing interview questions:\\n\\n1. **Describe the Product (P):**\\xa0Start by giving a brief overview of the product. This will help to set the context for the interviewer and give them a clear understanding of the product you are discussing.\\n    \\n2. **Choose an Objective (O):**\\xa0Next, identify the objective of the A/B testing experiment. This could be to increase engagement, reduce churn, or improve user experience, among others.\\n    \\n3. **Brainstorm Ideas (I):**\\xa0Now, it's time to brainstorm ideas that can help you achieve the objective you have identified. This is a critical step as it requires you to think creatively and come up with ideas that can make a real impact.\\n    \\n4. **Describe the Hypothesis (H):**\\xa0Once you have a list of ideas, it's time to form a hypothesis. A hypothesis is a statement that describes the relationship between two variables. It should be specific, testable, and have a clear direction.\\n    \\n5. **Describe the Experiment (E):**\\xa0Now that you have a clear hypothesis, you need to describe the experiment you will run to test it. This includes the groups you will use, the sample size, and the length of the experiment.\\n    \\n6. **Choose Criteria of Success (S):**\\xa0Next, identify the criteria you will use to determine if the experiment was successful. This could be a specific increase in engagement, a reduction in churn, or a specific increase in user satisfaction.\\n    \\n7. **Describe the Trade-Off (T):**\\xa0When conducting A/B testing, there are often trade-offs to consider. Identifying these trade-offs is important because it will help you make an informed decision about whether to implement the change or not.This includes potential negative impacts on other areas of the business.\\n    \\n8. **Prioritize the AB tests (P):**\\xa0Finally, prioritize the AB tests based on their impact and feasibility. This will help you determine which tests to conduct first and which tests to defer until later.\\n\\n\\nOnce we know the sample size, we can obtain the number of days to run the experiment by dividing the sample size by the number of users in each group. If the number is less than a week, we should run the experiment for at least seven days to capture the weekly pattern. It is typically recommended to run it for two weeks. When it comes to collecting data for a test,\\xa0**more is almost always better than not enough**.\\n\\n> Company X has tested a new feature with the goal to increase the number of posts created per user. They assigned each user randomly to either the control or treatment group. The Test won by 1% in terms of the number of posts. What do you expect to happen after the new feature is launched to all users? Will it be the same as 1%, if not, would it be more or less? (assume there’s no novelty effect)\\n\\nThe answer is that we will see a value larger than 1%. Here’s why.\\n\\nIn\\xa0**social networks**\\xa0(e.g. Facebook, Linkedin, and Twitter), users’ behavior is likely impacted by that of people in their social circles. A user tends to use a feature or a product if people in their network, such as friends and family, use it. That is called a\\xa0**network effect**.\\n\\n\\n\\nFor\\xa0**two-sided markets**\\xa0(e.g. Uber, Lyft, ebay, and Airbnb): interference between control and treatment groups can also lead to biased estimates of treatment effect. It is mainly because resources are shared among control and treatment groups, meaning that control and treatment groups will compete for the same resources. For example, if we have a new product that attracts more drivers in the treatment group, fewer drivers will be available in the control group. Thus, we’ll not be able to estimate the treatment effect accurately. **Unlike social networks where the treatment effect underestimates the real benefit of a new product, in two-sided markets, the treatment effect\\xa0**overestimates**\\xa0the actual effect.**\\n\\n\\nThere are many ways to tackle the spillover between groups and the main goal is to\\xa0**isolate users**\\xa0in the control and treatment groups. Below are a few commonly used solutions, each applies in different scenarios, and all of them have limitations. In practice, we want to choose the method that works the best under certain conditions, and we could also combine multiple methods to get reliable results.\\n\\n**Social networks:**\\n\\n- One way to ensure isolation is to create\\xa0**network clusters**\\xa0to represent groups of users who are more likely to interact with people within the group than people outside of the group. Once we have those clusters, we could split them into control and treatment groups. Check out\\xa0this paper\\xa0for more details on this approach.\\n- **Ego-cluster**\\xa0randomization. The idea originated from Linkedin. A cluster is composed of an “ego” (a focal individual), and her “alters” (the individuals she is immediately connected to). It focuses on measuring the one-out network effect, meaning the effect of a user’s immediate connection’s treatment on that user, then each user either has the feature or does not, and no complicated interactions between users are needed.\\xa0This paper\\xa0explains this method in detail.\\n\\n**Two-sided markets:**\\n\\n- **Geo-based**\\xa0randomization. Instead of splitting by users, we could split by geo-locations. For example, we could have the New York Metropolitan area in the control group and the San Francisco Bay Area in the treatment group. This will allow us to isolate users in each group, but the pitfall is that there will be a larger variance because each market is unique in certain ways through things such as the customer’s behavior, competitors, etc.\\n- Another method, though used less commonly, is\\xa0**time-based**\\xa0randomization. Basically, we select a random time, for example, a day of a week, and assign all users to either the control or treatment group. It works when the treatment effect only lasts for a short amount of time, such as when testing a new surge price algorithm performs better. It does not work when the treatment effect takes a long time to be effective, such as a referral program. It can take some time for a user to refer to his or her friends.\\n\\n\\n> We ran an A/B test on a new feature and the test won, so we launched the change to all users. However, after launching the feature for a week, we found that the treatment effect quickly declined. What is happening?\\n\\nThe answer is the novelty effect. Over time, as the novelty wears off, repeat usage will be decreased so we observe a declining treatment effect.\\n\\nNow you understand both novelty and primacy effects,\\xa0**how do we address the potential issues**? This is a typical follow-up question during interviews.\\n\\nOne way to deal with such effects is to completely rule out the possibility of those effects. We could run tests only on first-time users because the novelty effect and primacy effect obviously doesn’t affect such users. If we already have a test running and we want to analyze if there’s a novelty or primacy effect, we could\\xa0==1) compare new users’ results in the control group to those in the treatment group to evaluate novelty effect 2) compare first-time users’ results with existing users’ results in the treatment group to get an actual estimate of the impact of the novelty or primacy effect.==\\n\\n\\n\\nIn the simplest form of an A/B test, there are two variants: Control (A) and treatment (B). Sometimes, we run a test with multiple variants to see which one is the best amongst all the features. It can happen when we want to test multiple colors of a button or test different home pages. Then we’ll have more than one treatment group. In this case, we should not simply use the same significance level of 0.05 to decide whether the test is significant because we are dealing with more than 2 variants, and the probability of false discoveries increases. For example, if we have 3 treatment groups to compare with the control group, what is the chance of observing at least 1 false positive (assume our significance level is 0.05)?\\n\\nWe could get the probability that there is no false positives (assuming the groups are independent),\\n\\nPr(FP = 0) = 0.95 * 0.95 * 0.95 = 0.857\\n\\n**then obtain the probability that there’s at least 1 false positive**\\n\\nPr(FP >= 1) = 1 — Pr(FP = 0) = 0.143\\n\\nWith only 3 treatment groups (4 variants), the probability of a false positive (or Type I error) is over 14%. This is called the “**multiple testing**” problem. A sample interview question is\\n\\n> We are running a test with 10 variants, trying different versions of our landing page. One treatment wins and the p-value is less than .05. Would you make the change?\\n\\nThe answer is no because of the multiple testing problem. There are several ways to approach it. One commonly used method is\\xa0**Bonferroni correction**. It divides the significance level 0.05 by the number of tests. For the interview question, since we are measuring 10 tests, then the significance level for the test should be 0.05 divided by 10 which is 0.005. Basically, we only claim a test if significant if it shows a p-value of less than 0.005. The drawback of Bonferroni correction is that it tends to be too conservative.\\n\\n\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='744d2538-e976-4502-ad6d-51aae735c77e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n1. Define a product goal and its impact on your users\\nWhen defining your goal and its impact on customers, consider these five experiment components:\\n\\n- **The problem:**\\xa0what do your users already struggle with and need from your product?\\n    \\n- **The (possible) solution:**\\xa0what are potential solutions to that problem?\\n    \\n- **The benefit:**\\xa0what is the benefit of that solution, both for your users and for your business?\\n    \\n- **The users:**\\xa0which audience segment is this solution most relevant to?\\n    \\n- **The data:**\\xa0what will success look like? Which metrics are you looking to change?\\n\\n**Quantitative data**\\xa0will reveal potential gaps in your product experience (like a low task completion rate or high churn rate) or a declining metric (like a longer task completion time or a lower\\xa0NPS\\xa0than usual).\\n\\n**Qualitative data**\\xa0will show you how your customers feel while using your product, and where they struggle. For example, session recordings can reveal which areas of your product confuse users, and open-ended survey questions let your users explain why they are (or aren’t) taking specific actions.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='64aad427-baaa-4c53-b0e0-a51d1afc4a78', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n2. Build a hypothesis relevant to your goal\\nHere’s the structure you can follow to build your product experiment hypothesis:\\n\\n\"We believe that\\xa0**[a solution]**\\xa0will\\xa0**[the change that will happen]**\\xa0for\\xa0**[audience segment]**\\xa0because\\xa0**[reason for change]**.\"\\n\\nLet’s take the onboarding completion rate goal from the previous step. Here’s what a hypothesis might look like for that goal:\\n\\n\"We believe that\\xa0**reducing the number of suggested actions in the final three onboarding steps**\\xa0will\\xa0**increase the onboarding completion rate**\\xa0for\\xa0**new customers**\\xa0because it will\\xa0**reduce confusion and overwhelm**.\"\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='865c27ee-6a34-4a6a-8e87-e9132b85a2f7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n3. Choose KPIs to measure your experiment\\n\\nHere are some other examples of KPIs to inspire your thinking:\\n\\n- Conversion rate from free trial to paid subscription\\n    \\n- Customer Satisfaction Score (CSAT)\\n    \\n- Task completion time\\n    \\n- Task completion rate\\n    \\n- Form completion rate\\n    \\n- Churn rate\\n    \\n- Net Promoter Score® (NPS)\\n\\n- **Impression count**\\n- **Click-through rate**\\n- **Button hover time**\\n- **Time spent on page**\\n- **Bounce rate on the button's clickthrough link (assuming the button leads to a new webpage)**\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='072ee0b4-dd54-45eb-b9a8-6c4719118407', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n4. Set up experiment parameters\\nsample size and power \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='eed7a230-d2ea-4242-a405-6e94d4228ce3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n5. Run your product experiment\\n\\nSo, now you've got your experiment goal and hypothesis, and you know the KPIs you’re tracking and for how long. Next up: launching your experiment.\\n\\nFor this, you’ll need to set up some product experimentation tools. Here are some suggestions:\\n\\n- Google Optimize, a free experiment platform you can use to run A/B,\\xa0multivariate, redirect, personalization, and banner split tests\\n    \\n- Optimizely, a digital experience platform that enables tests like A/B, multivariate, and personalization campaigns\\n    \\n- Omniconvert, an experimentation tool that lets you run experiments for detailed user cohorts\\n\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='50b5c71f-c2b4-4965-9454-1a1b1e6377e0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n6. Review results to prioritize product updates and inform future experiments\\n\\nThere are two main possible outcomes of your experiment:\\n\\n1. **You’ve confirmed your hypothesis**, which means the product change you’ve tested should be rolled out to all users in that user segment.\\n    \\n2. **You’ve disproved your hypothesis**, meaning the outcome you’ve outlined in your hypothesis didn’t happen. That isn’t the end of the world—you can learn from it.\\n\\n\\n\\n\\nBalancing Network Effects, Learning Effects, and Power in Experiments\\n\\nMeet Dash-AB - The Statistics Engine of Experimentation at DoorDash\\n\\n**常见框架：**\\n\\n1. 要搞清楚我们想测试的核心指标是啥。你重视的是什么？你想怎么评估effect？\\n2. 要选择测试的significance level，power，length，计算出必须要的sample size\\n3. 控制对照组，跑实验\\n4. 分析实验结果，得出足够强壮的结论\\n\\n1. Setting Metrics - A good metric is simple, directly related to the goal at hand, and quantifiable. \\n2. Constructing Thresholds -What degree does your key metric must change in order for the experiment to be considered successful?\\n3. Sample Size and Experiment Length - how large of a group are we going to test on and for how long?\\n4. Randomization and Assignment - Who gets which version of the test and when? We need at least one control group and one variant group.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b323e27c-e458-4676-82fa-7cb5037301be', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Not running separate test for new vs. returning visitors**\\n\\nReturning visitors are loyal to your site. They are used to it with all of its conversion problems!\\n\\nHumans are creatures of habit. In many instances, we find that returning visitors convert at a lower rate when we introduce new and better designs.\\n\\n****Not considering mobile traffic****\\n\\n****Mobile vs desktop experience: the aftermath of testing****\\n\\n****Ignoring different traffic sources****\\n\\n****Mistake #1: Your test has too many variations****\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='23457709-badf-4a4e-926d-4a4e37f419d9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Problem 2: You stop testing at the first significant result.**\\n\\nThis is the statistical equivalent to taking your ball and going home. Unfortunately, when it comes to A/B testing, stopping your test as soon as you see a statistical significant result is not just bad sportsmanship, but it also produces completely invalid results.\\n\\n!Untitled\\n\\n**It’s common practice to pick 80% as the power and 5% as the significance level of the A/B test, that is 20% Type II error and 5% Type I error. However, the choice of a value of this parameter depends on the nature of the test and the business constraints.**\\n\\n****Step 3: Calculating minimum sample size****\\n\\n!Untitled\\n\\n****Analyzing A/B test results with Python****\\n\\n**parametric**\\xa0and\\xa0**non-parametric**\\xa0tests. The choice of the test depends on the following factors:\\n\\n- format of the primary metric (underlying pdf)\\n- sample size (for CLT)\\n- nature of the statistical hypothesis (show that a relationship between two groups merely exists or identify the type of relationship between the groups)\\n\\nThe most popular\\xa0**parametric**\\xa0tests that are used in A/B testing are:\\n\\n- **2 Sample T-test**\\xa0(when N < 30, metric follows\\xa0*student-t*\\xa0distribution, and you want to identify whether there exist a relationship and\\xa0*the type of relationship*\\xa0between control and experimental groups)\\n- **2 Sample Z-test**\\xa0(when N > 30, metric follows asymptotic\\xa0*Normal*distribution and you want to identify whether there exist a relationship and\\xa0*the*\\xa0*type of relationship*\\xa0between control and experimental groups)\\n\\nThe most popular\\xa0**non-parametric**\\xa0tests that are used in A/B testing are:\\n\\n- **Fishers Exact test (**small N, identify and you want to identify whether there exist a relationship between control and experimental groups**)**\\n- **Chi-Squared test**\\xa0(large N, identify and you want to identify whether there exist a relationship between control and experimental groups)\\n- **Wilcoxon Rank Sum/Mann Whitney test**\\xa0(small N or large N, skewed sampling distributions, testing the difference in medians between control and experimental groups)\\n\\nStandard Error and Confidence Interval for Non-parametric Tests\\nIn the case of parametric tests, the calculation of Standard Error and Confidence Interval is straightforward. However, in the case of Non-\\nparametric tests, the calculation is no longer straightforward. To calculate the\\nStandard Error and the Confidence Interval of a non-parametric statistical test\\nthat aims to compare the sample means or sample medians of control and experimental groups, one needs to use resampling techniques such as Bootstrapping and Boostrap Quantile method, respectively.\\n\\n[[standard deviation, error, and variance]]\\n\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f09a73cb-6de7-4b45-9835-5255ced1e8c8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Describe the Product**:\\n\\n**Facebook**\\xa0is a leading\\xa0**social media**\\xa0platform that offers users a platform to share and express different aspects of their life. With its multiple features such as Messenger, Marketplace, Groups, Games, and Jobs, Facebook provides a comprehensive experience to users.\\n\\nOne of the primary purposes of Facebook is to connect users with their friends and family. The platform offers three main ways for users to find friends:\\n\\n1. **Search:**\\xa0This feature allows users to search for friends by typing their name into the search bar.\\n    \\n2. **Find Friends:**\\xa0Facebook also offers a “Find Friends” feature that suggests friends based on the user’s contact information and mutual friends on Instagram.\\n    \\n3. **Friend Requests:**\\xa0Users can also receive friend requests from other users, making it easy for them to expand their network of friends.\\n    \\n\\n---\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a3744985-c884-444b-909e-d92ec3033a03', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Goal**:\\n\\nThe objective of Facebook is to encourage its users to expand their social network by adding more friends. In order to achieve this through A/B testing, I will be focusing on enhancing the \"Find Friends\" feature, which provides suggestions for potential friends. The aim is to simplify the process and make it easier for users to discover new connections.\\xa0\\n\\n---\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='35ef2aa4-4553-4646-bea5-d354f30309cd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**\\nUser Groups**:\\n\\n  \\nIn order to determine which user group to focus on for an A/B testing question, it is important to understand the different user groups and their behavior on the platform. There are three main user groups: New User, Moderate User, and Power User.\\n\\n1. **New User:**\\xa0This group of users is new to the platform and has not engaged much with it yet. They do not have many friends and there is not much information available about them.\\n    \\n2. **Moderate User:**\\xa0This group of users has established a set of friends but is not actively friending many people. They have a moderate level of engagement with the platform and there should be sufficient data to surface recommendations for friends.\\n    \\n3. **Power User:**\\xa0This group of users is highly active on the platform and friending people all the time. They have a large number of friends and are highly engaged with the platform.\\n    \\n\\nIn this case, it is recommended to focus on the Moderate User group, as they have enough friends to provide valuable data and insights, but are not as active as Power Users, making it easier to analyze their behavior and provide meaningful recommendations.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='13f9c6a8-ee11-429a-9577-421d16c2d30e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n**Name some methods to deal with missing value imputation?**\\n\\nSome popular methods include:\\n\\n- Drop the missing values\\n- Imputation Using (Mean/Median) Values\\n- Imputation Using (Most Frequent) or (Zero/Constant) Values\\n- Imputation Using k-NN\\n\\n神经网络其实往往没有SVM，SVM没有树模型效果好；而且神经网络比树模型的参数多得多\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='80c7386a-190d-48ee-9ca5-517b3ccd53e8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**WHEN DO WE OPT FOR RESAMPLING?**\\n\\n**Tip:**\\xa0This is another critical\\xa0****Data Science interview question.\\n\\nResampling is preferred during any of the following situations:\\n\\n1. When we need to pre-estimate the accuracy of an algorithm by taking a small subset of data or draw conclusions from it.\\n2. During performance significance tests, if we need to substitute labels of the data.\\n3. For validation of different models on subsets of a larger data.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f20b2f2e-1ab6-4375-9fad-749ef592694e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**WHAT ARE YOUR ASSUMPTIONS FOR LINEAR REGRESSION?**\\n\\nThis is one of the fundamental Data Science interview questions.\\n\\nI have three major assumptions.\\n\\n1. The data should have a linear relationship, which is pretty obvious because it’s linear regression and you cannot apply it on non-linear data just as is.\\n\\n2. There should be multivariate normality, and the residuals should be normally distributed.\\n\\n3. There should not be an unequal variance in the data.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='430548f0-6203-48f6-8b6f-d1a41104f5ba', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**HOW TO PREVENT OVERFITTING?**\\n\\n**Tip:**\\xa0This is one of the best Data Science interview questions for freshers\\xa0****that covers\\xa0****theoretical concepts**.**\\n\\nThe first thing we can do is to get more data. If that is not possible, we can opt for several techniques like cross-validation, **early stopping, ensembling, regularisation**, etc.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bf3e5625-4f5a-4a46-a960-fb1e6089370b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**What is the difference between supervised and unsupervised machine learning?**\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7a08f3f4-896e-4e33-b984-2c9c5af15221', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Supervised Machine learning:**\\n\\nSupervised machine learning requires training labeled data.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f5a23c69-a036-4d45-80bf-e216dc319bc9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Unsupervised Machine learning:**\\n\\nUnsupervised machine learning doesn’t required labeled data.\\n\\nUnlike supervised learning, unsupervised learning is used to draw inferences and find patterns from input data without references to labeled outcomes. A common use of unsupervised learning is grouping customers by purchasing behavior to find target markets.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7c149fd9-acdf-43db-a727-f9c81c0ea02e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**What is bias, variance trade off ?**\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cc2972a6-74a5-4320-86e7-7754eeb9f183', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Bias:**\\n\\n“Bias is error introduced in your model **due to over simplification** of machine learning algorithm.” It can lead to underfitting. When you train your model at that time model makes simplified assumptions to make the target function easier to understand.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9ab5fdde-3a65-4209-9094-c8ad437aad74', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Low bias machine learning algorithms - Decision Trees, k-NN and SVM**\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0824e07d-ffb3-4c7d-b2f1-1db9f5d356cf', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Hight bias machine learning algorithms - Liear Regression, Logistic Regression**\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3b7ded92-4088-47c5-8016-3ccdc55ab1f3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Variance:**\\n\\n“Variance is error introduced in your model **due to complex machine learning algorithm, your model learns noise** also from the training dataset and performs bad on test dataset.” It can lead high sensitivity and overfitting.\\n\\nNormally, as you increase the complexity of your model, you will see a reduction in error due to lower bias in the model. However, this only happens till a particular point. As you continue to make your model more complex, you end up over-fitting your model and hence your model will start suffering from high variance.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d5896875-d437-4fc5-a8f0-faec65c3f5f5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Bias, Variance trade off:**\\n\\nThe goal of any supervised machine learning algorithm is to have low bias and low variance to achive good prediction performance.\\n\\n1. The k-nearest neighbors algorithm has low bias and high variance, but the trade-off can be changed by increasing the value of k which increases the number of neighbors that contribute to the prediction and in turn increases the bias of the model.\\n2. The support vector machine algorithm has low bias and high variance, but the trade-off can be changed by increasing the C parameter that influences the number of violations of the margin allowed in the training data which increases the bias but decreases the variance.\\n\\nThere is no escaping the relationship between bias and variance in machine learning.\\n\\nIncreasing the bias will decrease the variance. Increasing the variance will decrease the bias.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d14f974f-4428-4bda-bc4a-58257a0cdcea', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Explain Decision Tree algorithm in detail.**\\n\\nDecision tree is a supervised machine learning algorithm mainly used for the\\xa0**Regression and Classification**.It **breaks down a dataset into smaller and smaller subsets** while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e660c414-d720-4796-9e15-69470511c18b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Why is dimension reduction important? 从储存，分析和visualisation三个方面看**\\n\\nDimensionality reduction is the process of reducing the number of features in a dataset. This is important mainly in the case when you want to reduce variance in your model (overfitting).\\n\\nWikipedia states four advantages of dimensionality reduction (see here):\\n\\n1. *It reduces the time and storage space required*\\n2. *Removal of multi-collinearity improves the interpretation of the parameters of the machine learning model*\\n3. *It becomes easier to visualize the data when reduced to very low dimensions such as 2D or 3D*\\n4. *It avoids the curse of dimensionality; overfiting problem*\\n\\nDimensionally cursed phenomena occur in domains such as numerical analysis, sampling, combinatorics, machine learning, data mining and databases. **The common theme of these problems is that when the dimensionality increases, the volume of the space increases so fast that the available data become sparse.** This sparsity is problematic for any method that requires statistical significance. In order to obtain a statistically sound and reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality. Also, organizing and searching data often relies on detecting areas where objects form groups with similar properties; in high dimensional data, however, all objects appear to be sparse and dissimilar in many ways, which prevents common data organization strategies from being efficient.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='24874a5c-ec17-464d-9e48-169fdaaeb8c9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**What is Entropy and Information gain in Decision tree algorithm ?**\\n\\nThe core algorithm for building decision tree is called\\xa0**ID3**.\\xa0**ID3**\\xa0uses\\xa0**Enteropy**\\xa0and\\xa0**Information Gain**\\xa0to construct a decision tree.\\n\\n**Entropy**\\n\\nA decision tree is built top-down from a root node and involve partitioning of data into homogenious subsets.\\xa0**ID3**\\xa0uses enteropy to check the **homogeneity of a sample**. If the sample is completely homogenious then entropy is zero and if the sample is an equally divided it has entropy of one.\\n\\n**Information Gain**\\n\\nThe\\xa0**Information Gain**\\xa0is based on the decrease in entropy after a dataset is split on an attribute. Constructing a decision tree is all about finding attributes that returns the highest information gain.\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d39fc383-ff77-40c9-9775-ec8d26da24b3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**What is pruning in Decision Tree ?**\\n\\nWhen we remove sub-nodes of a decision node, this procsss is called pruning or opposite process of splitting.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='180c143b-2f44-4bd8-9c1f-2815a74e326d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**What is Ensemble Learning ?**\\n\\nEnsemble is the art of combining diverse set of learners(Individual models) togather to improvise on the stability and predictive power of the model. Ensemble learning has many types but two more popular ensemble learning techniques are mentioned below.\\n\\n**Bagging**\\n\\nBagging tries to implement **similar learners on small sample populations and then takes a mean of all the predictions. In generalized bagging, you can use different learners on different population. As you expect this helps us to reduce the variance error**.\\n\\n**Boosting**\\n\\n**Boosting is an iterative technique which adjust the weight of an observation based on the last classification. If an observation was classified incorrectly, it tries to increase the weight of this observation and vice versa. Boosting in general decreases the bias error and builds strong predictive models. However, they may overfit on the training data.**\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='549e4e57-2861-4922-8e48-e03ff2c97b9e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**What is Random Forest? How does it work ?**\\n\\nRandom forest is a versatile machine learning method capable of performing both regression and classification tasks. It is also used for dimentionality reduction, treats missing values, outlier values. **It is a type of ensemble learning method, where a group of weak models combine to form a powerful model**.\\n\\nIn Random Forest, we grow multiple trees as opposed to a single tree. To classify a new object based on attributes, each tree gives a classification. The forest chooses the classification having the\\xa0**most votes**(Over all the trees in the forest) and in case of regression, it takes the\\xa0**average**\\xa0of outputs by different trees.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3cdfe964-18e0-4680-a122-913bca1b9e04', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**What is a Box Cox Transformation?**\\n\\nDependent variable for a regression analysis might not satisfy one or more assumptions of an ordinary least squares regression. The residuals could either curve as the prediction increases or follow skewed distribution. In such scenarios, it is necessary to transform the response variable so that the data meets the required assumptions. A Box cox transformation is a statistical technique to transform non-normal dependent variables into a normal shape. If the given data is not normal then most of the statistical techniques assume normality. Applying a box cox transformation means that you can run a broader number of tests.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c082d159-813b-4799-9e32-80ab9d5f1f6d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**How will you define the number of clusters in a clustering algorithm?**\\n\\nThough the Clustering Algorithm is not specified, this question will mostly be asked in reference to K-**Means clustering** where “K” defines the number of clusters. For example, the following image shows three different groups.\\n\\n!https://i.imgur.com/qgRH8Rm.jpg\\n\\nWithin Sum of squares is generally used to explain the homogeneity within a cluster. If you plot WSS for a range of number of clusters, you will get the plot shown below. The Graph is generally known as Elbow Curve.\\n\\nk-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. **k-means clustering minimizes within-cluster variances (squared Euclidean distances**), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='552545d2-578e-42f8-84f2-f711dcd93db6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**What is deep learning?**\\n\\n deep learning is just an extention of Neural networks. In neural nets we consider small number of hidden layers but when it comes to deep learning algorithms we consider a huge number of hidden latyers to better understand the input output relationship.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a53b6a22-2162-409e-8f02-53a8d0b3660a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**What are Recurrent Neural Networks(RNNs) ?**\\n\\nRecurrent nets are type of artifical neural networks designed to recognize pattern from the sequence of data such as Time series, stock market and goverment agencis etc. To understand recurrent nets, first you have to understand the basics of feedforward nets. Both these networks RNN and feedforward named after the way they channel information throgh a series of mathematical oprations performed at the nodes of the network. One feeds information throgh straight(never touching same node twice), while the other cycles it throgh loop, and the latter are called recurrent.\\n\\nRecurrent networks on the other hand, take as their input not just the current input example they see, but also the what they have percieved previously in time. The BTSXPE at the bottom of the drawing represents the input example in the current moment, and CONTEXT UNIT represents the output of the previous moment. The decision a recurrent neural network reached at time t-1 affects the decision that it will reach one moment later at time t. So recurrent networks have two sources of input, the present and the recent past, which combine to determine how they respond to new data, much as we do in life.\\n\\nThe error they generate will return via backpropagation and be used to adjust their weights until error can’t go any lower. Remember, the purpose of recurrent nets is to accurately classify sequential input. We rely on the backpropagation of error and gradient descent to do so.\\n\\nBackpropagation in feedforward networks moves backward from the final error through the outputs, weights and inputs of each hidden layer, assigning those weights responsibility for a portion of the error by calculating their partial derivatives – ∂E/∂w, or the relationship between their rates of change. Those derivatives are then used by our learning rule, gradient descent, to adjust the weights up or down, whichever direction decreases error.\\n\\nRecurrent networks rely on an extension of backpropagation called backpropagation through time, or BPTT. Time, in this case, is simply expressed by a well-defined, ordered series of calculations linking one time step to the next, which is all backpropagation needs to work.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a3fc4956-6965-4524-aa81-41257fdfa82f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**What is reinforcement learning ?**\\n\\n**Reinforcement learning**\\n\\n!https://i.imgur.com/w6tZJDg.png\\n\\nReinforcement Learning is learning what to do and how to map situations to actions. The end result is to maximize the numerical reward signal. The learner is not told which action to take, but instead must discover which action will yield the maximum reward.Reinforcement learning is inspired by the learning of human beings, it is based on the reward/panelity mechanism.\\n\\n**Regularization**\\n\\nR**egularization is the process of adding tunning parameter to a model to induce smoothness in order to prevent overfitting.** This is most often done by adding a constant multiple to an existing weight vector. This constant is often the L1(Lasso) or L2(ridge). The model predictions should then minimize the loss function calculated on the regularized training set.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='db887bc6-077e-40d3-a17d-6ded79f29d74', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Describe different regularization methods, such as L1 and L2 regularization?**\\n\\n!https://miro.medium.com/max/60/0*rFu6CXcUdMFIKQ1I.png?q=20\\n\\n!https://miro.medium.com/max/1340/0*rFu6CXcUdMFIKQ1I.png\\n\\nBoth L1 and L2 regularization are methods used to reduce the overfitting of training data. Least Squares minimizes the sum of the squared residuals, which can result in low bias but high variance.\\n\\nL2 Regularization, also called ridge regression, minimizes the sum of the squared residuals\\xa0**plus lambda times the slope squared**. This additional term is called the\\xa0**Ridge Regression Penalty**. This increases the bias of the model, making the fit worse on the training data, but also decreases the variance.\\n\\nIf you take the ridge regression penalty and replace it with the\\xa0**absolute**\\xa0value of the slope, then you get Lasso regression or L1 regularization.\\n\\nL2 is less robust but has a stable solution and always one solution. L1 is more robust but has an unstable solution and can possibly have multiple solutions.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3982d2f0-bee1-454f-924e-23b581583b5d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**What is TF/IDF vectorization ?**\\n\\ntf–idf is short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval and text mining. **The tf-idf value increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general.**\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7c71d31c-b1e0-491a-9e55-260e77639b38', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**What are some of the steps for data wrangling and data cleaning before applying machine learning algorithms?**\\n\\nThere are many\\xa0steps that can be taken when data wrangling and data cleaning. Some of the most common steps are listed below:\\n\\n- **Data profiling:**\\xa0Almost everyone starts off by getting an understanding of their dataset. More specifically, you can look at the shape of the dataset with .shape and a description of your numerical variables with .describe().\\n- **Data visualizations:**\\xa0Sometimes, it’s useful to visualize your data with histograms, boxplots, and scatterplots to better understand the relationships between variables and also to identify potential outliers.\\n- **Syntax error**: This includes making sure there’s no white space, making sure letter casing is consistent, and checking for typos. You can check for typos by using .unique() or by using bar graphs.\\n- **Standardization or normalization**: Depending on the dataset your working with and the machine learning method you decide to use, it may be useful to standardize or normalize your data so that different scales of different variables don’t negatively impact the performance of your model.\\n- **Handling null values:**\\xa0There are a number of ways to handle null values including deleting rows with null values altogether, replacing null values with the mean/median/mode, replacing null values with a new category (eg. unknown), predicting the values, or using machine learning models that can deal with null values.\\xa0*Read more\\xa0here.*\\n- **Other things include:**\\xa0removing irrelevant data, removing duplicates, and type conversion.\\n\\n> \\n> \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='766033cc-f916-4dc0-a85f-42ab640ef344', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**How to deal with unbalanced binary classification?**\\n\\nThere are a number of ways to handle unbalanced binary classification (assuming that you want to identify the minority class):\\n\\n- First, you want to reconsider the\\xa0**metrics**\\xa0that you’d use to evaluate your model. The accuracy of your model might not be the best metric to look at because and I’ll use an example to explain why. Let’s say 99 bank withdrawals were not fraudulent and 1 withdrawal was. If your model simply classified every instance as “not fraudulent”, it would have an accuracy of 99%! Therefore, you may want to consider using metrics like precision and recall.\\n- Another method to improve unbalanced binary classification is by\\xa0**increasing the cost of misclassifying**\\xa0the minority class. By increasing the penalty of such, the model should classify the minority class more accurately.\\n- Lastly, you can improve the balance of classes by\\xa0**oversampling**\\xa0the minority class or by\\xa0**undersampling**\\xa0the majority class. You can read more about it\\xa0here.\\n\\n**What is the difference between a box plot and a histogram?**\\n\\nHistograms are bar charts that show the frequency of a numerical variable’s values and are used to approximate the probability distribution of the given variable. It allows you to quickly understand the shape of the distribution, the variation, and potential outliers.\\n\\nBoxplots communicate different aspects of the distribution of data. While you can’t see the shape of the distribution through a box plot, you can gather other information like the quartiles, the range, and outliers\\n\\n!Untitled\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='031e6607-58bf-414a-8a3e-daef5156cf3f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**How to define/select metrics?**\\n\\nThere isn’t a one-size-fits-all metric. The metric(s) chosen to evaluate a machine learning model depends on various factors:\\n\\n- Is it a regression or classification task?\\n- What is the business objective? Eg. precision vs recall\\n- What is the distribution of the target variable?\\n\\nThere are a number of metrics that can be used, including adjusted r-squared, MAE, MSE, accuracy, recall, precision, f1 score, and the list goes on.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a43a3f2a-02ae-4e34-ad9e-9451c800cb7a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aa2dd4f5-44dd-4c3c-9736-e537c7d5f0f0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Why is Naive Bayes so bad? How would you improve a spam detection algorithm that uses naive Bayes?**\\n\\nOne major drawback of Naive Bayes is that it holds a strong assumption in that the features are assumed to be uncorrelated with one another, which typically is never the case.\\n\\nOne way to improve such an algorithm that uses Naive Bayes is by decorrelating the features so that the assumption holds true.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a5f4cada-6a38-4beb-9b7a-e2285f8bd9f5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**What are the drawbacks of a linear model?**\\n\\nThere are a couple of drawbacks of a linear model:\\n\\n- A linear **model holds some strong assumptions that may not be true in application. It assumes a linear relationship, multivariate normality, no or little multicollinearity, no auto-correlation, and homoscedasticity**\\n- A linear model can’t be used for discrete or binary outcomes.\\n- You can’t vary the model flexibility of a linear model.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e915b378-74ab-4c28-81b6-7c9aebaaea86', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Do you think 50 small decision trees are better than a large one? Why?**\\n\\nAnother way of asking this question is “Is a random forest a better model than a decision tree?” And the answer is yes because a random forest is an ensemble method that takes many weak decision trees to make a strong learner. Random forests are more accurate, more robust, and less prone to overfitting.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1182d1d0-de63-4de7-b8e5-1f64982330d6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**What are the assumptions required for linear regression? What if some of these assumptions are violated?**\\n\\nThe assumptions are as follows:\\n\\n1. The sample data used to fit the model is\\xa0**representative of the population**\\n2. The relationship between X and the mean of Y is\\xa0**linear**\\n3. The variance of the residual is the same for any value of X\\xa0**(homoscedasticity)**\\n4. Observations are\\xa0**independent**\\xa0of each other\\n5. For any value of X, Y is\\xa0**normally distributed**.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='929a7a64-cded-46b2-b1f3-1bb3f02dbf12', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**What is collinearity and what to do with it? How to remove multicollinearity?**\\n\\nMulticollinearity exists when an independent variable is highly correlated with another independent variable in a multiple regression equation. This can be problematic because it undermines the statistical significance of an independent variable.\\n\\n**You could use the Variance Inflation Factors (VIF) to determine if there is any multicollinearity between independent variables — a standard benchmark is that if the VIF is greater than 5 then multicollinearity exists.**\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c9089bdb-1da5-4cb4-a7f6-8c983eda0a40', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**What is a kernel? Explain the kernel trick**\\n\\nA kernel is a way of computing the dot product of two vectors\\xa0**𝐱**x and\\xa0**𝐲**y in some (possibly very high dimensional) feature space, which is why kernel functions are sometimes called “generalized dot product” [2]\\n\\nThe kernel trick is a method of using a linear classifier to solve a non-linear problem by transforming linearly inseparable data to linearly separable ones in a higher dimension.\\n\\n!https://miro.medium.com/max/1844/0*Fd6f7q0Vzh5O2Fk3.png\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ea7f3ecc-c1f5-4eab-933f-aee48f04fd94', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Difference between convex and non-convex cost function; what does it mean when a cost function is non-convex?**\\n\\n!https://miro.medium.com/max/60/1*OFdD62ot2WdkY4ubrTpMPw.png?q=20\\n\\n!https://miro.medium.com/max/873/1*OFdD62ot2WdkY4ubrTpMPw.png\\n\\nTaken from Cho-Jui Hsieh, UCLA\\n\\nA\\xa0**convex function**\\xa0is one where a line drawn between any two points on the graph lies on or above the graph. It has one minimum.\\n\\nA\\xa0**non-convex function**\\xa0is one where a line drawn between any two points on the graph may intersect other points on the graph. It characterized as “wavy”.\\n\\nWhen a cost function is non-convex, it means that there’s a likelihood that the function may find local minima instead of the global minimum, which is typically undesired in machine learning models from an optimization perspective.\\n\\nA\\xa0**long-tailed distribution**\\xa0is a type of heavy-tailed distribution that has a tail (or tails) that drop off gradually and asymptotically.\\n\\n3 practical examples include the power law, the Pareto principle (more commonly known as the 80–20 rule), and product sales (i.e. best selling products vs others).\\n\\nIt’s important to be mindful of long-tailed distributions in classification and regression problems because the least frequently occurring values make up the majority of the population. This can ultimately change the way that you deal with outliers, and it also conflicts with some machine learning techniques with the assumption that the data is normally distributed.\\n\\n**What is the Central Limit Theorem? Explain it. Why is it important?**\\n\\nStatistics How To provides the best definition of CLT, which is:\\n\\n> “The central limit theorem states that the **sampling distribution of the sample mean approaches** a normal distribution as the sample size gets larger no matter what the shape of the population distribution.” [1]\\n> \\n\\nThe central limit theorem is important because it is used in hypothesis testing and also to calculate confidence intervals.\\n\\n1）样本平均值约等于总体平均值。\\n\\n2）不管总体是什么分布，任意一个样本平均值都会围绕在总体平均值周围，并且呈正态分布。\\n\\n这就是中心极限定理，就是这么2句话\\n\\nTypes of selection bias include:\\n\\n- **sampling bias**: a biased sample caused by non-random sampling\\n- **time interval**: selecting a specific time frame that supports the desired conclusion. e.g. conducting a sales analysis near Christmas.\\n- **exposure**: includes clinical susceptibility bias, protopathic bias, indication bias.\\xa0*Read more\\xa0here.*\\n- **data**: includes cherry-picking, suppressing evidence, and the fallacy of incomplete evidence.\\n- **attrition**: attrition bias is similar to survivorship bias, where only those that ‘survived’ a long process are included in an analysis, or failure bias, where those that ‘failed’ are only included\\n- **observer selection**: related to the Anthropic principle, which is a philosophical consideration that any data we collect about the universe is filtered by the fact that, in order for it to be observable, it must be compatible with the conscious and sapient life that observes it. [3]\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='92413fa7-9f79-40dd-b908-112cb9929717', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Is mean imputation of missing data acceptable practice? Why or why not?**\\n\\n**Mean imputation**\\xa0is the practice of replacing null values in a data set with the mean of the data.\\n\\nMean imputation is generally bad practice because it doesn’t take into account feature correlation. For example, imagine we have a table showing age and fitness score and imagine that an eighty-year-old has a missing fitness score. If we took the average fitness score from an age range of 15 to 80, then the eighty-year-old will appear to have a much higher fitness score that he actually should.\\n\\nSecond, mean imputation reduces the variance of the data and increases bias in our data. This leads to a less accurate model and a narrower confidence interval due to a smaller variance.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c424dbc5-f253-4554-b5c1-460873ce729b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**What is an outlier? Explain how you might screen for outliers and what would you do if you found them in your dataset. Also, explain what an inlier is and how you might screen for them and what would you do if you found them in your dataset.**\\n\\nAn\\xa0**outlier**\\xa0is a data point that differs significantly from other observations.\\n\\nDepending on the cause of the outlier, they can be bad from a machine learning perspective because they can worsen the accuracy of a model. If the outlier is caused by a measurement error, it’s important to remove them from the dataset. There are a couple of ways to identify outliers:\\n\\n**Z-score/standard deviations:**\\xa0if we know that 99.7% of data in a data set lie within three standard deviations, then we can calculate the size of one standard deviation, multiply it by 3, and identify the data points that are outside of this range. Likewise, we can calculate the z-score of a given point, and if it’s equal to +/- 3, then it’s an outlier.Note: that there are a few contingencies that need to be considered when using this method; the data must be normally distributed, this is\\xa0not applicable for small data sets, and the presence of too many outliers can throw off z-score.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a2a33870-2c03-49aa-9cb6-b099cfadc93f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**How do you handle missing data? What imputation techniques do you recommend?**\\n\\nThere are several ways to handle missing data:\\n\\n- Delete rows with missing data\\n- Mean/Median/Mode imputation\\n- Assigning a unique value\\n- Predicting the missing values\\n- Using an algorithm which supports missing values, like random forests\\n\\nThe best method is to delete rows with missing data as it ensures that no bias or variance is added or removed, and ultimately results in a robust and accurate model. However, this is only recommended if there’s a lot of data to start with and the percentage of missing values is low.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5d3f6a25-2661-4c06-b535-919adfb7776a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Machine Learning Interview Questions**\\n\\n- *(Machine Learning) How do to find thresholds for a classifier?*\\n- *(Machine Learning) What’s the difference between logistic regression and support vector machines? What\\'s an example of a situation where you would use one over the other?*\\n- *(Machine Learning) Explain ICA and CCA. How do you get a CCA objective function from PCA?*\\n- *(Machine Learning) What is the relationship between PCA with a polynomial kernel and a single layer autoencoder? What if it is a deep autoencoder?*\\n- *(Machine Learning) What is \"random\" in random forest? If you use logistic regression instead of a decision tree in random forest, how will your results change?*\\n- *(Modeling) What is the interpretation of an ROC area under the curve as an integral?*\\n\\n\\'Random\\' in Random Forest refers to mainly two processes –\\xa0**Random observations to grow each tree**\\n. Random variables selected for splitting at each node.\\n\\nSVM tries to finds the “best” margin (distance between the line and the support vectors) that separates the classes and this reduces the risk of error on the data, while logistic regression does not, instead it can have different decision boundaries with different weights that are near the optimal point.\\n\\n- SVM works well with unstructured and semi-structured data like text and images while logistic regression works with already identified independent variables.\\n- SVM is based on geometrical properties of the data while logistic regression is based on statistical approaches.\\n- The risk of overfitting is less in SVM, while Logistic regression is vulnerable to overfitting.\\n\\nDepending on the number of training sets (data)/features that you have, you can choose to use either logistic regression or support vector machine.\\n\\nLets take these as an example where :*n = number of features,m = number of training examples*\\n\\n1. If\\xa0*n is large (1–10,000) and m is small (10–1000)*\\xa0: use logistic regression or SVM with a linear kernel.\\n\\n2. If\\xa0*n is small (1–10 00) and m is intermediate (10–10,000*) : use SVM with (Gaussian, polynomial etc) kernel\\n\\n3. If\\xa0*n is small (1–10 00), m is large*\\xa0(50,000–1,000,000+): first, manually add more features and then use logistic regression or SVM with a linear kernel\\n\\nGenerally, it is usually advisable to first try to use logistic regression to see how the model does, if it fails then you can try using SVM without a kernel (is otherwise known as SVM with a linear kernel). Logistic regression and SVM with a linear kernel have similar performance but depending on your features, one may be more efficient than the other.\\n\\nLinear SVMs and logistic regression generally perform comparably in practice. Use SVM with a nonlinear kernel if you have reason to believe your data won\\'t be linearly separable (or you need to be more robust to outliers than LR will normally tolerate). Otherwise, just try logistic regression first and see how you do with that simpler model. If logistic regression fails you, try an SVM with a non-linear kernel like a RBF.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a79fc7a1-c8fa-46e5-82e5-636c396ed910', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**a/b testin 常见错误**\\n\\nAB testing is essentially an experiment where two or more variants of a page are shown to users at random\\n\\nNot running tests for full weeks; 因为有seasonlity的影响\\n\\ntesting too many stuff; \\n\\n一些假设没得到满足；比如t test;', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='467b5d62-7143-4846-b2c8-1baacddfa14e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nBefore jumping in the visualizing steps, I asked the interviewer who are this dashboard for. The dashboard is for executive-level managers. To me, that means I need to show the highest-level metrics so that the managers know what is going on by looking at this snapshot. Then, I brainstormed the metrics to show. I came up with three to four metrics.\\n\\nI didn’t only build the charts and graphs. For each metric, I explained the rationale behind using this type of visualization and not the other. Talking out loud the process not only helps show the thought process but also makes me realize that some other types of charts express my metric better compared with the chart that I came up within the first place. Also, charts and graphs aren’t only pie charts, bar graphs, or line graphs. You can use typography, which means showing the pure key numbers in this case. The dashboard users can get the important figures immediately without having to find from the charts or graphs.\\n\\nA dashboard is not enough without the filters. I spent time thinking about appropriate filters, drawing those, and explaining to the interviewer. Some important filters include date range, locations/ markets, and time-frequency (where users can select a daily, weekly, or monthly view).\\n\\nAfter all, I summarized the key points about my dashboard, **including the target audience, important metrics, types of visualizations, and filters.** I also mentioned a potential way to further develop the dashboard in the future if have more variables in the data set or more stakeholders involved.\\n\\n1. **How can we visualize more than three dimensions of data in a single chart?**\\n\\nTo visualize data beyond three dimensions, we need to use visual cues such as\\xa0***color, size, and shape.***\\n\\n- Color is used to depict both\\xa0***continuous and categorical data***.\\n- Marker Size is used to represent\\xa0***continuous data***. Can be used for categorical data as well. However, since size differences are difficult to detect, it is not considered the most appropriate choice for categorical data.\\n- Shapes are used to represent\\xa0***different classes***.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6ac80c22-a418-4cbf-a823-bbfab82c9dc4', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**LAY OUT THE DIFFERENCES BETWEEN A HISTOGRAM AND A BAR GRAPH?**\\n\\n**Tip:**\\xa0This is one of the visualizations questions asked in Data Science interviews.\\n\\nThe bar graph is used for discrete data while the histogram is used for continuous data. In a bar graph, there is a space between the bars, while there is no space between bars in a histogram. That’s because the histogram is a continuous scale. Also, the order of the bars can be changed and sorted according to the requirements of the situation in a bar graph. In contrast, this is not a possibility in the histogram.\\n\\n1. **What is a Boxplot?**\\n\\nA Box and Whisker Plot (or Boxplot) are used to represent data distribution through their quartiles.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bea64432-be39-435c-8256-6a3530f9e614', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Boxplots vs Bar plots – points to remember:**\\n\\n- **Histograms**\\xa0are the best way to see the spread of your data.\\n- **Boxplots**\\xa0are the next best way.\\n- **Bar plots**\\xa0are the worst way. Don’t use them.\\n\\n!Untitled\\n\\n!Untitled\\n\\nI received a preview of the data sets. I had to build an imaginary dashboard using Google Drawings. You can draw charts in Google Drawings using a similar approach when drawing charts in Google Sheets.\\n\\nBefore jumping in the visualizing steps, I asked the interviewer who are this dashboard for. The dashboard is for executive-level managers. To me, that means I need to show the highest-level metrics so that the managers know what is going on by looking at this snapshot. Then, I brainstormed the metrics to show. I came up with three to four metrics.\\n\\nI didn’t only build the charts and graphs. For each metric, I explained the rationale behind using this type of visualization and not the other. Talking out loud the process not only helps show the thought process but also makes me realize that some other types of charts express my metric better compared with the chart that I came up within the first place. Also, charts and graphs aren’t only pie charts, bar graphs, or line graphs. You can use typography, which means showing the pure key numbers in this case. The dashboard users can get the important figures immediately without having to find from the charts or graphs.\\n\\nA dashboard is not enough without the filters. I spent time thinking about appropriate filters, drawing those, and explaining to the interviewer. Some important filters include date range, locations/ markets, and time-frequency (where users can select a daily, weekly, or monthly view).\\n\\nGuidelines on improving human perception include:\\n\\n- position data along a common scale\\n- bars are more effective than circles or squares in communicating size\\n- color is more discernible than shape in scatterplots\\n- avoid pie chart unless it is for showing proportions\\n- avoid 3D charts and reduce chartjunk\\n- Sunburst visualization is more effective for hierarchical plots\\n- use small multiples (even though animation looks cool, it is less effective for understanding changing data.)\\n\\nOne can answer this question in multiple ways: from technical points to mentioning key aspects, but be sure to remember saying these points:\\n\\n- Data positioning\\n- Bars over circle and squares\\n- Use of colour theory\\n- Reducing chart junk by avoiding 3D charts and eliminating the use of pie charts to show proportions\\n- And why sunburst visualisation is more effective for hierarchical plots.\\n\\n**How can you visualise more than three dimensions in a single chart?**\\n\\nUsually, the data is represented in the charts using height, width and depth in the images, to visualise more than three dimensions we make use of visual cues like colour, size and shape or sometimes animations for depicting changes through time.\\n\\n**Summary of Recommendations to improve data quality in Big Data projects:**\\n\\n- **Identify and prioritize the business use cases**\\xa0(then, use them to define data quality metrics, measurement methodology, improvement goals, etc.)\\n- Based on a strong understanding of the business use cases and the Big Data architecture implemented to achieve them,\\xa0**design and implement an optimal layer of data governance**(data definitions, metadata requirements, data ownership, data flow diagrams, etc.)\\n- **Document baseline quality levels for key data**\\xa0(think of “critical-path” diagram and “throughput-bottleneck” assessment)\\n- **Define ROI for data quality efforts**\\xa0(in order to create feedback loop on the ROI metric to improve efficiency and to sustain funding for data quality efforts)\\n- **Integrate data quality efforts**\\xa0(to achieve efficiency through minimizing redundancy)\\n- **Automate data quality monitoring**\\xa0(to reduce cost as well as to let employees stay focused on complex tasks)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1f517b87-578b-43fa-bb2f-f757c4b3b985', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n  \\n\\nfalse positive: 一个人对体育不感兴趣，但你给他推了；false negative: 一个人对体育感兴趣，但你没给她推\\n\\n  \\n\\n  \\n\\n算法模型评测指标\\n\\n**准确率**\\n\\n**推荐列表里，多少比例的文章，是用户读过的；判断用户对这么多话题感兴趣，哪些是真感兴趣的**\\n\\n**召回率**\\n\\n**推荐列表中，用户读过的文章，占用户阅读记录的比例；**\\n\\n覆盖率\\n\\n推荐列表里的文章，占文章库总数的比例\\n\\n  \\n\\n**Accuracy**\\xa0- Accuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations. One may think that, if we have high accuracy then our model is best. Yes, accuracy is a great measure but only when you have symmetric datasets where values of false positive and false negatives are almost same. Therefore, you have to look at other parameters to evaluate the performance of your model. For our model, we have got 0.803 which means our model is approx. 80% accurate.\\n\\n  \\n\\n  \\n\\n  \\n\\n余军说的产品经理的四大任务：\\n\\n需求；生产；销售；协调\\n\\n  \\n\\n企业通过“产品”这个关键媒介，以创造“用户价值” 的方式，有选择地和用户进行价值交换\\n\\n  \\n\\n用户不是自然人，而是需求的集合\\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\n现在想来，你的项目哪里做的不好？怎么改进？\\n\\n我们公司的产品你有了解吗？你觉得体验怎么样？\\n\\n为我们公司的产品提点建议。\\n\\n你的未来规划是怎么样的？\\n\\n你为什么要做产品经理？\\n\\n你觉得产品经理最重要的三个能力是什么？\\n\\n  \\n\\n你最欣赏的产品是什么？为什么喜欢\\n\\n你怎么看网易云音乐这款产品，分析一下\\n\\n如果让你做滴滴打车这款产品，你会怎么做？\\n\\n  \\n\\n  \\n\\n- BRD – 商业需求说明书\\n- MRD – 市场需求说明书\\n- PRD – 产品需求说明书\\n\\n  \\n\\n  \\n\\n我们的产品为**________**情况下的**________**用户提供**________**功能，解决他们的**________**需求，和**________**不同的是，用户在我们这里会得到更好的**________**。你把正在研究的产品套用到这个模板上，到最后可能就会有所提升\\n\\n  \\n\\n  \\n\\n  \\n\\n**#1** 电梯时间：在 **140** 个字内讲清楚你要做的产品愿景。\\n\\n  \\n\\n// 经常在媒体上看到，某创业者跟顶级 VC 共乘电梯就搞定投资，这里叫「电梯时间」也是类似道理，好产品应该能在 140 个字（一条 tweet）以内说清楚。\\n\\n  \\n\\n**#2** 是什么样的市场时机 **(opportunities)** 及其趋势，决定了当前的产品战略 **(strategy)**？不超过 **3** 条。\\n\\n  \\n\\n// 独特的时机和历史背景，尤其是巨大的时代要素，往往催生出独特的产品模式，所谓「时势造英雄」。为什么是现在来做这个事情，不是更早，或者更晚？没有 Smartphone 的全民普及，滴滴打车的产品设想就难以落地；没有以滴滴和腾讯为代表的巨头强势推广移动支付，共享单车这个并不新奇的模式很难在这一年爆发。  \\n\\n**#3** 目标用户 **(target users)** 是谁，他们有什么特征 **(characteristics)**？\\n\\n// 我们经常说，先想清楚你的目标用户是谁，再做具体的产品设计。不仅需要知道他们是谁，还要知道他们有哪些特征，尤其是连他们自己都忽略的那些特征（需求挖掘的一部分）。这要求产品经理能够做长期系统的用户观察、用户研究和用户访谈，也是产品设计能够产生差异化和亮点的源泉。\\n\\n  \\n\\n**#4** 成功的机会，或者说目标市场规模 **(market size)** 有多大？  \\n\\n// 所谓「小池塘里养不出大鱼」，行业规模决定产品天花板，有机会诞生 10 亿美金以上公司的行业，才有可能引起 VC 的注意。注意这里说的是「目标市场」的规模，比如你是做职业教育的，那么这个市场规模大小需要有针对性地核算，幼儿早教、K12 应试、留学等也属于「互联网教育」范畴，但就算他们的市场规模再大，也跟你做职业教育没啥关系。\\n\\n  \\n\\n**#5** 目标用户的核心目标 **(key goals)** 是什么？\\n\\n  \\n\\n// 核心目标是指他们最终希望得到什么？低价且丰富的商品？便捷省心的到家服务？能够真正约得到人？用户的目标可能是多样多层次的，需要辨识其中哪些是「伪需求」。\\n\\n  \\n\\n**#6** 目标用户现在的普遍解决方案是什么？他们在现有普遍接受的解决方案中会遇到什么问题？你会在哪些方面做得比现有方案好？\\n\\n  \\n\\n// 产品经理的使命不是「创造」一个需求，而是洞察到那些尚未被很好满足，有机会通过新的产品或技术手段提升效率、改进体验的已经存在的需求。在汽车诞生之前，用户的需求是「一匹更快的马」，但人们的出行需求是持续存在的，只是交通工具和效率在不断进化之中，新技术和新产品会刺激用户的需求更大程度地释放，这就是「创新」带来的社会价值。这里借用俞军老师的公式：用户价值 = (新体验-旧体验) - 替换成本，详细解读参照这篇文章，里面有一些很好的例子。\\n\\n  \\n\\n**#7** 你的价值声明 **(value proposition)** 是什么？\\n\\n  \\n\\n// 为目标用户提供了哪些足以让他们购买的系列价值点。\\n\\n  \\n\\n**#8** 有谁在试图解决同样或相似问题**?** 优势 **(strength)** 劣势 **(weakness)** 各在哪里？\\n\\n  \\n\\n// 知已知彼，并且搞清楚应该把谁当真正的竞争对手，搞错了目标的公司也不少。在快速发展和变化的时代里，真正的竞争对手往往有可能是从另一个赛道上跑过来的，从用户价值端取代 Yahoo! 的并不是另一个更好的新闻网站。互联网缩短了了解、调查甚至监控竞争对手的距离，基本上产品助理都知道 SWOT 分析模型，这里就不赘述了。\\n\\n  \\n\\n**#9** 描述你的解决方案，包括用户可在产品中完成的核心任务。\\n\\n  \\n\\n// 前两天跟一个创过业的资深产品经理+设计师朋友聊天，他发现 80% 的创业公司，都不能够正确地将产品目标定义清晰，并分解到产品设计、开发执行并上线的全过程，深以为然。这里的解决方案和核心任务，就定义了这款产品的核心功能，也就是最小化可行产品 (Minimum Viable Product, MVP) 版本的边界，做什么不做什么。\\n\\n  \\n\\n可以做一下这道填空题，形成一个流畅的句子，把上面的一些要素串起来：我们的产品为**________**情况下的**________**用户提供**________**功能，解决他们的**________**需求，和**________**不同的是，用户在我们这里会得到更好的**________**。\\n\\n  \\n\\n这几个空格分别填的是：应用场景、目标用户、核心功能、价值主张、竞争对手、核心目标。\\n\\n  \\n\\n**#10** 这个产品中的关系链是怎样的，人们如何建立并维系他们的关系链？\\n\\n  \\n\\n// 这个问题主要是针对社交网络、内容社区型产品。关系链是决定 SNS 成败的关键，因此值得单独考虑。\\n\\n  \\n\\n**#11** 为什么你适合做这个事情？有没有谁更适合做这个事情？\\n\\n  \\n\\n// 抛开时势环境因素，公司创始人、大公司里的项目负责人的个人背景和能力边界，直接决定产品成功率。往往取得巨大成功的创始人，他的过往工作、学习、个人生活积累都为他日后的成功奠定了基础，包括但不限于高新技术、专利、绝对垄断的资源、人才、学习能力。当然这不绝对，牛逼的创始人碰上合适的事情，会有意想不到的结果。\\n\\n  \\n\\n**#12** 该产品有技术驱动力 **(technical motivations)** 吗？可以是特定的算法、技术想法等等。不超过 **3** 条。\\n\\n// 评估技术可行性，技术是否是这个项目的壁垒之一？今日头条这个产品就有技术驱动力，包括但不限于足够多的用户行为数据、不断自我进化的推荐系统、聚集度较高的一流人才等。当然搜索引擎，纯电动车，以及无人驾驶、语音识别、机器人等 AI 领域的项目，都存在较高的技术驱动力和技术门槛。而美团、滴滴出行早期就相对没有技术驱动力。\\n\\n  \\n\\n**#13** 目前有何技术和非技术限制？你将如何解决或绕过？\\n\\n  \\n\\n// 技术限制比较好理解，比如技术攻关难度很大，核心技术尚不成熟；非技术限制，比如要做一个 UGC 社区，早期要找到一定数量、能持续生产优质内容的种子用户，才能让这个社区逐步 run 起来，这也算是一种非技术限制。\\n\\n  \\n\\n**#14** 大致的人力投入将是多少？\\n\\n  \\n\\n// 自己创业需要搞清楚这个问题，决定你在 12-18 个月里需要储备多少现金。要在公司内部启动一个新项目，跟老板合理争取 headcount 的前提，也是对人力投入要有清晰的估算，这里可以邀请 Tech Lead 一起来做评估。\\n\\n  \\n\\n**#15** 你打算用什么样的策略来发布 **(rollout)** 此产品，并推向市场？\\n\\n  \\n\\n// 结合 #3 定义的目标用户，以及 #8 定义的解决方案和核心功能，来决定产品的发布和推广策略。时下最精彩的案例，就是在解决城市单车出行的问题中，摩拜单车的策略是在车的制造上投入了更多成本（含 GPS 功能），追求质量可靠性和硬件寿命，但前期牺牲了铺车的速度；而 ofo 在明知车没有定位会影响用户查找和运营效率的情况下，以相对低的成本铺到了一个很大的数量（甚至导致部分用户不需要根据 GPS 找车）。这两种选择就代表了不同的 rollout 策略。再比如知乎早期几年为何一直坚持使用邀请制？Musical.ly 为何一开始就面向欧美市场？陌陌为什么是先推出附近的人再推出群组？\\n\\n  \\n\\n**#16** 哪些指标能够反映该产品的成功程度？并做简单估算。\\n\\n  \\n\\n// 应该看哪些核心数据，与产品提供的核心方案直接相关。这里做的简单估算，可以跟后期产品实际数据表现进行对比，来检验自己的估算是否准确客观，并且可以跟同行打听，知道同类产品不同数据表现的 benchmark。关于这个问题，这里不展开，推荐两篇由 Pinterest 早期产品经理、投资人写的极好的文章，第一篇里讲述了「核心行为」的重要性，以及如何定义；第二篇的第 #1 点描述了「决定度量什么」很重要。\\n\\n  \\n\\n**#17** 规划中的产品路线图 **(roadmap)** 是怎样的？\\n\\n  \\n\\n// 如果可以，设想一下在 MVP 推出后，可能会碰到哪些问题需要调整，以及在朝着更远目标的进程中，还有哪些重大版本的规划？讲出这些有利于给团队成员信心\\n\\n  \\n\\n  \\n\\n  \\n\\n《启示录》这本书第 11 章，为了评估产品机会，产品经理应回答如下十个问题：\\n\\n  \\n\\n1.  \\n\\n- 产品要解决什么问题？（产品核心价值）  \\n    \\n- 为谁解决这个问题？（目标用户）  \\n    \\n- 成功的机会有多大？（市场规模）  \\n    \\n- 怎样判断产品成功与否？（度量指标和收益指标）\\n- 2.  \\n    \\n- 有哪些同类产品？（竞争格局）  \\n    \\n- 为什么我们最适合做这个产品？（竞争优势）  \\n    \\n- 时机合适吗？（市场时机）  \\n    \\n- 3.\\xa0\\n- 如何把产品推向市场？（营销组合策略）  \\n    \\n- 成功的必要条件是什么？（解决方案要满足条件）  \\n    \\n- 根据以上问题，给出评估结论（继续或放弃）  \\n    \\n\\n  \\n\\n  \\n\\n#####\\n\\n  \\n\\n  \\n\\n  \\n\\n\\xa0产品经理需要做什么样的工作\\n\\n参考解析：主要负责三部分\\n\\n2.1、产品规划（包含：用户调研、数据分析等需求分析手段）\\n\\n2.2 、产品设计（包含：竞品分析、产品脑图、产品流程图、产品原型图、产品说明文档）\\n\\n2.3、产品落地（包含：需求评审、需求排期、项目跟进、上线走查等项目管理）\\n\\n  \\n\\n  \\n\\n**4**、 你认为什么是一款成功的产品\\n\\n参考解析：\\n\\n1）从用户的角度：能持续不断的解决目标用户的需求；用户操作简单方便，在使用产品后能够快速的有效的解决问题。\\n\\n2）从产品的角度：该产品相较于其他竞品有着独特的优势，产品能够持续不断的迭代以满足目标用户的需求。\\n\\n3）从商业的角度：公司的技术层面可以实现该产品，并且可以长期维护，持续完善；该产品可以为公司创造长期的可持续价值，包括但不限于利润，盈利模式，市场价值，战略地位等\\n\\n  \\n\\n  \\n\\n**5**、 实习期间有没有和设计**/**研发争执过，怎么解决的？\\n\\n参考解析：\\n\\n具体问题具体分析，如果是UI或产品对产品需求本身存在疑问，可参考如下：\\n\\n1）摆数据：证明做这个需求的必要性及完成之后的收益\\n\\n2）讲道理：说明这个需求跟公司整体战略的一致性\\n\\n3）向上沟通：如果基础沟通不能达成一致，可将双方领导拉到一起讨论\\n\\n说一下你理解的完整的产品流程是什么样子的？\\n  \\n\\n（**4**）学习能力\\n\\n因为互联网瞬息万变，前年流行互联网金融，今年流行直播、共享单车，产品经理需要不断的去体验，去学习才能不落伍。同时产品经理需要懂营销，懂技术、懂运营、懂设计，这些知识只有不断地学习才能掌握，所以产品经理需要学习能力，我平时会下一些APP，看一些书来充实自己。（你如果说上面的那句话，面试者有可能问你看了那些书，体验了那些APP，如果你没怎么准备，没怎么体验，建议不要说那句话。）\\n\\n（**5**）项目管理能力\\n\\n因为一个产品需要按时保质保量的上线，需要产品经理有一定的项目管理和团队协作能力，协调各方资源，保证产品按时按质上线。我通过XX锻炼出这种能力。\\n\\n（**6**）自我管理能力\\n\\n好的产品经理都不是等待别人分配的，需要主动收集需求，并把它转化成产品需求，需要一个比较强的自我驱动能力，而不是等着别人分配工作，设计、技术的工作都是由产品经理发出的，我通过XX锻炼出这种能力。\\n\\n（**7**）抗压能力\\n\\n很多锅需要产品经理背，很多委屈需要产品经理去受。例如：产品上线没有达到预期效果，开评审会的时候你的需求被大家批，这些都需要产品经理有足够的抗压能力和心理调节能力，我因为有XX经历，所以锻炼出这种能力\\n\\n  \\n\\n  \\n\\n说一款产品的时候要从它的战略层、范围层、结构层、框架层、表现层进行说明（用户体验5要素），这样会显得你比较专业，别人即使听不懂也会觉得你逻辑清晰，给你增加印象分！\\n\\n**12**、有许多需求，你是如何划分优先级的？\\n\\n可以从两个纬度四个象限进行划分，一个是紧急程度，一个是重要程度。按照优先级划分为重要紧急、不重要紧急、重要不紧急、不重要不紧急。\\n\\n  \\n\\n  \\n\\n为什么喜欢这款APP\\n\\n下面就以微信读书这款APP，以实际例子结合刚刚提到的2种回答思路和逻辑进行分析和解答：\\n\\n第一种逻辑\\n\\n首先分析微信读书这款APP的场景和需求，可以拆解成看书前、看书中和看书后3个场景来进行有层次的回答。\\n\\n1. 看书前：发现好书、价格优势。\\n2. 看书中：良好的阅读体验。\\n3. 看书后：社交需求、自我实现需求、知识回顾知识沉淀。\\n\\n（**1**）看书：阅读体验较好\\n\\n- 种类较为丰富的书籍，基本上能够满足自己的看书需求，找到想看的书籍（投资、经济、互联网、历史小说）。\\n- 沉浸式的阅读体验，干净简洁的页面，写想法、划线简单便捷的操作体验。\\n\\n（**2**）发现好书：自己经常会面临书荒，不知道读哪些书\\n\\n- 个性化推荐：根据阅读历史精准推荐，每日更新。\\n- 推荐维度：看书行为（加入书架、购买、看完、评论写想法、分享推荐、打分高）书籍种类推荐、书籍作者、对这本书感兴趣的人也在读。\\n- 推荐方法：基于书籍的推荐、基于作者的推荐、基于同类标签用户的推荐。\\n- 运营推荐：发现页面的每周推荐、书城的广告推荐、分类推荐、榜单推荐、专题推荐、热门推荐。\\n- 好友推荐：身边的微信好友都在读哪些书籍，希望和好友有共同话题，发现好的感兴趣的书籍。\\n- 随机推荐：摇一摇随机个性推荐（摇一摇得红包，超出预期）。\\n\\n（**3**）社交需求：和好友的想法交流，思想碰撞\\n\\n- 想法交流圈：类似朋友圈，点赞、评论、转发好友的想法，也可以通过想法圈表达自己读书的感悟，获得其他好友的认可，精神层面获得满足。\\n- 除了和微信好友交流，还可以和看同一本书的书友交流想法，可以查阅其他书友对于书籍的想法，引起共鸣。\\n\\n（**4**）自我实现的需求\\n\\n- 好友阅读时长的排行榜，类似微信运动排行榜，充分的激发了我们的阅读兴趣。\\n- 想法、书评等分享到想法圈，可以获得好友的点赞和评论认可。\\n\\n（**5**）价格优势\\n\\n- 因为可以每周通过阅读时长兑换书币，极大的降低了阅读的成本。\\n- 另外还可以获得书币红包。\\n- 免费书籍，限时特价等。\\n- 定价上有较大的优势，最新推出无限卡：月卡、季度卡、年卡等，比较实惠。\\n\\n（**6**）知识回顾、知识沉淀的需求\\n\\n- 避免看完后没有记忆点，很快的忘记或者没有收获，可以快速的回顾。\\n- 很方便快速的找到本书的自己之前的笔记、划线、想法、书签等。\\n- 可以看到热门的划线和其他书友的想法，获取新的知识。\\n\\n第二种逻辑：用户体验要素五要素\\n\\n（1）表现层：视觉设计，表现风格\\n\\n- 视觉交互效果较好，操作简单。\\n- 页面干净简单，没有过多的弹窗、banner广告等花里胡哨的东西，适合沉浸式阅读。\\n\\n（2）框架层+结构层：页面设计，信息展示\\n\\n- 页面设计和信息展示层级上来说比较清晰，操作简单。\\n- 发现、书架、想法、我的层级清晰，功能划分。\\n- 视觉焦点和信息层级比较清晰，主次分明，符合浏览习惯，用户使用时也不会眼花缭乱。\\n\\n（3）范围层：功能列表\\n\\n- 功能上来说比较好，用户体验佳。\\n- 哪些功能解决了你的需求，参考上述第一种逻辑中，哪些功能是比较好的。\\n\\n（4）战略层：APP的战略发展方向、商业模式\\n\\n- 语音知识付费的趋势，微信读书提供的听书的体验。\\n- 供应链整合，出版服务，IP打造。\\n- 商业模式：销售电子书、出版服务、电台服务、IP的打造', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='80da2a10-5624-4eeb-98e2-cc4591081066', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n1. goal, scope\\n\\nexplainability, latency and scale; current state of the problem; training and inference speed\\n\\nuser perspective: mobile or not; global market or not\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='66bbfecd-223d-43b0-9d09-1fa82249cdb3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n2. metrics: online and offline\\n\\nif its a ranking problem, use ranking metrics like MAP\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7df472ca-7db1-4705-bdcb-c356b729907f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n3. data collection： 想象一列数据长什么样\\n\\nhandle unbalanced data; NA and bad features; dimension reduction\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='04d1630c-8e86-477e-93ee-6e5993e3cb2d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n4. model architecture: different stages\\n\\nfinal layer: law and culture filter; bias mitigator; cold starter mitigator; downsample popular videos or user\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cdc62d1c-51a8-4e09-aa9a-8b9dfaa40574', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n5. feature engineering start from high level categories\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cc7c8c38-da32-43a7-8aa6-a9edabdccd86', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n6. model training and validation: baseline model; split in time\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ef594d90-6cac-4096-8e3e-b282cfa2a543', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n7. a/b testing and deploy by regions; model retraining; keep checkpoint, run only 1 epoch with new data\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5bd6b709-1515-4ab7-b7c6-2b3fd4698521', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n8. feature/model storage; containrized; deploy as api or package;; CI/CD, unit testing\\n\\ndf['bins'] = pd.cut(x=df['number'], bins=[1, 20, 40, 60, 80, 100],\\nlabels=['1 to 20', '21 to 40', '41 to 60',\\n'61 to 80', '81 to 100'])\\n\\n#Selecting a specific row\\n\\ndata.iloc[1]\\n\\ndata.iloc[:,0] # first column of data frame (first_name)\\n\\ndata.iloc[:,1] # second column of data frame (last_name)\\n\\ndata.iloc[:,-1] # last column of data frame (id)\\n\\ndata.iloc[0:5] # first five rows of dataframe\\n\\ndata.iloc[:, 0:2] # first two columns of\\n\\ndata[-2:] #last 2\\n\\ndata[:-2] #except last 2\\n\\nnewdf = df.sort_values(by='age')\\n\\ndata.drop_duplicates(subset='k1')\\n\\ndf.query('A > B')\\n\\n#Sort list by frequency\\ncounts = collections.Counter(lst)\\nnew_list = sorted(lst, key=lambda x: -counts[x])\\n\\n> lst = [1, 2, 45, 55, 5, 4, 4, 4, 4, 4, 4, 5456, 56, 6, 7, 67]\\nmax(lst,key=lst.count)\\n> \\n\\nDF_list2.groupby(['year','month']).agg(\\nlike=('likecount', 'mean'),\\nclicks=('clicksCount', 'mean'))\\n\\ndf = pd.DataFrame({'A': [1, 2], 'B': [10, 20]})\\n\\ndef is_delayed(x):\\n\\nreturn x > 0\\n\\ndata['delayed'] = data['arr_delay'].apply(is_delayed)\\n\\ndf = pd.DataFrame({'A': [1, 2], 'B': [10, 20]})\\n\\ndf1 = df.apply(np.sum, axis=0)\\n\\nprint(df1)\\n\\ndf1 = df.apply(np.sum, axis=1)\\n\\nprint(df1)\\n\\nOutput:\\n\\nA \\xa0 \\xa0 3\\n\\nB\\xa0 \\xa0 30\\n\\ndtype: int64\\n\\n0\\xa0 \\xa0 11\\n\\n1\\xa0 \\xa0 22\\n\\ndtype: int64\\n\\nli = [5, 7, 22, 97, 54, 62, 77, 23, 73, 61]\\n\\nfinal_list = list(map(lambda x: x*2 , li))\\n\\nprint(final_list)\\n\\nOutput:\\n\\n[10, 14, 44, 194, 108, 124, 154, 46, 146, 122]\\n\\nstring1 = 'abc'\\nstring2 = 'ymene'\\nlist(zip(string2, string1))\\n[('y', 'a'), ('m', 'b'), ('e', 'c')]\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aa6d9730-7135-4c0b-a2b9-5e0651fa7f7b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nHere's the hypothetical from the interviewer:\\n\\nFB launched a Zoom-like feature. It was generally well-accepted and its usage is growing.\\n\\nYou work at Instagram. How would you evaluate if IG should add that Zoom-like feature?\\n\\n(in other words, a synchronous communication app within a heterogeneous network (FB) is being evaluted for launch in an otherwise asynchronous homogenous network (IG).\\n\\nMy response:\\n\\nClarifying questions:\\n\\n- Can some people use the FB Zoom feature with a higher / different access level than others?\\n\\n- What requirements, minimums, or thresholds must be achieved in order to obtain higher access (such as:\\n\\no a Facebook business page,\\n\\no a Facebook business page with >1,000 followers\\n\\no a Facebook user with >500 edges (relationships since\\xa0Facebook can process one trillion edge graphs)\\xa0which the people (nodes) desiring higher access must have people\\n\\n- Higher access might include: the ability to invite more than 20 people (20 being the number the hypothetical provided); the ability to place other companies’ advertisements in the Zoom invitation, the Zoom meeting, and/or the Zoom follow up notification; the ability to place advertisements of the host’s company in the Zoom invitation, the Zoom meeting, and/or the Zoom follow up notification; a longer Zoom meeting time (>60 minutes for example)\\n\\n(clarifying questions partially answered but mostly deferred)\\n\\nI would review the data from the A/B experiment on this feature's usage and adoption at FB. I imagine this A/B test would have a dependent variable / control group / training set in machine learning (ML) (user behavior before Zoom feature) and an independent variable / test group / test set in ML (user behavior after Zoom feature). user behavior here would include the following variables which would be available in SQL tables for further analysis: frequency (did it increase or decrease after using this feature; in what ways did it increase or decrease (likes, comments, shares, marketplace (buying or selling), direct chat w/ other nodes (users), creation or removal of certain edges (relationships).\\n\\nI'd also analyze datasets which tracked:\\n\\n- how many invitees became hosts and set up their own FB Zoom meeting within 1,2,3,4 and +4 weeks;\\n\\n- How many invitees purchased one of the host’s products that were advertised in the Zoom meeting?\\n\\n- How many invitees purchased one of the non-host products that were advertised in the Zoom meeting?\\n\\nThe A/B experiment I'd design at IG:\\n\\nDependent variable / control group / training set: IG user behavior before Zoom feature\\n\\nIndependent variable / test group / test set: IG user behavior after Zoom feature\\n\\nuser behavior here would include the following variables which would be available in SQL tables for further analysis: frequency (did it increase or decrease after using this feature; in what ways did it increase or decrease (likes, comments, shares, marketplace (buying or selling), direct chat w/ other nodes (users), creation or removal of certain edges (relationships).\\n\\nLast, I'd run an SQL inner join of the networks of people who used FB Zoom and the networks of those same people on IG. I'd use clustering ML (entropy weight k-means) to then look for trends or patterns which might explain why the Zoom feature was used more (or less) on FB than IG; which gender uses the Zoom feature more; which network (IG or FB) leads to more Zoom invites being sent out; do IG or FB Zoom meetings lead to more purchases? What type of purchases (category)? What price range? How does price, category and frequency of purchase vary based on GIS or location data of Zoom host and location of invitees? Do the data suggest this might be growing into something like the Influencers feature?\\n\\nIf users have been randomly assigned to a variant, then the difference in sample sizes will impact the width of the confidence intervals around the average or proportional outcome. If there is a true difference in outcomes for users in the two different groups, then the difference might be more difficult to detect because the confidence interval around the smaller group will be wider. In this case, the estimate for the smaller group will have more variance but still be unbiased.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9bd2438b-6f23-48f6-a351-461f4fab4faa', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**10 A/B Testing Interview Questions and Answers to Ace Your Next Data Science Interview**\\n\\n1. ### **In what situation do we run an A/B test?**\\n\\nAns. A/B test is usually conducted to examine the success of a newly launched feature in a product or a change in the existing product. It leads to increased user engagement and conversion rate, and minimizes bounce rate. It is also known as split testing or bucket testing.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='36f94eaa-2f27-40cf-ad28-6300f0746095', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**2. Highlight one difference between one-tailed and two-tailed tests.**\\n\\nAns. One-tailed tests have one critical region, and on the other hand, two-tailed tests have two critical regions. In other words, one-tailed tests determine the possibility of an effect in one direction, and two-tailed tests do the same in two directions- positive and negative.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='14b5b5e1-5aca-46e5-a7a6-943f3d4a4688', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**3. What do you know about a null and alternative hypothesis?**\\n\\nAns. A null hypothesis is a statement that suggests there is no relationship between the two variable values. An alternative hypothesis simply highlights the relationship between two variable values.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2a7ba852-b54b-43be-8cfe-831f93ce1535', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**4. For how much time should you run an A/B test?**\\n\\nAns. One can estimate the run time by taking the number of daily visitors and the number of variations into consideration.\\n\\nExample: If the traffic on your website every day is, say, 40K the sample size is 200K, and there are two variants, then you should run the A/B test for 100 days = 200K/40K*2\\xa0\\n\\nFurthermore, one should run the A/B test for at least 14 days in order to compensate for the variations due to weekdays and weekends.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f5212b28-45e9-417a-bd7f-7058450ec89b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**5. Explain Type I and Type II errors.**\\n\\nAns. One observes a Type I error when they reject a null hypothesis that is indeed true for the population. It suggests that the test and control could not be distinguished but we proposed the opposite.\\n\\nOn the other hand, Type II error occurs when one rejects a null hypothesis that is indeed false for the population.It suggests that the test and control could be distinguished from each other but they could not identify the differences.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5f08787e-90fb-4d66-95aa-8ee21bc55437', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**6. What do you mean by p-value? Explain it in simple terms.**\\n\\nAns. The p-value is a number that reflects how likely one can find a particular set of observations if the null hypothesis holds true.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fa787871-9fd6-4fef-b446-74ae0caf045f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**7. Is it possible to keep the treated samples different from the assigned samples?**\\n\\nAns. Yes, it is possible to keep the treated samples different from the assigned samples. An assigned sample is one that becomes a part of the campaign. A treated sample on the other hand is a subset of the assigned sample on the basis of certain conditions.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f56a0ace-234b-4f55-b109-87b842dcb324', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**8. What do you understand by alpha and beta in terms of A/B testing?**\\n\\nAns. Alpha denoted the probability of type-1 error. It is called the significance level. And, beta denotes the probability of type-II error.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c733e9d4-16c2-445d-9154-547d29bcdbda', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**9. What is the relationship between covariance and correlation?**\\n\\nAns. The normalized version of covariance is called correlation. To know more in detail, check out\\xa0Correlation Vs Covariance in Data Science.\\xa0\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f36bd667-fd1d-4b13-85eb-1ce6da468e5a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**10. Describe the number one step in implementing an A/B test.**\\n\\nAns. Before conducting an A/B test for the audience, here are a few steps that you must follow:\\n\\n1. Prepare description for null and alternative hypotheses.\\n    \\n2. Identify guardrail metric and north star metric.\\n    \\n3. Estimate the sample size or least detectable effect for the north star metric.\\n    \\n4. Prepare a blueprint for testing.\\n    \\n5. Collaborate with instrumentation/engineering teams to appropriate tags in place.\\n    \\n6. Ensure the tags are working well.\\n    \\n7. Seek approval of the testing plan from Product Managers and engineers.\\n    \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f55fd971-5c75-4890-b713-3616116ec1fc', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nCommon A/B Testing Scenarios\\n\\nHere are a few A/B testing scenarios that might be asked in an A/B Testing interview to test your knowledge and expertise-\\n\\n1. ### Imagine you are working for an e-commerce company that wants to test the effectiveness of two different call-to-action buttons on their product pages. How would you design an A/B test to compare the conversion rates of the two buttons?\\n    \\n2. ### Suppose you are a marketer for a social media platform and you want to determine whether changing the color scheme of the platform's user interface will impact user engagement. How would you set up an A/B test to compare the engagement metrics between the original color scheme and the new one?\\n    \\n3. ### Consider you work for a content-based website and you want to test two different headline variations for an article to see which one generates higher click-through rates. How would you design an A/B test to compare the click-through rates of the two headlines?\\n    \\n4. ### Let's say you are a product manager for a mobile app and you want to test two different onboarding experiences to see which one leads to higher user retention. How would you set up an A/B test to compare the retention rates of the two onboarding experiences?\\n    \\n5. ### Imagine you are a growth marketer for a software company and you want to test the impact of different pricing structures on the conversion rates of your pricing page. How would you design an A/B test to compare the conversion rates of the two pricing structures?\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c0e94034-5334-4808-a192-ed7db8d4d5c5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Acquisition Metrics**\\n\\nAcquisition metrics are a set of metrics that are used to measure the effectiveness of marketing campaigns or activities in attracting and acquiring new customers. These metrics help businesses to evaluate the performance of their marketing efforts and optimize their strategies to maximize their return on investment (ROI) and increase their customer base.\\n\\nAcquisition metrics can include a range of different metrics such as cost per acquisition (CPA), conversion rate, click-through rate (CTR), customer acquisition cost (CAC), lifetime value (LTV), and many more. These metrics provide valuable insights into the success of different marketing channels and tactics, allowing businesses to make data-driven decisions to improve their marketing strategies and drive growth. Here are some examples of acquisition metrics:\\n\\n1. **Cost per acquisition (CPA):**\\xa0CPA is a metric that measures the cost of acquiring a new users. It is calculated by dividing the total cost of a marketing campaign by the number of new customers acquired during that campaign.\\n    \\n2. **Conversion rate:**\\xa0Conversion rate is the proportion of website visitors who complete a desired action, such as buying a product or filling out a form. It\\'s determined by dividing the number of conversions by the total number of visitors, and then multiplying the result by 100%.\\n    \\n3. **Click-through rate (CTR):**\\xa0CTR is a metric that measures the percentage of people who click on a link in an advertisement or email. It is calculated by dividing the number of clicks by the number of impressions and multiplying by 100%.\\n    \\n4. **Cost per click (CPC):**\\xa0CPC is a metric used in pay-per-click (PPC) advertising campaigns to measure the cost associated with each click on an ad. Essentially, CPC is calculated by dividing the total cost of the campaign by the number of clicks received.\\n    \\n5. **Customer acquisition cost (CAC):**\\xa0CAC is a metric that measures the cost of acquiring a new customer over a specific period. To determine the cost of customer acquisition, divide the total cost of sales and marketing activities by the number of new customers acquired during a specific time period.\\n    \\n6. **Return on investment (ROI):**\\xa0ROI is a commonly used metric in marketing that measures the effectiveness and profitability of a marketing campaign. Essentially, ROI is calculated by subtracting the total cost of the campaign from the total revenue generated and then dividing the result by the total cost of the campaign. This figure is then multiplied by 100% to express the ROI as a percentage.\\n    \\n7. **Lifetime value (LTV):**\\xa0LTV is a key metric used to evaluate the total worth of a customer throughout their relationship with a business. Essentially, LTV measures the total value that a customer will bring to a business over the course of their lifetime. The calculation of LTV involves multiplying two other key metrics: the average revenue per user (ARPU) and the average customer lifespan.\\n    \\n8. **Customer lifetime value (CLTV):**\\xa0CLTV is a metric that measures the total value of a specific customer over their lifetime. It is calculated by multiplying the average revenue per user (ARPU) by the average customer lifespan and multiplying the result by the customer retention rate.\\n    \\n9. **Time to Conversion:**\\xa0Time to Conversion is a metric that measures the time it takes for a user to convert into a customer from the first interaction with a product or service.\\n    \\n10. **Churn rate:**\\xa0Churn rate is a metric that measures the percentage of customers who cancel or do not renew their subscription or service during a given time period.\\n    \\n11. **Lead-to-customer conversion rate:**\\xa0Lead-to-customer conversion rate measures the percentage of leads generated that convert into paying customers.\\n    \\n12. **Marketing-originated customer percentage:**\\xa0Marketing-originated customer percentage measures the percentage of customers who came from marketing efforts, rather than from other sources.\\n    \\n13. **Market penetration:**\\xa0Market penetration measures the percentage of a target market that a business has captured.\\n    \\n14. **Net promoter score (NPS):**\\xa0The Net Promoter Score (NPS) is a customer satisfaction and loyalty metric. This measurement is obtained by taking the percentage of detractors away from the percentage of promoters.\\n    \\n15. **Revenue per user (RPU):**\\xa0RPU is a metric that measures the average amount of revenue generated per user.\\n    \\n16. **Number of new customers:**\\xa0Number of new customers measures the total number of new customers acquired during a specific time period.\\n    \\n17. **Average revenue per user (ARPU):**\\xa0ARPU is a metric that measures the average amount of revenue generated per user over a specific time period.\\n    \\n18. **Repeat purchase rate (RPR):**\\xa0RPR is a metric that measures the percentage of customers who make a repeat purchase.\\n    \\n19. **Social media engagement:**\\xa0Social media engagement measures the level of engagement and interaction on social media platforms, such as likes, shares, comments, and retweets.\\n    \\n20. **Organic traffic growth:**\\xa0Organic traffic growth measures the percentage increase or decrease in website traffic that comes from organic search results.\\n    \\n21. **Bounce rate:**\\xa0Bounce rate is a website analytics metric that measures the percentage of website visitors who navigate away from a website after only viewing a single page, without interacting with any other pages on the site. In other words, a \"bounce\" occurs when a visitor lands on a page of a website and then leaves without clicking on any other links or taking any further action.\\n    \\n    A high bounce rate can be an indication of several things, including:\\n    \\n    1. Poor user experience: Visitors may leave the website quickly if they find the content unengaging or difficult to understand.\\n        \\n    2. Slow page load times: Visitors may become impatient and leave the site if pages take too long to load.\\n        \\n    3. Irrelevant content: Visitors may leave the site if the content does not match their expectations or needs.\\n        \\n    4. Poor navigation: Visitors may struggle to find what they are looking for on the site and leave as a result.\\n        \\n22. **Channel effectiveness:**\\xa0It refers to how well a particular marketing channel, such as social media, email, or advertising, is performing in terms of reaching and engaging with its target audience, driving conversions, and generating a positive return on investment (ROI).\\n    \\n    Measuring channel effectiveness involves tracking and analyzing key performance indicators (KPIs) such as click-through rates (CTR), conversion rates, customer acquisition costs (CAC), customer lifetime value (CLV), and return on ad spend (ROAS).\\n    \\n23. **Traffic source distribution:**\\xa0It is a measurement of the sources through which website traffic is generated. It involves analyzing the proportion of website visitors that come from various sources such as search engines, social media platforms, email campaigns, direct traffic, and other referral sources.\\n    \\n\\n---\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f1bbab97-85de-421a-9cc7-541d4f744ef7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n**Activation Metrics:**\\n\\nActivation metrics are a set of metrics used to measure the effectiveness of a business's efforts to turn website visitors or app users into active customers or users. These metrics help businesses to evaluate how effectively they are engaging and retaining customers, and to identify opportunities for improvement in their customer activation strategies.\\n\\n1. **Activation rate:**\\xa0It measures the percentage of website visitors or app users who take a specific action such as signing up, making a purchase or completing a form. It helps businesses assess the effectiveness of their efforts to convert visitors into active users or customers.\\n    \\n2. **Time to activation:**\\xa0This metric measures the amount of time it takes for a new user to become an active customer or user, from the moment they sign up or download the app. It helps businesses understand how long it takes for users to start engaging with their product or service.\\n    \\n3. **Retention rate:**\\xa0Retention rate is a metric that tells businesses how many users are continuing to use their product or service over a specific time period, usually monthly or yearly. It helps businesses understand how effectively they are retaining customers and preventing churn.\\n    \\n4. **Onboarding completion rate:**\\xa0This metric measures the percentage of users who complete the onboarding process and become active users. It helps businesses understand how effectively they are onboarding new users and getting them to start using the product or service.\\n    \\n5. **Engagement rate:**\\xa0This metric measures the level of engagement of active users, based on the frequency and depth of user interactions with a product or service. It helps businesses understand how effectively they are engaging users and encouraging them to use the product or service more frequently.\\n    \\n6. **Churn rate:**\\xa0This metric measures the percentage of users who stop using a product or service over a specific period of time. It helps businesses understand how effectively they are retaining customers and preventing churn.\\n    \\n7. **Revenue per user (RPU):**\\xa0This metric measures the amount of revenue generated by each active user over a specific period of time, such as a month or year. It helps businesses understand the value of each user and their contribution to the overall revenue of the business.\\n    \\n8. **Average order value (AOV):**\\xa0This metric measures the average value of each transaction made by a customer, over a specific period of time. It helps businesses understand the spending habits of their customers and the average value of each transaction.\\n    \\n9. **Customer lifetime value (CLV):**\\xa0This metric measures the total value a customer brings to a business over the course of their relationship, taking into account their spending habits, loyalty, and retention. It helps businesses understand the long-term value of their customers and the potential revenue that can be generated from each customer.\\n    \\n10. **Referral rate:**\\xa0This metric measures the percentage of active users who refer new users to a product or service. It helps businesses understand how effectively they are leveraging their existing user base to acquire new customers.\\n    \\n11. **Trial-to-Paid Conversion Rate:**\\xa0Trial-to-paid conversion rate is a metric that measures the percentage of users who convert from a free trial to a paid subscription or purchase. This metric is commonly used in SaaS (Software-as-a-Service) businesses that offer free trials to potential customers.\\n    \\n    The trial-to-paid conversion rate helps businesses understand how effectively they are converting free trial users into paying customers.\\n    \\n12. **First-time User Conversion Rate:**\\xa0It is a metric used to measure the percentage of first-time visitors to a website or app who complete a desired action, such as making a purchase, signing up for a newsletter, or filling out a form. This metric is important because it indicates how effective a website or app is at converting new visitors into customers or users.\\n    \\n13. **Time to Value (TTV):**\\xa0It is a metric used to measure the amount of time it takes for a new user to realize the value of a product or service after they sign up or start using it. In other words, TTV is the time it takes for a user to experience the benefits of the product or service they have signed up for.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f433f478-5c59-4d93-b50c-5a9aa9b15a9a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n**Project Workflow**\\n\\nGiven a data science / machine learning project, what steps should we follow? Here's how I would tackle it:\\n\\n- **Specify business objective.**\\xa0Are we trying to win more customers, achieve higher satisfaction, or gain more revenues?\\n- **Define problem.**\\xa0What is the specific gap in your ideal world and the real one that requires machine learning to fill? Ask questions that can be addressed using your data and predictive modeling (ML algorithms).\\n- **Create a common sense baseline.**\\xa0But before you resort to ML, set up a baseline to solve the problem as if you know zero data science. You may be amazed at how effective this baseline is. It can be as simple as recommending the top N popular items or other rule-based logic. This baseline can also server as a good benchmark for ML algorithms.\\n- **Review ML literatures.**\\xa0To avoid reinventing the wheel and get inspired on what techniques / algorithms are good at addressing the questions using our data.\\n- **Set up a single-number metric.**\\xa0What it means to be successful - high accuracy, lower error, or bigger AUC - and how do you measure it? The metric has to align with high-level goals, most often the success of your business. Set up a single-number against which all models are measured.\\n- **Do exploratory data analysis (EDA).**\\xa0Play with the data to get a general idea of data type, distribution, variable correlation, facets etc. This step would involve a lot of plotting.\\n    - histogram of varaiables\\n    - correlation among variables\\n    - unique things that is important: the geographic distribution of income\\n- **Partition data.**\\xa0Validation set should be large enough to detect differences between the models you are training; test set should be large enough to indicate the overall performance of the final model; training set, needless to say, the larger the merrier.\\n- **Preprocess.**\\xa0This would include data integration, cleaning, transformation, reduction, discretization and more.\\n- **Engineer features.**\\xa0Coming up with features is difficult, time-consuming, requires expert knowledge. Applied machine learning is basically feature engineering. This step usually involves feature selection and creation, using domain knowledge. Can be minimal for deep learning projects.\\n    - create new features from existing ones\\n    - imputation: insert values where there might be missing\\n    - encode categorical variables: one hot encode; ordinal encode when one category is better than the other; sklearn.preprocessing里面都有这些encoder的直接实现\\n    - scaling: transform to the same numerical scale\\n- **Develop models.**\\xa0Choose which algorithm to use, what hyperparameters to tune, which architecture to use etc.\\n- **Ensemble.**\\xa0Ensemble can usually boost performance, depending on the correlations of the models/features. So it’s always a good idea to try out. But be open-minded about making tradeoff - some ensemble are too complex/slow to put into production.\\n- **Deploy model.**\\xa0Deploy models into production for inference.\\n- **Monitor model.**\\xa0Monitor model performance, and collect feedbacks.\\n- **Iterate.**\\xa0Iterate the previous steps. Data science tends to be an iterative process, with new and improved models being developed over time.\\n\\nWhy did you pick your model?\\nHow did you clean the data?\\nWhat type of validation tests did you perform\\non the data to prepare it for the model?\\nTell me about the assumptions of your model?\\nHow did you optimize your model?\\nHow did you implement\\nyour test/control?\\nTell me about how the underlying math in the model works.\\n\\n在这个例子中,选定目标运用了2种视角:\\n·第一种,用户视角,用户可以使用搜索功能高效的找到自己心仪的住宿产品。· 第二种,业务视角,通过提高搜索成功率,进而提升客户的下单转化率。\\n为了提高这个转化率,需要采取什么样的策略呢?\\n第一,返回与用户搜索值相匹配的搜索结果。\\n第二,提供有效的搜索结果排序。对于非标类产品,我们需要思考怎样把用户感兴趣的产品放在第一屏或者前三位,能够让用户一眼就看到他想搜索的产品。\\n第三,当搜索没有结果或者结果不足时,我们就要做有效的推荐。\\n\\n!Untitled\\n\\n在选取目标的时候,我们要意4则,即DUMB:\\n切实可行(Doable)\\n易于理解(Understandable)\\n可干预可管理(Manageable)\\n正向的有益的(Beneficial)\\n可以发现,以用户视角和业务视角出发制定的这两个目标,是符合DUMB原则的\\n\\n!Untitled\\n\\n(1)一级指标(Tier 1 Metrics)\\n一级指标必须是全公司都认可的、衡量业绩的核心指标。可以直接指引公司的战略目标,衡量公司的业务达成情况,本质上需要管理层和下级员工的双向理解、认同,且要易于沟通传达。\\n选择一级指标时,数量控制在5至8个最为合适。需要从公司和用户两个角度出发,与商业结果和公司战略目标紧密结合。比如GMV (GM=用户数*转化率*客单价) 、订单数量、周/日活跃用户数量等。\\n\\n(2)二级指标(Tier 2 Metrics)\\n二级指标是针对一级指标的路径分析拆解,是流程中的指标。当一级指标发生变化的时候,我们通过查看二级指标,结合一定的历史经验,能够快速定位问题的原因所在。\\n\\n!Untitled\\n\\n前段时间刚重写了一个 dl 任务，在此说下心得体会：\\n\\n1. 顺序上，先 dataset，检查基本的 transform，再搭 model，构建 head 和 loss，就可以把一个基础的、可以跑的网络就能跑起来了（这点很重要）；\\n2. 可视化很重要，如果是本地开发机，善用 cv.imshow 直观、便捷地可视化处理的结果；\\n3. 一个基础的 train/inference 流程跑通后，分别构建 1 张、10 张的数据用于 debug，确保任意改动后，可以 overfit；\\n4. 调试代码阶段避免随机性、避免数据增强，一定用 tensorboard 之类的工具观察 loss 下降是否合理；\\n5. 一般数据集最好处理成 coco 的格式，我的任务跟传统任务不太一样，但也尽量仿照 coco 来设计，写 dataset 的时候可以参考开源实现；\\n6. 善用开源框架，比如 Open-MMLab，Detectron2 之类的，好处是方便实验，在框架里写不容易出现难以察觉的 bug，坏处是开源框架为了适配各种网络，代码复杂程度会高一点，建议从第一版入手了解框架，然后基于最新的一边阅读一边开发。\\n\\n\\n\\n先给结论：以我写了两三年`pytorch`代码的经验而言，比较好的顺序是先写`model`，再写`dataset`，最后写`train`。\\n\\n在讨论码组件的具体顺序前，我们先分析每一个组件背后的目的和逻辑。\\n\\n`model`构成了整个深度学习训练与推断**系统骨架**，也确定了整个AI模型的输入和输出格式。对于视觉任务，模型架构多为卷积神经网络或是最新的`ViT`模型；对于NLP任务，模型架构多为`Transformer`以及`Bert`；对于时间序列预测，模型架构多为`RNN`或`LSTM`。不同的`model`对应了不同的数据输入格式，如`ResNet`一般是输入多通道二维矩阵，而`ViT`则需要输入带有位置信息的图像patchs。确定了用什么样的`model`后，数据的输入格式也就确定下来。根据确定的输入格式，我们才能构建对应的`dataset`。\\n\\n`dataset`构建了整个AI模型的**输入与输出格式**。在写作`dataset`组件时，我们需要考虑数据的**存储位置与存储方式**，如数据是否是**分布式存储**的，模型是否要在**多机多卡**的情况下运行，**读写速度**是否存在瓶颈，如果机械硬盘带来了读写瓶颈则需要将数据预加载进内存等。在写`dataset`组件时，我们也要反向微调`model`组件。例如，确定了分布式训练的数据读写后，需要用`nn.DataParallel`或者`nn.DistributedDataParallel`等模块包裹`model`，使模型能够在多机多卡上运行。此外，`dataset`组件的写作也会**影响训练策略**，这也为构建`train`组件做了铺垫。比如根据显存大小，我们需要确定相应的***BatchSize***，而***BatchSize***则直接影响学习率的大小。再比如根据数据的分布情况，我们需要选择不同的**采样策略**进行***Feature Balance***，而这也会体现在训练策略中。\\n\\n`train`构建了模型的**训练策略以及评估方法**，它是最重要也是最复杂的组件。先构建`model`与`dataset`可以**添加限制**，减少`train`组件的**复杂度**。在`train`组件中，我们需要根据训练**环境**（单机多卡，多机多卡或是联邦学习）确定模型更新的策略，以及确定训练**总时长**`epochs`，**优化器**的类型，**学习率**的大小与衰减策略，参数的**初始化**方法，模型**损失函数**。此外，为了对抗过拟合，提升泛化性，还需要引入合适的**正则化**方法，如`Dropout`，`BatchNorm`，`L2-Regularization`，`Data Augmentation`等。有些提升泛化性能的方法可以直接在`train`组件中实现（如添加`L2-Reg`，`Mixup`），有些则需要添加进`model`中（如`Dropout`与`BatchNorm`），还有些需要添加进`dataset`中（如`Data Augmentation`）。此处安利一下我们的专栏教程：数据增广的方法与代码实现。\\n\\n此外，`train`还需要记录训练过程的一些重要信息，并将这些信息可视化出来，比如在每个*epoch*上记录训练集的平均损失以及测试集精度，并将这些信息写入`tensorboard`，然后在网页端实时监控。在构建`train`组件中，我们需要随时根据模型表现进行参数微调，并根据结果改进`model`和`dataset`两个组件。\\n\\nThis is just a start when it comes to training neural nets. Everything could be correct syntactically, but the whole thing isn’t arranged properly, and it’s really hard to tell. The “possible error surface” is large, logical (as opposed to syntactic), and very tricky to unit test. For example, perhaps you forgot to flip your labels when you left-right flipped the image during data augmentation. Your net can still (shockingly) work pretty well because your network can internally learn to detect flipped images and then it left-right flips its predictions. Or maybe your autoregressive model accidentally takes the thing it’s trying to predict as an input due to an off-by-one bug. Or you tried to clip your gradients but instead clipped the loss, causing the outlier examples to be ignored during training. Or you initialized your weights from a pretrained checkpoint but didn’t use the original mean. Or you just screwed up the settings for regularization strengths, learning rate, its decay rate, model size, etc. Therefore, your misconfigured neural net will throw exceptions only if you’re lucky; Most of the time it will train but silently work a bit worse.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7b9715fa-2a72-4b20-a42a-c03deafd21aa', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n1. Become one with the data\\n\\nThe first step to training a neural net is to not touch any neural net code at all and instead begin by thoroughly inspecting your data. This step is critical. I like to spend copious amount of time (measured in units of hours) scanning through thousands of examples, understanding their distribution and looking for patterns. Luckily, your brain is pretty good at this. One time I discovered that the data contained duplicate examples. Another time I found corrupted images / labels. I look for data imbalances and biases. I will typically also pay attention to my own process for classifying the data, which hints at the kinds of architectures we’ll eventually explore. As an example - are very local features enough or do we need global context? How much variation is there and what form does it take? What variation is spurious and could be preprocessed out? Does spatial position matter or do we want to average pool it out? How much does detail matter and how far could we afford to downsample the images? How noisy are the labels?\\n\\nIn addition, since the neural net is effectively a compressed/compiled version of your dataset, you’ll be able to look at your network (mis)predictions and understand where they might be coming from. And if your network is giving you some prediction that doesn’t seem consistent with what you’ve seen in the data, something is off.\\n\\nOnce you get a qualitative sense it is also a good idea to write some simple code to search/filter/sort by whatever you can think of (e.g. type of label, size of annotations, number of annotations, etc.) and visualize their distributions and the outliers along any axis. The outliers especially almost always uncover some bugs in data quality or preprocessing.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='228a5e80-99ba-458c-aa5c-cce01d27853f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n2. Set up the end-to-end training/evaluation skeleton + get dumb baselines\\n\\nNow that we understand our data can we reach for our super fancy Multi-scale ASPP FPN ResNet and begin training awesome models? For sure no. That is the road to suffering. Our next step is to set up a full training + evaluation skeleton and gain trust in its correctness via a series of experiments. At this stage it is best to pick some simple model that you couldn’t possibly have screwed up somehow - e.g. a linear classifier, or a very tiny ConvNet. We’ll want to train it, visualize the losses, any other metrics (e.g. accuracy), model predictions, and perform a series of ablation experiments with explicit hypotheses along the way.\\n\\nTips & tricks for this stage:\\n\\n- **fix random seed**. Always use a fixed random seed to guarantee that when you run the code twice you will get the same outcome. This removes a factor of variation and will help keep you sane.\\n- **simplify**. Make sure to disable any unnecessary fanciness. As an example, definitely turn off any data augmentation at this stage. Data augmentation is a regularization strategy that we may incorporate later, but for now it is just another opportunity to introduce some dumb bug.\\n- **add significant digits to your eval**. When plotting the test loss run the evaluation over the entire (large) test set. Do not just plot test losses over batches and then rely on smoothing them in Tensorboard. We are in pursuit of correctness and are very willing to give up time for staying sane.\\n- **verify loss @ init**. Verify that your loss starts at the correct loss value. E.g. if you initialize your final layer correctly you should measure\\xa0`log(1/n_classes)`\\xa0on a softmax at initialization. The same default values can be derived for L2 regression, Huber losses, etc.\\n- **init well**. Initialize the final layer weights correctly. E.g. if you are regressing some values that have a mean of 50 then initialize the final bias to 50. If you have an imbalanced dataset of a ratio 1:10 of positives:negatives, set the bias on your logits such that your network predicts probability of 0.1 at initialization. Setting these correctly will speed up convergence and eliminate “hockey stick” loss curves where in the first few iteration your network is basically just learning the bias.\\n- **human baseline**. Monitor metrics other than loss that are human interpretable and checkable (e.g. accuracy). Whenever possible evaluate your own (human) accuracy and compare to it. Alternatively, annotate the test data twice and for each example treat one annotation as prediction and the second as ground truth.\\n- **input-indepent baseline**. Train an input-independent baseline, (e.g. easiest is to just set all your inputs to zero). This should perform worse than when you actually plug in your data without zeroing it out. Does it? i.e. does your model learn to extract any information out of the input at all?\\n- **overfit one batch**. Overfit a single batch of only a few examples (e.g. as little as two). To do so we increase the capacity of our model (e.g. add layers or filters) and verify that we can reach the lowest achievable loss (e.g. zero). I also like to visualize in the same plot both the label and the prediction and ensure that they end up aligning perfectly once we reach the minimum loss. If they do not, there is a bug somewhere and we cannot continue to the next stage.\\n- **verify decreasing training loss**. At this stage you will hopefully be underfitting on your dataset because you’re working with a toy model. Try to increase its capacity just a bit. Did your training loss go down as it should?\\n- **visualize just before the net**. The unambiguously correct place to visualize your data is immediately before your\\xa0`y_hat = model(x)`\\xa0(or\\xa0`sess.run`\\xa0in tf). That is - you want to visualize\\xa0*exactly*\\xa0what goes into your network, decoding that raw tensor of data and labels into visualizations. This is the only “source of truth”. I can’t count the number of times this has saved me and revealed problems in data preprocessing and augmentation.\\n- **visualize prediction dynamics**. I like to visualize model predictions on a fixed test batch during the course of training. The “dynamics” of how these predictions move will give you incredibly good intuition for how the training progresses. Many times it is possible to feel the network “struggle” to fit your data if it wiggles too much in some way, revealing instabilities. Very low or very high learning rates are also easily noticeable in the amount of jitter.\\n- **use backprop to chart dependencies**. Your deep learning code will often contain complicated, vectorized, and broadcasted operations. A relatively common bug I’ve come across a few times is that people get this wrong (e.g. they use\\xa0`view`\\xa0instead of\\xa0`transpose/permute`\\xa0somewhere) and inadvertently mix information across the batch dimension. It is a depressing fact that your network will typically still train okay because it will learn to ignore data from the other examples. One way to debug this (and other related problems) is to set the loss to be something trivial like the sum of all outputs of example\\xa0**i**, run the backward pass all the way to the input, and ensure that you get a non-zero gradient only on the\\xa0**i-th**\\xa0input. The same strategy can be used to e.g. ensure that your autoregressive model at time t only depends on 1..t-1. More generally, gradients give you information about what depends on what in your network, which can be useful for debugging.\\n- **generalize a special case**. This is a bit more of a general coding tip but I’ve often seen people create bugs when they bite off more than they can chew, writing a relatively general functionality from scratch. I like to write a very specific function to what I’m doing right now, get that to work, and then generalize it later making sure that I get the same result. Often this applies to vectorizing code, where I almost always write out the fully loopy version first and only then transform it to vectorized code one loop at a time.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b274bc53-1e12-4d23-8162-85742912acb2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n3. Overfit\\n\\nAt this stage we should have a good understanding of the dataset and we have the full training + evaluation pipeline working. For any given model we can (reproducibly) compute a metric that we trust. We are also armed with our performance for an input-independent baseline, the performance of a few dumb baselines (we better beat these), and we have a rough sense of the performance of a human (we hope to reach this). The stage is now set for iterating on a good model.\\n\\nThe approach I like to take to finding a good model has two stages: first get a model large enough that it can overfit (i.e. focus on training loss) and then regularize it appropriately (give up some training loss to improve the validation loss). The reason I like these two stages is that if we are not able to reach a low error rate with any model at all that may again indicate some issues, bugs, or misconfiguration.\\n\\nA few tips & tricks for this stage:\\n\\n- **picking the model**. To reach a good training loss you’ll want to choose an appropriate architecture for the data. When it comes to choosing this my #1 advice is:\\xa0**Don’t be a hero**. I’ve seen a lot of people who are eager to get crazy and creative in stacking up the lego blocks of the neural net toolbox in various exotic architectures that make sense to them. Resist this temptation strongly in the early stages of your project. I always advise people to simply find the most related paper and copy paste their simplest architecture that achieves good performance. E.g. if you are classifying images don’t be a hero and just copy paste a ResNet-50 for your first run. You’re allowed to do something more custom later and beat this.\\n- **adam is safe**. In the early stages of setting baselines I like to use Adam with a learning rate of\\xa03e-4. In my experience Adam is much more forgiving to hyperparameters, including a bad learning rate. For ConvNets a well-tuned SGD will almost always slightly outperform Adam, but the optimal learning rate region is much more narrow and problem-specific. (Note: If you are using RNNs and related sequence models it is more common to use Adam. At the initial stage of your project, again, don’t be a hero and follow whatever the most related papers do.)\\n- **complexify only one at a time**. If you have multiple signals to plug into your classifier I would advise that you plug them in one by one and every time ensure that you get a performance boost you’d expect. Don’t throw the kitchen sink at your model at the start. There are other ways of building up complexity - e.g. you can try to plug in smaller images first and make them bigger later, etc.\\n- **do not trust learning rate decay defaults**. If you are re-purposing code from some other domain always be very careful with learning rate decay. Not only would you want to use different decay schedules for different problems, but - even worse - in a typical implementation the schedule will be based current epoch number, which can vary widely simply depending on the size of your dataset. E.g. ImageNet would decay by 10 on epoch 30. If you’re not training ImageNet then you almost certainly do not want this. If you’re not careful your code could secretely be driving your learning rate to zero too early, not allowing your model to converge. In my own work I always disable learning rate decays entirely (I use a constant LR) and tune this all the way at the very end.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='10fb7f54-98dd-41ff-81ac-6f6581667b4f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n4. Regularize\\n\\nIdeally, we are now at a place where we have a large model that is fitting at least the training set. Now it is time to regularize it and gain some validation accuracy by giving up some of the training accuracy. Some tips & tricks:\\n\\n- **get more data**. First, the by far best and preferred way to regularize a model in any practical setting is to add more real training data. It is a very common mistake to spend a lot engineering cycles trying to squeeze juice out of a small dataset when you could instead be collecting more data. As far as I’m aware adding more data is pretty much the only guaranteed way to monotonically improve the performance of a well-configured neural network almost indefinitely. The other would be ensembles (if you can afford them), but that tops out after ~5 models.\\n- **data augment**. The next best thing to real data is half-fake data - try out more aggressive data augmentation.\\n- **creative augmentation**. If half-fake data doesn’t do it, fake data may also do something. People are finding creative ways of expanding datasets; For example,\\xa0domain randomization, use of\\xa0simulation, clever\\xa0hybrids\\xa0such as inserting (potentially simulated) data into scenes, or even GANs.\\n- **pretrain**. It rarely ever hurts to use a pretrained network if you can, even if you have enough data.\\n- **stick with supervised learning**. Do not get over-excited about unsupervised pretraining. Unlike what that blog post from 2008 tells you, as far as I know, no version of it has reported strong results in modern computer vision (though NLP seems to be doing pretty well with BERT and friends these days, quite likely owing to the more deliberate nature of text, and a higher signal to noise ratio).\\n- **smaller input dimensionality**. Remove features that may contain spurious signal. Any added spurious input is just another opportunity to overfit if your dataset is small. Similarly, if low-level details don’t matter much try to input a smaller image.\\n- **smaller model size**. In many cases you can use domain knowledge constraints on the network to decrease its size. As an example, it used to be trendy to use Fully Connected layers at the top of backbones for ImageNet but these have since been replaced with simple average pooling, eliminating a ton of parameters in the process.\\n- **decrease the batch size**. Due to the normalization inside batch norm smaller batch sizes somewhat correspond to stronger regularization. This is because the batch empirical mean/std are more approximate versions of the full mean/std so the scale & offset “wiggles” your batch around more.\\n- **drop**. Add dropout. Use dropout2d (spatial dropout) for ConvNets. Use this sparingly/carefully because dropout\\xa0does not seem to play nice\\xa0with batch normalization.\\n- **weight decay**. Increase the weight decay penalty.\\n- **early stopping**. Stop training based on your measured validation loss to catch your model just as it’s about to overfit.\\n- **try a larger model**. I mention this last and only after early stopping but I’ve found a few times in the past that larger models will of course overfit much more eventually, but their “early stopped” performance can often be much better than that of smaller models.\\n\\nFinally, to gain additional confidence that your network is a reasonable classifier, I like to visualize the network’s first-layer weights and ensure you get nice edges that make sense. If your first layer filters look like noise then something could be off. Similarly, activations inside the net can sometimes display odd artifacts and hint at problems.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='05c8bce7-30ef-4abb-b740-6a90fe68db08', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n5. Tune\\n\\nYou should now be “in the loop” with your dataset exploring a wide model space for architectures that achieve low validation loss. A few tips and tricks for this step:\\n\\n- **random over grid search**. For simultaneously tuning multiple hyperparameters it may sound tempting to use grid search to ensure coverage of all settings, but keep in mind that it is\\xa0best to use random search instead. Intuitively, this is because neural nets are often much more sensitive to some parameters than others. In the limit, if a parameter\\xa0**a**\\xa0matters but changing\\xa0**b**\\xa0has no effect then you’d rather sample\\xa0**a**\\xa0more throughly than at a few fixed points multiple times.\\n- **hyper-parameter optimization**. There is a large number of fancy bayesian hyper-parameter optimization toolboxes around and a few of my friends have also reported success with them, but my personal experience is that the state of the art approach to exploring a nice and wide space of models and hyperparameters is to use an intern :). Just kidding.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7af52050-1ec3-445f-8005-cfdb06acbf56', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n6. Squeeze out the juice\\n\\nOnce you find the best types of architectures and hyper-parameters you can still use a few more tricks to squeeze out the last pieces of juice out of the system:\\n\\n- **ensembles**. Model ensembles are a pretty much guaranteed way to gain 2% of accuracy on anything. If you can’t afford the computation at test time look into distilling your ensemble into a network using\\xa0dark knowledge.\\n- **leave it training**. I’ve often seen people tempted to stop the model training when the validation loss seems to be leveling off. In my experience networks keep training for unintuitively long time. One time I accidentally left a model training during the winter break and when I got back in January it was SOTA (“state of the art”).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aa5d943e-3a61-4eba-9645-e59a9e1ccbe0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='097c5858-6718-4a56-9485-f8009b5333c3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n我的analytics项目经历\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b2c7e1c1-bae0-4049-907a-9023adac15ae', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\ntiger recommender system show\\n**problem**: how to convince business to use it; business ppl are reluctant to change \\n**action**: \\n1. I created a dashboard in dbt, so they can enter product id and see recommended result; also created some example to explain direct and indirect correlation for the source \\n2. also created some example to explain direct and indirect correlation for the source\\n3. I showed comparison with our recommendation and the existing recommendations, look like our items make more sense \\n**impact**: the client is very satisfied and its a key component to convince client to convert; \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2462b792-dbdc-4f3c-a1aa-9fee4d42738c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\ntiger time series show simulation experiment\\n**problem**: how to convince business to use it; business ppl are reluctant to change \\n**impact**: the client is very satisfied and its a key component to convince the business  \\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='53711d8a-a750-4a81-9ea8-c0c70030360f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nDealmaker analysis\\nprovided many insights but can’t track its impact; analyzed what category of marketing email brings more clicks to our site; also keyword topic modeling;\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d7264ef0-6c1e-4fb4-8a50-e81d0ee45179', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nTiger 的项目 - recommender system\\n\\n**situation**: need to replace their external recommender system\\n\\n**impact**: no observable impact yet \\n\\n**challenge:** \\n1. majority of users are not logged in, so we can't do user - item embedding; have to do item based recom so need change model structure of NCF   \\n2. how to convince business to use it; business ppl are reluctant to change \\n\\n**about team**\\nabout 10 ppl; \\n\\n**action**: \\n- before setting up A/B testing, I created a dashboard in dbt, so they can enter product id and see recommended result; also created some example to explain direct and indirect correlation for the source \\n- Personally I am a coder of course, and I was usually assigned with relatively tough tasks because I am an efficient coder. \\n\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d61d89b2-caf3-4df7-bff0-5940cc2fabcd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nTiger 的项目 - time series sales quantity prediction\\n\\n**situation**: need a better model than our competitor; need to win over our client\\n- background: essential question for retail companies; DF; our client already have a vendor for it; but they want to improve; our team is full of phds so we took the task\\n\\n**impact**: our model is 10%+ better in terms of wape \\n\\n**challenge:** \\n1. need a better model; we looked up different advanced models, had discussion and used a smaller brand to test; split the task among the team on code pieces   \\n2. setup retail level model is difficult because we can't train 10000 * 100 stores time series; our online store only trained 10000 TS because its 1000 items; I created a store cluster mechanism and disaggregation pipeline \\n3. setup pipeline for a few thousands lines of code; resolve memory issue on databricks when doing parrall training with multiple GPUs; \\n4. for one brand and channel the result was particularly bad; changed to a simple arima model  \\n\\n**about team**\\nsupervise and leading a small team; what i learned: \\ndifferent ppl have diff personality & workstyle: \\nas detailed instruction as possible; clear success criteria and deadline, checkup point; if u can't finish it its ok but u need to show me whats the blocker\\n\\n**action**: \\n- big team around 10 ppl; im incharge of certain parts of the code like model evaluation etc. and leading two new members\\n- my daily job is assiging and reviewing tasks to my small team; run experiments on our model \\n- analytics: use dummy viable of rainy days to get better model performance; \\n\\n\\n What is Temporal Fusion Transformer\\n Temporal Fusion Transformer (TFT) is a Transformer- based model that leverages self-attention to capture the complex temporal dynamics of multiple time\\n sequences.\\n TFT supports:\\n Multiple time series: We can train a TFT model on thousands of univariate or multivariate time series.\\n Multi-Horizon Forecasting: The model outputs multi-step predictions of one or more target variables — including prediction intervals.\\n Heterogeneous features: TFT supports many types of features, including time-variant and static exogenous variables.\\n Interpretable predictions: Predictions can be interpreted in terms of variable importance and seasonality.\\n\\n gating mechanisms: skip over unused components of the model architecture\\n variable selection networks: select relevant input variables at each time step.\\n temporal processing of past and future input with LSTMs (long short-term memory)\\n multi-head attention: captures long-term temporal dependencies\\n prediction intervals: per default, produces quantile forecasts instead of deterministic values\\n\\n\\n\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a35ded60-2e63-44b0-b35d-090051e55b23', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n**raise prediction model**\\n\\n如果是有text的就加上text model layer; 如果没有的就用lightgbm\\n\\n**impact**:\\n\\n\\n**situation**: \\nbackground of company; we sell service or platform for startup to raise capital \\nit is mentioned on my ML brainstorming session; it generates 2 projects; after vs. before deal launch; one is time series; after deal launch; which can’t be done: \\n\\n**problem to solve:** have a tool to provide insight about companies; they don’t want to chase random boba shops \\n\\n\\n**Results**: \\n1. (>90% accuracy and F1 score)\\n1% more if the deal has text documents\\nrecall > 90% more important because they don't want to miss good deals;\\n1. pipeline: At the end state, **they only need to provide 2 numbers to identify company id on SEC website, pick which model to use regA or CF**; and the program will do everything and give them a prob of success; \\n2. Users are generally really excited about it;  I ask them to track how many calls it saves them to make; saved for reaching out;  raise success rate before vs. after using this tool \\n\\n\\n**Action**: **challenge**: **no existing outcome data of the capital raise**; **no existing feature internally**\\n\\n1. I scraped **dependent variable** from internal data, kingscrowd, 1-K form, and manual search; \\n\\nafter looking at features, I realized it can be classification on whether its going to be a success raise; also possible to be a regression problem on how much progress it can make on its goal; \\n\\n2. **predictor variables** from SEC, \\n\\n3. transformed data, train the model, deployed it by building a pipeline and a Django app;  includes textual data \\n2. **innovation**: esemble model； \\n\\ngrid search weight values between 0 and 1  with 0.05 interval for each ensemble member\\n\\nstacking: train the weights between the two models; **meta learner model is just a logisticregression， StackingClassifier；stacking是指用完全相同的feature输入不同model，所以我这个应该不是stacking**\\n\\n\\n**Metrics** & **measurable**: define the problem and the tool, they want to know if its a good company based on existing data; \\n\\nwhere do u find leads? what kind of companeis do u chase? \\n\\nMajority of them are companies that already filed SEC docs; they are highly likely to convert; they are likely to have well structured data\\n\\nsuccess standard is low: better than nothing; I ask them to track how many calls it saves them to make; saved for reaching out; raise success rate of the company they called should be higher than before; pretrained\\n\\n**具体工程上的实现**\\n1. s3 to store docs; django app; unit testing to see if the preprocessing pipeline is working; \\n2. unit testing code to make sure the feature pipeline is clean and tidy; if the xml is in our aws s3, its fast; \\n3. django app; hosted on aws EC2\\n\\n\\n**Tie in:**\\npretty typical ML project from start to end; the idea was triggered by my brain storm session; so its very practical and brings immediate value \\n\\n\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1d43540a-2eb3-4e2b-a2fa-697dc55053a6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**nlp annotation tool (hard project)**\\n\\n**impact**:\\n\\n**situation**\\n\\n4-5 people working on onboarding; \\n\\nthey need help to alliviate their work burden; \\n\\n**bottleneck**: rate of onboarding is limited by their work efficiency; \\n\\n**iterative process:** fully automation is too hard and not feasible, so I moved to what I can do; \\n\\n遗憾做不到的东西\\n\\nsimilarity based stuff; annotate Q labels next to each part of the documents; \\n\\nimpossible project to possible via rounds of meetings\\n\\n \\n**Results**\\n\\n[[multi label classification]] \\n\\ninput a pdf; and deal id\\n\\nresults are 3 folds: word doc with all Qs; a csv file; a highlighted pdf  \\n\\nat the end for Q blocks; close to 90% accuracy; 95% recall; precision is lower 80%+’ they care more about recall, meaning they can tolerate false pos; its ok if the model tells him that u should include this Q but it needs not; \\n\\n**Tie in:**\\n\\nshows my ability to make compromise and make useful tools \\n\\nshows my ability to digest and make use the STOA techniques to create tools that are useful \\n\\n**Metrics**\\n\\nbroader metrics: whether its better than their current work flow pure manually; reduce work time; \\n\\nspecific metrics: 80% accuracy at the classification model; \\n\\nits possible that tool can make things more complicated since u need input ur stuff into the tool and read the output of the model; \\n\\n**Action**\\n\\n**4 stages on this project**\\n\\nfirst stage is keyword highlighting: reading 40 page pdf vs. reading it with highlighted keywords to help fast human processing; techniq challenge is the ocr work; sequential OCR package pipeline; read image into word doc then highlight and save as pdf \\n\\ntopic modelling, unsupervised learning techinques to ; USA related; basic demo graphic info; company legal representation info; \\n\\n14 categories of content; jurisdiction; \\n\\n[[about tokenization]] \\n\\n**2nd stage was transforming txt generation problem to a multilabel classification on questions;  70+ questions** \\n\\n收集所有过去的问卷，把他们和input doc匹配上; input doc也是从g drive里爬下来的；\\n\\n预处理：\\n\\ncount sentence; 发现很多重复的句子出现在所有的doc里，直接删掉了；\\n\\nlower cases; lammatization; remove stop words; 虽然理论上deep learning不需要，但其实是有用的；\\n\\n优化词典vocabulary; 直接count了我的data里常用词，看bert vocab里有没有没有就加上\\n\\n把tabular feature加进来，用句子的方式\\n\\nscraped our own webpages to match the Q labels to pdfs; \\n\\n**discussed a new naming system with sales team**\\n\\nit took me a while to collect and organize the input data; its a mess in our google drive from the sales team; and match them into the questiionnare label output; \\n\\nthe model was a finetuned longformer model; I tried BERT first but its performance is worse; \\n\\ninvolves lots of detailed work about improving the model like removing identical text across the documents;  combine text and tabular features; \\n\\n我对模型做了改动让他能处理multi label classification problem; \\n\\n**3nd stage was  multilabel classification on question blocks (40+ question blocks)**\\n\\ndeployed with django and AWS; \\n\\n**4nd stage improved by pretraining; took me a while; lots of tech engineering challenge; no GPU, I asked trial credit from AWS** \\n\\nmeetings on needs: need better \\n\\ncollect data; which are inhouse documents; \\n\\nmany tricks like removing identical content in documents\\n\\ntrain and deploy\\n\\nfintuned BERT; to longformer model; 3-4% increase \\n\\npretrained myself - increase 1-2% \\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e688398c-44ef-4f67-abbc-5f822e2d5d98', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**investor insight score**\\n\\n**impact**:\\n\\n[[007 gradient boost and xgboost]] \\n\\n**situation**\\n\\nneed recommender system; but its not allowed for financial products; we can get around that by helping issuers pick better investors; \\n\\nmemory based CF不行，因为user之间的similarity不靠谱，user feature太少；而且这种non paramatric method准确度比较低\\n\\nmodel based CF 也不行，因为我就没有item-user matrix\\n\\n**Metrics**\\n\\nbaseline should be better accuracy than random guessing: 30% are funded;  so baseline is 70% accuracy; \\n\\n80% accuracy; \\n\\n**Action**\\n\\nxgboost model with tabular data; user related, deal related, company related, and interaction related features\\n\\n**challenge**: \\n1. **surprisingly few features I can use;** **main work is on feature engineering; kinda like kaggle challenge problem**\\n2. collaborating with other engineers ; put my model into docker; write test script to call server and local api 100 times vs. the result from my local model \\n\\n**mismatch between training and testing set** \\n\\nmany features needs to be computed like gender and ethnicity; \\n\\ndistance to the company; same state or not; geo location; find ways on how to use it; home state; \\n\\nstore the data on our snowflake databse; its a batch process; it reruns every data; \\n\\nruby on rail products; \\n\\nput everything in docker; **tests on local clients and serve clients;** \\n\\n**test with api tool;** write test script to call api 100 times vs. the result from my local model \\n\\n**Results**\\n\\nbatch inference every evening; \\n\\nput into our product as a feature; no good way to track results yet since its our using it; \\n\\n88% + accuracy and F1 score; recall and precision are balanced according to business needs;  \\n\\n**Tie in:**\\n\\nits not an internal tool; it faces our clients; \\n\\nexperienced testing to launching process; \\n\\ntest with api tool; write test script to call server and local api 100 times vs. the result from my local model \\n\\ngreen packer; 救急；\\n\\nThere was a data drift problem with my production ML model where we had a very unique large client whose investor features have too many missing values. I fixed the problem by retraining a simpler model for that client with fewer features.\\xa0\\n\\nthey want to access it on our web tool; that costs us some time; 我给了他们一张csv表救急\\n\\ni trained a new model with fewer features, add a model switcher in the API; \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c916ffed-6ea4-4d37-a8f0-1c57fb2053c3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nSocial media项目\\n\\nbackground/problem: Mkt这块，研究人们如何表达自己的人设的东西很多，比如我最近买了什么，人们会不会说，用什么方式来说，叫word of mouth。但研究别人如何来看待这些人设的很少。所以我这个项目是研究audience如何看待人们在socail media上发的东西。\\n\\n我觉得线上和线下的一个主要的区别是，social media上一个人的身份是很多元的，比如一个人可以展现他是父亲，是球迷，是摇滚乐粉丝，但线下其实其他人很多时候看不到这种多元化的人设，比如我一个经常和我一起上课的同学，可能我只能看到他是学霸这一点，并不太清楚他喜欢什么音乐这些。所以我研究线上，人们如何感知这种多元化的人设。两种研究方法：爬的大数据 和 lab study\\n\\n1. 人设太多了不好；八点几个label之后，印象就下降了；label 多了人们会觉得假\\n1. 爬的twitter上比较大的数据，我用word embedding，google之前开发的一个自然语言处理的软件包，来看每个人发的所有的posts，他的内容0的相似度，每个用户都会有一个相似度的打分，**相似度越低，好友越多**。\\n3. 我有个经济系的博士朋友爬了一些微信influencer account的数据，我找他要了一部分数据，用的topic modeling 找出每篇文章的topic 的分布，topic dispersion 越高就是越分散。发现一个号早期发的文章相似度越高，粉丝越多；后期反过来；\\n4. personal tag越多，好友越多 \\n\\n深入的nlp的方法我也有尝试，比如尝试了用bert作为一个feature extraction的工具，用pretrain的中文word embedding数据然后train最后一层layer来满足我的regression的cost function需求；当然这更多的是优化了regression prediction accuracy，不能提供多少insight\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8fbbbc56-f3d5-4386-a44d-497d3107dc4c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Sequential 项目**\\n\\nbackground: Hedonic adpataion是啥\\n\\n我们的研究主要研究对于负面的事件，人们会不会像正名事件那样也会逐渐适应。结果是，负面事件人们不会适应，会觉得越来越不爽。打个比方，外卖\\n\\n三类研究方法：有lab里的，有field data，多伦多地铁站，也有用爬虫爬下来的大数据\\n\\nLab: dictator game，独裁者游戏，一组人收到5毛钱，一组人收到1毛；\\n\\n大数据：连续pm2.5 高，人们的情绪反应的变化。自然语言处理的package，ntlk, stanford nlp,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='10476124-32bc-440e-a09b-1ef80b63eab9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nProbability and Information\\n\\n- What is Random Variable\\n-A random variable is a mathematical concept that represents a quantity or object that depends on some random event or process. For example, the number of heads in 10 coin flips is a random variable, because it depends on the outcome of each coin flip, which is random. A random variable can have different values depending on the possible outcomes of the random event or process.\\n\\n- What are some of famous Probability distributions: Bernoulli, Multinoulli, Gaussian, Laplace, Exponential, Mixture-of-Distributions(GMM)\\n- Expectation, variance, covariance and correlation\\n- How would you find the 95% confidence intervals for a given mean?: find 2sigma\\n-\\n- What is Information and Entropy?: Learning that a less likely event has happened is more informative then learning that a likely event has happened. Therefore we measure, self-information of an event given the prob distribution as I(X=x) = -log(P(X=x)) nats while Expected information in an event drawn from prob distribution is Entropy, H(x) = Ex~P) = -∑(p_i*log(p_i))\\n-\\n- Distributions that are nearly deterministic have low entropy otherwise high entropy\\n- What is KL Divergence: If we have two separate prob distributions P(x) and Q(x) over the same random variable, we can measure how different these two distribution are using the KL Divergence; Mathematically KL(P||Q) = Ex~P/Q(x)))\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0a203ae9-cb17-4d4f-885e-cee16e260eb2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nOptimization\\n\\n- What is difference between MLE and MAP estimation\\n-The difference between MLE and MAP estimation is that MLE is a frequentist method that maximizes the likelihood of the data given the parameters, while MAP is a Bayesian method that maximizes the posterior probability of the parameters given the data and a prior distribution12.\\xa0MLE does not take into account any prior information about the parameters, while MAP incorporates prior beliefs that can regularize the estimation34.\\xa0Both methods produce point estimates, but MAP can also be seen as a special case of MLE when the prior is uniform5. MLE and MAP can have different results depending on the data size and the prior distribution.\\xa0Generally, MLE is preferred when the data is large and the prior is not informative, while MAP is preferred when the data is small and the prior is informative\\n\\n\\n- Difference in optimization and regularization\\n- Optimization: refers to task of either minimizing or maximizing some function f(x) by altering x\\n- Regularization: Any changes made to the ML algorithm such that its performance on training data do not changes, but it improves for unseen test data. For instance, Regularization methods aim to mitigate this issue of overfitting by introducing additional constraints or penalties on the learning algorithm.\\n- What is the difference in convex and concave optimization?\\n- What are the benefits of convex function for optimization?\\n- Convex optimisation algos are applicable only to convex functions(function for which Hessian is Positive semi definite everywhere(Eigen values are positive or 0)). Such functions are well-behaved because they lack saddle points and all their local minima are necessarily global minima. Most functions in DL are difficult to express as Convex optimisation\\n\\n\\n- What are some of the commonly used optimization technqiues in neural networks: Gradient based optimization like GD, SGD, RMSProp, Adam, Ada, Momentum, Nestrov. Nature inspired optimization like Genetic Algo, PSO, Differential Evolution, Bee Colony algos.\\n-\\n- **Gradient Descent (GD)**: This is the simplest and most basic optimization technique, where the parameters are updated by taking a step proportional to the negative gradient of the loss function at the current point12.\\xa0GD can be slow and inefficient for large datasets, as it requires computing the gradient over the entire data at each iteration3.\\n- **Stochastic Gradient Descent (SGD)**: This is a variant of GD that updates the parameters by using a single or a small batch of randomly selected data points to estimate the gradient at each iteration12.\\xa0SGD can be faster and more robust than GD, as it can escape local minima and handle noisy data better3.\\xa0However, SGD can also be unstable and oscillate around the optimal point4.\\n- **Momentum**: This is a technique that adds a fraction of the previous parameter update to the current one, creating a momentum effect that accelerates the convergence and reduces the fluctuations of SGD12.\\xa0Momentum can be seen as simulating a ball rolling down a hill, where the ball gains speed and momentum as it goes downhill3.\\n- **RMSProp**: This is a technique that adapts the learning rate for each parameter by dividing the gradient by a running average of the magnitudes of recent gradients12.\\xa0RMSProp can prevent the learning rate from becoming too large or too small, and can handle non-stationary and sparse gradients better3.\\n- **Adam**: This is a technique that combines the ideas of momentum and RMSProp, by using both the running average of the gradients and the running average of the squared gradients to update the parameters12.\\xa0Adam can be seen as a general and effective optimization technique that can handle various types of data and problems3.\\n- **Ada**: This is a family of techniques that adapt the learning rate for each parameter based on the history of the gradients12.\\xa0Ada includes methods such as AdaGrad, AdaDelta, and AdaMax, which differ in how they compute and update the learning rate3.\\xa0Ada methods can be useful for sparse and noisy data, and for problems with different scales of features3.\\n\\n- How do we update the parameters using Gradient descent and why: During back propagation, we update parameters by subtracting negative of gradient wrt input. Negative of gradient points in a direction which minimizes the value of cost function\\n- What is the role of learning rate in gradient descent?: The LR must be small enough to avoid overshooting minimum and gaining uphill in directions with strong positive curvature\\n[[037 vip gradient decent backpropagation]]\\n\\n\\n- What are Jacobian and Hessian Matrices of a function?\\n-\\n- What is the role of second derivative in optimisation?: This is important because it tells whether a gradient step will cause as much of an improvement as we would expect based on gradient alone. It measures curvature I.e. concave or convex\\n\\n- What is Saddle point?\\n-\\n- What is Taylor series expansion?\\n- What is Newtons method?: Its a second order optimisation algorithm\\n\\n\\n- What is conditioning number?: Conditioning refers to how rapidly a function changes wrt small changes in its inputs. f(x) = A(-1)x and A has an Eigen value decomposition then its condition number = max[i,j] abs(𝜆i/𝜆j), i.e. the ration of largest and smallest eigenvalue. In multiple dimensions, there can be variety of second derivatives at a single point, because of second order derivative in each direction. The conditioning number of hessian measures how much the second derivative vary. Poor conditioning number implies poor performance of gradient descent as well.\\n- What are draw backs of using gradient descent?: GD fails to exploit the curvature information contained in the Hessian Matrix. Cost function of NN is neither convex or concave like sin(x) for x belongs to R with multiple maxima and minima\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='77186247-d2a9-4f18-9c66-9f3434c4b451', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGradient Descent, LR, Activation functions and Optimizers\\n\\n- ### Difference in GD, SGD, mini-batch GD\\n- GD: Optimisation algo, used to minimize some function by iteratively moving in a direction of steepest descent as defined by negative of gradient. Since in SGD, only one sample from dataset is chosen at random for each iteration, path taken by also to reach minima is noisy than vanilla GD. Goal is to reach minima fastly by taking greedy decisions\\n- Difference in GD, Momentum and Nestrov GD — **Momentum: Takes into account past gradients to smooth out the update by taking a exponential weighted average. Nestrov: It works on the principle of \"Look before you leap\"**\\n- How to tune LR learning rate in NN training? Are there any particular optimizers doings so?: Annealing LR: **Step decay, Exponential decay, Adaptive LR**\\n- Optimizer which adapt LR — Adagrad, RMSprop, Adam, AdamW=> Adagrad: **Parameters that have higher gradient or frequent updates, should have slower LR, so that we do not overshoot the minimum value, while params with lower gradient/infrequent updates should have higher LR. It decays LR very aggressively, so params will start reaching small updates because of decayed LR. It gets stuck when its close to convergence to minima**\\n- RMSprop: **It overcomes adagrad problem by being less aggressive to the decay of LR. It works by keeping an exponentially weighted average of the square of past gradients**\\n- Adam: adaptive momentum estimation. Combine idea of Momentum and RMSprop. Two params are beta_1, beta_2. Convergence through momentum, model do not get stuck in saddle point\\n- AdamW: Adam with Weight Decay Regularisation.\\n- Implement Softmax — Possible issue: Numerical stability for Overflow and Underflow\\n- What is difference between Cross entropy and Binary Cross entropy\\n\\t- Binary Cross entropy has only two possible options \\n- ### What is vanishing and exploding gradient?\\n\\t- Vanishing and exploding gradients are two common problems that occur when training deep neural networks with gradient-based methods. They refer to the situation where the gradients of the loss function with respect to the network parameters become either very small (vanish) or very large (explode) as they propagate through the layers of the network.\\n\\nVanishing gradients can cause the learning process to slow down or even stop, as the updates to the parameters become negligible. Exploding gradients can cause the learning process to diverge, as the updates to the parameters become too large and unstable.\\n\\n**There are several factors that can contribute to vanishing and exploding gradients, such as:**\\n\\n- The choice of activation functions: Some activation functions, such as sigmoid and tanh, have derivatives that are bounded between 0 and 1, or -1 and 1, respectively. This means that when the input values are large or small, the derivatives approach zero, which can cause the gradients to shrink as they multiply across layers. This is especially problematic for deep networks with many layers.\\n- The choice of weight initialization: If the weights are initialized too small or too large, they can cause the outputs and gradients of each layer to become too small or too large, respectively. This can lead to either vanishing or exploding gradients, depending on the direction of the multiplication. A good practice is to initialize the weights such that the variance of the inputs and outputs of each layer is roughly the same.\\n- The choice of optimization algorithm: Some optimization algorithms, such as gradient descent, can be sensitive to the magnitude and direction of the gradients. If the gradients are too small, the algorithm can get stuck in local minima or plateaus. If the gradients are too large, the algorithm can overshoot the optimal solution or oscillate around it. A good practice is to use adaptive learning rate methods, such as Adam, that can adjust the learning rate based on the history of the gradients.\\n\\n**There are also several techniques that can help mitigate or prevent vanishing and exploding gradients, such as:**\\n\\n- Using non-saturating activation functions: Some activation functions, such as ReLU and its variants, have derivatives that are either 0 or 1, or a constant value, respectively. This means that they do not shrink or grow the gradients as much as sigmoid or tanh, and can avoid the saturation regions where the derivatives are close to zero. However, they can also introduce other issues, such as dying ReLU or leaky ReLU.\\n- Using batch normalization: Batch normalization is a technique that normalizes the inputs of each layer by subtracting the mean and dividing by the standard deviation of the mini-batch. This can reduce the internal covariate shift, which is the change in the distribution of the inputs of each layer due to the updates of the previous layers. This can make the network more stable and less sensitive to the initialization and learning rate.\\n- Using gradient clipping: Gradient clipping is a technique that limits the magnitude of the gradients by either rescaling or truncating them if they exceed a certain threshold. This can prevent the gradients from becoming too large and causing numerical instability or divergence.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2779f310-7f87-4d7e-8992-35a349bc388e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nWhat is ReLU activation? Are there any pros and cons of using it?\\n- Pros: Avoid & rectifies vanishing gradient. Less computationally expensive than tan or sigmoid. Cons: Can only be used within hidden layers. Some gradients can be fragile during training and can die. It results in Dead neuron. The neurons which go in that state, will stop responding to the variation in error. \"Dying ReLU\" problem: If a neuron\\'s output is consistently negative, the gradient is consistently zero, and the neuron effectively becomes inactive. Remedy is to use Leaky Relu or p-ReLU, Its an attempt to fix \"dying ReLU\" by having small negative slope when x is negative\\n\\nSome of the pros of using ReLU activation are:\\n\\n- It is non-linear, which means it can capture complex patterns in the data.\\n- It is computationally efficient, which means it can speed up the training process of the network.\\n- It avoids the vanishing gradient problem, which means it can prevent the gradients from becoming too small and slowing down the learning.\\n\\nSome of the cons of using ReLU activation are:\\n\\n- It can cause the dying ReLU problem, which means it can lead to some neurons becoming inactive and never firing again.\\n- It can blow up the activation, which means it can produce very large outputs that can cause numerical instability or divergence.\\n- It is not differentiable at zero, which means it can cause problems for some optimization algorithms that rely on smooth gradients.\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='10005113-fb26-4fbd-9cb6-c7e88ef72317', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nWhat is squashing in Sigmoid activation\\n- Problem with sigmoid — Saturate quickly and kill gradients\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='98380bf1-497c-4fb7-bfe1-3d95f6803a19', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nWhat are some of the regularization techniques we can use while training a NN? — Data Augmentation, Using Dropout, Early Stopping, L1/L2 regularization, Batch Normalisation, Skip-connection: allows gradients to bypass certain layers, helping to address vanishing gradients\\n- Can I use L2 regularization to deal with vanishing gradient problem while training NN? — This will worsen the problem, by shrinking weights towards 0. Its main purpose is to control the complexity of the model and encourage smaller weights, rather than directly addressing vanishing gradients.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='30b40c6b-bc56-4e77-8303-aff54a5e9913', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHow to create ensemble of NN, in a practically feasible manner? — Using dropout creates an ensemble of NN\\n- What if all weights are initialized with same value in a NN? — Each hidden unit gets same value and same activation output then. Same derivative of cost function during back prop. No learning/Under-fitting\\n\\n- How does weight initialization impact the training process of a neural network? What are some of the techniques? — It can affect how quickly the network converges to a solution, the quality of the solution reached, and whether the network gets stuck in local minima. Xavier/Glorot Initialization: This method initializes weights with values drawn from a Gaussian distribution with zero mean and a variance, He Initialization, Uniform Initialization\\n- What if I initialize all weights to 0? — **Initializing all weights to zero can cause issues since all neurons will learn the same features and gradients will be equal.** This results in symmetric weights and no learning capacity.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d9479c77-ea62-486c-9641-e0b1ea0e54b2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nDescribe the purpose and benefits of batch normalization in neural networks. — It normalize the activations of hidden layer, so that the weights of next layer can be updated faster. It tackles the problem of internal covariate shift\\n- What is internal covariate shift — It refers to the change in the distribution of activations within a neural network layer during training due to the changing input distribution.\\n- Can we do not use Batch Normalization in RNN or Transformer architectures — Batch Normalization was originally designed for feedforward neural networks and convolutional neural networks (CNNs). It aims to normalize the activations of each layer by adjusting them to have zero mean and unit variance. RNNs process sequential data over time steps. **Traditional BN treats each time step as a separate batch, which can break the temporal dependencies in the data. Techniques like \"Layer Normalization\" or \"Sequence Batch Normalization\" have been proposed to address this issue.**\\n- **What will be the last layer activation function and loss-function for multi-label and multi-class classification — Multi-class: Softmax with cross entropy, Multi-label: Sigmoid with Binary cross entropy**\\n- What is Catastrophic forgetting — The process of learning new information interferes with or overrides the previously learned information. Common in Sequence learning networks. As the network adapts to the new tasks, the weights and representations that were useful for the previous tasks might be altered → degradation in performance\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6413eb76-a35a-4a87-96b1-d7089dddcd61', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nTransformer\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c1078ba3-de06-4379-84ea-13a456390212', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nWhat challenges do RNNs face when processing long sequences, and how does the Long Short-Term Memory (LSTM) architecture aim to address these issues?\\nOne of the main challenges that RNNs face when processing long sequences is the\\xa0**vanishing gradient problem**. This means that the gradients of the loss function with respect to the network parameters become very small as they propagate through the layers of the network. This can cause the learning process to slow down or even stop, as the updates to the parameters become negligible. This also makes it difficult for the network to capture long-term dependencies in the data, as the information from distant time steps gets diluted or lost.\\n\\nThe LSTM architecture aims to address this issue by introducing a\\xa0**gating mechanism**\\xa0that enables the network to learn when to retain or forget information from previous time steps. The LSTM consists of four components: an input gate, a forget gate, an output gate, and a memory cell. The input gate decides how much of the new input to add to the memory cell. The forget gate decides how much of the previous memory cell to keep or discard. The output gate decides how much of the current memory cell to output. The memory cell stores the long-term information that is passed across time steps.\\n\\n\\n- Transformers use attention. Attention helps to draw connections between any parts of the sequence, so long-range dependencies are not a problem anymore.\\n- What is attention?\\n- What is the purpose of attention mechanisms in neural networks?\\n-\\n\\n- Why do we need Positional embeddings in transformer? — Since transformer model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension as the learned word embeddings, so that the two can be summed.\\n- What is input to a transformer model for MT?\\n- **How can I tokenise the text before training the Transformer model — Areas of concern: 1) Unit Selection, 2) Vocab Size, 3) handling languages with complex morphology, 4) out-of-vocabulary words, 5)Word level or White space tokenisation, 6) Character leve**l: Text is tokenized at the character level, where each character becomes a token. Used in ELMO using 1-D convolution, 7) Sub-word level: To handle rare or OOV words/character sequences, 8) Byte-pair encoding: BPE works by iteratively merging the most frequent pairs of characters or subword units in the text. It starts by treating each character as a token and then merges pairs of tokens based on their frequency until a specified vocabulary size is reached. Its a greedy algo. 9) Word piece: WordPiece also breaks down words into subword units, but it performs the splitting based on maximizing a predefined criterion (such as likelihood or entropy) while maintaining a specified vocabulary size. Its an optimal algo.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5d2b586d-05a0-4db7-bd01-ffa94e740f84', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nWhat different kind of attention mechanisms are utilized in Transformer?\\n- Self-attention in Encoder: All of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder\\n- Cross-attention in Decoder: The queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence\\n- Why do we use Multi-headed self attention?\\n- What is masked multi-headed attention?\\n- What are QKV in attention(self and cross)?\\n- How to create mask for masked multi-head attention? — Create upper triangular matrix with negative infinity values and apply exp operation i.e. all values in the input of the softmax which correspond to illegal connections\\n- What are different decoding strategies for language modelling task — 1) Greedy decoding, 2) Beam Search: This strategy maintains a fixed number (beam width) of partial sequences in parallel. At each step, the model generates multiple candidate tokens and keeps the top candidates based on their probabilities, 3) Random Sampling, 4) Tok-K: This strategy limits the sampling to the top-k most likely tokens at each step. This allows for controlled randomness while ensuring that the generated sequences are coherent and contextually relevant. 5) Top-p/Nucleus sampling: It keeps the top-p most likely tokens at each step, where p is a probability threshold. This technique helps in maintaining diversity while controlling the number of options considered, 6) Temperature Scaling: Using Softmax temperature. Higher temperature values introduce more randomness, while lower values make the selection more deterministic.\\n- Why do we use Layer Norm in Transformer and not Batch Norm?\\n- In batch normalization, we use the\\xa0_batch statistics_: the mean and standard deviation corresponding to the current mini-batch. However, when the batch size is small, the sample mean and sample standard deviation are not representative enough of the actual distribution and the network cannot learn anything meaningful.\\n- As batch normalization depends on batch statistics for normalization, it is less suited for sequence models. This is because, in sequence models, we may have sequences of potentially different lengths and smaller batch sizes corresponding to longer sequences.\\n- How BERT and GPT are different?\\n- What tasks BERT was pre-trained upon?\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='107e9a28-e973-4f86-9d79-45540e7d3b5a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nWhy do we consider bi-directional context in BERT and not in GPT?\\n\\nWe consider bi-directional context in BERT and not in GPT because bi-directional context can help the model understand the meaning and relationship of words in a sentence better than uni-directional context. For example, if we have a sentence like “He saw a bear in the woods”, bi-directional context can help the model infer that “bear” is a noun and not a verb, and that “he” is the subject and not the object of the sentence. Uni-directional context, on the other hand, can only use the words before or after the current word, and may miss important information from the other side.\\n\\nBERT is a bi-directional model because it uses a masked language modeling objective, which randomly masks some words in the input and predicts them using the context from both sides. This allows BERT to learn from the whole input sequence at once, and capture long-range dependencies and complex semantic relationships. BERT also uses a next sentence prediction objective, which predicts whether two sentences are consecutive or not, using the bi-directional context of both sentences.\\n\\nGPT is a uni-directional model because it uses a causal language modeling objective, which predicts the next word in the input given the previous words. This allows GPT to learn from the left-to-right context of the input, and generate coherent and fluent text. However, GPT cannot use the right-to-left context of the input, and may miss some information that is relevant for the prediction. GPT also uses a transformer decoder, which uses an attention mask to prevent the model from seeing the future words in the input.\\n\\n\\n- What is KV cache? — The KV cache in Transformers is a memory-efficient technique that stores key (K) and value (V) vectors from the encoder's output to optimize the attention mechanism during decoding in autoregressive language models, leading to faster and more efficient generation of tokens.\\n- What is the time complexity of attention mechanism?\\n- Can we reduce this time complexity?\\n- Why there is Shift right token operation when giving inputs to decoder — Input to the decoder is the target sequence shifted one position to the right by the token that signals the beginning of the sentence. The logic of this is that the output at each position should receive the previous tokens (and not the token at the same position, of course), which is achieved with this shift together with the self-attention mask.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0c7fcddf-96c6-4b7e-8944-d0f9ac9ba3ff', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nDiffusion Models\\n\\n- Most of the questions along with results are mentioned with response here in\\xa0this article. Noting down the questions ahead.\\n- Are there any assumptions for noise functions added at each step of Diffusion?\\n- Why only the noise added at each step is gaussian. Can I use any other probability distribution of noise?\\n- Why diffusion models perform better than GANs, Is there any intuition?\\n- What is the role of skip-connections in U-net architecture and how are they different from skip-connections in ResNet?\\n- Why the variance schedule have small value(i.e. β_t << 1) or why the step size is small?\\n- Why we need to do so many reverse steps to obtain clear image. What benefit it provides?\\n- Let's compare different Generative architecture such as VAE, GANs and Diffusion. When should one prefer to use one architecture over the other?\\n- How is stable-diffusion different from DALLE-2 or IMAGEN model architectures. What added advantage do it provides?\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2fbe8911-9241-4959-88cc-ceac6b65bdde', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nTell me a time when u run A/B test and discover insight\\n\\nTell me a time ur analysis uncovered insight\\n\\nTell me about your ETL experience \\n\\n\\n\\n\\n**每一类问题都可以clarify**\\n\\n\"What do you mean by favorite product? \\n\\nDo not go deep into details at first, especially the technical details. Keep your answer clear and straight to the points.\\n\\xa0Give a high level introduction about the example and focus on the impact and results first. If the interviewers are curious about how you have achieved the results, they will ask follow-up questions\\n\\n- Good Communicator\\n- Confidence & Humbleness\\n- Go-Getter\\n- Ability to Follow Rules & Protocols\\n- Independent & Autonomous\\n- Works well with Others\\n- Problem Solver\\n- Commitment to the Role & Company\\n- Leadership Qualities\\n- Positive Representative of the Company\\n\\n不要发散，比如腾讯的那个要付费的；\\n\\n不能weak;\\n\\n不要有大概，似乎，需要bluff；老板会不耐烦；\\n\\n不要用长句；人们希望对方更快更直接，宁愿对方听不懂来问；\\n\\n犀利的话，不要去柔和，这样体现自己的心理承受力；\\n\\n先结论，再解释；\\n\\n**要有树一样的定力；即便有人开玩笑，也不要带进去了，这根线要绷着**\\n\\n**如果对别人有威胁的，就谦虚一些，如果对别人没有威胁的，就自大一些**\\n\\n一定要有切换力：现在不说我了，说说你；\\n\\n**多用1，2，3；**\\n\\n**回答问题的时候，用一二三，再具体解释**\\n\\n\\n\\n\\n\\n1说话要留气口，提供reaction的空间:\\n面试在脱离了“考试”的性质后,毕竟是一场对话。面试官也不是npc，不是工具人，你需要保持一定的互动，ta才能听得下去。\\n同时,留气口的互动就像是冥想时的钟声一样,起到了时不时把ta的注意力拉回来的作用。\\n 2讲故事也要top-down:讲故事的时候,通过回忆的方式边回想边讲,很\\n容易说的很\"线性\" (linear),但对于听者来说,\\n没有对故事后面进程的 expectation 或者\\nvisualization，根本就没办法跟上你的思路。\\n具体到讲故事的\"术\"上,我们可以做的就是:准备故事的时候要用框架(比如常用的star);并在开始讲之前,用一两句话把故事大纲总结清楚,让面试官知道接下来两分钟的期待。\\n\\n 3善用\"flag words\":\\nflag words这类词是指,能让听者清晰地知道马上要期待什么的一种表达方式,从而减轻ta的认 知负荷（cognitive load)。\\n比如对于时间和数字的指示。举个例子: \"After I figured out what the key challenge was, I decided to dissect the big problem into three smaller steps.\" 讲完这句，面试官不仅了解了你的动机,还能有所准备:你马上要说三点非常重要的内容了。\\n4尽可能保证选词和语调的精确度：\\n有很多常见问题例如: \"你的三个最大的优点/缺点是什么\",这类问题可能不需要立刻讲故事,但 肯定需要 concise的 key words 来总结，需要提前好好打磨你要说的具体词汇/短语,并且语气要着重强调。这点可能没法做到立竿见影，但确实是咱们长期可以练起来的。\\n\\n\\n\\n Things you want to show:\\n - Show introspection (identify your role in creating the problem)\\n - Show that you act on introspection (identify the lesson)\\n - Show empathy (your colleagues are your ingroup)\"bias towards action”\\n\\n\\n\\n对于技术问题，先答最主流的答案，再详说自己的理解，不然人家会以为我不知道主流答案；比如how to deal with overfit: regularizaiton\\n\\nCan you tell me more about what XYZ is up to? What\\'s the tightest bottleneck at the moment?\\n\\nCan you tell me more about what\\xa0*your team at XYZ*\\xa0is up to?\\n\\n自己不确定答案的项目类问题，请教一下这种比较specific的问题 或者你直接问问当时聊的mgr有没有什么建议\\n\\n别人在介绍自己业务的时候，如何体现自己的passion\\n\\nThank you for updating me on my candidacy. I truly appreciate the opportunity and It\\'s been a pleasure getting to know the growth operations\\nteam and learning more about Gorgias.\\nWhile this role did not work out, I hope it is okay we can stay connected. I would like to check in with you in the next two months or so and see there\\'s another opportunity that might better match my skill set and qualifications.\\nAgain, thanks for everything and I wish you and the team all the best during this busy recruitment time.\\nSincerely,\\nAngela\\n\\n**prepare question: what would you approach solving a real problem**\\n\\n1. Did you begin by stating what the goals of the feature are before jumping into defining metrics? What is the point of the Mentions feature?\\n2. Are your answers structured or do you tend to talk about random points?\\n3. Are the metrics definitions specific or are they generalized in an example like “I would find out if people used Mentions frequently.”\\n\\n1. Why are you passionate about voice messaging services?\\n2. Could you tell me about a time you experienced a technical challenge when working with relational databases? What did you do to fix it?\\n3. Could you tell me about the chatbot that you built for the city of boulder?\\n    1. What problem were you trying to solve and how do you think your product solved it?\\n    2. Where/how did you collect your data? What problems did you run into?\\n    3. Did the fact that the inquire boulder faq page is dynamically generated complicate things?\\n    4. Walk me through the process that you used to clean your data\\n    5. Which libraries did you find helpful when tokenizing your raw data?\\n    6. How did you evaluate the performance of your model?\\n    7. What is lemmatization, and why did you use it versus stemming?\\n        1. https://stackoverflow.com/questions/1787110/what-is-the-true-difference-between-lemmatization-vs-stemming\\n        2. Lemmatisation is closely related to stemming. The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications.\\n    8. Describe how and why you used bag of words and TFIDF in your model\\n    9. What methods did you use for similarity matching the user questions to the answers in the dataset, and why? What approaches were you deciding between?\\n    10. Describe your tech stack--could you talk a bit about Google dialogflow and how you used it as a knowledgebase intent, as well as the data pipeline from a customer query to the answer they get?\\n    11. How did you productionize and deploy your chatbot?\\n    \\n    change your resume according to the JD; IF the company says random forrest, put it in your resume; \\n    \\n    be excited in the process\\n    \\n    show you are able to grow; familiar with the latest packages\\n    \\n    be prepared with the \\n    \\n    ask question: tell my about your company culture in 2 - 3 words\\n    \\n    ; 如果别人问r u familiar with aws analytics. 你不知道，你可以说 i am more familiar with google analytics, they are similar \\n    \\n    reach to hiring manager if possible, so you can skip the recruiter; \\n    \\n    get to face to face asap \\n    \\n    when networking, ask ppl how they get to the role; what is their exp?\\n    \\n     \\n    \\n    **我有精读能力，我有研究能力，更重要的是，我有“读不懂但可以读完，而后反复读，进而读的更懂”的能力**\\n    \\n    需要体现自己的妥协，也要体现自己坚持对的\\n    \\n    show that it is less likely to happen in the future \\n    \\n    show u can deal with difficult ppl because they always pop up \\n    \\n    what type of weaknesses cannot be told: only talk about weakness that you have addressed \\n    \\n    \\n    \\n    **I had a tendency to not checking with ppl as often as I should; I set a timer with different ppl about that** \\n    \\n    need all infor before action, lead to procrastination\\n    \\n    self determine deadline and make it accountable to other ppl \\n    \\n    broad knowledge and \\n    \\n    sit down and solve problems; find solutions; learn new tech and skills and implement it fast\\n    \\n    overthinking and focus too much on details; assumptions behind all analysis\\n    \\n    don\\'t blame yourself: I didn\\'t hit my kpi; maybe we can have the conversation in private;\\n    \\n    don\\'t convey you accept abuse', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3fd66833-ce1c-441e-939e-e1d5ec69cc2e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n\\n\\n\\n\\n\\nim familiar with the conceptual framework of mkting; have interned in marketing research agency before; I am familiar with what they are doing in general \\n\\n\\nSTP：segmentation, targeting, positioning \\n4P\\n\\n\\n!Untitled\\n\\n\\n\\n\\n\\n\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3358e0c4-56c3-42e4-839a-b95786b11219', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n6 Common Attribution Models\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c3bb950e-0f14-4438-b3d3-66aa94630e73', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n1. Last Click\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b18216d8-808a-48e4-b131-eefd166a7fc1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n2. First Click\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2dcec1ac-9136-4455-a76b-460a65b284c9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n3. Linear\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='28dbb69d-fe18-4271-87c7-3fcc31dc1746', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n4. Time Decay\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bcd258e9-a61c-41b4-b8c3-e3315fdaa628', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n5. Position-Based\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='04bc5ec9-db09-447e-aa35-1dbea0e5bf5b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n6. Data-Driven (Cross-Channel Linear)\\n\\nGoogle Analytics 4 has a unique data-driven attribution model that uses machine learning algorithms.\\n\\nCredit is assigned based on how each touchpoint changes the estimated conversion probability.\\n\\nIt uses each advertiser’s data to calculate the actual contribution an interaction had for every conversion event.\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='91e68ffd-e6b1-45c7-bcce-e3a01ba9b88b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n\\n\\n\\nHow does a CPG create value?\\n\\nMost CPG companies pioneered the 5-step model to create value and drive growth.\\n\\n1. Perfected mass-market brand building and product innovation. This capability achieved reliable growth and gross margins that are typically 25 percent above non-branded players.\\n\\n2.Built relationships with mass retailers that provide advantaged access to consumers. By partnering on innovation and instore execution and tightly aligning their supply chains, FMCG companies secured broad distribution as their partners grew.\\n\\n3.Entered developing markets early and actively cultivated their categories as consumers became wealthier. This proved a tremendous growth source — generating 75 percent of revenue growth in the sector over the past decade.\\n\\n4. Designed their operating models for consistent execution and cost reduction. Most have increased centralization to continue pushing costs down. This synergy-based model has kept general and administrative expenses at 4 to 6 percent of revenue.\\n\\n5. Used M&A to consolidate markets and create a basis for organic growth post-acquisition. After updating their portfolios with new brands and categories, they applied their superior distribution and business practices to grow those brands and categories.\\n\\nHowever, several industry trends have disrupted this traditional value creation model so much that the CPG firms need to reinvent their business and operating models over the next decade to thrive in the industry. More on the industry trends are covered in the section \"CPG Market Outlook and Industry Trends.\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7887bfe8-899f-4711-8976-f93289e549c8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n[[my interview feedback]]\\n\\n[[ML interview 课程笔记]]\\n\\nMachine Learning-Powered Search Ranking of Airbnb Experiences\\n\\nCreating a Modern OCR Pipeline Using Computer Vision and Deep Learning\\n\\nMachine Learning-Powered Search Ranking of Airbnb Experiences\\n\\n注意这个baseline的设置\\n\\n**Implementation details:**\\n\\xa0In this stage, our ML model was limited to using only\\xa0*Experience Features,*\\xa0\\nand as a result, the ranking of Experiences was the same for all users. In addition, all query parameters (number of guests, dates, location, etc.) served only as filters for retrieval (e.g. fetch Paris Experiences available next week for 2 guests), and ranking of Experiences did not change based on those inputs.\\n\\n**Stage 2: Personalize**\\n\\nThe next step in our Search Ranking development was to add the\\xa0*Personalization*\\xa0capability to our ML ranking model.\\n\\n**Testing the ranking model:**\\xa0\\nWe conducted an A/B test to compare the new setup with 2 models with Personalization features to the model from Stage 1. The results showed that Personalization matters as we were able to\\xa0**improve bookings by +7.9%**\\xa0\\ncompared to the Stage 1 model.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6204a589-e368-4329-a5d8-12f281d6a207', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGeneral suggestions\\n\\nim not sure this right now; i can look it up; never say sth ur not sure\\n\\nfor more senior roles, u have to lead the conversation; \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a7c54634-7ba0-4560-b06f-ea18e71dce16', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nS**tage 1: get objective setup for the interview;**\\n\\n- define the goal and scope, any existing system? how are things done right now\\n- is an ML solution really needed?\\n- 这个模型的结果，是拿来干嘛的？谁会用到它\\n- how will the search be accessed? voice or audio or both?\\n- restrictions,  time/resource constraint; Explainability and simplicity might be valuable here.\\n- how do u evaluate the project in general\\n    \\n    For most consumer applications, there’s two high level business objects: **engagement and revenue.** You want to be a participant in working through how to translate these goals into system objectives. So if the interviewer says they want to increase user engagement, you can start a discussion breaking that down: new user retention, increasing session length, number of sessions, etc. \\n    \\n    Most products have a North star user action that’s used as a proxy for engagement or satisfaction, like the number of interactions with Instagram posts per day. You don’t need to get into specific KPIs yet (save that discussion for online evaluation.) \\n    \\n    At this stage, the goal is to spend a little bit of time gathering information and showing off some of your product sense. You might not need all of the information you get, but it will help you think about the problem and give you a bit of time to consider different angles.\\n    \\n- **scale and latency:** how fast do u want the inference to be; what segement of user will use this product?\\n    - Next, you can discuss product requirements.\\n    - Is this feed being calculated in real time or can you use batch?\\n    - Does this apply to all user segments? How many users are there?\\n    - How quickly do items get stale? (eg. do we need to content that’s only a few seconds old?)\\n    \\n    You also want to bring up technical scaling requirements (don’t make assumptions, it’s key to clarify this out loud):\\n    \\n    - How many items are you dealing with?\\n    - How many users are there?\\n    - What are the peak number of requests per second? Or day?\\n    - Hammer out timing SLAs (eg. we’ll incorporate user actions into recommendations within X seconds/minutes/hours)\\n    - There’s no need to explicitly ask that, but you should call out any assumptions you’re making. You can also clarify any preconceived notions you have about the solution. For example, you might call out that it seems important to be able to show recent posts from friends, but you could get clarity on what ‘recent’ means, posts from the last 5 minutes or the last 24 hours. What’s the oldest content you would recommend?\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8bac779b-5e61-467f-8044-395bf3f6b53b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nask about the design from the user perspective;\\n\\nare they going to see it on mobile or desktop; \\n\\nmodel might be different on mobile and desktop\\n\\n!Untitled\\n\\n!Untitled\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3c04b533-8197-4e85-b686-f9c3cd86cf1e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nS**tage 2 metrics: offline and online**\\n\\n[[Classification and regression metrics]]\\n\\ntarget level of performance: if we have existing system, use it as baseline; if not, use intuition, the model should be better than intuition \\n\\nsome metrics should not fall; some ones u want to increase\\n\\nOffline Metrics:\\n\\n- Precision (the fraction of relevant instances among the retrieved instances)\\n- Recall (the fraction of the total amount of relevant instances that were actually retrieved).\\n- discuss precision and recall which is more cost effective;\\n- Ranking Loss.\\n- Logloss.\\n- 不同的Device，recommend的数量和metrics不一样，手机上更重视top items;\\n\\nOnline Metrics:\\n\\n- different levels: look at the ad for some time 5 sec; click on the ad; third level of metrics like purchase;   可以是一个连续变量而不用是0,1；买了东西就是3，只看了一眼是1\\n- Use A/B testing to compare.\\n- Click Through Rates (CTR).\\n- Watch time.\\n- Conversation rates.\\n- what metrics has a strong correlation to business needs like revenue; what metrics has a strong correlation to user exp; engagement and retention;\\n- more long term metrics like monthly active user; weekly or monthly cohort retention rate graph\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6a56df95-ec3f-4609-8d82-db64e73c2828', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nStage 3: data collection\\n\\n- User’s interaction with the existing system (if any)\\n- Human labelers/Crowdsourcing\\n- Specialized labelers\\n- Synthetic data\\n\\n[[deal with unbalanced data]] \\n\\n依据经验决定，需要采样多少正负样本，1：2, 1:3都有可能\\n\\ndeal with sparse data: pca SVD; *Perform Feature Selection and Feature Extraction*; remove feature; \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f37348b2-894f-45ff-9f8a-47c63de074ce', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Data Source**\\n\\n****Automate the ETL process with DAGs****\\n\\nwhat are the data available; external sources; how do u collect;\\n\\nhow do u extract, transform, and load it?\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6ecbc8df-7cd5-4e3b-a954-1156fe6f7f4a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nS****tage 4. Exploratory data analysis****\\n\\ndimensionality reduction\\n\\n[[dimension reduction methods]]\\n\\n[[k means clustering]] \\n\\nexploratory analysis 的过程可以包括什么:\\n\\ncorrelation plot\\n\\nclustering \\n\\nfactor analysis to group features into 3 or 4 factors; eign value>1\\n\\nSince you have limited data, I would suggest a more inflexible statistical model that makes more assumptions about the data generating process.\\n\\n- Studying the features (mean, median, histogram, moments, etc.)\\n- Exploring the relationship between features (covariance, correlation, etc)\\n- Applying dimensionality reduction (such as PCA) to get rid of redundant parameters\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5cecaece-8009-48fd-a066-67eb0fca0fe1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nData preprocessing\\n\\ndata transform 的过程可以包括什么：\\n\\ndrop duplicates, NA, or use average to replace NA; \\n\\nremove outliers; \\n\\nnormalize the numeric data; \\n\\nencoding categorical data\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b5170e02-abe5-4a87-8abd-2d43c1b957f2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n**Standardize data**\\n\\nWe need to standardize our data (zero mean and unit variance) so a specific feature's magnitude doesn't affect how the model learns its weights.\\n\\n- Numeric values: demeaning, scaling (eg. dividing by the max or standard deviation), removing outliers (eg. values greater than the 99th percentile). There’s also binning, quantization etc.\\n- Categorical values: If a categorical value has a dozen or fewer values, you can use one hot encoding. Eg. the OS the user’s client is using. If you need to represent a variable with lots of possible states, it gets unwieldy to use one-hot encoding so you can switch to an embedding.\\n- Text: There’s a lot of valuable data in text. It’s good to have some basic understanding of NLP concepts so you can discuss how to present text to a model. It’s become common place to use embeddings for this, so understand how systems like BERT and Word2Vec can be used. You don’t need to be an expert, but a bit of knowledge of common techniques will make this so much smoother.\\n- Complex: How can you represent complex items like a user’s history of viewed posts as a numeric vector? Similar to text embeddings, you can build up a feature by representing each post as its own embedding, then combining the embeddings (through aggregation or concatenation.) For modern AI systems, it should be clear that embeddings play a huge role, so you should understand the math and concepts behind this.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='164722d1-cefd-49ea-99cb-c97e1ab50db9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n**stage 5: the architecture;**\\n\\n[[search engine design ]]\\n\\n[[Two stage recommender system]]\\n\\n[[Collaborative filtering & Recommender system basics]] \\n\\n[[time series 4 深度之眼课程]] \\n\\n用lightgmb就要做特征处理，比如把年月日当做特征\\n\\nFB prophet 模型外部变量的影响只能是线性的; LSTM先进一些\\n\\ntime series 最复杂的模型：MQRNN, deepAR， wavenet\\n\\n```\\n## 第一组特征： 历史数据最后一周的销量和促销活动的金额\\ndf_sample['sales_lw'] = df_sample.groupby(['dept'])['sales'].shift(1)\\ndf_sample['promotion_lw'] = df_sample.groupby(['dept'])['promotion_sales'].shift(1)\\nfeature_cols = feature_cols + ['sales_lw','promotion_lw']\\n\\n## 第二组特征： 上一个周期（即去年同一周）的销量和促销活动金额\\ndf_sample['sales_ly'] = df_sample.groupby(['dept'])['sales'].shift(52)\\ndf_sample['promotion_ly'] = df_sample.groupby(['dept'])['promotion_sales'].shift(52)\\nfeature_cols = feature_cols + ['sales_ly','promotion_ly']\\n```\\n\\nembed all of the transcript information sentence by sentence in a semantic vector space using BERT (bidirectional encoding representation transform), a transformer based-NN thats popular for embedding an understanding of sentences.\\n\\nstore the vectors for each sentence and their podcast id in something like faiss, fb’s library for efficient similarity search.\\n\\nthrow metadata in elastic search.\\n\\nembed the user queries into the embedding space and find candidate sentences and their constituent docs.\\n\\nalso search elastic search for podcasts matching on metadata. combine query results.\\n\\n每个stage的model 用到的feature不一样\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\nIt turns out that using WordCNN with trainable embeddings initialized by Word2Vec trained on customer tickets yields the best performance. This result makes sense, because the application is more like keyword spotting and activation than language model or sequence generation, and WordCNN models the former better and LSTM models the latter better. In other words, we care more about the presence of certain phrases and the order of words. Therefore, we decided to use WordCNN as our version 2 algorithm.\\n\\n!Untitled\\n\\nfasttext训练很快；他是训练的web embedding models like CBOW and skipgram\\n\\n***FastText***\\n\\xa0is a lightweight library designed to help build scalable solutions for text representation and classification.\\n\\nAfter considering the challenges in infrastructure, ML, and the adversarial space we settled for a\\xa0**multi-stage multi-model approach**\\n\\xa0where there are two stages and different models for each stage and type of spam. The first stage is used to identify the subset of photos that are most likely to contain spam; the models in this stage are tuned to maximize spam\\xa0recall\\n\\xa0while filtering out most of the safe photos. Essentially, this step changes the label distribution of the data fed into the second stage and in doing so it significantly reduces\\xa0ham\\n/spam class imbalance and removes many potential false positives (consider the following: we do not perform inference on a large subset of photos in the second stage, and the final set of false positive is only limited to the false positives generated by the second stage, which may or may not intersect with the false positives generated in the first stage). The second stage is where the actual classification of the content happens;\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ae288df0-19a3-4682-878f-215ea967e357', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nstage 1: recall\\n\\ntwo tower model: cocatenate user and prod emb after; ANN approximate nearest neibough to save time\\n\\n[[Two stage recommender system]] \\n\\nother recall funnels, like geo location; fuzzy matching elsticsearch from search query;  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='97c0ae7c-0477-4a5a-acc3-c0c5789c36d3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nstage 2: ranking; candidate generation model\\n\\nuser, product, statistic (how many ppl liked this video; how many videos this user liked in the past 30 days), and contextual features; \\n\\ncocatenate user and prod emb after before \\n\\n****Multi-gate Mixture-of-Experts (MMoE)****\\n\\n[[diversity recommender 多样化问题]]\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bb19b3c3-b9cb-4bfb-8484-e45c74f57d97', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nstage 3: rearranging  and diversity\\n\\ncold start方法：\\n\\n- content based info; look alike; 用类似的item的推荐，放到新item上\\n- multi arm bandit methods, 比如uppeer confidence bound; 一开始new item will have a high ucb score, so it will be recommended;\\n- 直接给流量倾斜，比如先给1000 impression，到一定阶段后recommender就可以pick up了\\n\\n[[冷启动 cold start 推荐系统]] \\n\\n新用户的召回list生成：geo; look alike; 兴趣标签；等他有了一些行为之后他的用户emb就成型了就可以用大模型处理了\\n\\n如果分数最高的前十条都是篮球，不可能全给他推篮球相关内容；\\n\\nDPP Improving the Diversity of Top-N Recommendation via Determinantal Point Process\\n\\n[[UCB upper confidence bound]]\\n\\nTraining:\\n\\n- User behavior is generally unpredictable and videos can become viral during the day. Ideally, we want to train many times during the day to capture temporal changes.\\n\\nInference:\\n\\n- For every user to visit the homepage, the system will have to recommend 100 videos for them. The latency needs to be under 200ms, ideally sub 100ms.\\n- For online recommendations, it is important to find the balance between exploration vs. exploitation. If the model over-exploits historical data, new videos might not get exposed to users. We want to strike a balance between relevance and fresh new content.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='473eea77-4036-4bef-9c0f-f9ff9187cea7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nReal time or not\\n\\nOne of the most important design decisions is whether the system is real time, pre calculated batch or some hybrid. Real time systems limit the complexity of the methods available while batch calculations have issues dealing with staleness and new users.\\n\\nsmall online system can be based on search query\\n\\nfor different types of users, use different model; \\n\\nheavy user use one version; \\n\\nlight user use only contextual features like geo location\\n\\nrecall logic: what are the top 30 vid belongs to this users favorite genre; or geo \\n\\nif u are making recommendation, accuracy is usually not good; u should look at ranking metrics; if your recommended top one is bad, its a more serious problem than the 10th recommended one\\n\\nWhy is MRR metric better for mobile use cases, whereas NDCG and MAP better for desktop use cases?\\n\\nMRR only concern about top 1 most relevant item.\\nHowever, NDCG and MAP not only take top 1 relevant but also rest of relevant items.\\nYou can check those property when you check those equations.\\n\\n[[ranking metrics  MAP MRR NDCG]]\\n\\n Why is MRR metric better for mobile use cases, whereas NDCG and MAP better for desktop use cases?\\n\\nMRR only concern about top 1 most relevant item.\\nHowever, NDCG and MAP not only take top 1 relevant but also rest of relevant items.\\nYou can check those property when you check those equations.\\n\\nin each user session, the first version of recommend list will be the offline model output; after ur behavior, u gets the results fine tuned by online model, like ur search behavior etc. \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='72940702-ed97-49be-b59c-ad58e3f63b76', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Additional Considerations:**\\n\\n- layers applied at the end of model depends on religion, geo location etc.\\n- Biases: Are we sampling from a large enough subset of demographics, if not maybe we can group the largest and set others to be OOV demographics.\\n- Any concerns with privacy/laws? We may need to anonymize or remove data depending on privacy concerns.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d7438763-d69c-4028-9611-47cd292ee9f5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nS**tage 6: features**\\n\\n- Entity-related features such as cuisine type, price range, popularity, quality, and rating\\n- Consumer-related features such as cuisine/taste preference, vegan status, and affordability\\n- Consumer-entity engagement features such as view/click/order history, rating, and reorder rate； including dynamic features like lastN features; behavior in the current session;\\n- Context-related features such as delivery ETA/distance/fee, promotion, day part, week part, holiday, and weather conditions; geo features\\n\\n**一个牛逼的训练emb的方法**\\n\\nhttps://doordash.engineering/2018/04/02/personalized-store-feed-with-vector-embeddings/\\n\\nFor store2Vec, we embed stores as vectors using the word2vec (CBOW) algorithm from\\xa0gensim package\\xa0with the following modification.\\n\\n1. each store is a word in our vocabulary and\\n2. each sentence is a list of stores viewed together in a user session.\\n\\nFor word context, we found a context window size of 5 to work the best. As quality constraints, we enforce minimum thresholds on number of stores in a session and number of sessions a store appears in.\\n\\nThis gives us vectors for every store. Then to generate vectors for a consumer, we sum the vectors for each store they ordered from in the past 6 months or 100 orders. To then determine the distance between a store and a consumer, we take the cosine distance between the store’s vector and the consumer’s vector.\\n\\nThis store2vec distance feature is one feature we added to our training pipeline for recommendations. The training pipeline consists of the following stages.\\n\\n!Untitled\\n\\ncompetitor data available?\\n\\nWhen using GBDT, one does not need to worry much about scaling the feature values, or missing values (they can be left as is). However, one important factor to take into account is that, unlike in a linear model, using raw counts as features in a tree-based model to make tree traversal decisions may be problematic when those counts are prone to change rapidly in a fast growing marketplace. In that case, it is better to use ratios of fractions. For example, instead of using booking counts in the last 7 days (e.g. 10 bookings), it is better to use fractions of bookings, e.g. relative to the number of eyeballs (e.g. 12 bookings per 1000 viewers).\\n\\n- (ML Pipeline: Data Ingestion) Think of Data ingestion services/storage\\n- (ML Pipeline: Data Preparation) Feature Engineering - encoding categorical features, embedding generation etc.\\n- (ML Pipeline - Data Segregation) Data split - train set, validation set, test set\\n\\n• Handle missing values or outliers: usually, we can drop outliers and if there is a lot of data then we can drop missing values, if data is limited, then we can impute data via average (or any other method for dealing with missing values). Talk about this with your interviewer remember it is supposed to be a discussion.\\n\\n• Balancing positive and negative training examples: if you notice that there probably will be a very big imbalance then you should discuss ways to solve this: up-sampling, under-sampling as well as other techniques like SMOTE.\\n\\n• If we are using a deep neural network, then we do not need feature selection. If we need, tree-based estimators can be used to compute feature importances. Additionally, we can use the L1 norm for regularization so that some feature coefficients would be zero.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1636b450-05dd-4729-b4e1-5ffc67bd0488', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nLevels of features\\n\\nThese are the high level sources of signals. Eg. if you’re personalizing the results of a Youtube video search, there’s a few data sources for features:\\n\\n- The user doing the search\\n- Each video that can be recommended\\n- Interactions between the user and each video; like if the user click on the video before\\n- The search query\\n- location based features;\\n\\nIt’s worth it to be pretty exhaustive in naming all the high level data sources you can think of, there shouldn’t be that many (less than a dozen.)\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ffdff33b-52dc-46d3-9a7e-8fe2fbd887e9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**High Level Features**\\n\\nWithin each data source, you can iterate on the types features available. It’s good to call out some example specific features, but it would take too long to be exhaustive about these. Eg. for a Facebook user you have features like:\\n\\n- Demographics: Age, gender, nationality, language\\n- Session: time in session, number of items viewed\\n- Post History: items liked, items disliked, posts created\\n- Friends and following: How many friends do they have, posts liked by friends, celebrities/interests followed\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='11605df0-6820-4c74-b7ee-b08570693598', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGeo features\\n\\nproximity to city - proximity to tourist places/ eat outs - proximity to other hotels\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0d4da1c2-2b04-4c75-9bde-0396bd0b306b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nNLP features\\n\\n[[VIP word2vec]] \\n\\n[[文本预处理  nlp data preprocessing]] \\n\\nnlp concept like reviews, embeddings, bert, gpt 3; yelp reviews ; \\n\\ninteraction between customer and restaurant features; nlp for reviews; \\n\\n1. Brainstorm user features, item features and sources of data signals at the company you’ll be interviewing at. It’s possible you’ll get asked something completely unrelated, but you’ll be thankful if these do come up.\\n\\n- (ML Pipeline - Model Train and Evaluation) Build a simple model (XGBoost or NN) - How to select a model? Assuming its a Neural Network 1. NLP/Sequence Model - start: LSTM with 2 hidden layers - see if 3 layers help, - improve: check if Attention based model can help 2. Image Models - (Don't care right now) 3. Other - start: Fully connected NN with 2 hidden layers - Improve: problem specific\\n- (ML Pipeline - Model Train and Evaluation) What are the different hyperparameters (HPO) in the model that you chose and why?\\n- (ML Pipeline - Model Train and Evaluation) Once the simple model is built, do a bias-variance tradeoff, it will give you an idea of overfitting vs underfitting and based on whether overfit or underfit, you need different approaches to make you model better.\\n\\nThere are a few options for parallelisation:\\n\\n- The simplest form is to have a\\xa0**dedicated pipeline for each model**, i.e. all models run concurrently.\\n- Another idea is to parallelise the\\xa0**training data**\\xa0i.e. the data is partitioned and each partition has a replica of the model. This is preferred for those models that they need all fields of an instance to perform the computation (e.g. LDA, MF).\\n- A third option is to parallelise the\\xa0**model**\\xa0itself i.e. the model is partitioned and each partition is responsible for the updates of a portion of parameters. It is ideal for Linear models, such as LR, SVM.Finally, a\\xa0**hybrid**\\xa0approach can be used, combining one or more options. (For more info I recommend you read\\xa0this publication).\\n\\nwhy do you remove features: performs better, easier to understand, faster to run; \\n\\n[[select and remove features ]] \\n\\n[[data sparsity problem]]\\n\\n!Untitled\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9d1072b6-62f2-4d7a-ac9d-81e05bc23507', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n\\n\\n- 特征交互层：特征交互层就像上文总结的Wide&Deep类模型一样，可以有很多部分。比如，FM可以来提取特征之间的二阶交叉，举例，拿用户性别的Embedding和候选主播的Embedding来做二阶交叉，FM可能会学到男用户喜欢看女主播。CIN是多阶交叉，可以拿多个特征放到CIN中进行高阶特征组合。Attention就像DIN那种方式，可以把过去交互过的主播和当前要预测的主播放到Attention中。这样当前预测主播是跳舞，可以把用户过去看过的游戏主播权值训练的小一些，过去看过的跳舞主播权值大一些，这样就达到了筛选优化的作用。还有简单的方式，就是把这些Embedding全都连接起来，接一个全连接层。\\n- 特征拼接层：特征拼接层就是把特征交互层得到的Embedding向量拼接在一起，然后接个全连接。这个全连接可以起到降维的作用，把Embedding的维度降小一点。因为，多目标输出层中还有多个tower。\\n\\n多目标输出层：类似MMOE的结构，可以设计一个专家网络，这样每个任务从特征拼接层的输出中学到不同的东西，避免造成各任务之间相互干扰的情况。常见的任务有：点击，用户是否点击了这个主播。观看，可以定义观看多少秒以下是无意义的观看，多少秒以上是有意义的观看，并且可以把观看时长作为一个权重进行加权训练。送礼，送的是正样本，没送的是负样本，送了多少可以当成权重进行加权训练。多任务训练的好处就是可以用全样本数据，且原始输入层中的Embedding也会训练的更加充分\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1fa9f480-40a7-43da-923c-b2769a3ac966', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nS**tage 7: model selection and evaluation**\\n\\n[[  partial dependence plots PDP  ]] \\n\\n[[xgboost vs. random forrest]]\\n\\nYou can refer to the following link\\xa0XGBOOST is slower than Random Forest\\xa0on the Xgboost Github. Its a weakness of GBT's in general when there are many classes.\\n\\nThe reason is that gradient boosting requires that you train [number of iterations] * [number of classes] trees, whereas random forest only requires [number of iterations] trees.\\n\\nGenerally,\\xa0**`scale_pos_weight`**\\xa0is the ratio of number of negative class to the positive class.\\n\\nSuppose, the dataset has 90 observations of negative class and 10 observations of positive class, then ideal value of\\xa0`scale_pos_weight`\\xa0should be 9.\\n\\n[[lightGBM and XGBoost vs. catboost]] \\n\\nMajor Differences between the two is that LightGBM uses a novel technique of Gradient-based One-Side Sampling (GOSS) to filter out the data instances for finding a split value while XGBoost uses pre-sorted algorithm & Histogram-based algorithm for computing the best split.\\n\\nGOSS assumes that data points with small gradients tend to be more well trained. This means that it is more efficient to concentrate on data points with larger gradients.To attenuate the problem of biased sample it also randomly samples from data with small gradients.\\n\\n·\\xa0**Gradient-Based One-Side Sampling (GOSS) — Focus on number of data points**\\n\\nDifferent gradients play different roles when computing the information gained in a decision tree. Data instances with larger gradients contribute more to the information gained and vice versa. Therefore, the best approach\\xa0***to train a model is to keep instances with large gradients and cut out the small gradients when sampling the data set.***\\xa0This will achieve higher accuracy in the information gained than down-sampling a data set using a purely uniform random sampling technique. This treatment is useful if the data set has a large range of information gained across data points which is generally the case with large data sets.\\n\\n[[    Factorization Machines and matrix factorization    ]]\\n\\nGBDT gradient boosting decision tree\\n\\n树状模型自带特征筛选的属性；对输入特征分布没有要求，在连续特征上表现良好，省去了人工转换的过程；计算复杂度比LR低很多\\n\\nGBDT+LR排序模型中输入的特征都是稠密的通用特征,记忆能力较差,所以引入支持高稀疏特征的FM模型。\\n计算复杂度为O(N).\\nGBDT+FM上线后,各项效果均比 GBDT+LR提升了4%~6%\\n\\nFM可以自动处理交叉项，所以可以降低LR的复杂度\\n\\n!Untitled\\n\\n!Untitled\\n\\nOnce you have arrived at the modeling component, you should first give a baseline model (if possible). Typically you would have a baseline model that does not require machine learning. For example, a good baseline for our earlier prompt would be to recommend the most popular products to the users. This “model” will always be easy to implement and you now have a baseline that all your other models should outperform.\\n\\nThen, speak about traditional ML models that are quick to train for e.g. logistic regression or decision trees.\\n\\n[[  partial dependence plots PDP  ]]\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='71aee26a-8e31-4dc4-8e50-ba6f0e821ae9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n**Content vs Collaborative signals for head & tail users**\\n\\nFor a hybrid model to work effectively, it should use both\\xa0**content and collaborative signals to achieve the best of both worlds**. For head users with a good number of collaborative signals it should rely more on these signals whereas for tail users it should rely more on content-based features. We wanted to validate whether this was indeed happening in our hybrid model.\\n\\nTo perform this analysis, we picked\\xa0**representative features for content and collaborative signals**\\n. Review text based similarity and matrix factorization score were the top features in the model and it made sense to pick these as representative features. We use\\xa0Partial Dependence plots\\n\\xa0(PDPs) against these features which shows the average prediction on the entire dataset when a feature is set to a particular value.\\n\\nSince this is a ranking task, we chose\\xa0**Normalized Discounted Cumulative Gain\\xa0(NDCG)**\\xa0and\\xa0**Mean average precision\\xa0(MAP)**\\xa0as metrics. The hybrid approach was compared against a couple of baselines:\\n\\n1. Popular businesses in the user’s location - available for both head and tail users\\n2. Matrix factorization - available only for head users\\n\\nIn order to mimic production settings using historical data, we created test sets which are in the future of the model’s training period (i.e. both feature generation period and label period were shifted into the future for the test set).\\n\\nAt first, we look at the relative improvement from the business popularity baseline at different values of rank (i.e. rank k=1, 3, 5, 10, 20, 30, .., 100). We find that the model\\xa0**more than doubles**\\xa0the NDCG and MAP metrics compared to a “locally popular” baseline at k=1!\\n\\nHere is the high level approach to establishing baselines:\\n\\n我常用的一种办法：unbalanced data, all positive cases; auc will be 0.5 \\n\\n1. Start with the simplest possible baseline to compare subsequent development with. This is often a random (chance) model.\\n2. Develop a rule-based approach (when possible) using IFTTT, auxiliary data, etc.\\n3. Slowly add complexity by\\xa0*addressing*\\xa0limitations and\\xa0*motivating*\\xa0representations and model architectures.\\n4. Weigh\\xa0*tradeoffs*\\xa0(performance, latency, size, etc.) between performant baselines.\\n5. Revisit and iterate on baselines as your dataset grows.\\n\\n- Logistic regression: Fast to train, very compact, but only finds linear relationships between features.\\n- Gradient boosted trees: Better performance than logistic regressions, can find non-linear interactions, typically doesn’t require much tuning.\\n- Deep Neural Networks: produces state of the art solutions, deals with non-linearities, requires lots of tuning and can require a lot of computing resources to train.\\n\\nalgorism: first step use very fast simple algo; recall; \\n\\n2nd stage, get 10 out of 500; tree based approach; \\n\\nif its small dataset, one step is enough; \\n\\n1. feature engineering; try to build one new features at least\\n2. logistic regression on stage 1; maxmize recall; \\n\\n**2nd phase; tree based approach or NN;** \\n\\noverfit with few data points first first than regularization\\n\\ni assume we have unlimited computing resources; \\n\\n- **Training time:**\\xa0How much training data and capacity is needed to build our predictor?\\n- **Evaluation time:**\\xa0What are the SLA that we have to meet while serving the model and capacity needs?\\n\\nmodel validation to pick the best model; \\n\\nThe training set is what we’ll use to fit the model for each possible value of the\\nmanually-set parameters,\\n• the validation set is what we’ll use to determine parameters that require manual\\nsettings,\\n• and the test set is what we use to evaluate our results for reporting, and get a sense\\nfor how well our model will do on new data in the real world.\\n\\nNOTE: do not forget to give the pros and cons of each approach you speak about. Example:\\n\\n**Model A:**\\n\\n- A short explanation of the model, the hyper-parameters and the loss function\\n- Pros of model A\\n- Cons of model A\\n\\nAgain, you should ask the interviewer if they would like you to explain how to productionize this component.\\n\\n- **Loss function selection:**\\xa0CrossEntropy, MSE, MAE, Huber loss, Hinge loss\\n- **Regularization:**\\xa0L1, L2, Entropy Regularization, K-fold CV, dropout\\n- **Backpropagation:**\\xa0SGD, ADAGrad, Momentum, RMSProp\\n- **Vanishing gradient**\\xa0and how to address it\\n- **Activation functions:**\\xa0Linear, ELU, RELU, Tanh, Sigmoid\\n- **Other issues:**\\xa0Imbalanced data, Overfitting, Normalization, etc\\n\\n- **data parallelism**: workers received different slices of the larger dataset.\\n    - *synchronous training*\\xa0uses\\xa0AllReduce\\xa0to aggregate gradients and update all the workers weights at the end of each batch (synchronous).\\n    - *asynchronous training*\\xa0uses a universal parameter server to update weights as each worker trains on its slice of data (asynchronous).\\n- **model parallelism**: all workers use the same dataset but the model is split amongst them (more difficult to implement compared to data parallelism because it's difficult to isolate and combine signal from backpropagation).\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cd8e89b8-bfcc-4aa5-913c-19fba816883c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n**Optimization of NN**\\n\\nDistributed training strategies are great for when our data or models are too large for training but what about when our models are too large to deploy? The following model compression techniques are commonly used to make large models fit within existing infrastructure:\\n\\n- **Pruning**: remove weights (unstructured) or entire channels (structured) to reduce the size of the network. The objective is to preserve the model’s performance while increasing its sparsity.\\n- **Quantization**: reduce the memory footprint of the weights by reducing their precision (ex. 32 bit to 8 bit). We may loose some precision but it shouldn’t affect performance too much.\\n- **Distillation**: training smaller networks to “mimic” larger networks by having it reproduce the larger network’s layers’ outputs.\\n\\n!Untitled\\n\\nIf your data does not fit into your RAM, use a data pipeline. It works like this:\\n\\n1. Read a single batch of data from disk into memory\\n2. Process that batch (e.g. normalize/scale/crop/...)\\n3. Feed that batch to the model and perform optimization step\\n4. Repeat from 1. until all samples in the dataset have been used.\\n\\n- **Offline evaluation:**\\xa0The performance of the model on the hold-out samples of the dataset. During dataset collection, the data was divided into train, test, and validation subsets. The idea is to analyze how well the model generalizes the unseen datasets. You can also carry out K-fold cross-validation to find the performance under different subsets of data. The model that performs well for the selected KPI is selected to be implemented and deployed.\\n- **Online evaluation:**\\xa0The first step of deploying the trained model in real-world scenarios (after it has been evaluated offline) is to carry out A/B testing. The trained model is not quickly put out to face the real-world data at large. It's far too risky. Instead, the model is deployed on a small subset of scenarios. For example, say the model designed was to match an Uber driver with the rider. In A/B testing, the model will say only be deployed in a smaller geographical region instead of the entire globe. This beta version of the model will then be compared with the existing model over a longer period of time and if it results in an increase in the performance of the business-related KPI (such as more DAU/MAU for the Uber app, better user retention, and eventually an improved uber revenue for that are) then it will be implemented on a larger scale.\\n\\n• *representation*: use term frequency-inverse document frequency\\xa0(TF-IDF)\\xa0to capture the significance of a token to a particular input with respect to all the inputs, as opposed to treating the words in our input text as isolated tokens.\\n\\nEach application's baseline trajectory varies based on the task. For our application, we're going to follow this path:\\n\\n1. Random\\n2. Rule-based\\n3. Simple ML\\n\\nWhen choosing what model architecture(s) to proceed with, what are important tradeoffs to consider? And how can we prioritize them?\\n\\n- `performance`: consider coarse-grained and fine-grained (ex. per-class) performance.\\n- `latency`: how quickly does your model respond for inference.\\n- `size`: how large is your model and can you support it's storage.\\n- `compute`: how much will it cost ($, carbon footprint, etc.) to train your model?\\n- `interpretability`: does your model need to explain its predictions?\\n- `bias\\xa0checks`: does your model pass key bias checks?\\n- `time\\xa0to\\xa0develop`: how long do you have to develop the first version?\\n- `time\\xa0to\\xa0retrain`: how long does it take to retrain your model? This is very important to consider if you need to retrain often.\\n- `maintenance\\xa0overhead`: who and what will be required to maintain your model versions because the real work with ML begins after deploying v1. You can't just hand it off to your site reliability team to maintain it like many teams do with traditional software.\\n\\nWe can also baseline on your dataset. Instead of using a fixed dataset and iterating on the models, choose a good baseline and iterate on the dataset:\\n\\n- remove or fix data samples (false positives & negatives)\\n- prepare and transform features\\n- expand or consolidate classes\\n- incorporate auxiliary datasets\\n- identify unique slices to boost\\n\\n[[logistic regression]]\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='52d5f17f-9053-4949-a0cc-e7142b8783a6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nStage 8: **Model training**\\n\\n**Distributed Training, Tuning, and Evaluation**\\n\\nWe need to train, tune, and evaluate models with hundreds of billions of examples. As well, the models will have billions of parameters. To ensure that our system can properly handle models of this caliber, we\\'ll need to implement:\\n\\n- Data Parallelism\\n- Model Parallelism\\n- Checkpointing\\n\\n[[overfitting regularization   防止模型过拟合的方法汇总  ]]\\n\\nThis is where the technical knowledge will be evaluated. Make sure you are familiar with the different aspects of ML training and are comfortable talking about them in-depth. The interviewer might even ask you how you will combat say overfitting, or why didn\\'t you use regularization, and if you did which one did you use and why, etc. Topics include but are not limited to\\n\\n- **Loss function selection:**\\xa0CrossEntropy, MSE, MAE, Huber loss, Hinge loss\\n- **Regularization:**\\xa0L1, L2, Entropy Regularization, K-fold CV, dropout\\n- **Backpropagation:**\\xa0SGD, ADAGrad, Momentum, RMSProp\\n- **Vanishing gradient**\\xa0and how to address it\\n- **Activation functions:**\\xa0Linear, ELU, RELU, Tanh, Sigmoid\\n- **Other issues:**\\xa0Imbalanced data, Overfitting, Normalization, etc\\n\\n[[048 learning rate and warm up schedular]] \\n\\nThere are many\\xa0schedulers\\n\\xa0schedulers to choose from but a popular one is\\xa0`ReduceLROnPlateau`\\n\\xa0which reduces the learning rate when a metric (ex. validation loss) stops improving. In the example below we\\'ll reduce the learning rate by a factor of 0.1 (`factor=0.1`\\n) when our metric of interest (`self.scheduler.step(val_loss)`\\n) stops decreasing (`mode=\"min\"`\\n) for three (`patience=3`\\n) straight epochs.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f0d2ac89-bad9-4caa-a686-d6f5e5165301', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nS**tage 9: ab testing and deployment**\\n\\n- If the model inference is served real-time via an api, use a first-in, first-out queue. Workers pick and process jobs out of the queue, performing training or inference, and returning predictions to the user when done.\\n- If the task does not require real-time inference, the prediction can be made and orchestrated every day in batch and stored in database. The user can get queried results from the database.\\n- If the model is small, it is possible to move the ML models right to the edge devices and make a prediction on the devices.\\n\\n[[ab test 步骤]] \\n\\n- productiozing the model\\n    - examine predictions in real time to ensure the distributions of our real time predictions is same distribution of our training steps\\n    - speed of prediciton requriement;\\n        - productionize as a batch process; nightly ETL job and run every night\\n        - < 1h; productionize it as a service/API; usually REST Api\\n    - launch an experiment\\n        - ab test experiment design\\n            - target metric: profitability\\n            - assumptions of ab test:\\n                - independence: whether it holds\\n                - normal distribution\\n                - randomization\\n                    - pick unrelated covariates and the means\\n            - power analysis to make sure we can detect the effect\\n        - run in a single market first and expand it to the whole population gradually, from 5% to 10% to 20%; if not ideal we stop\\n- \\n\\nif its a brand new recommendation system without prior model as benchmark, we can still do ab test with prior status;\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='248bffaa-d55d-4a79-954e-a72f3695e92b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Concept drift**\\n\\nBesides the input and output data drifting, we can have the actual relationship between them drift as well. This concept drift renders our model ineffective because the patterns it learned to map between the original inputs and outputs are no longer relevant.\\n\\n**ML OPs for Modeling:**\\n\\n1. Repeatability of Experiments:\\n- ML Flow\\n- KubeFlow\\n\\n2. Parallelize hyper-parameter tuning: Google Cloud, Azure, AWS\\n\\n3. Model Versioning:\\n\\n- DVC\\n- Amazon SageMaker\\n- Google Cloud AI Platform\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='208b55cf-b53c-46f3-8b13-5b8e2caf0854', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nStage 10: iterative model improvement; after online evaluation; error analysis\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c6aec7d0-8a98-49b9-bb7e-c63d058343ac', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n1. data lake; feature and model store. data pipeline\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7d6ad6c6-3d9b-4974-8780-fbc2a68dfc35', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n2. Containerized with docker\\n\\n- Serve models as Endpoint/ Model as an API\\n- Model as a package [Batch processing]\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='eeb974c5-b7a9-4075-8ccc-189f34bb2f22', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n3. CI/ CD [Automated testing and ] MLOps\\n\\n- Unit testing.\\n- Model testing/ Cross Validation\\n- model auto update after retraining with new data; compare model performance and launch the best model\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2b73aa64-1b36-4755-ad97-ae6540e155e0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n4. Deployment/ Distribution\\n\\nKubernetes\\n\\n- Orchestrate the applicatiom.\\n\\nAWS EKS, Nomad/Qamel, MLFlow\\n\\nship model to another country to train; \\n\\nscale up or down in morning vs. evening time for workers/instances \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7c4a22a0-9de7-4dd3-97e6-2d8f25dc0ba3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n5. Monitoring and Alerting\\n\\nWeight and Biases、 ML Flow monitor offline metrics change \\n\\nPrometheus/ Grafana/ Cloudwatch/ Azure Monitoring Solution (Azure Data Factory/ Kusto) These are for monitoring GPU performance etc. \\n\\ncollect both performance data and user data (how many usage etc.)\\n\\ncomponents in the big model that are not; stage 1 or stage 2 model going wrong; \\n\\nfeature not working properly; it represent not i expected; \\n\\ndata level problem: specific subset might be causing the problem; \\n\\nhow i want to select the sample to do ab testing; \\n\\nintelegent subset of sample might be better; more heavy users; select dt gro people; \\n\\n1. Online A/B testing: talk about performing an A/B test using the online metric(s) you mentioned earlier.\\n2. Where to run inference: if we run the model on the user’s phone/computer then it would use their memory/battery but latency would be quick, on the other hand, if we store the model on our own service we increase latency and privacy concerns but removes the burden of taking up memory and battery on the user’s device.\\n3. Monitoring Performance: some measurements that we should log would be error rates, time to return queries and metric scores.\\n4. Biases and misuses of your model: Does it propagate any gender and racial biases from the data?\\n5. We should mention how often we would retrain the model. Some models need to be retrained every day, some every week and others monthly/yearly. Always discuss the pros and cons of the retraining regime you choose.\\n\\nDepending on the use-case, scores can also be delivered to the client asynchronously i.e. independently of the request:•\\xa0**Push**: Once the scores are generated, they are pushed to the caller as a notification.•\\xa0**Poll**: Once the scores are generated, they are stored in a low read-latency database; the caller periodically polls the database for available predictions.\\n\\nIn order to minimise the time the system takes to serve the scoring when it receives the request, two methods are employed:• the input features are stored in a low-read latency in-memory data store,• predictions precomputed in an offline batch-scoring job are cached for easy access [this is depending on the use-case, as offline predictions might not be relevant].\\n\\n**ML OPs for Data:**\\n\\n1. **Storing data:**\\n- If there are any object storage (images, videos, etc.) needs : Amazon S3, GCP Cloud Storage\\n- Databases for metadata as well as structured (tabular) data: MySQL, Postgres, Oracle\\n- Feature Store (store and access ML features) (Offline): FEAST, Amazon SageMaker Feature Store, Hopsworks Feature Store\\n- Data Versioning: DVC, Pachyderm\\n\\n**2. Data Ingesting and Transform:**\\n\\n- Ingesting: Offline data → can query your databases, Online data → we need high throughput and low latency so we should use online streaming platforms like Apache Kafka and Apache Flume\\n- Transform Features: Apache Spark, Tensorflow Transform\\n\\n**3. Orchestration Platforms:**\\n\\n- Airflow\\n- Kubernetes\\n\\nThe architecture for truly deploying an ML model is this:\\n\\n1. Backend server receives a request from user’s web browser. It’s wrapped up in JSON but semantically would be something like: “Tomorrow is Wednesday and we sold 10 units today. How many customer support calls should we expect tomorrow?”\\n2. Backend pushes the job {Wednesday, 10} into a queue (some place decoupled from the backend itself, such as Redis in the case of MLQ). The queue replies with “Thanks, let’s refer to that as Job ID 562”.\\n3. Backend replies to the user: “I’ll do that calculation. It has ID 562. Please wait”. Backend is then free to serve other users.\\n4. The user’s web browser starts displaying a ‘please wait’ spinner.\\n5. Workers — at least, ones that are not currently processing another job — are constantly polling the queue for jobs. Probably, the workers exist on another server/computer, but they can also be different threads/processes on the same computer. Workers might have GPUs, whereas the backend server probably does not need to.\\n6. Eventually, a worker will pick up the job, removing it from the queue, and process it (e.g. run {Wednesday, 10} through some XGBoost model). It’ll save the prediction to a database. Imagine this step takes 5 minutes.\\n7. Meanwhile, the user’s web browser is polling the backend every 30 seconds to ask if job 562 is done yet. The backend checks if the database has a result stored at id=562 and replies accordingly. Any of our multiple horizontal backends is able to serve the user’s request. You might imagine that the shared database is a single point of failure, and you’d be right! But separately, we provisioned replicas and some failover mechanism, maybe sharding/load balancing, so it’s all good.\\n8. After five minutes plus a bit, the user polls for a result, and we are able to serve it up.\\n\\nA team wants to A/B test multiple different changes through a sign-up funnel. For example, on a page, a button is currently red and at the top of the page. They want to see if changing a button from red to blue and/or from the top of the page to the bottom of the page will increase click-through.\\nHow would you set up this test?\\n\\nTwo Options: - Run a multiple variant test - Run a simultaneous test\\n\\n1. Calculate the desired effect size of our change\\n2. Calculate the required sample size & duration of the experiment to hit the desired effect\\nsize\\n3. Ensure proper tracking of CTR within our homepage\\n4. Ensure proper experiment framework to randomize between treatment/control 5. If we want to run a simultaneous test, we'll need to have a framework for measuring the\\ninteraction effects. We can:\\nMeasure each variant individually against control\\nFor each variant, calculate the values of the interaction term to determine influence of the other experiment\\nBenefit is we get more power with the simultaneous test. And we can understand what would happen if we rolled both variants out.\\n5. If we wanted to either run tests separately, this would give us the benefit of\\ninterpretability of our results. At the cost of potential power improvements and delay in\\nresults.\\n6. We can apply variance reduction techniques like stratification or adding covariates to\\nreduce the effect of external factors.\\n\\n!Untitled\\n\\n!Untitled\\n\\nWith online learning, how can we encode new feature values without retraining from scratch?\\n\\nWe can use clever tricks to represent out-of-vocabulary feature values such encoding based on mapped feature values or hashing. For example, we may wan to encode the name of a few restaurant but it's not mapped explicitly by our encoder. Instead we could choose to represent restaurants based on it's location, cuisine, etc. and so any new restaurant who has these feature values can be represented in a similar manner as restaurants we had available during training. Similarly, hashing can map OOV values but keep in mind that this is a one-way encoding (can't reverse the hashing to see what the value was) and we have to choose a hash size large enough to avoid collisions (<10%).\\n\\nWhat are some tasks where batch serving is ideal?\\n\\nRecommend content that\\xa0*existing*\\n\\xa0users will like based on their viewing history. However,\\xa0*new*\\n\\xa0users may just receive some generic recommendations based on their explicit interests until we process their history the next day. And even if we're not doing batch serving, it might still be useful to cache very popular sets of input features (ex. combination of explicit interests leads to certain recommended content) so that we can serve those predictions faster.\\n\\n!Untitled\\n\\n!Untitled\\n\\nUsing Machine Learning to Predict Value of Homes On Airbnb\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='015367c4-9f0f-4b32-aa94-a4a940a27fe8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n[[ab test 步骤]]\\n\\n\\n24 A/B Testing Interview Questions in Data Science Interviews and How to Crack Them - KDnuggets\\n\\nInterview Query | 45 Statistics & A/B Testing Interview Questions (Updated for 2022)\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2aa2868c-9b21-4a88-a8c7-d3808fce5754', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHow to answer Product Strategy questions?\\n\\nWhen answering product strategy questions in a product management interview, it is essential to follow a structured approach to ensure that you provide a comprehensive and well-thought-out response. Here is a step-by-step guide that you can use to answer product strategy questions:\\n\\n1. **Clarify the Question:**\\xa0Make sure you fully understand the question being asked by the interviewer. Ask for clarification if necessary to ensure you address the question correctly.\\n    \\n2. **Describe the Company & Product:**\\xa0Provide an overview of the company and the product. This will help to set the context for your answer and demonstrate your understanding of the company and its offerings.\\n    \\n3. **List your Strategic Choices:**\\xa0Identify and list the various strategic choices available to the company. These could include expanding the product line, entering new markets, or repositioning the product.\\n    \\n4. **List your Evaluation Criteria:**\\xa0Determine the key criteria that will be used to evaluate the different strategic choices. This could include market size, competition, financial viability, and customer needs.\\n    \\n5. **Evaluate each Strategic Choice:**\\xa0Analyze each strategic choice based on the evaluation criteria you have established. Identify the pros and cons of each option and evaluate the potential impact of each choice on the company and its product.\\n    \\n6. **Make a Recommendation:**\\xa0Based on your evaluation, recommend the best strategic choice for the company and its product. Explain your reasoning and provide evidence to support your recommendation.\\n7. \\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f0460e07-67ff-4729-9a78-3df913279e66', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n1. Clarify the Question\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c96e10fa-bb0d-41e6-85cb-9af1e6f78c05', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n2. Goal\\n**Goal:**\\xa0Design a bicycle renting app to promote sightseeing, transportation, cycling and reduce traffic & pollution.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='25bcbc71-a37e-44b4-b58b-3bc028bc47df', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n3. User Segments; 可以用不同的标准来segment users; 比如可以用广告主的类型，用户的demo, or user habit\\n1. **Tourists:**\\xa0This is the primary user group, comprising solo travellers, groups of friends, and families. Their main objectives are to explore popular tourist destinations in the city, such as museums, zoos, monuments, and other sightseeing spots. The app aims to provide a convenient and affordable way for tourists to explore the city at their own pace while reducing traffic congestion and pollution.\\n    \\n2. **Guides/Travel Agencies:**\\xa0This user segment may want to include bike rentals as an optional service for their customers. The app can provide a platform for these users to make bookings and manage their rentals.\\n    \\n3. **Locals:**\\xa0This segment of users would use the app primarily for daily travel and errands. By renting bikes instead of driving or using public transportation, locals can save time, money, and reduce their carbon footprint.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='10a7a7ab-6953-498d-99e0-02fb4d6a6f2d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n4. User Journey:\\nThe user journey in a bike renting app would typically involve the following steps:\\n\\n1. Registration or Login:  \\n    The user opens the app and either registers or logs in using their existing credentials.\\n    \\n2. Search for Rental Shop:  \\n    The user selects their preferred location and searches for rental shops nearby or in the specific location they want to visit.\\n    \\n3. Browse Available Bikes:  \\n    The app displays a list of available bikes in the selected rental shop, along with their prices and other details such as size, type, and features. The user can browse through the options and select the bike they want to rent.\\n    \\n4. Book and Pay:  \\n    Once the user has selected their preferred bike, they can book it by choosing the rental duration and paying for it using a secure payment gateway. The app provides a confirmation of the booking and a receipt for the payment.\\n    \\n5. Pick up the Bike:  \\n    The user goes to the selected rental shop to pick up the bike. They may need to show their ID and booking confirmation to verify their identity and rental details.\\n    \\n6. Ride the Bike:  \\n    The user takes the bike and starts riding it. They may use the map feature in the app to navigate to their desired destination.\\n    \\n7. Return the Bike:  \\n    Once the rental period is over, the user returns the bike to the rental shop, following the instructions provided in the app. The rental shop may verify the bike's condition and check for any damages.\\n    \\n8. Review and Rating:  \\n    After returning the bike, the user can leave a review and rating for the rental shop, which can help other users make informed decisions.\\n    \\n9. Support and Assistance:  \\n    The app provides customer support and assistance in case of any issues or concerns during the rental period or after returning the bike.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6ffdcb5c-b052-454a-9b1b-e5587bdbc483', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n5. Pain Points\\n\\nThe first step in tackling any question is to clarify the scope of the problem. Sometimes, the question may already be specific enough that you can proceed without further clarification,\\xa0_e.g. Should Facebook have a dating feature?_\\xa0. However, in cases where the question is too broad, it\\'s important to ask for clarification to ensure that you understand what is being asked. For example, if the question is \"_What should be the product strategy of Company X moving forward?_\" you might ask, \"Are we talking about the overall product portfolio or a specific product within the portfolio?\" or \"What are the key metrics that we should be considering when defining its product strategy?\"\\n\\nAsking these types of clarifying questions can help you understand the scope of the problem and ensure that you are addressing the right issues. It\\'s also important to ask questions about the current status of the company and the product, such as \"_**WHY**\\xa0is the company considering a shift in strategy?_\" or \"_What are the current pain points that users are experiencing with the product?_\" Understanding these key details can help you prioritize the right strategy.\\n\\nNote that the interviewer may sometimes respond by giving you more freedom to choose the path to move forward. In this case, it\\'s important to take a step back and think critically about what the company\\'s missions and goals are and what you believe is the best approach to achieve those goals. Ultimately, the goal is to demonstrate that you can think creatively and strategically about product problems and come up with effective solutions.\\n\\n- **Q)**\\xa0Can we include both normal bicycles and e-bikes?  \\n    _A) Yes, both types of bikes are included.  \\n    _\\n    \\n- **Q)**\\xa0Are we targeting any specific geography?\\xa0  \\n    _A) Consider a specific country of your choice (say India).  \\n    _\\n    \\n- **Q)**\\xa0Are we targeting any specific platform or OS?\\xa0_like, App/Web, Android/IOS._  \\n    _A) Upto you to decide.  \\n    _\\n    \\n- **Q)**\\xa0Is the bike rental service only for tourists, or can anyone rent a bike?  \\n    _A) Anyone can rent it.  \\n    _\\n    \\n- **Q)**\\xa0Are international and national tourists both included in the target audience?  \\n    _A) Yes, both types of tourists are included.  \\n    _\\n    \\n- Q) Does the project scope include designing the physical aspects of the bike rental service?  \\n    _A) No, only the app design is within scope.  \\n    _\\n    \\n- Q) Why do we want to design & develop the app?\\xa0  \\n    _A) To promote easy sightseeing and transportation and reduce traffic & pollution._\\n\\n\\n\\n\\n零售的本质是一个公式：流量x转化率x利润率\\n\\n!Untitled\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6d992371-7295-4cb0-8e9e-8ed18f3e53c9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n场景一：非实验场景策略效果评估\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3beeb3fd-cbd6-4a91-84a1-221a5bb39514', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n\\n\\n策略、功能以及活动的效果评估，几乎是每个产品、运营、分析师日常工作的一大重点。大到重要功能上线，小到一个文案修改，都可以去衡量这个策略的效果，比之前提升了多少，对整体数据贡献了多少。不仅衡量了工作产出，也为之后的优化方向提供沉淀与依据。AB实验是回答这个问题的最优解，但是当无法做实验时，研究如何利用历史数据进行因果分析评估也变的非常重要。\\n\\n非实验场景的策略评估方式的核心思想就是：**人为创造一个虚拟对照组与策略上线数据做比较估计策略真实效果**。\\n\\n效果回归本质上属于统计学中的因果推断问题。在统计科学中，因果推断要解决的问题本质是**剥离我们所不关心的外部变量对结果的影响**，从而精准估计到我们所关心的策略因素对结果的单一影响。在不能做AB实验的场景下，通常有两种完成这件事情的思路：\\n\\n- **构造相似群体（Matching）**：这种思路假设在未被实验策略影响的样本中存在一些样本和被实验策略影响的样本存在同质性。只要我们想办法找到这些相似的样本作为虚拟对照组，就可以控制外生因素。这种思想最经典的方法是PSM(倾向得分匹配法)；\\n- 构造虚拟现实（Synthetic Control）：这种思路认为策略的影响其实是策略上了之后的指标表现和“假设策略没上”的平行时空中指标表现的diff。因此，只要通过建模方法构建出假设策略没上的虚拟时空的指标水平，即可评估实验策略收益。典型的方法包括合成控制法、Causal Impact等；\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='03529bb0-7276-410b-9974-c700e420344a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3816cdb0-bf96-4453-898d-78dec7a5e19f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nPSM\\n\\nPSM的思路是**对策略命中的每一个用户，都在策略未命中用户中找到和他近似的一个用户**。通过这种方式，策略命中用户和近似找到的对照用户就有了可比性。PSM 作为一种 matching 方法，最大的目的就是控制实验组和控制组的观测变量。举个简单例子，我们想对比吸烟的人和不吸烟的人的健康状况，我们观察了两组人群作为样本，可能会出现以下两种情况:\\n\\n- 恰好我们选取的吸烟的人群更年轻，身体本身就更健康，而不吸烟的人群整体年龄偏大，或者身体本身比较虚弱，最后观察一段时间得出吸烟无害的结论。\\n- 吸烟的人群也喜欢喝酒，吸烟的人群本身也不注重自己的健康等等一系列其他因素干扰，导致得出吸烟有害健康的结论。\\n\\n这两种结论都是不够科学的，因此用 matching 的思路可以缓解上面的情况，而 PSM是 matching 最常用的方法。具体应用可以分为以下几个步骤：\\n\\n- **计算未被策略影响的每个用户倾向性得分**：这一步本质是建模问题，因变量为是否被干预Treatment，自变量为用户特征变量。做一个建模预测策略发生概率(逻辑回归/XGB/RF等)；\\n- **得分匹配**：有了每个用户的倾向性得分，针对目前的被策略命中的用户，匹配得到一个近似相同的用户，组成对照组。\\n- 修剪：筛选掉倾向性得分极值。常见的做法是保留得分在某个区间的用户，例如实验组和对照组用户得分区间的交集，只保留区间中部的 90% ，可能原始得分在0.05-0.95的用户。\\n- 匹配：实验组对对照组根据得分进行匹配的时候，比较常见的有几种方法。knn: 进行 1 对 K 有放回或无放回匹配。radius: 对每个策略影响用户，匹配上所有得分差异小于指定 radius 的用户。\\n- 设置得分差异上限：匹配用户的时候，要求每一对用户的得分差异不超过指定的上限min(delta(score))<caliper。\\n- \\n\\nhttps://mmbiz.qpic.cn/mmbiz_png/5EpmeGExLQcI5gmYykZqNXtqySEmjFxjXas7dnNwWHI4WoL1TpMesKcNeeVuJP9ibuQjcjCbhklboHF0tuM6Y8A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1\\n\\n- **平衡性检验**：看倾向性得分在匹配前后的分布、以及特征在匹配前后的 QQ-Plot。匹配后的实验组和对照组的倾向性得分分布更加接近，变量分布也更接近。\\n- **因果效应估计(策略效果)**：我们的目标是推断策略组用户的平均处理效应 ATT (Average Treatment Effect on the Treated)。现在我们已经构造出了一对接近同质的实验组和对照组了，我们可以直接比较匹配后的实验组和对照组。\\n\\n举个🌰，业务在上线‘主播连麦PK功能’后，发现使用连麦PK功能的用户比未使用此功能的用户开播率高4.3%，于是认为该功能提高了主播开播率。这个场景该如何做效果评估？特征选取：\\n\\n- 自变量：用户基础画像、行为特征\\n- 因变量：是否使用该功能\\n\\n按照1:1的匹配比例，最终匹配上26w对用户数据。其中策略组用户开播率13.1%，对照组为11.2%，两组diff为1.9%，假设检验通过。因此从数据角度证明该功能确实能提高开播率，但提升效果为1.9%而不是4.3%。\\n\\nhttps://mmbiz.qpic.cn/mmbiz_png/5EpmeGExLQcI5gmYykZqNXtqySEmjFxjfu9QtpEfnzA74uwbDFFRgicXbzLIxb8k23J47DoEfxn7NgoMI383iayA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='54c48f79-6ef6-4fdd-a336-44b148e0edc4', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0b693b25-6db5-4f81-a3a5-d78af52da82e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nSCM(合成控制法)\\n\\n当treatment施加到一个群体或者地区上时，很难找到单一的对照组，这时可以采用合成控制方法构造虚拟对照组进行比较。原理是构造虚拟对照组，**通过treatment前的数据上学习的权重，拟合实验组在实验开始前的数据，模拟实验组用户在没有接受实验情况下的结果**，构造合成控制组，实验开始后，评估实验组和合成控制组之间的差异。\\n\\n假设我们想要衡量疫情对于上北京某类商品的GMV的影响。若按照传统的matching方法，我们需要选择一个和北京疫情前的dau、gmv等最相似的非疫情城市作对比。但用合成控制法的话，我们需要从全国所有的非疫情城市中进行筛选，用同样的疫情前一段时间的各类维度数据与北京进行拟合匹配，使得合成后的「虚拟北京」在各个维度上尽可能真的接近「真北京」。最终选取拟合效果好的：北京=0.1城市A+0.2城市B+0.3城市C+P ；\\n\\n概括的说，我们要找到策略干预单元在不被策略干预情况下的反事实时间序列曲线：\\n\\nhttps://mmbiz.qpic.cn/mmbiz_png/5EpmeGExLQcI5gmYykZqNXtqySEmjFxj10Jpy72sOnDE2GBVRbqsl8mbfWYlmgX02qWCElDfhFwknVGYRib9ibcQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1\\n\\n在合成控制中，我们没有很多样本但是我们有很多时间点信息。所以我们可以做回归拟合，将每个数据点翻转输入矩阵，然后样本会成为变量，我们将结果表示为样本的加权平均值，就像下面的矩阵乘法：北京=0.1上海+0.2天津+0.3广州\\n\\n我们尝试评估疫情对北京的某品类GMV影响，我们为其找了其他30个非疫情城市。现在，我们可以将**因变量定义为北京GMV，将自变量定义为其他城市GMV进行回归**。\\n\\n注意几个点，首先策略干预后，合成控制的虚拟北京GMV超过了真实北京GMV，这表明疫情降低了北京的销售数据。其次，疫情前的时期是完美拟合的，这表明我们的合成控制可能发生过拟合。因为我们的对照组用了30个城市，所以我们的线性回归模型有30个参数可以使用，这给模型提供了过多灵活性，产生过拟合风险。此时可以使用Ridge或者Lasso回归来解决此问题，这里不过多赘述。\\n\\n有了合成控制的虚拟北京后，我们就可以将策略效果估计为策略结果与合成控制结果之间的差距，即 真北京-虚拟北京：\\n\\nhttps://mmbiz.qpic.cn/mmbiz_jpg/5EpmeGExLQcI5gmYykZqNXtqySEmjFxjO2UfybycpNicay1llXoeJvPNvWbb7nmo80BtfGgxgCGvw70MFTEjpnw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1\\n\\n由于合成控制法的样本量通常较小，因此在确定我们的结果是否具有统计学意义时，可以使用cross validation交叉检验方法。每次我们置换我们的策略组和对照组，由于我们只要一个策略城市，这意味着对于每个城市，我们要假装它是被疫情影响过的，其他则是对照。\\n\\n通过对所有城市应用合成控制，我们可以估计所有城市的合成状态和真实状态之间的差距。对于北京来说，这就是策略效果。对于其他非疫情城市，这就像安慰剂效应。如果将所有安慰剂效应与北京的疫情干预效果一起绘制，根据所有城市策略干预效果的分布，我们可以计算北京效应量的p值。在我们估算的所有其他30种安慰剂效应中，没有一个高于北京的效应量，所以p值为非常接近于0，具有统计显著意义。\\n\\nhttps://mmbiz.qpic.cn/mmbiz_jpg/5EpmeGExLQcI5gmYykZqNXtqySEmjFxjpB8ko6RKSXxm0tzQz7nk56LpJDKXCuOBQy77cgIgFcWd8ibr2Q1rCGA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f2ed0704-65ea-4b3b-b82c-243d74b3fa10', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aa10026b-41d9-4711-ba7a-be9b05ff912b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6e387633-a0eb-4d67-9b95-f578f7311c42', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n场景二：实验场景下的正向用户下探\\n\\n在做策略时，我们需要探究策略对于不同用户的异质性影响(HTE)，哪些用户对策略更敏感更容易被影响以及影响有多少，更好的归因和理解不同的用户群。通常情况下，我们结合实验来做分析。比如在实验中，**我们需要挑选出来那些实验效果显著的用户，去分析他们的特征，以及找到敏感用户，帮助我们了解策略的影响，作出下一步迭代**。\\n\\n针对这类问题，之前常用的方法是去针对用户做维度下钻，但是效率极低且并不自动化很难遍历所有特征组合去分析。\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8b8a961c-6eee-4b12-87fb-fd0255398ded', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nCausal Tree基本原理\\n\\n这时我们可以结合机器学习的方法去解决此类问题，这里选择因果决策树(Causal Tree)方法。Causal Tree是一种直接对目标进行建模的方式，它改进了传统决策树的优化目标和指标分桶方式，以达到最大化分桶的异质性因果效应，同时调整误差的效果。\\n\\n首先，它会把数据分成训练集和估计集，一部分训练集去构造树，另一部分估计集去估计因果效应和方差。其次，在树的分区方式上，使用各个节点的方差对目标函数加以修正。给定训练集Str= {(Xi,Di,Yi)}，其中Di=1代表实验组，Di=0代表对照组，目标是预测E( (Yi(1) - Yi(0)) | Xi)。其中\\n\\nhttps://mmbiz.qpic.cn/mmbiz_jpg/5EpmeGExLQcI5gmYykZqNXtqySEmjFxjRoSMiamBpWIYAvgONfy8bZ3yGicJ59pibp0MvLsWic8V5n6lXKrmwRUH9A/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1\\n\\n本种方法须满足条件独立假设(CIA)即给定特征，用户是否在处理组和我们关心的结果是互相独立的。本模型的结果易于理解，可以得到某一个叶子(用户群)的因果效应结果，但是如果需要处理高维变量的话能力相对较差，最终分桶效果可能相对较，且无法检验，所需要模型使用者真正了解问题和数据的产生过程。在实际应用时，ab实验分析通常是满足CIA的，且一般不需要处理过于高维变量，所以本方法在实验异质性因果效应探究问题应用较为广泛。\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8571eede-8148-4d7e-93ab-1b2b1b31c230', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9a86db2a-3cf9-41b5-8462-9ef996b4344c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nCausal Tree🌰\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6208be7c-6d8f-45f9-b7b9-78aa05d6f5fd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n\\n\\n某业务做了产品优化实验，但实验各项消费数据表现较差，以APP平均使用时长为例，我们能否用Casual Tree找出一些群体的消费者，使得我们在这些消费者身上有正向实验收益？\\n\\n特征选取：\\n\\n- 自变量：性别、年龄、新老用户、是否安装竞品、是否是创作者\\n- 因变量：实验后两周每日APP平均时长(取log)\\n\\n通过建模，我们发现Causal Tree的第一个分裂点是用户是否是创作者，说明创作者受到了更大的实验负向影响。最终树将用户分为了10个节点(10个桶)，负向因果效应最大的组为第10组(非创作者+未安装竞品+0-23岁)，APP平均时长降低了16%。负向因果效应最小的组为第4组(创作者+未安装竞品+0-12岁)，APP平均时长上涨了7%；\\n\\n我们将每个组的分群因果效应均画出，没有找到正向收益置信的用户组。但是有些用户群体，实验没有对他们造成很大的负向影响。\\n\\nhttps://mmbiz.qpic.cn/mmbiz_png/5EpmeGExLQcI5gmYykZqNXtqySEmjFxj1jP6y2kyJiaLxD2K0ibf3ghheHLHBOiaGeq6TX15jX1Nheedn5xBpwbCg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4efa3026-fc73-4973-996c-a1905c00830a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='41a617db-00d2-482a-81c1-27ebe8efa6d9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n场景三：策略敏感人群探究\\n\\n目前业界流量竞争已经进入白热化，个性化策略已经渗透到了生活中的方方面面，不论是识别营销敏感人群去推送刺激其消费的优惠券，还是为某类视频爱好者针对性推送其所爱的垂类内容。而如何找到真正的策略敏感人群，将更多的预算/资源投入到可以带来‘增量’的用户上，以提升整体roi，成为了后时代精细化运营的关键，Uplift Model可以尝试解决这一类问题。\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='842a3a0b-6cbf-4ebc-bdf3-2544a4ee8ec2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nUplift Model基本原理\\n\\n用简单的例子来介绍此模型，假设我们是个电商平台，一件标价300元的商品，用户的购买率为6%。现有一批预算可以给用户发放10元的优惠券以提升用户购买率，那么这批优惠券应该发送给平台的哪些用户呢？\\n\\n此时我们脑海中有四类用户：\\n\\n- Persuadables：不发送优惠券则不买，发送优惠券则购买；\\n- Sure things：不论是否发送优惠券均会购买；\\n- Lost causes: 不论是否发送优惠券均不会购买；\\n- Sleeping Dogs: 不发送优惠券会购买，发送优惠券反而不买；\\n\\nhttps://mmbiz.qpic.cn/mmbiz_png/5EpmeGExLQcI5gmYykZqNXtqySEmjFxj5barMcsiaPpXlydaUyeu8yJp7tpmU7vUgYNz3EptRVdf3ibtCSctxjSw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1\\n\\n左上的Persuadables(说服型)类用户被发券干预后购买率得以提升，是我们真正想要进行干预的营销敏感用户。Uplift模型要解决的问题就是**通过建模预测的方法精准的去对这四类用户进行分群**。\\n\\n对于单个用户来说，无法同时观测到在有干预和没有干预两种情况下的表现，这也是因果推断中的反事实的问题。这时可以从用户的角度来对平均因果效应做估计，假如我们有两组同质用户，对其中一组用户发券另外一组不发券。之后统计这两群人在购买转化率上的差值就可以被近似认为是平均因果效应。\\n\\nUplift建模需要服从CIA条件独立假设，因为样本在特征上分布需要较为一致，因此AB实验是Uplift Model建模过程中非常重要的前置条件。\\n\\n- 首先，我们选取部分用户(小流量实验，样本量足够建模)随机分为实验组和对照组，对照组不发优惠券，实验组发放优惠券，用户最终是否购买为一个0-1变量；\\n- 然后，对整体实验数据用户购买行为进行建模；\\n- 最后，再用小流量实验训练得到模型对我们需要预测的全量用户进行条件平均处理效应估计，预测其发放优惠券所带来的增益值；\\n\\n假设有N个用户，用户i在没有优惠券的购买结果为Yi(0)，在有优惠券时购买结果为Yi(1)，此时发送优惠券对该用户的增益就是uplift score (i)=Yi(1)-Yi(0)。**当uplift score为正值时，说明干预项对用户有正向增益作用**，也就是上文所提到的Persuadables(说服型)用户。\\n\\n此外，Uplift模型通常有几种建模方式：S-Learner、T-Learner、Class Transformation等等。模型评估方法有Uplift柱状图、QINI曲线等方式。针对几种建模及评估方式可详细参考该把优惠券发送给哪些用户？一文读懂Uplift模型。\\n\\n最后，我们总结一下Uplift模型可能的应用场景：\\n\\n- **精准定位策略敏感人群：**我们希望找出来一些对干预项（例如发券、投放、内容推荐等）比较敏感的用户，继而对其进行精准策略/营销；\\n- **测算收益空间：**Uplift模型可以帮助我们测算如果对策略做一些人群向优化，业务收益将会提升多少；\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='79262bc6-228f-45ee-a3e8-7fbad77ac1d1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nUplift Model🌰\\n\\n例如我们做了短剧类视频屏蔽Holdout实验，整体大盘DAU下降-0.5%，但大盘用户结构却未看清哪些用户是短剧核心用户，哪些用户的短剧类视频推荐分发的提升空间比较大，我们需要对大盘用户进行分层。\\n\\n特征选取：\\n\\n- 自变量：用户基础画像、行为特征、整体视频消费行为、短剧消费行为、短剧内容偏好等；\\n- 因变量：用户是否是短剧核心用户会反映在短剧屏蔽前后的活跃度变化上，因此需要用活跃度这个指标来定义用户正负样本。正样本：策略上线后一个时间周期 相比策略上线前一个时间周期，活跃天数差值>0，负样本反之；\\n\\n训练好模型后我们对大盘所有用户进行短剧增益预测，得出了每个用户的基于短剧分发的策略增益分数Uplift Score，然后对其进行分桶观测：\\n\\nhttps://mmbiz.qpic.cn/mmbiz_png/5EpmeGExLQcI5gmYykZqNXtqySEmjFxjw3YOB1HXhVibXCCNDRDhEchRplxmsFpgGebrwQ1vgfn6ODAr8va05Qg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aafd06c6-cd8f-4a7f-b398-dfaf2e653e89', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='eab6ba00-3bf1-4ac7-8963-2de2ed85262b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n场景四：因果影响指标分析\\n\\n很多时候因果推断会遇到**混淆变量**的问题，比如我们想要去分析直播推荐多样性(指标D)对用户活跃度(指标Y)的影响，但此时存在很多变量X既与D相关又与Y相关。\\n\\n解决这类问题传统的方法是用X对Y做线性回归，X的参数就是影响效应，或者是上XGboost看Shap值等。但传统的方法会依赖很多强假设例如不能多重共线性等，强假设下得到的估计不一定合理。所以这种场景下传统的指标影响分析方法将不满足业务需求，双重机器学习(Double Machine Learning)为这个问题提供了解决的思路。\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='89421a45-da8b-4037-bc19-ca835aba2eb2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nDouble Machine Learning基本原理\\n\\n在介绍DML之前我们先用最简单的例子来讲讲它所解决的问题：我们要估计冰淇淋价格与其销量间的因果效应。我们的数据集中特征X包括温度、成本和一周中的周几三个变量，Treatment T为价格，outcome Y为销售量。其中，T影响Y，X影响T和Y，即存在混淆。通过可视化，我们可以很明显看到，在周末(weekday=1和7)的时候，价格比平常要高很多，即星期几这个特征就是价格与销量之间的混淆变量。\\n\\nhttps://mmbiz.qpic.cn/mmbiz_png/5EpmeGExLQcI5gmYykZqNXtqySEmjFxj4Z5qYU9HDiaiaQVtvFrxtjejsuyuS2ToGxnJeSu1NmPVmhjOb8W0Ytfg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1\\n\\n一种简单的去偏方法就是线性回归，我们拟合一个线性回归模型，然后固定其他变量不变，去估计平均因果效应(ATE)：\\n\\nhttps://mmbiz.qpic.cn/mmbiz_png/5EpmeGExLQcI5gmYykZqNXtqySEmjFxjGVoic5abiaCQEySkSoR5FibmkHZnWhF6bYcyp9z0otK6G4HXMPQX2PQnw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1\\n\\n但特征X与Y的关系可能是非线性的，如温度temp，当温度升高时，人们可能都去沙滩玩耍，买冰淇淋吃，销量Y升高，但当温度过高时，人们可能只想呆在家，这时销量Y就下降了。因此，我们**不可以直接做线性回归，而需要用残差建模的方式去建立因果模型，**残差的思想就是DML的理论基础。\\n\\nDML是一种处理基于观测数据进行因果建模的方法，它可以去除偏差。根据Frisch-Waugh-Lovell定理，假设我们要估计Y = β·X+ θ·D+ E 中D的系数θ项:\\n\\n- 先直接用X对Y做回归，得到X估计Y得到的残差，即剔除控制量X对Y的影响；\\n- 再用X对D做回归，得到X估计D得到的残差，即剔除控制变量X对D的影响；\\n- 最后用上面D的残差对Y的残差做回归，就可以得到最终想估计的系数θ项，也就是ATE(D对Y的因果效应参数)；\\n\\n虽然DML可以去混淆，让我们可以只关注T对Y的影响。但是在T和Y残差化后，我们仍然使用的是线性模型。当价格只需在小范围内变化时，这种方法可能还适用。然而通常情况是，在价格较低的时候，价格增加1元，需求量可能减少2个，而在价格较高的时候，价格增加1元，需求量可能只减少1个，**边际效应会逐渐递减**。\\n\\n显然，这是一种非线性关系。这时，我们可以通过对目标函数进行转换，实现非线性建模。在非线性CATE估计时，DML估计的是CATE的局部线性近似(导数)。举个例子，假设我们通过模型对一个顾客估计出CATE=2，说明价格减少一个单位，销量会多出2个单位。但我们不能据此就做出决策直接在价格上减少10个单位。因为当价格过低时，CATE可能就就会从2变成0.5。因此在处理非线性的CATE的时候，需要注意不同Treatment下的CATE可能是不同的。\\n\\n双重机器学习假设所有混淆变量(既影响D又影响Y的变量)都可以被观测，所以在做特征选择时要格外注意尽量将这些特征加入模型。同时，机器学习自带的正则化可以达到高维变量选择的效果，我们可以通过拆分训练集和测试集的方式来解决过拟合带来的偏差，提高估计准确性。\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='82c802e1-eef9-4338-82df-4c285aa7a6ae', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nDouble Machine Learning🌰\\n\\n用户APP消费时长一直是优化视频推荐的主要目标，但随着消费需求的多样化，优化用户时长对用户活跃度的边际效应逐渐递减。\\n\\n目标：\\n\\n1. 我们需要探索还有哪些潜在的用户行为或者哪些内容对用户活跃度有正向因果影响；\\n2. 这些潜在抓手的因果效应都是多少，以判断其优先级；\\n\\n由于用户活跃度和非常多指标都存在相关性（混淆变量较多），因此不能直接回归，需要用DML来计算潜在抓手指标对用户活跃度的因果效应，比较因果效应大小来判断优先级；\\n\\n特征选取：\\n\\n- X：统计第一个时间周期的用户消费行为特征（习惯偏好、消费行为、互动行为、消费内容垂类、作者相关画像信息等）\\n- D：统计第二个时间周期的用户消费应为特征（D为需要计算因果效应的特征）\\n- Y：第三个周期的用户活跃天数\\n\\n建模步骤：\\n\\n- 随机选取第二个时间周期的活跃用户，拆分训练集和测试集，关联第一个时间周期的特征指标及第三个时间周期的因变量；\\n- 用训练集数据，用XGB将X对Y做回归，在测试集上得到X对Y的残差，这步可以剔除控制变量X对Y的影响；\\n- 用训练集数据，用XGB将X对D做回归，在测试集上得到X对D的残差，这步可以剔除控制变量X对D的影响；\\n- 用测试集数据，对每个待评估指标D，用线性回归模型拟合上面的D残差～Y残差，得到每个D特征的因果效应系数θ，θ表示D指标每增加1%，用户的周活跃度将提升θ%；\\n\\nhttps://mmbiz.qpic.cn/mmbiz_png/5EpmeGExLQcI5gmYykZqNXtqySEmjFxjVB2SBxT353caFRaZzBygkj5mVCYbIK33bDpVHEJGwbRFR0NZnfTqtA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1\\n\\n剔除不显著的特征，整体来看按照提升用户活跃情况的边际效应大小的逻辑来确认优化手段的优先级，优化内容的分发垂类结构>提升用户的内容互动量>优化内容生产者结构。\\n\\nLet\\'s say that Facebook would like to change the user interface of the composer feature (the posting box) to be more like Instagram. Instead of a box, Facebook would add a \"+\" button at the bottom of the page.\\n\\nHow would you test if this is a good idea?\\n\\n**hint 1**\\n\\nWe can begin thinking about this question in terms of the metrics that Facebook wants to improve. Which metrics relate to posting, and which specific posting metrics would correspond to Instagram’s style rather than Facebook’s?\\n\\nIt helps to bring it back to the goal of each platform. What is Facebook’s mission and what is Instagram’s?\\n\\nApple\\n\\nSuppose you’re given millions of users that each have hundreds of transactions and these millions of transactions are for tens of thousands of products. How would you group the users together in meaningful segments?\\n\\n如果你有几百万用户，每个用户都会发生数百笔交易，这些交易存在于数十种产品中。你该如何把这些用户细分成有意义的几类？\\n\\n举个例子：用户购买股票交易的这个过程\\n\\n数据分析角度：多维分析\\n数据挖掘角度：特征\\n数据统计加的：假设检验，\\n用户分群\\n普通用户\\n普通粉丝\\n忠实用户\\n核心用户\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\n- Map out key problems with reasonable hypotheses\\n- Design high-level solutions with sound rationales\\n- Understand the potential pros and cons of each solution\\n- Know what questions to ask next\\n\\n!Untitled\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a5d8f126-d03a-45aa-96fa-268a4da53d7e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n**3.1 The BUS framework**\\n\\nThe BUS framework is a three-step approach to the product process.\\n\\n1. Business objectives\\n2. User problems\\n3. Solutions\\n\\nLet’s say your product sense interview question asks you to build an apartment hunting app. Here, the interviewer needs to hear that in order for the app to be successful, the three most important components are (1) business model, (2) competition, and (3) user segment.\\n\\n1. **Business model:**\\xa0You must mention that it’s a two-sided marketplace and that a solution must resolve marketplace-inherent issues like the cold-start problem.\\n2. **Competition:**\\xa0You should recognize that apartment hunting has many incumbents (e.g. Craigslist, Zillow), so meaningful product differentiation is key.\\n3. **User segments**: Apartment hunting is a space that spans many needs and user segments (e.g. college students to families), so it’s important for the solution to focus on a narrow and clearly defined user segment.\\n\\n- **product design questions**\\xa0will ask you to design a new product from scratch. They're to test if you can combine a strong eye for design and UX with an understanding of business objectives.\\n- **product improvement questions**\\xa0are testing the same thing, but instead you'll be asked how you'd improve and existing product.\\n- **product strategy questions**\\xa0come from a more high-level perspective . They test your ability to understand competitive markets and to create a product roadmap that responds to the business strategy.\\n\\n**Example questions asked in Facebook PM product sense interviews**\\n\\n**Product design questions**\\n\\n- Design a social travel product for Facebook\\n- Design a jobs product for Facebook\\n- Design a product to help users find a doctor on Facebook\\n\\n**Product improvement questions**\\n\\n- How would you improve Facebook? (answer)\\n- Pick a Facebook app / any product — how would you improve it?\\n- How would you improve Facebook groups?\\n- How would you improve Facebook birthdays?\\n\\n**Product strategy questions**\\n\\n- Facebook events is struggling. How would you turn it around? (answer)\\n- Should Facebook enter the dating / jobs market?\\n- How would you monetize Facebook marketplace / messenger?\\n- What should Facebook do next?\\n\\nYour interviewer will be looking for several things:\\n\\n- How do you approach prioritization? In other words, how you do make sure the right things get done first?\\n- How do you navigate trade-offs — both for your product and across the Facebook app/ecosystem?\\n- How do you make data-based decisions? What would you want to measure? Which metrics are important? Which aren’t?\\n- How do you turn analytical results into actionable decisions?\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='536b15b6-e86a-4d9f-b3cc-c9fdee5225b7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Example Execution Questions**\\n\\n- Android weekly usage dropped all of a sudden. How do you find out what happened?\\n- Friending is slowing down, how do you find out what\\'s happening?\\n- If you were responsible for News Feed, what goals would you set? How would you evaluate success?\\n- How would you decide whether to show someone an Ad vs. a \"People You May Know\" story in the News Feed?\\n- Imagine you are the PM for FB Live (or the News Feed or another Facebook product)… What suite of metrics would you want to see for the product? What should the goal of the team be? What are the pros and cons of the goal you selected? Why is that the right goal for the team?\\n\\n**1. Work within a structure**\\n\\nYou’ll need to show you can approach complex problems methodically, always keeping your original goal in mind. Have a framework to structure your thought process, and talk your interviewer through it step-by-step.\\n\\n**2. Break the problem down into pieces**\\n\\nQuestions such as \"How would you improve Facebook?\" are so vast, they’re impossible to properly answer unless you break them down into smaller components. Your framework will help you here.\\n\\n**3. Make intentional design choices**\\n\\nYou\\'ll need to constantly be able to refer back to your original objective. That\\'s your North Star, and the design choices you make should take you toward it.\\n\\n**4. Empathize with users**\\n\\nTo make the right product decisions, you’ll have to understand who your users are, how to segment them, and how to target the right ones.\\n\\n**5. Talk about trade-offs**\\n\\nThere will be numerous approaches and solutions to the problem the interviewer poses, and you’ll have to make tough choices. Discuss them within a\\xa0prioritization framework\\xa0to show the rationale behind your decisions.\\n\\n**6. Know your metrics**\\n\\nWhile the product sense interview isn’t as metric-focused as the execution interview, you’ll still need to give a strong explanation of how you’re going to measure success.\\n\\n**7. Turn ambiguity into specifics**\\n\\nIt’s not enough to just have an interesting discussion with the interviewer. You’ll need to turn ideas into specific product solutions.\\n\\n**8. Show creativity**\\n\\nShowing a structured, coherent approach is only half the battle: try to offer creative, innovative solutions that go beyond the obvious.\\n\\n**9. Think at scale**\\n\\nTo state the obvious, Facebook is huge. Its products are used by billions of people. So as you outline your answer, make sure you’re thinking big.\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0d11cb32-05c0-4f1e-bee5-ce3e31c83c37', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Experiment Design**\\n\\n- What do you mean by A/B testing?\\n    - https://dimensionless.in/data-science-interview-questions-with-answers/\\n    - As well as being perhaps the most accurate tool for estimating effect size (and therefore ROI), it is also able to provide us with causality, a very elusive thing in data science! With causality we can finally lay to rest the “correlation vs causation” argument, and prove that our new product actually works.\\n    - https://towardsdatascience.com/data-science-you-need-to-know-a-b-testing-f2f12aff619a\\n    - Type I error\\u200a—\\u200aor falsely concluding that your intervention was successful (which here might be falsely concluding that layout B is better than Layout A). Also known as a false positive result.\\n    - Type II error\\u200a—\\u200afalsely concluding that your intervention was not successful. Also known as a false negative result.\\n    - http://www.cs.cornell.edu/courses/cs578/2006fa/design.html\\n    - https://www.analyticsvidhya.com/blog/2017/05/41-questions-on-statisitics-data-scientists-analysts/\\n- Which of the following measures of central tendency will always change if a single value in the data changes?\\n    - Mean, median, mode, or all?\\n    - A - The mean of the dataset would always change if we change any value of the data set. Since we are summing up all the values together to get it, every value of the data set contributes to its value. Median and mode may or may not change with altering a single value in the dataset.\\n- What does it mean for a result to be statistically significant?\\n    - https://www.quora.com/What-kind-of-A-B-testing-questions-should-I-expect-in-a-data-scientist-interview-and-how-should-I-prepare-for-such-questions\\n    - Statistically significant is the likelihood that a relationship between two or more variables is caused by something other than chance.\\n    - Statistical hypothesis testing is used to determine whether the result of a data set is statistically significant.\\n    - This test provides a p-value, representing the probability that random chance could explain the result; in general, a p-value of 5% or lower is considered to be statistically significant.\\n    - Statistical significance is used to accept or reject the null hypothesis, which hypothesizes that there is no relationship between measured variables. A data set is statistically significant when the set is large enough to accurately represent the phenomenon or population sample being studied. A data set is typically deemed to be statistically significant if the probability of the phenomenon being random is less than 1/20, resulting in a p-value of 5%. When the test result exceeds the p-value, the null hypothesis is accepted. When the test result is less than the p-value, the null hypothesis is rejected.\\n- What is a confidence interval?\\n    - In statistics, a confidence interval (CI) is a type of interval estimate, computed from the statistics of the observed data **that might contain the true value of an unknown population parameter.**\\n    - The interval has an associated confidence level that, loosely speaking, quantifies the level of confidence that the parameter lies in the interval. More strictly speaking, the confidence level represents the frequency (i.e. the proportion) of possible confidence intervals that contain the true value of the unknown population parameter.\\n    - In other words, if confidence intervals are constructed using a given confidence level from an infinite number of independent sample statistics, the proportion of those intervals that contain the true value of the parameter will be equal to the confidence level.\\n    - https://en.wikipedia.org/wiki/Confidence_interval\\n    - \\n    - [[diff in diff and instrumental variables IV]]\\n- What are instrumental variables and why are they important for experiment design?\\n    - https://www.quora.com/What-are-some-data-science-interview-questions-Do-they-include-canonical-algorithm-questions-such-as-search-graphs-data-structures-etc\\n    - In statistics, econometrics, epidemiology and related disciplines, the method of instrumental variables (IV) is used to estimate causal relationships when controlled experiments are not feasible or when a treatment is not successfully delivered to every unit in a randomized experiment.[1] Intuitively, IVs are used when an explanatory variable of interest is correlated with the error term, in which case ordinary least squares and ANOVA give biased results. A valid instrument induces changes in the explanatory variable but has no independent effect on the dependent variable, allowing a researcher to uncover the causal effect of the explanatory variable on the dependent variable.\\n    - Instrumental Variables regression (IV) basically splits your explanatory variable into two parts: one part that could be correlated with ε and one part that probably isn’t. By isolating the part with no correlation, it’s possible to estimate β in the regression equation:Yi = β0 + β1Xi + εi.\\n    - https://www.statisticshowto.datasciencecentral.com/experimental-design/confounding-variable/\\n- When building a recommender system, how do you know if your model is working? Is there a rigorous way to test your model?\\n    - Is there a more rigorous —> number associating with match?\\n        - Manually rate random matches then see how ranks compare\\n            - Compare to random guessing\\n            - Benchmark is almost always random guessing\\n        - Take a list of 10 sentences then 1 test sentence —> then rank those sentences —> “is the best match pair of sentences in the top 3 of what the model picked out” —> the top x and top n in recommender systems\\n            - Getting creative with measuring the success of your model?\\n            - As a user what matters to me!\\n\\n- What proportion is more than 2.0 standard deviations from the mean?\\n    - 95.45%\\n- What proportion is between 1.25 and 2.1 standard deviations above the mean?\\n    - \\n        \\n        https://lh3.googleusercontent.com/PaFTE14_OF-SQGzoRpiJxtA2k8-dza1aisdEhj9arcJlCerAK19KQe2DLDYd7olEdTuU9kqrPBAbb6VRv9_9ZRCu5U4qTMgOIhsXhR9iYmIh5vJ3GGWd-JEYkumX1h_V9nmx_K6T\\n        \\n- What term refers to the standard deviation of the sampling distribution?\\n    - The standard error (of the mean) is the standard deviation of the sampling distribution from the sample mean.\\n- What is the shape of the sampling distribution of r? In what way does the shape depend on the size of the population correlation?\\n    - The shape of the sampling distribution of r is usually (negatively) skewed, unless r is 0. The reason for this is that r cannot take on values greater than 1.0 or less than -1.0, so its distribution cannot extend as far in one direction as it can in the other direction. The greater the value of p, the more pronounced the skew of r’s distribution. To convert Pearson’s r to a value that’s normally distributed, we can use a transformation called Fisher’s z transformation.\\n    - The shape of the sampling distribution of r is negatively skewed. Higher the size of the population correlation, more pronounced the skew.\\n    - http://onlinestatbook.com/2/sampling_distributions/samp_dist_r.html\\n- When solving problems where you need the sampling distribution of r, what is the reason for converting from r to z\\'? (relevant section)\\n    - The reason we convert r to z’ is to transform the variables into ones that have a normal distribution. Without a normal distribution, it would be very difficult to compute probabilities and the standard errors of the sampling distribution of r. Thus, by using the Fisher method, we can transform r into a variable that is normally distributed and that has a known standard error of and is thus much easier to work with.\\n- What is the p-value?\\n    - When we execute a hypothesis test in statistics, a p-value helps us in determine the significance of our results. These Hypothesis tests are nothing but to test the validity of a claim that is made about a population. A null hypothesis is a situation when the hypothesis and the specified population is with no significant difference due to sampling or experimental error.\\n    - https://www.educba.com/statistics-interview-questions/\\n- When should you use a t-test vs a z-test ?\\n    - https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/hypothesis-testing/t-score-vs-z-score/\\n    - Use a t-test when you have a smaller sample, N<30 , and a z-test when you have a larger sample\\n    - Both are used for hypothesis testing, but for a z-test, you don’t need to know the SD of your population.\\n    - \\n        \\n        https://lh3.googleusercontent.com/Ce7-NfnjU3Vl8PuHHqP0jJFNMySGyZp7OlUKzya7B-573ZKUPOBvtrY3_xLHURLGXnwmYB2dRp2svXLtjRCFbf33_0w9gvJJgcBdevlND2Pm48Cb7A7JmDGHdnawjvqV2j9w8BCF\\n        \\n- How is power defined in hypothesis testing?\\n    - power is the probability of correctly rejecting the null hypothesis. We’re typically only interested in the power of a test when the null is in fact false. This definition also makes it more clear that power is a conditional probability: the null hypothesis makes a statement about parameter values, but the power of the test is conditional upon what the values of those parameters really are.\\n    - https://apcentral.collegeboard.org/courses/ap-statistics/classroom-resources/power-in-tests-of-significance\\n    - \\n        \\n        https://lh5.googleusercontent.com/52nYhqos80JydcvTvawu5s2B-RIbgAocPU9v7pFYWdsM9RT-MJwCCWnNbyvyZNeaiBoxRjewjiCth27SZnNvYJMSj4VmCKKoF69bD3QNz9rwKWz7aF7K-2DMy__W0VblV5IiHv09\\n        \\n    - Power is the probability of rejecting the null hypothesis when in fact it is false.\\n    - Power is the probability of making a correct decision (to reject the null hypothesis) when the null hypothesis is false.\\n    - Power is the probability that a test of significance will pick up on an effect that is present.\\n    - Power is the probability that a test of significance will detect a deviation from the null hypothesis, should such a deviation exist.\\n    - Power is the probability of avoiding a Type II error.\\n- What is the difference between data science, ML and AI?\\n    - In computer science, **artificial intelligence** (**AI**), sometimes called **machine intelligence**, is intelligence demonstrated by machines, in contrast to the **natural intelligence** displayed by humans and other animals. Computer science defines AI research as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.[1] More specifically, Kaplan and Haenlein define AI as “a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation”.[2]Colloquially, the term \"artificial intelligence\" is used to describe machines that mimic \"cognitive\" functions that humans associate with other human minds, such as \"learning\" and \"problem solving\".[3]\\n    - Understanding the nature of intelligence in general\\n    - https://en.wikipedia.org/wiki/Artificial_intelligence\\n    \\n    !Untitled\\n    \\n    为了绕开观测数据因果推断的问题，我们引入了准实验。从目前因果推断整体的分析框架中可以看到准实验所处的位置，左图包含实验数据和观测数据的因果推断。其中，在观测数据的因果推断中，我们会优先看数据是否满足DID（Differences In Difference，双重差分）、工具变量和断点回归的前提要求。如果满足，会优先使用这三种方法；如果不满足，才会使用PSM（Propensity Score Matching，倾向评分匹配）和混淆PSM方法。这种优先级的原因是相比于PSM，前三种方法绕开了混杂因子，这是唯一的也是最重要的区别。因此它们依赖的假设在业务层面更容易得到满足，同时也很容易被检验，这样的结论也更容易被信服。我们把上面的三种方法称为准实验方法\\n    \\n    DID在腾讯看点中是一个常用的方法，我们用DID发现了在极端天气下，天气资讯对用户留存的影响。去年8月6号，是台风黑格比经过的时间，我们希望在这样极端的天气下，推送天气的咨询是否能提升用户留存。\\n    \\n    对于这个问题，我们首先想到如下实验：\\n    \\n    - 实验组：8月6号曝光天气的用户\\n    - 对照组：8月6号未曝光天气的用户\\n    \\n    结论：曝光天气的用户次留相比于未曝光天气的用户次留高了20%。\\n    \\n    事实上，这个结论肯定是错误的。因为曝光天气和未曝光天气这两组用户本身就不平衡，因为我们通常是给活跃用户曝光。因此，这样得到的结论是带有混淆偏差的。\\n    \\n    因此，我们又想到如下实验：\\n    \\n    - 实验组：前期未曝光天气，8月6号曝光天气的用户作为实验组\\n    - 对照组：前期未曝光天气，8月6号未曝光天气的用户作为对照组\\n    \\n    结论：曝光天气的用户相比于未曝光天气的用户在受到干预之后，次留扩大了1.4%\\n    \\n    基于上述结论，我们判断天气内容的曝光对次留是有因果效应的。为什么说这就是因果效应呢？双重差分中，第一层差分指的是实验组和对照组在实验前后的差异，我们在右上图看到了实验前的平行性是满足的，可以认为混淆变量对实验组和对照组的第一重差分是相等的，那么影响第二重差分（实验组和对照组差分的差分）的因素就只有干预本身了。因此，我们可以通过二次差分得到一个因果效应，也就是这里的1.4%。\\n    \\n    !Untitled\\n    \\n    针对这类问题，我们提出一套通用的观测数据因果推断分析方式来给出答案。我们主要关注三个问题，第一个问题启动重置对下一次的使用有没有影响？\\xa0第二个问题 一段时间的启动重置下来对用户的未来的打开次数，活跃，收入是否有影响？\\xa0前两个问题解决后，我们关心是否存在部分人群能够既不影响体验，又不影响收入增加和其他功能的导流。这三个问题又称为短期影响、长期影响和用户异质性分析。\\n    \\n    考虑前面给出的分析框架，我们发现都有相应的解法。\\n    \\n    **①\\xa0短期影响**\\n    \\n    由于用户是否被启动重置，只取决于用户的访问时间在40分钟右侧还是左侧，那么对于这类问题很适合用断点回归的方式解决。\\n    \\n    **②\\xa0长期影响**\\n    \\n    长期影响依赖于很多混淆变量，它适合用PSM、混淆控制的方式处理。前面提到，如果我们考虑PSM和匹配方法有一个难题——它的结论很容易被挑战，因为不存在遗漏的混杂因子是无法被证明的。如何解决这个问题是个难点。\\n    \\n    **③\\xa0用户异质性**\\n    \\n    异质性分析的前提是实验数据，或者说准实验数据，如何去获并分析短期和长期干预的准实验数据呢？同时在我们的场景中，我们关注多个指标和解释性，异质性没有一个直接可以满足的方法。那么现有的下钻分析和uplift能满足这样的目标吗？\\n    \\n    针对这三个问题，我们分别进行阐述\\n    \\n    !Untitled\\n    \\n    !Untitled\\n    \\n    针对长期问题，可以画出如上因果图，考虑一段时间启动重置累积后对用户的影响。长期问题的难点是无法绕开遗漏的混杂因子。比如，我们通过混淆控制的方法去解决这个长期问题，我们先尝试控制用户的活跃度，使其在一段时间内的访问次数都是21，发现击中比例越高的用户的访问天数越多。如果访问次数已经囊括了所有的混淆变量的话，这个结论就是正确的。事实上我们发现，当访问次数都是21的时候，击中比例越高的用户，相当于他们的间隔都比较长，也就是他们是低频高日活型的用户，而击中比例越低的用户，他们正好是高频低日活型的用户。也就是说，我们控制了访问次数，却没有控制住用户的访问模式。这样得出的结论也是错误的。\\n    \\n    当然，我们可以用PSM把这些所有可能的混淆变量一步步都考虑进去。但同样会存在两个问题，一是局部性问题，PSM匹配的样本只是样本中的一小部分，无法代表整体样本，二是遗漏的混杂因子的问题依然无法解决。下面给出我们的解决办法。\\n    \\n    !Untitled\\n    \\n    **准实验：**\\n    \\n    在短期的断点回归中，我们可以看到因为访问间隔会随机地落在40分钟的左右两侧，因此在40分钟邻域构成一个准实验。从业务的视角看，这个准实验是用户无法感知这次访问距离上一次是过了39分钟还是41分钟，他是无法感知到这个差异的，这导致来访的用户的各种变量也是随机分配到这个区间的。那么这个邻域是否能一定程度地扩大呢，能否扩大到30到50分钟或者20到60分钟呢？\\n    \\n    **邻域选择：**\\n    \\n    邻域的选择是置信度和随机性的折中。当范围越大的时候，我们覆盖的样本就越多，但随机性会变差。当范围越小的时候，随机性很好，但覆盖的样本很少，从而置信度会受到质疑。最终，选择了20到60分钟这个区间。我们还通过特征平衡性来证明这两个区间的样本在各项重要特征上都是比较接近的。\\n    \\n    **构造变量：**\\n    \\n    因为我们已经证明了用户的访问行为落在20到40分钟和40到60分钟是一个几乎随机的事件，那么我们可以基于这个事件去构造一个长期的随机变量，就是用户在一段时间内落在40到60分钟的次数除以落在20到40分钟的次数，用这个比例作为长期的准实验变量\\n    \\n    !Untitled\\n    \\n    我们用上表按照长期的击中比例来分组，我们发现两组用户在两周内各项数据都没有明显差异。也就是说，我们的长期Rate比例是与各种混淆因子独立的，也就是T独立于X。那么我们可以证明，Rate和活跃天数Y的因果性是等于相关性的。在右图做了大量证明，我们说明了准实验变量的相关性是等于因果性的，我们就可以直接去观测T和Y的关系，也就是我们构造出来的Rate和活跃天数的关系。\\n    \\n    !Untitled\\n    \\n    !Untitled\\n    \\n    如果说整体上的结论，短期整体和长期整体的结论是显而易见而且直觉的，那么第三个问题细分人群的结论就不是那么显而易见了，异质性分析的前提是实验或者准实验。前面，我们已经构造了准实验变量，创造了无偏样本。下面，我们希望通过异质性分析找到不同人群在不同干预措施下的不同效果，然后去去改善策略。\\n    \\n    比如，我们发现主动打开为主的活跃用户在被启动重置打断后的活跃度和收入都出现了下降，那么对于这类用户我们就应该下架策略。又比如，我们发现启动重置打断不仅会增加频繁打开信息流用户的活跃度和信息流的时长，还不影响他们的搜索时长，那么对于这类用户我们就可以执行启动重置策略。\\n    \\n    这里的难点是我们的目标指标有多个，包括搜索时长、信息流时长、收入。同时，用户的标签维度很高，包括主动打开、频繁打开信息流等。同时，我们要把这样的结论通过算法解释并满足通用性。需要同时满足这四个要求是个难点\\n    \\n    !Untitled\\n    \\n    通过调研发现，这四个要求是很难同时满足的。从前面的分析框架中，我们可以看到，异质性分析主要包括下钻分析和Uplift分析。在下钻分析和Uplift分析的调研中，我们发现了解释性、通用性和细粒度之间矛盾。下钻分析有比较好的解释性但通用性比较差，因为它不太适合处理连续变量，而且它一旦遭遇维度比较高的问题会有搜索效率的问题。Uplift在通用性和研究粒度上没有问题，但是它的解释性较差，比较适合高维和复杂业务。可以看到，在我们的问题中Uplift更加满足要求。\\n    \\n    我们继续调研Uplift发现，Transform outcome的方法是更满足我们的要求的，它相比Meta-learner有更高的准确性，同时相比于Direct uplift model有更低的实验成本，但问题是，它只适合于单指标的建模。那么多指标的uplift的建模，我们目前了解到的只有Mr-uplift方法。它的实验方法是用多个outcome组成一个新的outcome，然后对新的变量建模。这个转换是不可逆的，也就是说我们的变量对原始的outcome的uplift是无法被复原的。因此我们发现，只有Transform outcome最满足我们的要求，下面我们对其进行改造\\n    \\n    !Untitled\\n    \\n    我们的算法目标有3个：\\n    \\n    - 多指标的实验uplift拟合\\n    - 模型可解释\\n    - 算法通用、可处理高维度\\n    \\n    下面，我们用伪代码来呈现我们是怎么达到以上目标的，主要是四个步骤：\\n    \\n    Step1：在数据处理后，先通过Transform outcome去转换我们原始的Y和G。新的变量会被称为Y*和G*。然后对新的变量分别用CatBoost拟合模型。\\n    \\n    Step2：输出模型的重要特征，并选择出现次数最多的，用前15个或前10个解释细分人群。\\n    \\n    Step3：通过两个模型预测的uplift的正负值划分四个象限，比较不同象限的人群在Step2中得到的重要特征的均值差异，得到一个定性的结论。\\n    \\n    Step4：通过Step3的定性结论做一个单维度的搜索，得到定量的结论。然后输出每个维度子人群的uplift的绝对量值以及置信度。\\n    \\n    !Untitled\\n    \\n    我们通过Gini Score来评估这4种模型方案的准确性，黑线代表的是随机实验的效果，蓝线代表的是当前模型的效果，与黑线构成的面积越大效果越好。红线是理论上能够达到的最大值，但是它不能说明是最优效果，只能说是一个量高。我们发现Transform outcome加CatBoost的模型效果最好，Gini面积达到了0.1387，比单模型方法的效果好两倍\\n    \\n    !Untitled\\n    \\n    拿短期异质性来举例，我们希望知道不同上下文的访问行为在被启动重置打断后，在搜索使用时长和总使用时长上有没有什么不同的表现。首先，我们根据算法画4个象限图，我们根据总时长和搜索使用时长分别建立一个uplift模型，横轴为搜索使用时长的uplift，纵轴为总使用时长的uplift，每个点表示一次不同上下文的访问行为。那么第一象限代表被启动重置后，其总时长和搜索使用时长都会有提升，第三象限代表被启动重置后，其总时长和搜索时长都会有明显的下降。\\n    \\n    接着，我们得到这两个模型的重要特征，然后对比四象限的人群在这些重要特征的均值上的差异。对比第一象限和第二象限，我们发现第一象限的人群搜索时长相比于第二象限的人群搜索时长的占比更低，这说明启动重置策略对搜索时长占比较高的用户可能会下降搜索意愿。对比第一象限和第三象限，我们发现第一象限的打开方式有多种，而第三象限的打开方式主要是主动打开，这说明对主动打开的用户，启动重置策略会引起反感，不仅会降低搜索意愿并且对信息流导流不感兴趣。对于这类用户，我们需要采用下架策略。这样的定性结论到底是正确还是错误，我们还需要定量验证。\\n    \\n    对前两次打开方式做一个细分，每一种上下文我们都区分实验组和对照组。通过对比这四种细分上下文实验组和对照组的总时长和搜索时长的差异，得到真实的离线数据的总时长uplift和搜索时长uplift。最终来确定量化的uplift和置信度。\\n    \\n    这是短期异质性的四象限分析算法效果，长期异质性也是一致的\\n    \\n    !Untitled\\n    \\n    最终我们得到，整体上短期和长期的启动重置策略都有副作用。但区分用户看，可以发现可以对搜索活跃度较低的用户保持现有策略，对搜索活跃度相对较高的用户下架现有策略。更加精细化地，我们可以区分不同session上下文的行为。到这里，我们已经说完了异质性的结论。在分析过程中，我们发现启动重置对搜索用户的影响更大。因此，我们特别对产品平台上做了建议，就是在搜索用户搜索完退出再返回时，切换时增加一个动画，提醒用户之前的上下文已经被收纳到这个窗口里了，让用户主动选择是继续之前的上下文还是来到新的信息流页面。\\n    \\n    到这里，我们就已经解决了之前提出的3个问题，我们用断点分析解决短期影响，用uplift解决长期影响，用改良的准实验构造解决用户异质性。', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='15dd671b-87ee-4f18-98d5-4592f7b38143', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**不写模型的工作怎样？**\\n\\n1. **my work exp showed me that fancy model is not that important; making impact is important;**\\xa0\\n2. **business ppl are reluctant to change; convince ppl to make use of DS tools are more important than creating tools; temporal fusion transformer model; I asked the client to provide me stats to run the simulation**\\xa0\\n3. no opportunity to showcase my business sense\\n\\n\\n\\nThe framework I recommend also has four steps:\\n\\n- **Goal**\\n- **Impact**\\n- **Challenges**\\n- **Finding**\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a59ab5df-f643-4901-a800-06a98e59ca9b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGeneral 回答问题的要点\\n\\n别人在介绍自己业务的时候，如何体现自己的passion\\n\\n不要follow ur first intuition; 自己花几分钟时间想几个idea, 选最好的\\n\\n可以从fix data and fix model两方面来回答，还包括换用loss function, use log, etc. \\n\\n**想象data 每一行是怎样的**\\n\\n给自己5分钟practice回答behavioral问题\\n\\n**永远可以有一个简单的框架，然后确认我的框架没问题再开始**\\n\\nTake a minute or two to frame your response:\\n\"I\\'d like to take a minute to gather my thoughts on a solution if that\\'s okay with you.\"\\n\"To address this problem, I would like to discuss data preparation, statistical method and a business recommendation. Are you okay with this?\"\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='93fd89e3-eac6-4042-a04e-a936e0161bd2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHave you ever found an analysis that was the opposite of what you were hoping for? If not, suppose you performed an analysis where a client was hoping for one outcome, but in fact you got the opposite of what they were hoping it would show? How did you or would you handle this?\\n\\nonce the hypothesis contradicts the fact, the hypothesis is wrong, not the fact\\n\\nmight be problem or model or data or evaluation pipeline; \\ncheck from backwards: 80% problem is on evaluation pipeline, so start from here;   \\n\\n1. double check the analysis first; change a simple model to see if its problem of the model; see if results are the same across models\\n2. if no problem with code, find out why: might be problem of data, might be certain subset of the sample that drives the effect; check distributions and correlations see if they fits intuition \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cd1a439d-4b90-4d1f-be8c-bed6377dc224', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n体现我展现数据的沟通能力\\n\\nin general I am a good communicator; I would say by far my meetings and presentaitons are well received in general \\n\\nconvince people with data; \\n\\n1. convince them my model is good, my analysis is valid; \\n2. convince them a project is promising so we need to do it \\n\\n\\n1. for b 充分了解别人的需求，比如我一开始提了一些idea别人兴趣不大 based on what our data can do，组织了一个brain storming session focus on what people need and look back at our data or collect data\\n2. one leason is about explainbility; people love to know why; 需要用explainable model 比如xgboost; 就算用不可解释的模型也要加上可解释模型\\n3. another lesson is that ppl don't care much about model; one thing u can do is make ppl feel fancy, for instance, when i explain the temporal fusion transformer, i mentioned its the same underlying mechanism behind the LLM and chatgpt   \\n4. built a linear model for in session prediction, showed how different behaviors when added up, can contribute to likelihood of purchase  \\n\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='40e9060e-2d50-429f-8973-b58ab566f397', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nSalary expectation\\n\\nbased on my understanding of the mkt, I am considering 140 k to 160 k\\n\\nbut im willing to negotiate depending on the benefits and entire compensation package you would be providing\\n\\n*From what I know about the the industry and the position, I think somewhere in the area of $XX – $XX.”*\\n\\n*compensation isn’t the only thing that matters to me and I’d love to learn more about the job, the company, and the work environment here.*\\n\\n*but I’m definitely open to negotiating based on the entire compensation package.”*\\n\\n“*I’d like to learn more about the position and the duties, and what the team’s like, before discussing money. But may I ask what salary range you’re considering for this position?”*\\n\\nDelivered politely, you’ll demonstrate that your priority is learning whether the role is really what you’re looking for — which every employer will respect. And your deft invitation to share the budgeted salary range will be difficult to resist.\\n\\nIf the employer’s salary range is in the area you were considering, or even higher, thank them for sharing the information and confirm that the figure’s in your ballpark. If it’s a little less, say it’s at the lower end of what you were hoping for, but you’d still like to talk about the job.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='992dae2c-7cd3-4806-9486-059fc44f5ddb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n为何离开上家公司 why left\\n\\ni felt that I have accumulated a year exp, made the transition from academia to the industry successfully; im confident that i can get things done and bring value; it’s time for a new step forwards; \\n\\nin terms of tech stack, infrastructure, and team, Yelp is certainly a better and more challenging place for my career prospect;  \\n\\nmy current company is not data centric and won’t be in the near future; its prob a valid strategy, but not the best for my own career development \\n\\n I was the first hire of the data team still 2-3 people; CTO told us that data projects is not the focus of the company in the next couple months; \\n\\n下面这段可以不说\\n\\ncompany is not willing to invest in data projects: \\n\\nno gpu infrastructure; company is not willing to spend $1000 +  for several percentages of improvement on the model accuracy; its valid to reject my request; but there is uncertainty for exploratory data science work;\\n\\nI still managed to get it done  \\n\\nnot focusing on data science is totally valid and prob the best strategy for the company; but its not the best for my career growth; \\n\\nFirst and formost; be honest about my current situation; laid off by the company; \\n\\nmy previous company build tools for startups to raise capital; because of the high interest rate, very few startups actually able to raise $, so the company is struggling  \\n\\ncompany laid off the entire data team, including me \\n\\nI planned to leave the company before the lay off, because data science is not a core part of the product at least for now; the CTO also agree with it; I felt many projects get limited resources and support, I have been efficient in terms of collecting data, explore how we can use existing inhouse data and external data, and building models, the deployment of models tends to be very slow, which honestly is a little bit frustrating to me\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a9a709d7-e9d7-4e08-9772-fbc812de4f1a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n**my advantage;**我的优势优点\\n\\n**1. fast learner and  self motivated;** \\n\\n**can get things done and bring value to the company when facing uncertain needs**, in a messy startup environment with little resources or support \\n\\nfor instance, **I learned django from scratch and finished the web app in a week or two;**\\n\\n**When doing research , I need to teach myself to learn and adopt various tools and resorting help from the literature and other ppl. I learned the whole nlp stuff myself. good at NLP; not only fine tune, tried to pretrain large language model myself\\n**independently solve problems by exploring new methodologies;\\n\\n2. my business and product sense is strong; in my previous position at the startup, when ppl introduce a new feature or the mkt guy says we are launching a new compaign, compared to other engineers; besides the product manager, it is usually me who talks most; I often make product suggestions during meetings;\\n\\n我不想go out of my place; I know new functions requries resources and they have their plan; 而且因为是B2B产品其实我不太知道用户怎么想的；只有和data science完全相关的点我才会很主动的提意见in a assertive way\\n\\nthe $ I spent on IOS apps on iphone is more than the price of a new iphone\\n\\n3. besides tech skills, I got rigorous tranining in social science so I am pretty good causal inferences, design experiments etc. My coding exp started from econometrics; \\n- I have designed maybe at least 100 + experiments. \\n- i know the perripheral things about data science which is sth other DS people often lack exp. like how to collect data, how to label data; I hav**e recruited labelling workers in China to label the ads in my wechat data**. \\n- I understand what's the limitation of different datasets and knows how to explain it with analysis of human behavior. \\n\\nFor instance, when I read indsutry reports I can always find problems or potential problems. Because the way the collect and define data are problematic; \\n\\nWhen I am in Tencent and train the classification model, I spend lots of time to select labels that are both statistically robust and easy to use and understand for the users. *mutually exclusive and collectively exhaustive*\\n\\nI am extremely sensitive to potential bias or false definition in data and varaibles when I read business news and reports. Guess u can call it an expertise in data collection and control data quality. My prefossor told me that it’s the tradition of the chicago school of economics where we emphasize a lot on data collection quality to avoid garbage in and garbage out.\\n\\n优点，知识和研究方法上，知道的很多，很多时候可以给身边的人一些insight；老朋友大厂员工，在公司内部做一个出海分享。他好我说了一下他的想法和体验比较散乱。我就告诉他了一个网站，跨文化研究出一个score，告诉他用这个framework来讲，后来他讲完效果不错也很感谢我\\n\\n bring insights to co workers;**\\n\\n**Self motivated**\\n\\n**博士生就是自我驱动的；很多时候我们博士生经常交流如何催导师回邮件的心得，开玩笑是，连哄带骗**\\n\\n我是那种能做成事的人。我会想尽办法调动我身边的资源，投入十分的精力把事往前推。我是比较自我驱动的，不是那种别人推着走的\\n在tiger 我的老板马上就开始让我带team,因为他知道我是推着别人走的\\n\\n**我有精读能力，我有研究能力，更重要的是，我有“读不懂但可以读完，而后反复读，进而读的更懂”的能力**\\n\\n自我介绍 (三分钟)\\n\\n**问题分析：**这道题主要考察应聘者的逻辑思维、语言表达、自我认知等能力。\\n\\n**参考建议：**\\n\\n第一，条理清晰，层次分明，突出与岗位要求相吻合的技能、个人所长、行为风格、实际经验等。\\n\\n第二，现场表达必须与个人简历所写保持一致。\\n\\n第三，控制时间，一般不超过 3 分钟。\\n\\n第四，尽量口语化，语言平实可信。\\n\\n“Walk me through your resume/CV”\\n\\n“Tell me about yourself.”\\n\\n\\nmy previous company build tools for startups to raise capital; because of the high interest rate, very few startups actually able to raise $, so the company is struggling  \\n\\ncompany laid off the entire data team, including me \\n\\nI planned to leave the company before the lay off, because data science is not a core part of the product at least for now; the CTO also agree with it; I felt many projects get limited resources and support, I have been efficient in terms of collecting data, explore how we can use existing inhouse data and external data, and building models, the deployment of models tends to be very slow, which honestly is a little bit frustrating to me\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='972f5a52-3149-4202-a173-7fbc156ce6e8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n自我介绍 tell me about urself\\n\\nalright other info about myself: the past year I work as MLE dealmaker; I was the first hire of the data science team, and I completed several projects that starts from defining business problem, data collection to product deployment; \\n\\nI have experience with typical ML project like recommender system and more advanced ML products built on SOTA nlp models like transformer based models \\n\\nI am pretty confident that I can get things done with early stage data infrastructure, messy data, little resources or help; \\n\\n三个项目：raise prediction; investor insight score; nlp tool\\n\\nbesides the three projects, I also did a bunch of analytics and visualization projects for mkt and finance, strategy team for instance; since I am the only data guy in the company. Those things are not deployed model into the products, more like reports & documents as the output that provides insights; \\n\\na little bit about my edu background: I did my undergrad in engineering and found myself more interested in social science. So I had a double major in economics and continued to marketing programs in graduate school. In my phd, I studied consumer behaviour using data to answer marketing questions, my work involves a lot of data analysis and I do coding everyday in my five years during the phd.\\n\\nWhy i want to change job: better platform opportunity; \\n\\nI heard of this opportunity from my friend \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a12bf593-2615-42ec-947c-6a49fb1fac32', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n我在dm学到了啥：\\n\\n1. tools and procedure for ML projects; before I join the company, I didn’t even know how to use git; \\n2. how to work as a team; to push through projects and make things happen; \\n\\n不要越权去做事，不要干涉别人，比如设计问卷的人\\n\\n做完我手上的事，主动和人讨论，顺便催他的进度; push product manager to push other ppl\\n\\n工作流程 \\n\\nmake sure to ackowledge other ppl on meetings\\n\\n 1. define needs, brainstorm session; guide other ppl to understand DS; see if DS can solve the problem, define metrics with product & business ppl, \\n\\ndraft project documents to make everything concret;\\n\\n1. organize people: put up tickets that involves everyone and assign tasks to different people; timeline of tasks; \\n2. my work: collect data; baseline model, improve it with new data and model architect;  how to deploy it; build the pipeline from our own database and external data source; using aws to store intermediate data; build end tools like django web app, use docker if neccessary  \\n3. don’t intervene too much; can raise suggestions, but only insist on stuff relevant to me\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8d26a186-4992-4dfe-8629-9ceebaab256e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n我在tiger学到了啥：\\n1. complex model; the project is a couple thousands line of code; pipeline \\n2. lead a small team; subordinate; make sure they can produce results ; ackowledge their work; make sure they can learning sth and grow profesionally; \\n3. some ppl are more self driven some are not, arrange 1on1s for more shy colleague \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='395e55e5-4812-419d-acbd-624868592676', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n**Tell Me About A Time You Received Constructive Feedback**\\n\\nElaborate to the interviewer how did you react when you received the feedback. Show that you maintained a calm and composed attitude and were not defensive to the person who gave you the feedback.\\n\\nExplain to the interviewer how you evaluated and decided your plan of action. Elaborate on the strategy that you used to improve yourself based on the criticism.\\n\\nspeak too fast when I am not sure about the answer; trying to get around with it; \\n\\nshould look them into eyes; \\n\\nwhen ppl challenging me, I tend to speak really fast and get by the question \\nmy professor told me not to defensive to ppl's critisism and questions; if ur idea and research is good,  critisism would only make it better; if ur idea is crap, u don't want to stick to it \\n**use data to support ur response; just tell u don’t know and will look it up; ask ppl back on how to resolve the problem? should I look into more data or read another theory**\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d2929b96-1102-410f-9a94-b553a1a6fdb3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n****Tell Me About A Time When You Had To Give Someone Difficult Feedback****\\n\\n1st ask on slack; 2nd ask on jira; then schedule 1on1 \\n\\n*he acknowledged the problem, and I asked if there was a reason for the lack of progress.* \\n\\nShe expressed that she had been aware of the problem, but wasn’t sure how to fix it.\\n\\n*After she explained that a family caregiving situation was the issue; and he was taking frequent day offs;* \\n\\nI suggested him to change his slack status on day offs; and ask if he needs to quit the current project; \\n\\nI know negative feedback is hard to hear, especially when it’s someone’s first attempt at a task, so I wanted to make it a little easier.\\n\\nI met with this employee and started the conversation with something good she had done.\\n\\nI took note of a couple of strong points in the newsletter; the lead story she had chosen and the organization of her material were both excellent and I told her that before mentioning the formatting issues.\\n\\nMany people teach the “sandwich” approach to giving critiques or corrections:\\n\\n1. Start with positive\\n2. Proceed to the negative feedback\\n3. End with something positive\\n\\nThe positive is the bread, and the negative is the meat/cheese.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0b6a973f-33b8-4d4b-bb6f-20d96cd2f755', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nWhat drives you? What are you passionate about?\\n\\n1. driven by curiosity of understanding human behavior and how socirty works with data; \\n2. driven by empowering people with tools created by AI and DS tools; 本质上是帮别人看清how things work with data; for instance, they only reply on intuition to judge if a company is good or bad; \\n\\nbut in fact its a very biased process without themselves knowing it; sales的人教我的；他们会mention the founder is from florida; \\n\\n1. 谈到我的转行；我觉得做教授偏离了这个目的；too far away from the real world; 我的实习腾讯也要我大家都喜欢我；\\n2. 我自学nlp的经历；先自己上网课打好基础让自己问的问题不那么蠢，不准选课，我就去蹭课，然后假装学生去TA session，问具体应该怎么操作我的nlp的东西；\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5d8ccbeb-d2ec-4f97-8245-c6de2650b081', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n体现我的主动性\\n\\n- if engineering side is slow, I will do some analytics projects; 我是第一个把G analytics data整理出来的人\\n- 我自己找到了aws的人要的free trail credit; pretrain large model;\\n\\nproduct ppl未经我允许把probability分了档，出现很多人都是D grade\\n\\ni asked page view data; asked to add into next month's nps survey\\n\\n主动加了google map api来leverage geo features，gender ethnicity judged by name; 我如何操作的Update csv; 肯定有更好的办法\\n\\nIn the past year I did some projects and internship and felt that it’s better for me to go to the industry than staying in academia, so I am looking for jobs in data science and machine learning.\\n\\nI am an active investor. used the 1st generation of crowdfunding many times. I have been following the news of the 2nd generation, wefunder, startengine. One of friend told me that he was not optimistic with this type of angel investing platforms because  , I saw guys are trying to build CRM tools for crowdfunding, so I am quite interested. The job description is very broad, so I figured that I need to chat with you guys to learn more about your company and the position. \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2de5ba94-6e6d-4d7b-aa4f-0fbb4fdc6d0e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nWhy I am a better fit to the industry: I want to build product in a data driven way;\\n\\n为何业界更适合我：我能感受到自己的impact, 10 people read it; 我可以get things done; \\n\\nI can get things done and ppl like my work; In Tencent I completed 2 projects, one NLP and one user research project in 3 month as an intern. Ppl like my work;\\n\\nI am a good coder. I love code; reflected by the take home; \\n\\nI love tech products; I often send emails to developers about product suggestions; saw bedrock before on product hunt;\\n\\nI have a very strong sense of business and tech products that companies really like; It happens to me all the time that when I was doing intern in a group, ppl from other group wants to grab me to attend their meetings for some insight, because they know that I know lots of stuff. \\n\\n艾默生的名言\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a655275d-0c70-4713-b734-e4dc16a6794f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n**what defines me as a data scientist**\\n\\n1. My 1.5 year work exp shows that I can get things done, even with startup messy data infrastructure; lot of times I can handle tricky problems\\n2.  I am a fast learner and can adapt fast; I built django web app so non-tech people can use my model; 2 weeks; \\n3. very faimilar with nlp techinques and can use the SOTA models like BERT in deep learning\\n4. besides tech skills, I got rigorous tranining in social science so I am pretty good causal inferences, design experiments etc. I have designed maybe at least 100 + experiments. \\n5. i know the perripheral things about data science like how to collect data, how to label data; I understand what's the limitation of different datasets and knows how to explain it with analysis of human behavior. For instance, when I read indsutry reports I can always find problems or potential problems. \\n\\n90  vs. 00, 90 like apple more than Chinese cellphone brands; 00 reverse; because of more positive attitude to China. Counfunding factor; primary school middle school; they are more obedient and the survey is dissemtemed in school; can be reverse causality that because they use cheaper chiense phone so they like it more; \\n\\nI hav**e recruited labelling workers in China to label the ads in my wechat data**. When I am in Tencent and train the classification model, I spend lots of time to select labels that are both statistically robust and easy to use and understand for the users. mutually exclusive and collectively exhaustive; multiple label vs. single label classification\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e4777559-796a-4816-b74b-2726cf026ac5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n**Why are you interested in this role?**\\n\\nI love fintech; I love to wear multiple hats, closer to product etc.; \\n\\nYour product is like an intersection of finance and marketing, it's something that I know very well. \\n\\nI love mkting; feels like home; \\n\\ngood engineering culture; lonely \\n\\nclearly has nlp element in it; the contextual targeting \\n\\n[[lightGBM and XGBoost vs. catboost]]\\n\\n**Why yelp**\\n\\n1. i heard of this opportunity from my friend Palermo;\\n\\nhe said many good words about the projects and ML team at yelp; \\n\\nHe shared similar acadamic background with me we grad from same school with a PhD. We took many similar courses.\\n\\nwant to be surronded by talented people;\\n\\n1. frequent user of yelp; I am a foodie;  love to experiment new restaurant; pretty good cook  \\n2. although in theory I can do ML in any type of company, I would prefer to work on problems that I am more excited about; lots of review ; NLP element in it; location data etc;\\n3. In fact I have used Yelp Kaggle competition data before in one of my academic research; Kaggle competition;\\xa0 how to be a good influencer; exteme attitudes brings u more follower than neutral attitudes, contolling other factors;\\n\\n\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='11d61cc3-da00-49c1-846d-42221dbb06e3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n****What would you do in a case where you disagree with your manager?****\\n\\n**understand what is causing the disagreement.** As much as you are eager to make your case, take a step back and try to understand the perspective your manager has on the topic\\n\\nMake sure you hear your manager out and understand the rational. “it is impossible for two rational ppl to not having agreement”\\n\\nlet go of my ego and emotions\\n\\n**It is always best to remember your role is to help your manager and the team do the best work possible for the product**, the team and the people in in the team. It is never about being proven with the best idea. If you disagree with the managers decision, you should register your disagreement but as long as you are in the role you need to execute on the direction provided as provided it does not violate your morale campus and company policy.\\n\\nI will have a discussion with him regarding why he disagrees with my solution. I will try to present him a plan about my solution. why I think its the best approach and take his feedback. I will try to analyze his approach and find the best in both approach. Will try to discuss with him and come up with an approach which is agreeable to both parties. The idea is refining the solution and approach instead of personally taking the disagreement. I will make sure that we are attacking the problem instead who provides the solution.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dd2b999e-3ee0-48d1-bd4b-824ca9eb80f6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n****How do you sell an idea to senior management? If you use slides, what would the content include?****\\n\\n**Cover Slide w/ catch phrase**\\n\\n**Problem Slide**\\xa0- Hook. Present the problem in a clear and concise manner. Create empathy with the senior managers.\\n\\n**Key Findings that support the problem**\\n\\n- Use data\\n- Use user research - existing/untapped\\n- Use research- TAM, Competition, Future trends etc.\\n\\n**Goal/objective statement**\\n\\n**Proposed Solution -**\\xa0Who are you building for? What are you building?\\n\\n**Market Research**\\xa0on existing solutions/ alternatives\\n\\nHow this supports the current/future\\xa0**company mission**\\n\\n**Value Add- Impact**\\xa0that this will have:\\n\\n- Business\\n    - Financial - Rev? Expenses\\n    - Competitive advantage\\n    - Effort- Resources required\\n- User\\n    - Customer/user may be different\\n    - Customer satisfaction\\n\\n**Metrics**\\xa0Key / North Star metric for a product\\n\\n**Summary**\\xa0**Backup slides:**\\n\\nAt our best, product managers can see the problems nobody new how to define and build solutions that are so simple, people wonder why it didn\\'t exist before. Contrary to what the job title may suggest, Product managers don\\'t just manage \"products,\" but investigate the human experience to understand, define and bring to life the tools, services, and products that the world is hungry for.\\n\\nI became a product manager because I wanted to solve real problems that took all of this and more into consideration. I want to work with people, understand what motivates them, what moves them, and what blocks them to achieve what they want. I believe product managers are in a privileged position to work with people of all skills and expertise, and bring it together to synergy with the insight of the customers pain points and needs. When this happens, it\\'s like magic.\\n\\n腾讯\\n\\n1. collect data across dept; and lots of data preprocessing change stop word, define my own ways for word segmentation; 2. tried traditional models and more advanced models open sources solutions within company\\n\\n3. classification label; \\n\\n**欧莱雅**\\n\\nGoal: 客户提一些问题，我们用数据来回答 帮他们做了初步的数据分析的pipeline\\n\\n分析上不是很深入，但比较繁杂；数据量比较小；times series 的数据。常见的arima这类方法都有尝试。把他tabular data也做了\\n\\n1. Lots of data merging and combination; 整理数据；写了上千行；将近一半吧；欧莱雅天猫店的数据\\n2. answer clients\\' question with data: mainly tabular analysis and time series analysis (small dataset so not very good results); predict what drives sales revenue and profit; based on sales and marketing activiy variables, like banner, streamer, price promotion; identify what is best\\n\\nprice elasticity comparison across products and categories etc.; \\n\\n1. post hoc experiements with existing data; economics we call it diff and diff; compare the effect of sales promotion across 100+ different products; how does the promotion effect differ across their online store vs. retailer store\\n\\n比如直播对日常销售的冲击，双11对日常销售的冲击；结果我可能不太方便告诉你；直播有没有什么长期效果（对比各个产品，有些直播较少的）；直播对brand image有没有负面影响\\n\\n学到了：处理客户各种各样的需求，用data回答这些问题；对传统行业的客户如何和他们沟通\\n\\n**加拿大政府部门的项目**\\n\\n目的是：给通信公司一个指导，让他们了解消费者care的东西，扩大网络覆盖的范围和质量\\n\\nDV: satsifaction \\n\\n1. They are looking for causal explanation; regression type of analysis； factor analysis, combined the measures to create a story \\n2. conjoint analysis \\n\\n我主要做了clustter analysis; conjoint analysis；都是比较typical的mkt research的东西\\n\\n年轻人更care网速和价格，老年人更看重品牌和服务质量；偏远地区省份的人看重网速和限制；\\n\\n**关于这个工作**\\n\\n- 有一部分研究人行为的。我有信心可以contribute，只是看说我怎么在工作中leverage我的skill\\n- 有一部分沟通成果的，做ppt的，我觉得我也不错。之前实习老板给我的反馈来看，离职了之后老板还发打电话问我有个图咋做的。关键是逻辑吧，在学术圈里可能是最好的了，presentation效果一直都很不错\\n- 写code还挺强的，如果有什么处理数据啥的我也可以做\\n\\n**最大的优点；缺点**\\n\\n**缺点**\\n\\n稍微有点完美主义吧，也是做学术带来的，过分注重细节了。学术圈的人背景一致。公司里就不会了。**最近帮欧莱雅**做一个天猫网店的项目，人家不太care，missing value我是这样处理了一下，但其实客户都不是很care这些，所以后来如果我做这个处理或者不做这个处理模型出来的结果差不多的，这些东西和客户沟通的时候就不用说了，文字上的东西说清楚留着备案就好；\\n\\n缺点，学术圈带来的。和朋友说话的时候体现出来的，各种idea都觉得不是不可能。说话沟通的时候不肯定。it is likely that; might，比如你说A会导致B，加一堆限定词，在什么情况下，我认为A的发生会让B的发生概率上升\\n\\n因为怕不符合真理，怕被同行怼； 在业界大家care的是你研究的结果说的话执行之后的效果如何，以及和别人和团队的沟通效率，不应该怕被怼or不准确\\n\\n**Too focused on details. It’s a habit from the academia; like sometimes I explain how I dealt with missing value but the clients are not interested in the missing value**\\n\\n**Too cautious; because the academia is full of critisism; sometimes toxic; in the industry, people care only about the results and communication efficiency is really important, so it’s not ideal to keep your thoughts to yourself**\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='771be9c0-bb6a-45ec-830f-cedd305f40ee', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nteamwork work in a team example\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4ad263f9-f28e-42a8-a512-52d2e301c8de', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n**teamwork exp of data science**\\n\\n2-3 people; i am usually the one who pushes others; relatively successful; \\n\\n我主动要了aws credit，来train larger model; \\n\\n**Self motivated**\\n\\n**博士生就是自我驱动的；很多时候我们博士生经常交流如何催导师回邮件的心得，开玩笑是，连哄带骗**\\n\\n**DM的经验：learned many leasons;** \\n\\n刚开始都是see what can be done with current data; what \\n\\n**backwards from the need side is better; 我自己组织了一个ML meeting; help ppl see the value of DS on their work;** \\n\\n本质上要到重视DS的组织里 + get more ppl on board to the project \\n\\n- I am usually the one pushing the project to realization; not sure if its norm in the industry\\n- make sure other engineers see the value ; engs are not very concerned with end user at least in my company; I guess not particularly technologically challenging or rewarding;\\n- **make sure my end user is on my side and they need the tool, so they will help me push the product manager and other engineers;**\\n- **make sure my boss can help me, so other ppl will listen to boss**\\n- **make sure ppl will use it and measure feedback on my products;**\\n\\navoid reinvent wheels within the company;\\n\\nask permission from CTO and do it; \\n\\niterate fast; \\n\\n- Have a framwork of managing teams and the project; plan for meeting checkup and plan B for emergency; set a schedule for meetup; won't be too pushy;\\n- Culture; keep a balanced tension at work; don't want ppl to be too pressured and can't be too lose; want to be friends to those ppl; find an excuse to checkup\\n- know their skill level; if the guy knows python well and he does not know R well and does know pca factor analysis; leave some buffer\\n- 对上和对下；对上就是要保持沟通，多问；对下就是要主动给指导但不要pushy\\n\\n**如何体现我自己的好奇心**\\n\\n我自己的想法，是不是一定越复杂的模型layer越多就performance training error越好；\\n\\n对于过深的神经网络是怎么处理的。人们发现，网络过深反而会导致traing error增大，说明是back proporgation的信息传递出了问题，前面网络的信息传不到后面了，所以出现了residual and dense connection两种techinque, 来保证信息传递；后面也出现了很多变体，比如两层的传递项。虽然我自己还用不上，但我知道多数很深的网络都会用到这两种技巧之一。然后dense connection因为会把维度扩大，所以用的较少\\n\\n!Untitled\\n\\n!Untitled\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0ff7fe14-1944-4d74-b6af-e1eeebaf4be3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**我的business sense 有多好**：\\n\\nmy mentor says she has a project relevant to marketing. that I find the three promising suggestions that she had worked for more than a month\\n\\nHow to be different from forbidden city; where to find the breakthrough point\\n\\nFrom category view: mature category, use design to be unique; niche category, which criteria to have to select category; from consumer view;\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d044c5ba-3341-457f-81a0-cbf185932a3e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nmentorship you’ve given and received\\n\\n**given**: research assistants and lower grade phd students;\\n\\nsituation: they work for me as volunteer; no pay\\n\\ntask: make them feel rewarding and learn sth; its my responsibility\\n\\naction: 1. diverse task: try not put repetitive work to the same person; give my best guidance 2. they want to learn data analysis; organized workshop; organized slack channel, and answer Qs on the channel every few days; 3. I also helped them with grad school application etc.\\n\\nresult: they learned how to use R because they work for me; they refer their friends to work as my RA; very good relationship; ; we were still in contact; one Indian student become a data science guy after graduation; she said she was so afraid of math and coding before the workshop;\\n\\nmy professor is still using my slides and video to train his student in R\\n\\n**received: CTO and other colleagues;** \\n\\nsituation: tech stack and infrastructure; git pull request\\n\\ntask: learn about how to work as an engineer \\n\\n**Naming of things** \\n\\n`module_name`,\\xa0`package_name`,\\xa0`ClassName`,\\xa0`method_name`,\\xa0`ExceptionName`,\\xa0`function_name`,\\xa0`GLOBAL_CONSTANT_NAME`,\\xa0`global_var_name`,\\xa0`instance_var_name`,\\xa0`function_parameter_name`,\\xa0`local_var_name`,\\xa0`query_proper_noun_for_thing`,\\xa0`send_acronym_via_https`.\\n\\naction: 1. we discussed what he think I should learn; what I think I want to learn; we pick the overlap like docker, git, how to do code review etc. Also there are things that we didnt expected but I still found out need to learn; like naming of things; \\n\\n1. I read books, videos, list my questions on a 1on1; I ask samples from him; which project on our git has the docker module; I ask at our engineer team channel if I don’t understand something specific about a project;  \\n2. learn from mistake: large file git made an error and cleaned previous git history; he fixed it and expalined to me why it happened; I created a dummy git repo myself, everything I need to do I do it at my fake repo first; \\n\\nresult: I can work as an engineer with others; pretty confident with git; reviewed other people’s python code \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dd9763cb-2e96-4961-97c8-f6d43a0c3cf9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n****What\\'s your favorite product and why?****\\n\\n\"What do you mean by favorite product? Are you thinking specifically hardware, software, or a feature within those, or something non-electronic?\\xa0**Dealer\\'s Choice.**\\n\\n\"Are you asking why I love this product, or to explain why this product is a market leader independent of how i feel about it?\\xa0**Talk about why YOU love this product**.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a1378c9e-b08b-4765-8cca-1bb759b75925', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n****Tell me about a time you had to make a decision to make short-term sacrifices for long-term gains.****\\n\\n**situation**: 我发现有人有重复的data需求；直接跑一遍就行了\\n\\n**action**: \\n\\norganized a workshop \\n\\nhelp people use SQL vs. help them get data myself; \\n\\nsnowflake makes me realize its easy to implement; u don’t need to setup environment \\n\\nwaiting until we migrate to snowflake so its easier to get started; \\n\\n**result**: \\n\\nalthough not many come to the workshop, it has its effect \\n\\ntradeoffs as I was holding off different managers to show the right picture; \\n\\n现在有人告诉我说VP sales is really good at SQL\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6fdcd151-3d11-4291-abb9-916f58268dae', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n****Tell me about a time you handled a difficult stakeholder****\\n\\n**situation**: green bay packer; no investor insight score\\n\\n**task**: make the client happy; by give him the score;\\n\\n**action**:  \\n\\n 1. 我和cto，client manager, Product manager讨论解决办法\\n\\nanother meeting with him向他解释他的dashboard为何么有investor insight score；解释他项目的特殊性；feature is different \\n\\n1. 当晚给了他一个csv应急；和工程的人商量如何给他的页面设置单独的model backend; \\n\\n新加的feature用不上，老的Feature只能用上一部分在model里；\\n\\n1. created a roadmap to him: we will give u a csv tomorrow; give u in 3 days; \\n\\n\\n\\n**我学到的**：\\n\\n1. speak directly with the stakeholder to understand need; \\n\\n2. give them a plan about when; what to deliver; \\n\\n3. engage again to make sure we were on the same page\\n\\n最后决定说需要建立不止是模型的，还有feature pipeline的flexibility; \\n\\nMy approach to dealing with difficult stakeholders has always been:\\n\\n1. Engage - Directly engage with the stakeholder, meet or chat\\n2. Listen - Listen to what they have to say, patiently.\\n3. Understand - Understand their POV, even if it is impossible at some times\\n4. Ask - Ask clarifying questions. Why? When? What?\\n5. Engage again - Keep them in the loop until there is closure\\n\\nFor example, we were in the final stages of a very important, strategic project for our organization. I was leading the product efforts. We had completed 80% of the project and were racing towards the toughest 20% with 3 weeks left. I got a pretty heated email from one of the senior leaders in the organization that this project is a total failure and that we should stop all efforts towards this project. The task in hand was 2 fold. Continue taking the project to the finish line. Bring the difficult stakeholder into the vision of the project. Bonus - excite them about the project. The following were the specific actions I undertook:\\n\\n1. Worked with the stakeholder's direct reports and identified all their concerns. - Engage/Listen/Understand/Ask\\n2. Verified against the backlog to see if all of their concerns have been properly prioritized in terms of - in progress, done, backlog and won't do. - Engage again\\n3. Verified with the stakeholder's direct reports that the previously set up sync-cadences work. - Ask\\n4. Built a dashboard that talked about the positive business KPIs due to the 80% launch and projected the full benefits.\\n5. Exposed the backlog visibility to add in new projects that could be leveraged using this project.\\n6. Presented all these data to the senior leader on the above. - Engage again\\n\\nThe senior leader had a few clarifying questions, but this time after consuming the above information they were more conducive to hearing out our plan of action and also started suggesting some ideas on the future roadmaps and improvements. They were happy with the results and were hoping to engage on the concerns that we disagreed on.\\n\\n \\n\\nWhen dealing with difficult stakeholders, I normally do my best to understand their point of view like how they would like to explain and collaboratively arrive at a result that is in best interest for the company.\\n\\nFor example, there was a time when I presented a demo of a key project in our big internal conference. Sales folks was stoked by it and wanted to share with the customers soon. However, the conference ended with a twist that I wasn't prepared for. Our GM wanted us to release the project in a week’s time. While I felt strongly that the product was not ready enough, I first checked with him as to why he thought quick release was desired. He wanted to capitalize on the excitement given to sales people quickly. He thought I was risk averse in delaying the release. I could understand his point and asked him for a day's time to analyze the situation deeply and return. I stepped out of the meeting and immediately triaged all the bugs along with effort estimation by partnering with my engineering team. I pulled a meeting with my engineering manager and support manager to assess the pros and cons of earlier release. We concluded that even if we had extra testing resources and faster security reviews, we cannot launch in a week and if we do so, it would result in more support costs and stall us on our future release plans. I knew this would not give him any pleasure. I then partnered with my marketing manager to draft an internal campaign targeting sales, when we launch the product in a month’s time. I created a launch plan deck calling out the risks, opportunities and out GTM strategy. I first rallied across other key stakeholders to buy into the delayed release before presenting to the GM.Then later when I presented to the GM, he was quite convinced. We managed to release the project earlier than a month and it was very well received. We got 3 top enterprise customer testimonials appreciating this project.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='57aff6c7-22ca-4eb1-8b56-e50224472807', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n****What did you like and dislike about your previous role?****\\n\\nlike: Academia to the industry; good for transition, because I had to cover lots of stuff in a startup environment; In general I was doing machine learning, which was great, I gets to do typical ML stuff like recommender system; I was also encouraged to do NLP stuff which is what I am passionate about; glad I managed to deploy nlp models into our products; \\n\\nsince I have little support and weak infrastucture, I developed many other skills. Like I am really good with web scrapping right now, because I have to collect external data; I learned to write up django app to deploy ML models; Also learned how to work with other engineers like using git, docker, jira assign tickets, and confluence\\n\\nwhat I don’t like is support is too weak; I am the only DS/ML guy and simply feel lonely; company had no intent to hire more people; Not ideal for my career growth  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c00d8a55-9313-4961-b8d7-b6bab78aa99a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n****Tell me about a time you received negative feedback and how you dealt with it.****\\n\\nAs a PM i received a feedback from my program manager on my style of verbal communication. It is about me speaking faster when i wanted to get away with a topic that i wasn't confident (may be not backed up with data, or still in process of getting detailed insight of a problem etc.). Whereas when I'm confident I tend to speak slowly or more assertively that made people to follow easily.\\n\\nI welcomed that feedback so from then on when I'm not confident in a topic I became more assertive to let people know that i'm still working on it and get back later once I have all facts. A small tweak but made myself more confident as a better PM\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='569dca28-ef14-4b37-bb40-f510576b058e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n****Tell me about a time when you had an idea you proposed was not agreed on****\\n\\nDisagreement --> persistent ---> more data insights ---> positive relationship ---> mentor/trust\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b2e45dbd-118e-4c14-b113-c01ffefbe64a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n**Could you tell me about a time when you faced competing priorities?**\\n\\n**situation:** \\n\\nbug with a model for a particular client vs. data analysis insight job vs. new model at hand; \\n\\n**task** 我考察的三个重要因素： **importance vs. urgency vs. resources/effort required; needs communication to determine them**\\n\\nwhen we have 2 week sprint it is also how we plan on projects; \\n\\nbug with a model: **high urgency and importance, low effort on my side**\\n\\nI talked to our client manager and learned the issue with the model; find out its urgent; evaluated effort required from myself and other engineer via meeting; \\n\\nfound out the bug, finished my part in a few hours; delivered a csv as a temporary fix; \\n\\ndata analysis insight job：**high urgency low importance; not sure about effort;** \\n\\nimmediatly; I showed him how to look it up on the external website; and gave him a result with our own data\\n\\nfinished scrapping external website for a more thourough analysis the next day\\n\\nHigh Urgency + High Impact > high urgency + low impact > low urgency + high impact \\n\\nTo prioritize multiple projects, I will start with talking to stakeholders of each Project to identify if any of these Projects Goals are aligned with the goals and objectives of the organization. If any of the project is aligned with overall goals of Organization then I will give Prioritize that Project with Number#1 Priority.\\n\\nFor Remaining Projects, If any 2 of the Projects are dependent on each other or in sequence then that will be easy pick to identify which one can be done first or second.\\n\\nFor Projects with competing Priorities, I will talk to stakeholders to find the Urgency (High, Med, Low) of the Project to find how sooner stakeholders want that Project. And then i will ask them the overall Impact (High, Med, Low) of the Project to all end users. OF course we need to find some quantitative data to justify the values of Impact/Urgency. Like, Impact to 1000's of users. Very Urgent due to some financial or data or regulatory requirement.\\n\\nI will use below 4 pillars to rate and prioritize those 5 projects; \\n\\n- Analyze the importance/impact of those 5 projects, to our team and to the whole company maybe\\n- Find out if those 5 projects have any dependency between each other\\n- Clarify the timeline of each project; once I got the timeline, communicate with other stackholders both verbally and on tickets, so everyone knows what to expect from me for each projects;\\n- Identity the status of resources to work on those 5 projects and also the status of their dependencies\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d6f70b6b-8292-4ace-8b97-c92779083caf', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\ndifficulty; how you’ve managed roadblocks or tough situations and what you learned;**关于困难**\\n\\n**situation**\\n\\nnlp project; two roadblacks: first is better define need to make it a feasible problem for data science; \\n\\nsecond is a tech resource problem \\n\\n**how I solved it:**\\n\\n- the original requirement is not feasible technically; need to assign labels next to content;\\n- after some meetings, we refined the need of the project in general and planned two stages;\\n\\n first stage helps the new recruit asap; second stage I can’t guarantee result but worth a try; \\n\\non top of 2nd stage, I also made two more rounds of iteration; \\n\\n **solving 2nd problem:** \\n\\nI first tried to work on the code to see if it works; \\n\\nthem scrapped a few g data; estimate gpu ram needs; 一方面需要通过parameter计算ram用量，number of parameter * 4B = 20MB; 另一方面问题在于input text长度；我的主要问题是text太长；\\n\\nevaluated different options for online GPU; discussed with our CTO and the strategy person about parternships \\n\\nasked for $1000 test credit; \\n\\nuse different techniques to minimize ram usage to fit my need\\n\\n**result**: improve %2 \\n\\nTalk about a difficult problem you’ve had to solve. How did you solve it?\\n\\nPython coding不会；之前都是用R但这个问题用不了R；\\n\\n- 我自己学，然后不停的用边缘的学习迫近那个问题；先一步步来，自己看网上资料和网课，这样问的问题不会很蠢或者很泛泛。问经济系的同学；\\n- 去旁听课，假装自己是学生，去问计算机系的助教；\\n- 说服导师雇佣了一个RA专门帮我debug；\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3a45cae3-d076-40b3-aa37-90213d22f178', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n****Tell me about a time you disagreed with someone and how you resolved it. conflict;****\\n\\n**situation**: tag system的问题: they want full flexible tags; I feel it won’t help the data science\\n\\n- conflict: different understanding of user experience\\n\\n**task**: 本质上goal略有不同；我希望data science feature更多，他们觉得full flexibility is best for user; 我要站在他们的角度说我的方案是更好的用户体验\\n\\n**action**\\n\\n我说服了他们这不行，拿我在tencent的经历；说服他们用户也希望有一些代表性的tags；比如你注册social media的时候你也不希望，直接白板让你随便填；给你一个list of 10 tags feels easier, and its easier for ppl to use this function\\n\\n**result**\\n\\n找到一个中间道路：leave the flexible option but give a set of tags; \\n\\n- we discussed 2 solutions: add a custom item or combine similar tags for analysis; agreed\\n- we met with a long term client and created a set of tags; we still allow free input but will iterate over time;\\n- not many users use the feature; for those who use, current sets is enough; turns out people use tags that only themselves understand; invalid for analysis\\n\\n**takeaway**\\n\\n- In general the key takeaway here was that both of us were working towards a shared goal of improving product and . 站在别人的角度才能说服他\\n- 我自己也要提醒自己保持open；data science is an empirical science; 但自己有信心的时候就得说 One should not get fixated with one idea and should be open to evaluating counter arguments as well\\n\\nmy business and product sense is strong; compared to other engineers; besides the product manager, it is usually me who talks most; I often make product suggestions during meetings;\\n\\nI persuaded the product manager to change free tags to tags system; \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6d665b52-6fe9-4f7a-aa82-33d6ec1a92ca', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\ncollaboration 体现我的合作能力\\n\\nhow to listen, and how to express my own opinion; how to find the common goals and achieve together\\n\\nSome interpersonal skills you might mention include:\\n\\n- The ability to take on leadership roles\\n- The ability to motivate yourself and others\\n- Working well on teams and in cooperative environments\\n- Demonstrating responsibility\\n- Flexibility\\n- Empathy\\n- Knowing how to express sympathy\\n- Dependability\\n- Knowing how to ask for clarification as needed\\n- Possessing effective communications skills\\n- Knowing how to listen actively\\n\\n*My coworker and I expressed our frustrations with each other, and we both acknowledged that we misheard each other. I suggested implementing a color-coding system to use on a shared spreadsheet for future projects like this one. This new system works well for us, and we haven\\'t had any similar issues since this incident.\"*\\n\\n****How would you describe your collaboration skills?****\\n\\nI am a self motivated persuasive person; group projects, im the guy who take a lead and push other people; \\n\\nI enjoy help and supporting people; my unique background in social science; brainstorming session I am good; \\n\\nI appreciate being supported; \\n\\nAs long as you\\xa0**keep it professional**\\n, however, and do not start pointless conflicts just because they believe in another God, or vote for another candidate in the elections, you should be fine. That’s exactly what I want to do–I want to\\xa0**focus on work**\\n, the\\xa0**goals we try to reach as a team**\\n, and not on things that separate up. This has worked well for me in school, and I am sure it will work fine in the job.\\n\\n我说服sales team 换naming system; \\n\\n什么文件是必须的，必须的放在一个文件夹里；非必须的在另一个夹子里\\n\\ntime; version; fuzzy matching, who put this in这些我不需要match但是有用;\\n\\n和data engineer商量data base document；比如你不能到哪都命名DM-data;  \\n\\n*I believe my collaboration skills are strong because I am always willing to listen to other people’s ideas, even if they differ from mine. I also think it’s important to be open-minded about different perspectives and opinions. In my last position, I had a disagreement with another employee over which software we should use for our project. Instead of immediately insisting on using the software I preferred, I listened to their concerns and asked questions to better understand why they didn’t want to use the software I suggested. We eventually came up with a compromise where we would use both software programs.*\\n\\n*\"I\\'m very comfortable in team settings. I find that group work brings out some of my best qualities. I always looked forward to our brainstorming sessions in my previous position working with a marketing team. I consistently offered more suggestions than others. Not all my ideas were followed up on, of course, but many of the team members liked hearing about them.*\\n\\n*I enjoy working as part of a team because I find I\\'m more successful and engaged at work when I know I have a team to support me. I also like to make time during my workday to work independently. To support both team and individual goals, we met for collaboration in the mornings and used some time after lunch to work on my tasks independently before coming back together at the end of the day to complete team objectives.”*\\n\\n*The characteristics I feel are essential include mutual respect, clear communication, a connection with common goals, and a willingness to contribute.*\\n\\n*\"I like to listen to my teammates during meetings first to get an idea of their perspectives before giving my input on strategies or methods for planning and initiating project tasks. We also use collaboration techniques that help all team members provide input and share their ideas. This way, each individual has an influence on the success of our projects.\"*\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='930541f5-5c92-43ff-8f1d-2d8130ed7f2f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nDescribe a situation where you had to work with a colleague you didn’t get along with.\\n\\n**situation**\\n\\nI would first talk to them privately in a non-confrontational manner, using “I” statements to suggest that there might be a problem that we should resolve together. I’d also do my best to determine the root of the issue and to see if I or other team members could improve this person’s productivity.\\n\\n\\nCTO suggested me to ask opinion from the UX designer about how many options should we put on the page for ppl to enter; \\nux designer; \\n\\nhave to get the opinion from him; three options vs. 5; position on the mobile phone \\n\\n**challenge**\\nthis friday - next monday - next \\nhave a lot of different opinion about the experiment plan \\n\\n**action**\\n\\nask him on slack; on jira ticket publicly \\n\\nbefore chat with him I asked the CTO; book a 1on1; non confrontational manner; \\n\\nwhat are the blockers exactly? turns out he was not happy about some details of the exp design; \\nconvince him about the exp design; \\n\\nwhy I select this industry for data collection; backed by data from other competitor websites; \\n\\nwhy I believe at the first stage, 300 people is enough sample size; considering length of exp; conversion rate   \\n\\n**result**\\n\\nI did took some of his suggestions but also rejected some; I know exp better than him; \\n\\nresolved his concerns;\\n\\nhe had a pretty thorough report the next day; we were able to move the project forward; \\n\\n**situation**: 有个product manager完全不推进项目；I proposed a new price formating; had meetings with all stackholders; \\n\\npreparation work: exp design; data collection plan; analysis plan; \\n\\nhe was responsible for decide the tool for ab testing \\n\\nits the first time I meet him; \\n\\n**task**\\n\\nget the ab testing running; \\n\\n**action**\\n\\n1. try to be professional; past due on the ticket; I ask him whats blocking him first personally, then on the open jira ticket; he just didn’t reply messages; \\n2. trying to be nice to him and friendly; how much I love philiphno food and there are several great restaurants that I have been to \\n3. finally I asked him for a 1on1 meeting and asked him why; \\n\\n**result**\\n\\nwhat are the blockers exactly?\\n\\nturns out he had some personal issues and was having day offs frequently at that time; and he was sorry about it; \\n\\nI asked him to be open about those things and assign the task to other ppl if he can’t commit his time and energy that time; \\n\\ni asked CTO if I can do it? can I make the decision myself about the tool? he says better to leave it to product team\\n\\nhe did quit from that project after our meeting; and the product lead took the job; but decided not to proceed with it; \\n\\n*“I haven't had any major disagreements with teammates, but whenever we had differing ideas about defining the success of our projects, we always took the time to clarify expectations, communicate effectively, and provide individual input for the group to consider. I've found that connecting with my teammates and listening to their perspectives helps me find ways to compromise so we can focus on our important shared goals.”*\\n\\nThere’s no rule that says everyone in a business has to like each other, but they do have to work together regardless of their feelings. Interpersonal team dynamics play a huge role in successful collaboration, and it is important to identify individuals who can navigate these sometimes treacherous waters.\\n\\nWith this collaboration interview question, you are looking to get the person talking about their approach to difficult colleagues. You don’t want to hear simple or cavalier answers, as these situations are often difficult to resolve.\\n\\nRed flags include interviewers who “don’t really have conflicts,” or stories that end with some version of “so I don’t really see him/her around anymore.” The reality is that conflict is baked into the collaborative process. A person who won’t recognize productive disagreement, or deals with it by avoidance, is not likely going to help their team.\\n\\nWhat you want to hear is an answer that shows the interviewer has the team’s needs at heart, not just their own.\\n\\nInstead of blaming the other person for the problem, these candidates are taking action to repair or revitalize the failing relationship. They will describe how they took initiative to make changes on their end, and worked with the other person to resolve the unproductive conflict.\\n\\nOftentimes interviewers with good answers will talk about how they came to understand their problematic colleague’s point of view. This is a very healthy sign that the candidate will be able to troubleshoot and settle this common threat to collaboration.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c9a90f2d-996d-4a8e-894c-70c5dcd356b4', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Tell me about a time you worked well as part of a team.**\\n\\n\\n\\n\\n\\n我和data engineer; \\n\\n1. 我提需求；和他们简单讨论后觉得可行的话，是否一定要create a pipeline for this？是不是一次性的？对公司其他人是否有价值，create tickt on jira; 具体列出我理想的table 长什么样子，有几个column都是啥；更新的频次如何; \\n2. 他们执行; test if it works; \\n3. make it a function or feature; I will draft the documents and update our variable databse; they will improve on the doc; \\n4. 学到的东西：保持充分的沟通和项目update；这在小team容易实现，要延伸到大项目上\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4c146223-6721-41f0-8b39-f5e12861a45e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nnlp project如何合作的\\n\\n主要是我向别人提需求；因为我是唯一的DS guy; I need to find a way out for collaboration; 我来组织jira；\\n\\n1. Convinced CTO; 提的要求有点不合实际；basic task is to save time for current process; define needs with onboarding team; Minimal viable product；写成文稿给CTO看\\n2. Asked help from client onboarding team to prepare data for me; 搞定training data: 和sales team 合作重新命名所有的文件；要到之前的quesionnaire;\\n3. Asked help from data engineer; tech tools：tech support engineer负责aws的；教我处理了各种tools；sagemaker notebook it logs me out every 8 hours; we figured out a way to run it offline;\\n4. 建成这个pipeline; data engineer教我搞定了文档的存储；front end engineer django app的布置\\n5. in the end, the output are 3 documents:\\n\\n我的经验：\\n1. ticket越详细越好；document及时更新让大家看到我的进度，必要时主动邮件抄送所有相关的人；我也会主动汇报别人的进度；\\n2. 约定时间和人check up progress 这样人家不会感到被驱使，重点是了解blocker；用lets work it together 的态度，而不是用我指示你做什么的态度\\n3. align everyone’s interest on the same project, importance of DS;\\n4. 有些重要的会议involve CTO进来用manager的力量来督促别人\\n\\n意见不一致只要充分交流确保目标一致都可以解决\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b9d2cd16-f1ee-491b-849a-4092be130817', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nDo you prefer to work independently or as part of a team – and why?\\n\\n- i am a pretty independent worker; from my academic exp; not because I chose, its just I am used to it;  but I really enjoy working with ppl;\\n- it’s nessasary to work as a team to find value for ML; sometimes u have to say no to other ppl’s needs on DS; unrealistic or unnessary needs\\n- I enjoy both help others and other ppl helping me; clearly I have a lot to learn in DS or as an engineer; glad to find that I also have a lot to offer to other ppl;\\n- braimstorming sessions I am prob one of the most talkative; my product sense and business sense is pretty good;\\n\\n**What to listen for**\\n- An understanding of the benefits of both approaches and the different situations in which they work best (ex: working independently during focused sessions and collaborating to brainstorm ideas and gather feedback)\\n- Fluidity and comfort working either independently or in a group setting\\n\\n**What this question assesses**\\n- Ability to take initiative when working solo, and to build rapport and foster constructive and effective relationships with teammates\\n- Leaning too heavily on the team for direction if they signal that they struggle to work alone\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ca298338-d9f7-4a42-ab4b-fa294298bfad', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**What role have you played in team situations?**\\n\\nim the one who push others; \\n\\n1. define goals and resources; separate goals; always have a clear timeline and responsibility allocation\\n2. always have a backup plan\\n3. set up checkin time points for blockers; don’t rush everythign at the end\\n4. communication skill: I have done this, what do u think; what’s ur progress? \\n5. 尽量了解每个人的长处短处和需求；allign people’s interest together; so they can appreciate ur proposal more; figure out each team members needs; if they just want to get it done its fine; I want good grades, they don’t; I can’t have high expectations on him\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dbd7e1bf-438f-414a-8b88-d98132e185c9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\ncareer goal; areas of interest in tech and your future aspirations; 5年职业规划; ****What are your job expectations?****\\n\\n- ****Define your goals**** I enjoy ML/DS very much; I am sure this field will be growing; to be a senior manager under the current path; 现阶段还不知道自己要成为技术专家还是管理者；两方面都努力进步because growth opportunity is huge in both direction for me;\\n- ****Summarize your plan for achieving your goals****\\n1. hard skills, become a tech expert: two side: one side is to get deeper, better understanding of SOTA in nlp via reading and coding; besides work, I also started work on my personal side project on nlp \\n\\nanother side is to digest existing knowledge and skills better by knowing their limitations and advantages in practice; I have gained some exp in the startup environment; those things can only be achieved by experiences; \\n\\n1. soft skill: how to get things done in DS in general; everything is a collaborative work in the modern society; 两方面任务：how to discover and define needs that can be resolved by ML; how to leverage resources to get things done; no concrete plan for this; all I did is keep reminding myself to be open minded and be proactive in communication; \\n\\nhow to get resources from others, how to work with them and how to give out and help others; how to align myself properly in the small and big team; action: 如何进步：总结复盘自己的项目经历；多和人交流，包括公司内和公司外的人\\n\\n- soft skill involves lots of other stuff besides career growth, including improving emotional intelligence and empathy, being authentic and over communicate etc. I am pretty interested in this kind of personal growth in general\\n1. from what I learned from previous rounds of interview and my friend palemo; *your company encourages employee growth. Last time I discussed mentorship and career development opportunities with the interviewer. I would love to prove my value at this company and make good use of some of these opportunities. Hopefully, I could work my way up and be an admirable DS/ML leader.*\\n\\n- 2 types of DS: how to convince people with data; how to leverage models to empower products and companies; I had some exp in both of them, but certainly has a lot to learn\\n\\n**问题分析：**考察候选人对自己未来发展的设想、职业生涯的规划能力。\\n\\n除非是目标非常明确的人，或者有多年工作经验的职场人，不然很难回答清楚，那么怎么说才能回答好这个问题呢？\\n\\n不要说“几年当主管”，\"几年当经理\"毫无意义。\\n\\n**参考建议：**\\n\\n第一，介绍自己认真思考过这个问题，自己的规划是基于目前的实际情况来设计的。\\n\\n第二，在工作方面，突出自己打算通过积极完成工作任务，积累各方面的经验，让自己成为这个领域的专业人士，也希望有机会能够带领团队，成为优秀的管理者，为单位做出更大贡献，获得双赢。\\n\\n第三，在学习方面，打算在专业领域做进一步学习和研究，将实践经验与专业知识相结合，为自己的职业成长做好铺垫，打好基础\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='58c4be23-336e-4bd4-812e-be7478c61f57', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nTime you dealt with failure; mistake 失败\\n\\n**situation**: I proposed a new price formating; need to change front end and a ab testing tool to test the new formating\\n\\nI proposed the idea because our competitors are doing it; designed the ab testing plan very carefully and had a long documents for it; analysis plan; if this happens what can we conclude etc. \\n\\n150% and 200% of our baseline; presented and approved; asked our legal team about potential risk of running it; \\n\\n**task: front end change; find a ab testing tool and run it**\\n\\nthe current tool only support CTR; the exp needs comparison of numeric values about \\n\\nI proposed using amplititude or google optimize; \\n\\n**result**: \\n\\npartially failed: finished one stage, which is changing the price format; \\n\\nthey just launched the new format without testing; with my proposed number; significant increase\\n\\n**why failed**:  I created the ticket, CTO told me let him organize it and assign tasks to different people; \\n\\nnot enough engineering resources for ab testing; no need for other projects for now\\n\\n**responsible:**\\n\\n**what I did to make it happen?**\\n\\nim partially responsible \\n\\nI did not insist to push and follow the projects frequently\\n\\nI failed to connect with other stakholders and engineers about the needs and importance of the ab testing; just let CTO handle it but its not his priority; \\n\\n**what I learned/make sure it won’t happen again:** \\n\\n- need to better understand and manage engineering resources beforehand; also need to know  that resources can be more if I fight for it and align with other people’s interest; for instance, find other needs for ab testing so people will be more interested to develop a good tool for ab testing\\n- many people don’t appreciate the value of data science, they have their lay believes that new feature is always better without testing;\\n- I organized data science workshops and brain storming session to help ppl understand importance of data science; translated into several projects; 主动找finance, sales people 聊how data science can help them; even with simple ways, like I generated some table with descriptive statistics for them\\n- 反思我自己，也存在disconnection with user；I asked them to involve me with meetings with a long term client in another project and learned a lot\\n\\nTell me about a time you failed. How did you deal with the situation?\\n\\nFailed the paper submission;\\n\\n1. don’t let yourself down emotionally. Understand it is very normal. 通过和导师交流说明被拒绝是极其正常的事。情绪上修正。\\n2. 坐下来很理智的分析为啥被拒绝；its not gonna work；我们猜了一下reviewer是谁；是不是通过收集一轮数据就可以解决？Reviewer推荐的一些方法已经试过了，不行；现有条件下有没有可能达到原有的目标；如果达不到\\n3. 我的目的要不要调整？可能只能发一个不是顶级的期刊；和我的dissertation不相符；他的目的是什么？\\n4. 多问问人，多找机会在公开场合present收集一些意见，先暂停这个项目，有新的想法再做打算；\\n5. how to stop this from happening in the future; not a theory driven paper; effect driven paper\\n\\n**situation**: recruit participants for a study; 200 in 3 weeks; got fewer than 100; didn’t expect to be so few people; have to tell them to come back next semester\\n\\nIts my responsibility since I was responsible for the task; two professors working on this project; \\n\\n**Why failed**: \\n\\nprofessors also do not have much exp; we have our standard way of recruit participants online; we had a meeting about it \\n\\n- I bought chocolate, and tiny suverniors like a pen that has a rotman logo on it; print out posters, which has QR code and our email;\\n\\nHonestly I tried myself for two days and it seems work; 40 people \\n\\n- I taught the RAs how to do it and saw how they act at campus; after 2 weeks it didn’t work\\n\\n**What I learned:**\\n\\n- over confidence; thought it will be successful and had no backup plan; daily registration are likely to decrease over time\\n- system of tracking; by the end of week 1, what should we do if the number is not expected\\n- \\n\\n招募被试；没料到这么少，这么麻烦；最后和导师交流之后他说要踢掉这些人；按计划实施之后招到的被试很少；\\n\\n我和我的研究助理费了很大功夫；包括在fb上；在校园里面给人发巧克力，一些线上的渠道；后来导师觉得太少了；\\n\\nRA在执行上有问题，保证了训练到位，但有一个新来的我导师招的人纯不靠谱；我没有用不同的二维码给他们俩\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5dab43e9-3989-4662-a87b-f0701734c311', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n**问我希望工作环境是什么样子**\\n\\n小团队；low ego; 坦诚高效沟通（教授不回邮件）；目标一致；有一些有趣的人\\n\\n软硬件上的支持，我相信XX比学校给的多; tech mentor; a little bit nudge is enough; When I first started to learn web scrapping, bug I coundn't understand, I go to TA session pretend that I am a student; \\n\\nThe reason I turn down the offer from tencent is that the team; \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='42f85d80-302f-430d-bb4a-10a04b767416', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**你觉得自己是什么性格的人？**\\n\\n比较安静，认真，努力，自我驱动力比较强的人；\\n\\ncan fight and get through rough time; have my way of dealing with difficulties; \\n\\na nice guy; pursuasive guy 有能力影响他人; Many ppl think I am an influencial person but I honest think I am a introvert guy \\n\\ninteresting and high curiosity, self motivated fast learner; geek in many different things; read a lot; my friends say they can often got inspirations from me; \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='76c2c52c-fdc6-4d42-b1b8-b52b2d0db5bf', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**如何说服别人**\\n\\nmy business and product sense is strong; compared to other engineers; besides the product manager, it is usually me who talks most; I often make product suggestions during meetings;\\n\\nI persuaded the product manager to change free tags to tags system; \\n\\n2 solutions: iteration or combine similar tags for analysis;  \\n\\nshe created a set of tags and asked me opinion about it; we still allow free input but will iterate over time; \\n\\nturns out people use tags that only themselves understand; invalid for analysis\\n\\n说服导师做我想做的research idea；用data and theory; 稍微先做一点；\\n\\n自己越懂，就越能说服别人；自己要先做一些身先士卒\\n\\n强迫导师雇佣了一个RA专门帮我debug；他没股用过CS的RA，他只觉得需要雇佣RA在实验室里帮忙收数据或者去大街上发问卷，他从没想过这种问题，就是找一个CS的RA来帮忙；\\n\\n眼下：说服他两点：花不了多少钱；一个study，就得几百块；RA一个学期顶多3，500刀；code还是我来写，他就帮我debug和解决我没法解决的问题；\\n\\n愿景：他想的收数据的方式都是很传统的，比如在实验室里收和找咨询公司买数据；借助我这个项目帮他完成big data的转型\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6f9c4daa-9ea8-44b3-b3bc-150e43491ac9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n对上管理，对下管理\\n\\n**对下**\\n\\n**定目标（团队目标和个人目标结合），给方法（书面的步骤，模板），多监控沟通，用被人觉得舒服的方式，比如邮件说我写了一部分了你写了多少**\\n\\n**导师说没问题我就去干了，我没问他具体细节**；事实是，他自己都没招过，他assume这样操作就行，操作细节也一概不知；我的错误在于，assume他知道怎么操作，听他的就行，虽然自己简单实验了一下招募的操作，但是没估算具体的效果\\n\\n结果：发邮件说因为先到先得的政策。我只能说，我们下次有事，提前叫你保证你的收入和你的position。我们的reputation受损了；以后招募被试没那么容易了。\\n\\n**事前的操作上细节的沟通，对deadline不清楚，对结果的predictioin是否一致，如果出现了这个结果怎么办，如果那个结果怎么办；**\\n\\n**如果干一周只有这么多，那怎么办？设置检查点；改止损的时候止损，该改变策略的时候改变；**\\n\\n**对下属要了解，要留有预案以防下属办事不力**\\n\\n关于coding；如果你们team有处理的需求的话，我完全可以做；\\n\\n总而言之，我很想加入是因为；\\n\\n我自己挺有体会的；对方在抱怨的时候，表示赞同，做了一些不愉快的工作；证明我比别的博士准备的更好；\\n\\n**担心我进业界是不是难以磨合，我要向人保证我ok；不要帮学术讲话；企业的考虑方式是，你是值得的，我就愿意把好东西给你来招募你；不是说我不要我不care什么；**\\n\\n**不知道是否可以让我做一些更advanced的活；不要说FB之类的其他公司；对方怎么想的不好说；就说我听说过其他公司，博士可以xxx，可能性；**\\n\\n**暑期的时间，向你们学习一下，如果觉得合适的话，可以考虑之后加入你们；**\\n\\n**只表示我欣赏腾讯，没考虑过别的公司**\\n\\n**不能讲我毕业回不回国；要确信我回国；**\\n\\n受虐狂；我要让人跪下 vs. 我要给他跪下\\n\\n不可以讲嗯嗯和点点点；要用好，ok\\n\\n不要用验证了我的结论了“吧”这样的\\n\\n准确率是多少：直接给我个答案；\\n\\n不要过分谦虚；不要说，不了解算法细节；\\n\\n问具体怎样验证的？：验证方法有，1，2；具体的细节我不太清楚\\n\\n不要有和业界隔开的心理；不要强调我和业界的差别，要帮别人省时间，帮别人；\\n\\n**Situation: Describe the context or situation. Explain where and when this group project took place.**\\n\\n**Task: Explain the mission of the group – describe the particular project you were working on. If there was a problem in the group, explain that problem or challenge.**\\n\\n**Action: Describe the actions you took to complete the project or solve the particular problem.**\\n\\n**Result: Finally, explain the result of the actions taken. Emphasize what your team accomplished, or what you learned.**\\n\\n**关于teamwork**\\n\\n**清楚两方都知道职责是啥，deadline是啥，expectation 的结果是啥，有一些意外方案，如果结果那样，应该怎么办；如果结果这样，该怎么办；保持工作进度的update；有一个meeting email 回复的安排。每个人的工作方式都有个调整期。我觉得我不是特别死板的人，我可以调整。**\\n\\n1. Talk about a time when you had to work closely with someone whose personality was very different from yours.\\n\\n比较Pushy的教授喜欢老来催我弄得我有点不舒服；老来问我读了什么paper啊这种；\\n\\n我有点希望按自己的思路来；需要读很多paper的，要通过海量的知识摄入拎清楚自己的逻辑，然后才能说得比较自信清楚。后来我聊了这个问题，说我要是paper没读好没读全会自己没信心。后来交流过后他说，他之前一个学生不催就不干活，美国人，最后找不到工作啥的；我每周1-2次update，不需要完美，大概读到了什么theory\\n\\n**别人犯错了你咋办？**\\n\\n硕士的时候，几门课都碰到这种情况。我自己还好因为我个人的情绪管理能力还ok。但对整个team不公平，很影响士气.\\n\\n- 弄清楚为啥他miss deadline，并不是说那个人 蠢，哥大的mba；其实就是不是很care那门课，目标就是和你不一致；\\n- 一方面明确deadline，任务责任，结果是什么；以邮件书面的形式，meeting brief，这样大家都有压力些；\\n- 另外可以建立以下个人关系吧，这也有帮助，喝个咖啡啥的。比如两周的task，过一周问一下Update结果会好很多，我也不能用leader的语气，只能以合作者的身份，前提是我自己做了一些东西的情况下，我发邮件问下他；我说我的内容做了这些这些，可能和你做的部分有点重合，你做了多少？\\n- \\n- 另一方面营造一个团队氛围吧；邮件组里形成那个沟通讨论的氛围吧，无形中给人压力。\\n\\n如果说deadline 已经错过了，就想办法补救吧，不行的话只能我自己上，以后同类的任务就多留些空余时间来，或者找更有能力的人来\\n\\n**若领导布置了大量的工作，而完成时间又十分有限，为了完成任务，您怎么办？**\\n\\n**问题分析：**考察候选人的时间管理能力。\\n\\n**参考建议：**\\n\\n第一，分清任务的轻重缓急，紧急又重要的任务先完成。\\n\\n第二，发动团队其他成员，借力完成。\\n\\n第三，鼓励老人带新人，提高工作效率。\\n\\n**Tips：**实在是过重，以上方法全部用上了都不行，可以与领导协商，先完成几成，其他不重要的任务可以缓办。\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f175b121-f03e-4194-bf76-13cb870d819c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**面试入坑题：怎样看待加班？**\\n\\n**问题分析：**考察候选人的责任心和职业道德。五花八门的回答：\\n\\n“我不愿意接受无意义加班”\\n\\n“没问题，随时都可以加班”\\n\\n**参考建议：**\\n\\n第一，任何一家单位都有可能要加班。\\n\\n第二，自身的工作任务没有完成，加班是理所当然的，当然，**自己会不断提高专业技能，以尽量减少不必要的加班**，之前也是这么做的。\\n\\n第三，如果遇到紧急任务或突发情况时，需要加班，自己会尽己所能，希望能够尽快顺利地完成团队面临的任务。\\n\\n**Tips：**表现出自己愿意牺牲自己的一部分个人时间，提升个人能力，为公司创造更多利益；明确岗位是否需要经常加班，表明自己态度。\\n\\n**leadership**\\n\\n情况：有点半死不活；五个人以上啥的；要被取缔边缘的\\n\\n我当电影协会会长时候的经历；第一我有面试，招新的时候阵仗很大海报做的比较fancy找一个同济学交互设计的同学给他钱做的，一般娱乐类社团都没面试的\\n\\n第二比较民主，open的那种赋权吧。每个人自己感兴趣的活动，他来负责，保证时间投入不大的情况下，我全程参与来把关。每周一起看电影，然后邀请一位同学做几页ppt讲电影，让同学自己选他喜欢的，但有底线比如豆瓣7分以上，打分人数要1w以上，不然他会选特别奇怪的电影；豆瓣写影评，互相点赞小组；每个月一次去北影看电影；去看指环王3部曲\\n\\n一开始来的人少我也不知道什么情况；后来才知道\\n\\n结果：人数长得挺快的，不到十个人发展到骨干人员都十多个，其他边缘一点的50个人；我那两年发展的很不错嗯；现在还和一些社团骨干有不错的关系\\n\\n比较难的，或者说要求身份的，比如邀请北影的老师来讲座啥的，只能我自己来；\\n\\n我自己RA team；四个人；难点在于不给钱的\\n\\n我学习了一些老师的RA的经验，但用处不大，因为我没钱的\\n\\n- 明确需求\\n- 个人关系要很好；请吃饭，聊他们学业聊他们追女生的事情；他们care的东西；喜欢研究的就多聊点；希望往简历上贴金的就帮他们；帮他们改简历和文书，教他们coding；workshop；我拿ppt问了我导师他的意见。\\n- 形成一个小团体的感觉：课表和打工表给我；没课的话我expect你能来；丑话说前头； 教他们coding；强制要来，逼着他们学点什么\\n- 结果：学生反馈效果还不错，他们会主动给我推荐靠谱的学弟学妹次年再给我当RA。\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='70a4263e-21d3-4e7a-88ce-35aacf4dea2c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**关于和boss的关系**\\n\\nTell me about a time when you worked under close supervision or extremely loose supervision. How did you handle that?\\n\\n每周meet一次；但他突然生娃了，不会信息；我struggle了两个月；和他skype聊了一次，和他说我想推进项目的愿望；自己run study，理论上是不应该的，因为他是合作者，他和我说，如果不通过他的检查，他可能不会pay；然后我和他商量了下，我自己自己pay，他到时候看情况来compensate我；如果他觉得是不该run的就不需要pay；他后来还是补偿了我；另一方面我不停的更新新的数据，让他也知道我在干活，他作为合作者也有紧迫感\\n\\n突然每天给我发邮件催。我有点不太适应。就问他什么情况；他要升职了，需要这个paper马上投出去；很senior的教授也是人。各有苦衷；没睡好吗？倒苦水，他自己的肺有毛病，孩子升学；\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6ab850c5-8d77-402c-afd8-cbb9278d708b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**关于stress**\\n\\n最近没有；博士期间都可以通过合理的时间安排让自己不那么streesful\\n\\nVery stressful；修双学位，实习，社团，还要考托福（比如托福班就不去了退钱），和女朋友吵架；行为上调整schedule砍掉unnecessary时间：；列出来priority，尽力和各方沟通清楚我的边界在哪里，哪些能推掉就推掉；实习的老板说三天就三天，其他工作做不完的我得留在宿舍做；那段时间经常刻意不带手机出门节省每一秒，末班地铁上还在看书做题。\\n\\n生理上：尽力保持生理上的健康；心态上：尽力了就好；不要把时间浪费在焦虑上。行动是解决焦虑的唯一办法；尽全力就好，如果最后结果不行，也ok；当时的结果还不错，申请啥的；有压力代表你在成长，没有压力就出问题了; anything desirable in life will bring u stress; \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7cfce6df-7d7c-4233-98e2-f1d31d266a26', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**关于如何adapt**\\n\\nDescribe a time when your team or company was undergoing some change. How did that impact you, and how did you adapt?\\n\\n每周meet一次；但他突然生娃了，不会信息；我struggle了两个月；和他skype聊了一次，和他说我想推进项目的愿望；他身体也有点不好我也不能太逼她；\\n\\n自己run study，我会发你，你没时间回我也会发出去；他和我说，如果不通过他的检查，他可能不会pay；然后我和他商量了下，我自己自己pay，他到时候看情况来compensate我；如果他觉得是不该run的就不需要pay；他后来还是补偿了我；另一方面我不停的更新新的数据，让他也知道我在干活，他作为coauthor也有紧迫感\\n\\n**Describe a scenario when you were persuaded to change your mind about something?**\\n\\n比较Pushy的教授喜欢老来催我弄得我有点不舒服；老来问我读了什么paper啊这种；\\n\\n我有点希望干自己的；需要读很多paper的，要通过海量的知识摄入拎清楚自己的逻辑，然后才能说得比较自信清楚。后来我聊了这个问题，说我要是paper没读好没读全会自己没信心。后来交流过后他说，他之前有个美国学生不催就不干活；我是中国人不是，我每周1-2次update，不需要完美，大概读到了什么theory\\n\\n政治制度；民主自由洗脑的，读了很多书发现民主自由在20世纪以前是个贬义词；本科的时候，不知道要念什么phd，去图书馆疯狂读书；每个国家的制度都有可取之处，每个人也是；每个人也不需要按一种或者几种模式来生活。\\n\\n**谈谈您是否曾经需要给团队成员一些建设性的批评意见？您是怎么做的？**\\n\\n这是学术圈教我的一件好事；\\n\\nTotally opposite 学术观点，fully respect each other; 人是不是本质上prosocial的\\n\\n我觉得你这里可以这样改进：你的assumption如何，我的assumption是怎样的，我有没有理解对；你的分析方法是什么，我的分析方法；你的逻辑链；\\n\\n必须给可以实操的建议；比如再收一次data，怎么收；你这个数据可能有问题，你应该加一些什么问题，应该用什么统计方法处理数据，你去看哪本书\\n\\n**您是否曾经需要让抗拒某一项目或想法的人员认同该项目或想法？您是怎么做的？**\\n\\n讨论过后他不喜欢；我直接给了data，我自己的钱；因为一般情况下都是教授出钱；另一方面我给了一个theory structure, 表示我读了很多paper，他不好意思拒绝\\n\\n请举例说说你是如何和性格完全不同的同事一起紧密合作的（work closely with someone with a personality which was very different from yours）？\\n\\nContext： 需要完成pre; 两个人，极不靠谱\\n\\n一周之前就给他和我安排任务：写一个summary；过三天ppt完成一半；\\n\\nThe goal is to earn a good score;\\n\\nclaire 特别pushy\\n\\n请告诉我你如何面对团队合作时出现的矛盾（face a conflict while working on a team）？是如何处理好的？\\n\\n你是否尝试过从某个不是有求必应的（not responsive）同事那里获取信息？\\n\\nContext： 需要完成pre; 两个人，极不靠谱；The goal is to earn a good score;\\n\\n一周之前就给他和我安排任务：写一个summary；过三天ppt完成一半；\\n\\n强化我们俩的私交，叫他一起去喝酒\\n\\n我告诉他如果不能在哪天respond我，结果会是什么。\\n\\n我希望体验完全不同的工作模式；我技能，知识上学的东西很多；希望能给公司创造价值\\n\\n希望看到我的能力能如何给公司创造价值\\n\\n**关于时间管理**\\n\\nDescribe a long-term project that you managed. How did you keep everything moving along in a timely manner?\\n\\n符合我的motivation的\\n\\n不符合我的motivation的，就设置强制时间，每周二周三上午必须干这个；番茄时间这种办法。利用别人来约束自己\\n\\n**关于使用逻辑思维**\\n\\nGive an example of an occasion when you used logic to solve a problem.\\n\\n建立逻辑链；不同的概念之间的；通过literature；A 和 B之间的关系；因为A导致C；C导致B；如何证明，操纵C，来让AB间的关系消失\\n\\nA和B之前还可能通过D产生关系，然后我通过操纵D，发现AB之间的关系不受影响\\n\\n排除一切不可能的原因，剩下的那个就是唯一的原因\\n\\n**关于自我驱动**\\n\\nTell me about a time you set a goal for yourself. How did you go about ensuring that you would meet your objective?\\n\\n对纯知识感兴趣比较有好奇心，会继续念书；认识很多外经贸的朋友，大家都觉得我特别懂；先打算学社会科学 ，转行> 经双，有点做学术的想法 > 经济学不见得太适合 > 读书海量的 >行为科学 > marketing，找实习，清华找研究经验，申请硕士，结果是最好的项目\\n\\n**关于创造力**\\n\\nTell me about a problem you solved in a creative way.\\n\\nMultiple identity theory; word2vec;\\n\\n**一年时间把新的项目做出点东西，到能投会议的阶段**\\n\\n读paper，找到research idea，找到感兴趣的老师；招募新的RA，开始收集新的数据，新的研究方法；投会议；同步写paper\\n\\n**关于讲故事**\\n\\nGive me an example of a time when you were able to successfully persuade someone to see things your way at work.\\n\\n工作内容上\\n\\nGive theory + logic; give data and show my effort; what have I done; if he didn’t do as much, he might\\n\\n人们不信，觉得负面情绪负面效果很大；我用data证明，负面情绪比没有偏好带来的负面效果更大；理论上也介绍了dehumanization，这个mkt的人不熟\\n\\n工作方式上\\n\\n比较Pushy的教授喜欢老来催我弄得我有点不舒服；老来问我读了什么paper啊这种；\\n\\n我有点希望干自己的；需要读很多paper的，要通过海量的知识摄入拎清楚自己的逻辑，然后才能说得比较自信清楚。后来我聊了这个问题，说我要是paper没读好没读全会自己没信心。后来交流过后他说，他前一个学生不催就不干活，后来出路不太好；我每周1-2次update，让他放心；取一个折中的方案；\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3372a8c4-8e2f-4af6-a2d2-5f4350067ba6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Any question for us?**\\n\\nWould you say you learn more from your co-workers or on your own?\\n\\nWhat are the strategies that you like?\\n\\nTell me about your proudest professional accomplishment.\\n\\n转行；我的导师严格来说不算这个细分领域的；研究social cognition的人也都认识了有很多交流；Expert on social cognition of SNS; I know most theories and existing scientific results;\\n\\n•\\tYour origin story. Explain your background and overall experience. Practice and iterate: your first version of your origin story will spend way too much time on something no one cares about. I had to ask myself: Does anyone really care that you got a Philosophy degree before getting a Physics degree, Susan? The answer is no! I only bring up that additional piece of biographical information with audiences who have already indicated a relevant interest—otherwise, I anchor on the piece of information I know (from practice) hooks people’s attention. Yes, it’s unfortunate that we have to package ourselves into small, digestible boxes that aren’t representative of all our interests and passions and weird quirks. But people can only digest so much information, so find one strong narrative track and stick to it. You should have a 30-second, 90-second, and 3-minute version of your origin story. Then, when you’re in an interview scenario, you must choose in the moment how detailed you are going to be, and which length of pitch to use. (If this is too much structure for you, don’t get stuck in your head about it. Just practice, practice, practice talking about yourself, and note when you’re going down a narrative path that seems to drag.)\\n\\n•\\tThe stories of your past projects. Come with at least three and rank them in your mind (which is your favorite, which is your strongest). Make sure you highlight the motivation for the project (the hook of why it interested you or why it was important), along with the methodology and the outcome. How you choose a problem is one of the most compelling parts of your past work, so don’t skip it and jump right in to talking about clustering algorithms. Similarly, you want to cap the story off by telling us the Grand Conclusion: What was the outcome of the project? What was its impact? How did the story end? For your project-based stories, be able to give the high-level summary as well as the detailed version, so that you can give your interviewer the level of detail their question calls for. Don’t be afraid to include a human element (“my team was really stumped on how we were going to [x], because the executives wanted [y]…”). Collaborative problem solving is a key part of every single business with >1 employee.\\n\\n•\\tThe story of a time you solved a problem or took initiative to fix a process that was broken. Your goal is to prove you can drive value for the company in question, that you can spot opportunities and execute on them. Explain why the problem arose or why the process was broken. This is the hero’s journey of business problems. (“The good people of [company] were in the thrall of [broken process]. I set out to save them. But the problem was worse than I thought! After hours of terrible effort, inspiration appeared. I solved the problem and the business was saved.”) When you reach your Grand Conclusion, quantify your results—the man-hours you saved, the revenue you drove, the number of people you helped, etc.\\n\\n•\\tA story of conflict resolution, or a time you struggled with something and got through it. Lots of interviews involve an “overcoming obstacles” type of question. This question may focus on interpersonal conflict (“tell me about a time you butted heads with a team member”) or on general challenges (“tell me about a time you encountered a problem you didn’t know how to solve”). Think carefully about what in your background can show (1) that you can work with others in the face of personality differences or disagreements, and/or (2) that you can move forward and be successful when faced with a challenge you haven’t met before.\\n\\n•\\tYour story of your future, within the context of the company with which you’re interviewing. Tell the story of what excites you or interests you about the company. Describe the value you can see yourself adding to the team. Don’t be braggadocious, but do consider how your unique talents would serve the team (frame things of being of service and being useful if you worry that you’re coming off as overly arrogant). Describe the value you think you’d get out of working there: how inspired you are by the rest of the team or the company mission, or what you can learn from the problem space. Just don’t bullshit, though—be genuine.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c0b4c553-2b3a-4664-849e-6083039f446a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**How could you collect and analyze data to use social media to predict the weather?**\\n\\n**How would you design the people you may know feature on LinkedIn or Facebook?**\\n\\n- Find strong unconnected people in weighted connection graph\\n    - Define similarity as how strong the two people are connected\\n    - Given a certain feature, we can calculate the similarity based on\\n        - friend connections (neighbors)\\n        - Check-in’s people being at the same location all the time.\\n        - same college, workplace\\n        - Have randomly dropped graphs test the performance of the algorithm\\n    - News Feed Optimization\\n        - Affinity score: how close the content creator and the users are\\n        - Weight: weight for the edge type (comment, like, tag, etc.). Emphasis on features the company wants to promote\\n        - Time decay: the older the less important\\n1. **How would you predict who someone may want to send a Snapchat or Gmail to?**\\n- for each user, assign a score of how likely someone would send an email to\\n- the rest is feature engineering:\\n    - number of past emails, how many responses, the last time they exchanged an email, whether the last email ends with a question mark, features about the other users, etc.\\n- Ask someone for more details.\\n- People who someone sent emails the most in the past, conditioning on time decay.\\n\\n**How would you suggest to a franchise where to open a new store?**\\n\\n- build a master dataset with local demographic information available for each location.\\n    - local income levels, proximity to traffic, weather, population density, proximity to other businesses\\n    - a reference dataset on local, regional, and national macroeconomic conditions (e.g. unemployment, inflation, prime interest rate, etc.)\\n    - any data on the local franchise owner-operators, to the degree the manager\\n- identify a set of KPIs acceptable to the management that had requested the analysis concerning the most desirable factors surrounding a franchise\\n    - quarterly operating profit, ROI, EVA, pay-down rate, etc.\\n- run econometric models to understand the relative significance of each variable\\n- run machine learning algorithms to predict the performance of each location candidate\\n\\n**In a search engine, given partial data on what the user has typed, how would you predict the user’s eventual search query?**\\n\\n- Based on the past frequencies of words shown up given a sequence of words, we can construct conditional probabilities of the set of next sequences of words that can show up (n-gram). The sequences with highest conditional probabilities can show up as top candidates.\\n- To further improve this algorithm,\\n    - we can put more weight on past sequences which showed up more recently and near your location to account for trends\\n    - show your recent searches given partial data\\n\\n**Given a database of all previous alumni donations to your university, how would you predict which recent alumni are most likely to donate?**\\n\\n- Based on frequency and amount of donations, graduation year, major, etc, construct a supervised regression (or binary classification) algorithm.\\n\\n**You’re Uber and you want to design a heatmap to recommend to drivers where to wait for a passenger. How would you approach this?**\\n\\n- Based on the past pickup location of passengers around the same time of the day, day of the week (month, year), construct\\n- Ask someone for more details.\\n- Based on the number of past pickups\\n    - account for periodicity (seasonal, monthly, weekly, daily, hourly)\\n    - special events (concerts, festivals, etc.) from tweets\\n\\n**How would you build a model to predict a March Madness bracket?**\\n\\n- One vector each for team A and B. Take the difference of the two vectors and use that as an input to predict the probability that team A would win by training the model. Train the models using past tournament data and make a prediction for the new tournament by running the trained model for each round of the tournament\\n- Some extensions:\\n    - Experiment with different ways of consolidating the 2 team vectors into one (e.g concantenating, averaging, etc)\\n    - Consider using a RNN type model that looks at time series data.\\n\\n**Given training data on tweets and their retweets, how would you predict the number of retweets of a given tweet after 7 days after only observing 2 days worth of data?**\\n\\n- Build a time series model with the training data with a seven day cycle and then use that for a new data with only 2 days data.\\n\\n1. **What are some differences you would expect in a model that minimizes squared error, versus a model that minimizes absolute error? In which cases would each error metric be appropriate?**\\n- MSE is more strict to having outliers. MAE is more robust in that sense, but is harder to fit the model for because it cannot be numerically optimized. So when there are less variability in the model and the model is computationally easy to fit, we should use MAE, and if that’s not the case, we should use MSE.\\n- MSE: easier to compute the gradient, MAE: linear programming needed to compute the gradient\\n- MAE more robust to outliers. If the consequences of large errors are great, use MSE\\n- MSE corresponds to maximizing likelihood of Gaussian random variables\\n\\n**What are various ways to predict a binary response variable? Can you compare two of them and tell me when one would be more appropriate? What’s the difference between these? (SVM, Logistic Regression, Naive Bayes, Decision Tree, etc.)**\\n\\n- Things to look at: N, P, linearly seperable?, features independent?, likely to overfit?, speed, performance, memory usage\\n- Logistic Regression\\n    - features roughly linear, problem roughly linearly separable\\n    - robust to noise, use l1,l2 regularization for model selection, avoid overfitting\\n    - the output come as probabilities\\n    - efficient and the computation can be distributed\\n    - can be used as a baseline for other algorithms\\n    - (-) can hardly handle categorical features\\n- SVM\\n    - with a nonlinear kernel, can deal with problems that are not linearly separable\\n    - (-) slow to train, for most industry scale applications, not really efficient\\n- Naive Bayes\\n    - computationally efficient when P is large by alleviating the curse of dimensionality\\n    - works surprisingly well for some cases even if the condition doesn’t hold\\n    - with word frequencies as features, the independence assumption can be seen reasonable. So the algorithm can be used in text categorization\\n    - (-) conditional independence of every other feature should be met\\n- Tree Ensembles\\n    - good for large N and large P, can deal with categorical features very well\\n    - non parametric, so no need to worry about outliers\\n    - GBT’s work better but the parameters are harder to tune\\n    - RF works out of the box, but usually performs worse than GBT\\n- Deep Learning\\n    - works well for some classification tasks (e.g. image)\\n    - used to squeeze something out of the problem\\n\\nWhat are the steps for wrangling and cleaning data before applying machine learning algorithms?\\n\\n在应用机器学习算法之前纠正和清理数据的步骤是什么？\\n\\n数据预处理：缺失值，脏数据，异常点检查 和 处理\\n\\n数据归一化：最大-最小归一化 ，Z-分数, 对数log，分段归一化，排序归一\\n\\n特征选择： Filter（基于相关统计量）， Wraper（特征子集搜索）， Embedding（lasso,ridge） ， 降维\\n\\nHow do you measure distance between data points?\\n\\n如何测量数据点之间的距离？\\n\\n标称数据：Jaccard\\n\\n序数数据：可以变换成 数值数据 或者 标称数据\\n\\n数值数据：p-范数，余弦相似性\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\nWhat features would you use to build a recommendation algorithm for users?\\n\\n你会使用什么功能来为用户构建推荐算法？\\n\\n用户分群\\n普通用户\\n普通粉丝\\n忠实用户\\n核心用户\\nCollaborative filtering\\nContent-based filtering\\nHybrid recommender systems\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\n1. Pick any product or app that you really like and describe how you would improve it.\\n    \\n    选择任何一个你真正喜欢的产品或应用程序，并描述如何改善它。\\n    \\n\\nHow would you find an anomaly in a distribution ?\\n\\n如何在分布中发现异常？\\n\\n参数法：高斯模型\\n\\n非参数法：直方图，箱形图 ，散点图\\n\\n聚类：稀疏的簇是异常的可能性比较大\\n\\n分类：One-Class SVM ， KNN\\n\\nHow would you go about investigating if a certain trend in a distribution is due to an anomaly?\\n\\n如何检查分布中的某个趋势是否是由于异常产生的？\\n\\n问题描述的不清楚\\n\\n“某个趋势是有异常产生的”\\n\\n对比下 Spearman 和 pearman 计算相关系数\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\nWhat metrics would you consider using to track if Uber’s paid advertising strategy to acquire new customers actually works? How would you then approach figuring out an ideal customer acquisition cost?\\n\\n你会考虑用什么指标来跟踪 Uber 付费广告策略在吸引新用户上是否有效？然后，你想用什么办法估算出理想的客户购置成本？\\n\\n(1)\\n\\n付费前后的海盗指标对比\\n\\n详细点说下底层的数据模型怎么实现（E-R模型，维度模型）\\n\\n(2)\\n\\n举个实际例子说明我这个行业是这样计算获客成本的：\\n\\n指标：\\n\\n平均每个新增活跃用户净利润/年\\n\\n平均新增有效“有效户”净利润/年\\n\\n算法：\\n\\n用户与利润增长模型\\n\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\nWhy do you use feature selection?\\n\\n为什么要使用特征选择（feature selection）？\\n\\n业务角度：可解析性，输入对输出的影响程度\\n\\n算法角度：维度灾难，降维，降低学习任务的难度\\n\\n特征选择方法：\\n\\nWhat is the effect on the coefficients of logistic regression if two predictors are highly correlated? What are the confidence intervals of the coefficients?\\n\\n如果两个预测变量高度相关，它们对逻辑回归系数的影响是什么？系数的置信区间是什么？\\n\\n系数影响：增大回归系数的方差\\n业务上： 可解释性变差\\n系数的置信区间：变小\\n\\n什么是方差膨胀因子 (VIF)？\\n\\nWhat is the effect of having correlated predictors in a multiple regression model?\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\nWhat’s the difference between Gaussian Mixture Model and K-Means?\\n\\n高斯混合模型（Gaussian Mixture Model）和 K-Means 之间有什么区别？\\n\\nK-Means：非参数方法，非概率模型，相似度角度衡量\\n\\nGMM：参数方法，概率模型，概率角度衡量\\n\\nK-means is a special case of Mixture ofGaussian, and Mixture of Gaussian is a special case ofExpectation-Maximization.\\nThe biggest difference between K-meanand GMM in practice is:\\nK-Mean only detect spherical cluster.\\nGMM can adjust its self to ellipticshape cluster.\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\nHow do you pick k for K-Means?\\n\\n在 K-Means 中如何拾取 k？\\n\\n根据业务理解\\n代价函数RMSE 与K的函数图 ，曲线拐点。\\n其他方法：Unsupervised-model-11.pdf\\nHow do you know when Gaussian Mixture Model is applicable?\\n\\n你如何知道高斯混合模型是不是适用的？\\n\\nQuora:GMM vs K-means\\n\\n概率模型：指对样本的概率密度分布进行估计，\\n\\n分类问题：输出不是确定的分类标记，而是得到每个类的概率。\\n\\n对样本中的数据分别在几个高斯模型上投影，就会分别得到在各个类上的概率。然后我们可以选取概率最大的类所为判决结果\\n\\n理论上通过增加Model的个数，可以用GMM近似任何概率分布\\n\\nGMM 由 K 个 Gaussian 分布组成，每个 Gaussian 称为一个“Component”，这些 Component 线性加成在一起就组成了 GMM 的概率密度函数：\\n\\nAssuming a clustering model’s labels are known, how do you evaluate the performance of the model?\\n\\n假设聚类模型的标签是已知的，你如\\n\\n何评估模型的性能？\\n\\n分类问题，混淆矩阵，准确率，召回率等\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\nWhat’s the difference between L1 and L2 regularization?\\n\\nL1 和 L2 正则化之间有什么区别？\\n\\n贝叶斯角度：\\n\\nL1 代价函数相当于拉普拉斯先验，根据最大后验估计进行回归系数求解\\n\\nL2是高斯分布先验进行，最大后验的求解\\n\\n对回归系数影响\\nL2 penalizes one big weight more than many small weights.\\nL1 doesn’t.\\nSo with L2, you tend to end up with many small weights, while with L1, you tend to end up with larger weights, but more zeros.\\n\\n[[Differences between L1 and L2 as Loss Function and Regularization]]\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\nName and describe three different kernel functions and in what situation you would use each.\\n\\n点出及描述三种不同的内核函数，在哪些情况下使用哪种？\\n\\nDescribe a method used in machine learning.\\n\\n随意解释机器学习里的一种方法。\\n\\nHow do you deal with sparse data?\\n\\n如何应付稀疏数据？\\n\\nRidge can handle both sparse and nonsparse data.\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\nHow do you prevent overfitting?\\n\\n如何防止过拟合（overfitting）？\\n\\nPenalty methods\\nHoldout and Cross-validation methods\\nEnsembles\\n\\nHow do you deal with outliers in your data?\\n\\n如何处理数据中的离群值？\\n\\n对比下drop 和 不drop 对模型结果产生影响\\nOutliers: To Drop or Not to Drop\\nrobust statistic\\nHow to Deal with Outliers in Your Data\\nHow do you analyze the performance of the predictions generated by regression models versus classification models?\\n\\nclassification models: 混淆矩阵\\n\\nregression models : RMSE ,MAP\\n\\nHow do you assess logistic regression versus simple linear regression models?\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\n!Untitled\\n\\nWhat’s the difference between supervised learning and unsupervised learning?\\n\\nWhat is cross-validation and why would you use it?\\n\\n什么是交叉验证（cross-validation），为什么要使用它？\\n\\nHoldout Method 缺点：\\n样本不足\\n样本不够随机，训练处的模型不够robust\\nCross-validation:\\n\\nRandom Subsampling\\nK-Fold Cross-Validation\\nLeave-one-out Cross-Validation\\n作用：\\n\\n模型和参数选择\\n性能评估\\nWhat’s the name of the matrix used to evaluate predictive models?\\n\\n用于评估预测模型的矩阵的称为什么？\\n\\n混淆矩阵\\n\\nWhat relationships exist between a logistic regression’s coefficient and the Odds Ratio?\\n\\n逻辑回归系数和胜算比（Odds Ratio）之间存在怎样的关联？\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\nWhat’s the relationship between Principal Component Analysis (PCA) and Linear & Quadratic Discriminant Analysis (LDA & QDA)\\n\\n主成分分析（PCA）与线性判别分析（LDA）、二次判别分析（QDA）之间存在怎样的关联？\\n\\n都是降维的方法\\n\\nPCA\\n\\n非监督\\n投影方向，使得数据尽可能的分散开，数据的方差最大\\nLDA，QDA （生成模型）\\n\\n监督\\n投影方向使得数据尽可能分类开来\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\n!Untitled\\n\\nWhat’s the difference between logistic and linear regression? How do you avoid local minima?\\n（行业分析师）逻辑与线性回归有什么区别？如何避免局部极小值？\\n\\n区别对比见IBM第4题\\n\\n解决local minima问题：用 cross entropy loss作为cost function ,它是covex function\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\n!Untitled\\n\\nHow would you build a model to predict credit card fraud?\\n\\n如何构建一个模型来预测信用卡诈骗？\\n\\n分类问题\\nLogistic Regression\\n特征选择\\nHow do you handle missing or bad data?\\n\\n如何处理丢失或不良数据？\\n\\n忽略该记录\\n全局变量替换确实值\\n均值 ，中位数，最可能的值代替缺失值，\\nRobust 算法\\nHow would you derive new features from features that already exist?\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\nSuppose you were given two years of transaction history. What features would you use to predict credit risk?\\n\\nWhy does SVM need to maximize the margin between support vectors?\\n\\nExplain what heteroskedasticity is and how to solve it\\n\\nWhy not logistic regression, why GBM?\\n\\n- Simulate a bivariate normal\\n- Derive variance of a distribution\\n\\nRF meaning random forest? If so, they are correct that multi-collinearity is not a problem for random forest models as a predictor.\\n\\nHowever, random forests are typically not used for media mix models because they only really \"care\" about predicting the outcome, not building a theoretical model of the data generating process - this is why multi-collinearity isn\\'t really an issue.\\n\\nLet\\'s say you have 2 variables in the model: total media spend and a holiday index and you are predicting sales. These features are probably colinear - your brand probably spends more around holidays, and both are going to be correlated with sales. If you look at e.g. SHAP plots, partial dependency plots, variable importance (i.e. the tools people usually use to \"interpret\" random forest models) then both holiday and sales will show up as \"highly influential\" because they both are - but these tools won\\'t tell you the effect of marketing spend while controlling for the holiday index simply because this is not how a random forest model gets constructed from the data.\\n\\nYou should be using some sort of GLM. Linear regression is a standard for MMM in industry for the reasons I\\'ve described above, especially the Bayesian versions of it, including penalized variants such as Ridge regression. I have also seen Facebook\\'s Prophet model being used since it is a linear regression model masquerading as a time series model (and as such may be able to model/control for seasonal-type effects more elegantly than a standard GLM).\\n\\nTL;DR RF is not really appropriate for a MMM. You should figure out how they are estimating the effect sizes for media spend on sales with their RF model and think critically about why that method doesn\\'t really give the information that you want to provide to your marketing team.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ebea699c-221f-4df6-b90a-dfc0f36a2454', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nWe at Facebook would like to develop a way to estimate the month and day of people\\'s birthdays, regardless of whether people give us that information directly. What methods would you propose, and data would you use, to help with that task?\\n\\n- date and # of friends\\' post on his wall with \"birthday\"-relevant keywords\\n- Birthday post from highly relevant users: eg. twins, parents\\n- time stamp of other important date: eg. age when enroll high school', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7bcc4fba-4688-43d1-b004-9e013fc5c9cc', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nClarify:\\n\\n- What is the toolbar change? Location, look, buttons, results?\\n- Why are we changing the toolbar? Are we looking to have users engage within the site more? Are we looking to serve them more relevant content?\\n\\nAssuming we're changing the toolbar for the sake of having users search more: I would consider two metrics to be important 1. click rate on the toolbar 2. number of searches executed\\n\\nusage frequency of search bar and other alternative search methods like filtering; repeated search - search success rate\\n\\nmetrics of the product in general: chrun rate, daily active time, daily active user\\n\\n1. Click after search:\\n- the more clicks the better\\n1. Time on site after search\\n2. Number of click / searches with toolbar per session.\\n3. Number of upvotes after search.\\n4. Click links in answer after search.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c091e3f7-d134-4f13-9a5d-761235ce4df3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nYou are compiling a report for user content uploaded every month and notice a spike in uploads in October. In particular, a spike in picture uploads. What might you think is the cause of this, and how would you test it?\\n\\n1. Clarify: happens on all channels/platforms/geography/user segment.\\n2. Goal: find the root cause of the spike in video uploads and validate our hypothesis.\\n3. Internal Factors: No problem with metrics, no new feature/promotion/partnership/video duplication\\n4. External Factors: Not seasonal, no global events/hardware 5: Competitor: No bad PR/ regulatory ban,\\xa0**so the answer is their Picture upload function is having problem. All their user then come to us in October.**', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fd042157-785d-4051-87aa-9dee55e6515e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHow do you map nicknames used in Facebook users names (Pete, Andy, Nick, Rob, etc) to real names?\\n\\nFirst approach - 1) check what other facebok platforms they are on - instagram, whatsapp 2) If they are using rela names there then we can fetch it\\n\\n2nd approach - 1) Do reverse match fo those with full names on fb how they are calling themselves on instagram, whatsapp 2) from gathering that mapping we can fill the nicknames of those missing\\n\\n3rd approach - people do mention nickname along with full names. so use that intelligence too to predict nicknames.\\n\\n1. It is string matching type of problems.\\n2. So we can use regular expression (generally used in NLP, pattern matching etc) but it will give all possible matches but all these must not be correct.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3e79e2fc-72c1-4412-a6ee-f42dc193e807', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nTags: VIP\\n\\nWhat metrics would you use to track whether Uber's strategy of using paid advertising to acquire customers works?\\n\\n广告就是三类metric: ads metrics: download, sign up and the ads we paid (conversion rate and churn rate), user level metrics, LTV, CSC; \\n\\nCSC: marketing costs metrics, Cost per thousand impressions, Cost per click, Cost per acquisition, Cost per ride\\n\\nMetrics of Successful Paid Advertising Success Metric: \\n\\n- app downloads/sign-ups traced from advertisement link\\n- Conversion Rate: # of app downloads or sign ups / total paid advertising impressions\\n\\nGuardrail Metric:\\n\\n- Customer Acquisition Cost (CAC): how much does it cost to acquire a new customer?\\n- LTV / CAC: determines how much a new customer will cover the expenses needed to acquire them (ideally have this ratio over 1)\\n\\nA/B Test\\n\\n- Test group (seen ad) vs. Control (not seen ad)\\n- Value: avg. app downloads/sign-ups -Test type: independent t-test (compares two means from separate populations) to determine whether the mean of the test group is statically different from the control (e.g. is the mean # of downloads for people who see paid advertisement higher than those who do not)\\n\\nFactors of Consideration -Population size of each group (use conversion rate if different) -Channels of Advertisement (Display Ads, SEM, Social etc.) - channel and impressions should be constant to speak to CAC and LTV ratio. -Eliminate irrelevant factors (Does the test group even engage with this type of channel?)\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='05c80ef9-9af9-485e-9ba4-17bb86f51826', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\nHow could you use GPS data from a car to determine the quality of a driver?\\n\\n- follows the speed limit\\n- doesn't take wrong turns\\n- reaches the destination in the estimated time\\n\\n1. Avg. Speed: how long does it take them to get from point A to point B?\\n2. Degree of Turns: how sharply does an individual turn on corners? At what speed are they taking turns (compare the differences in position to time)?\\n3. Lane Switching/Swerving: how often does a vehicle position shift slightly when going straight at similar speeds?\\n\\n\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='da17c7dd-1006-4b84-80ed-ed16a48b7900', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\nHow would you leverage Spotify's collected data to improve the app experience?\\n我可以想到的features: demographic features and music pref features non time sensitive, user behavioral features time sensitive\\n\\nDV: time spent on spotify; \\n\\nClarify\\n\\n- We're trying to improve app experience for the user, correct? I see ways in which we could use collected data for Spotify's own purposes but want to make sure.\\n- What are the business objectives?\\n    - Do we want users to spend more time on the app?\\n    - Share more songs?\\n    - Use personalized playlists more often?\\n    - Listen to more podcasts or music?\\n- How can I access this data? What kind of user collected data do we have?\\n\\nConstrain\\n\\n- Say the goal is to get users to listen to more podcasts given their music listening history (note: I would work off of the interviewer here, this is the approach I'm taking)\\n- Goal: I will be looking at how we can use user listening data in order for our product team to push users towards related podcasts to increase podcast user hours.\\n\\nPlan/Method\\n\\n- I'll pull\\n    - users, their music habits (genre, time spent, have they listened to podcasts before, do they listen to music podcasts)\\n    - podcasts and their genres, are they music podcasts\\n- I'll calculate the correlation between these podcasts and the listening habits of their users, and using something like a linear regression model to calculate their correlations\\n\\nGoal\\n\\n- If we find a high correlation between music listening habits and podcasts habits of users, we can leverage user data to push more relevant podcasts to users - increasing time spent on the app.\\n- This has been done to create curated playlists, but with Spotify's recent foray into buying exclusive podcasts, we can continue to improve this effort via a targeted approach using collected data.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a0f46105-d5e8-4eeb-9133-543784bdf45f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nDescribe how to engineer the heatmap telling taxi drivers where to go to maximize their probability to get a client. How do you define which area will have high demand next and who do you want to go there?\\n\\nTo plot heatmap we will collect area wise information and driver information. We will first divide a city into different regions.\\n\\n1. By visually inspecting or by running a clustering algorithm we will create n numbers of regions based on historical rides demand data.\\n2. Once we have n regions we will run n time series forecasting models to predict future ride demands per region.\\n3. Now we will collect driver information: We will calculate driver-region pair weight vector that will signify a driver's willingness to travel to each region.\\n\\n4. To calculate such driver-region weight we will collect following pieces of information: 1) Driver's distance to each cluster. 2) If driver is old driver then we will calculate proportion of her/his rides to individual regions. If driver is new driver then we will ask her/him to rate top five regions she/he willing to travel. 3) To help generate more revenue we will collect average ride revenue per region. 4) If driver is old driver then we will collect average star for a driver from each region. 5) To incorporate other region level factors such as crime rate.\\n\\nWe will convert all of this information into single weight value (0-1) for each driver-region pair.\\n\\nWe will then multiple this weight vector to region wise demand vector to later plot a heatmap.\\n\\n##############################\\n\\n1. why their company would build a potential product.\\n2. understand the target audience.\\n3. how you will solve this edge case.\\n- This new feature could be helpful for Lyft and its drivers for connecting drivers and riders faster, thus generating more revenue for Lyft and its drivers.\\n- KPIs may include drivers’ earnings, total company revenue or the average time a driver waits between two customers.\\n- An edge case for your solution would be when there’s a major event in the city, such as a music festival. As music festivals are not always held at the same place during the same dates, it could increase the demand for Lyft rides exponentially for a couple of days then die down immediately, which is hard to predict if you just use past data. Real-world data collection of major events like these need to be incorporated.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d7633653-e5ef-44cd-881f-bcaef64edb99', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nTags: VIP\\n\\n1. 先明确定义\\n2. 宏观环境和基线；了解这个波动是否正常\\n3. 拆解问题，从设备，人群，地域等角度提出一些可能性；不同情况下，分子，分母分别会怎么变化\\n4. 根据上面的答案，提出自己最有可能的hypothesis来解释这个变动\\n5. 提出要验证这个hypothesis，需要什么data；\\n6. 提出metrics，如果这个metrics多大，就证明或者证伪了我的hypothesis\\n\\nLet's say you're working on the ads team at Facebook. Fill rate in ads is defined as the number of overall impressions divided by potential opportunities.\\nLet's say you see that the fill rate has dipped by 10%. What would you look into?\\n\\nThis could be a couple of different things. \\n\\n1. Before anything, I'd want to understand how we\\ndefine an impression. Does merely viewing an add count, or does the user need to interact with the ad to be counted? Given that Facebook makes the majority of its revenue from ads,\\naddressing this potential discrepancy is tantamount towards Facebooks continued success.\\nFrom there, we can brainstorm some possibilities.\\n2. First I'd ask what this 10% change is relative to (Daily, Weekly, Monthly?). From there we have\\nsome better ground to stand on but we should also clarify what else is known. \\n3. Is this coming from a particular region? Does this hold constant across all devices/platforms (mobile vs.\\ndesktop)? Is this decrease true for all advertiser campaigns, or are only a subset of advertisers\\nbeing affected? Has FB seen an increase in users since our last numbers were crunched (i.e. last week in the that this is a weekly change).\\nGiven these factors, we can better understand this problem by answering those questions. \\n4. First we should look to answer the macro questions. - Look at fill rate globally. Is a particular region responsible for this change? If so investigate further - Look at net user change. Do we have an influx of new users coming in? If so, that means our denominator has gotten bigger and our numerator might just need some time to balance out - Is the decrease specific to platform? \\n5. A difference in fill rate based on mobile vs. desktop could imply many things, even bugs in how an ad is being run.- Look at week over week/day over day change (whatever is appropriate given\\nthe rate interval). Looking at the data this way gives us some intuition as to what is a normal fluctuation is. This type of calculation should flatten out any activity that is to be expected, and highlight anything that is new or unusual.\\n6. These questions will help narrow down our larger questions. Once we have answers to these we\\ncan ask things like\\nIn the case of regional differences, were there any particular events or holidays going on there that could have affected overall user activity and thus impressions?\\nIs a specific advertiser or set of advertisers causing this decline? Such an effect could imply a number of things, potentially even a bug that these users are experiencing. This situation would encourage a deeper dive into who these advertisers are to address their needs. To summarize, answering the reasoning behind this decrease could benefit from understanding\\nwhere the problem is coming from, what's going on Facebook, and what normal activity looks\\nlike for us.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='442bb7c7-c9f3-48d8-a8fc-72c1c481e736', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\nWhat's the distribution of the number of daily trips in NYC looks like?\\n\\nit should have two peaks with large number in suumer, spring and autumn, small peak for winter; \\n\\nnormal distribution because if we collect a very large number of days date, it should follow the law of large number ???\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a55fae91-781d-4393-a220-1b545a48467d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nLet\\'s say you\\'re a product data scientist at Facebook. Facebook is rolling out a new feature called \"Mentions\" which is an app specifically for celebrities on Facebook to connect with their fans.\\n\\nHow would you measure the health of the Mentions app? And if a celebrity starts using Mentions and begins interacting with their fans more, what part of the increase can be attributed to a celebrity using Mentions versus what part is just a celebrity wanting to get more involved in fan engagement?\\n\\nLet\\'s first break down some structure on what the interviewer is looking for. Whenever we\\'re given these open-ended product questions, it makes sense to think about structuring the questions with well-defined objectives so we\\'re not switching between different answers.\\n\\n*1. Did you begin by stating what the goals of the feature are before jumping into defining metrics? What is the point of the Mentions feature?*\\n\\n*2. Are your answers structured or do you tend to talk about random points?*\\n\\n*3. Are the metrics definitions specific or are they generalized in an example like “I would find out if people used Mentions frequently.”*\\n\\nLet\\'s start with the first question. We shouldn\\'t be jumping into metrics but thoughtfully thinking about what metrics matter when analyzing the goal of the new feature rollout by Facebook. Let\\'s think about what happens when a celebrity starts using Mentions.\\n\\nDoes it drive increases in engagement in celebrities and regular Facebook users? The Mentions app is essentially a way for users to connect to their favorite celebrities easier which describes a two-way marketplace effect. There needs to be enough celebrities using mentions to engage users and enough users for a celebrity to stay engaged.\\n\\nOne way we can test this is by\\xa0**exposing mentions posts/stories/Q&As to a portion of the users in the top of their newsfeed**. Do these users stay engaged on the platform longer?\\n\\nWe have identified engagement as the thing we want to track. So now we can implement some more defined metrics. We can measure retention in a form of daily active users or weekly active users and compare this metric against the users that do not get the \"Mentions\" test. Does the weekly active user count go down or up in the test compared to the control?\\n\\nNow let\\'s look on the celebrity side. Anothing thing to measure is if the Mentions feature is taking away from any other part of the Facebook app.\\xa0**Is engagement down on other features with Facebook when celebrities are using Mentions?**\\xa0If we can cohort by overall usage of Mentions as a percentage of total usage on Facebook, we can analyze if celebrities using Mentions increases the total engagement of the celebrity on Facebook.\\n\\nAgain, we can define our metric specifically. In this case we can\\xa0**cohort celebrites into different buckets depending on how much they use Mentions**and then look at their average first month retention on the platform, second month retention, etc...\\n\\n**Example:**\\n\\n% of FB usage as Mentions = <10%1st month churn = 20%2nd month churn = 10%\\n\\n% of FB usage as Mentions = 10 to 20%1st month churn = 15%2nd month churn = 7%\\n\\n% of FB usage as Mentions = 20 to 30%....\\n\\nThe churn rate is measuring retention of celebrities\\xa0split up into\\xa0**cohorts by the percentage of time that they spend on the mentions app**. If we model general retention curves, we\\'ll see a natural decrease to a plateau after X number of months calculated by: (active celebrities in month X) / (total celebrities that signed up in month 1).\\n\\nIf you imagine each cohort as a percentage of time that user spends on the mention app as <10%, 10%-20%, 20%-30%, etc.. and then model the retention curves for each cohort on a graph over time with the X axis as the number of months since they joined Facebook and the Y axis as the % of active celebrities/total celebrities, then if we see a slower and slower drop in retention rate, we can attribute the increase in celebrity mentions usage as an increase in retention.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5bc7a5d3-881b-4840-9d5f-44d5852be8af', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n面试题 overbond\\n\\n!Untitled\\n\\n```python\\ndef getMaximumProfit(price, profit):\\n    max_p = -1\\n    cur_p=0\\n    for i in range(len(profit)):\\n        for j in range(i+1, len(profit)):\\n            for k in range(j+1, len(profit)):\\n                if price[i] < price[j] < price[k]:\\n                    cur_p = profit[i] + profit[j] + profit[k]\\n                    max_p = max(max_p, cur_p)\\n    return max_p\\n\\ndef getMaximumProfit(price, profit):\\n    # Write your code here\\n    price = [0] + price\\n    profit = [0] + profit\\n    dp:list[tuple[int, int, int]] = [(0, 0, 0)]\\n    for i in range(1, len(price)):\\n        pos = 0\\n        for j in range(len(dp)):\\n            if price[dp[j][2]]  sum((profit[i] for i in dp[pos])):\\n                pos = j\\n        dp.append((dp[pos][1], dp[pos][2], i))\\n    return max((sum((profit[_] for _ in i)) for i in dp))\\n```\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n```python\\ndef getMaxUnits(boxes, unitsPerBox, truckSize):\\n    # Write your code here\\n    t = sorted([(unitsPerBox[i], boxes[i]) for i in range(len(boxes))], key = lambda x : -x[0])\\n    ans = 0\\n    for i in t:\\n        ans += min(truckSize, i[1]) * i[0]\\n        truckSize -= i[1]\\n        if truckSize <= 0:\\n            break\\n    return ans\\n```\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n```python\\nclass IncreasingList:\\n    def __init__(self):\\n        self.list = []\\n\\n    def append(self, val):\\n        \"\"\"\\n        first, it removes all elements from the list that have greater values than val, starting from the last one, and once there are no greater element in the list, it appends val to the end of the list\\n        \"\"\"\\n        while (len(self.list) and self.list[-1] > val):\\n            self.list.pop()\\n        self.list.append(val)\\n\\n    def pop(self):\\n        \"\"\"\\n        removes the last element from the list if the list is not empty, otherwise, if the list is empty, it does nothing\\n        \"\"\"\\n        if len(self.list):\\n            self.list.pop()\\n\\n    def __len__(self):\\n        \"\"\"\\n        returns the number of elements in the list\\n        \"\"\"\\n        return len(self.list)\\n```\\n\\n!Untitled\\n\\n!Untitled\\n\\n```python\\ndef getMinimumCost(arr):\\n    # Write your code here\\n    res = 0\\n    cur_cost = 0\\n    l=0\\n    r=0\\n    \\n    for i in range(len(arr)-1):\\n        first = arr[i]\\n        second = arr[i+1]\\n        if abs(second-first) > res:\\n            res = abs(second-first)\\n            l = i\\n            r = i + 1\\n    mid = (arr[l] + arr[r]) // 2\\n    cur_cost += (arr[l] - mid) * (arr[l] - mid)\\n    cur_cost += (arr[r] - mid) * (arr[r] - mid)\\n    \\n    for i in range(len(arr)-1):\\n        \\n        if i == l:\\n            continue\\n        first = arr[i]\\n        second = arr[i+1]\\n        cur_cost += (first - second) * (first - second)\\n    return cur_cost\\n\\ndef getMinimumCost(arr):\\n    # Write your code here\\n    pos = 0\\n    ans = 0\\n    for i in range(len(arr) - 1):\\n        if abs(arr[pos + 1] - arr[pos]) < abs(arr[i + 1] - arr[i]): #找到差距最大的一个间隔，就知道插入的位置了；插入的值一定是这个间隔的中点；\\n            pos = i\\n        ans += (arr[i + 1] - arr[i]) ** 2\\n    return ans - int((arr[pos + 1] - arr[pos]) ** 2 / 2)\\n```', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ccd18cbe-7ba0-4127-bfb5-7222f1e4ccd7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"yelp 面试\\n\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\nGPT-3 reached the great milestone of showing that unsupervised language models trained with enough data can multitask to the level of fine-tuned state-of-the-art models by seeing just a few examples of the new tasks.\\n\\n  \\n\\nchatgpt的主要突破是RL; with human response; 之前一直有个belif是RL不适合NLP;\\xa0\\n\\n  \\n\\na system that’s been trained only on the form of language can’t a priori achieve meaning and understanding.\\n\\nSTACKroveflow封杀他因为，他不能用自己的数据训练自己\\xa0\\n\\n  \\n\\n  \\n\\nMRR: Mean Reciprocal Rank； 专注在第一个；\\n\\nMAP: Mean Average Precision；给前排的更大权重\\n\\nNDCG: Normalized Discounted Cumulative Gain\\n\\n  \\n\\n  \\n\\nDeterminantal Point Process\\n\\nUCB upper confidence bound\\n\\npartial dependence plots PDP\\n\\n  \\n\\ndata lake/feature store > containerized models serve as api or package (batch processing) > CI CD\\xa0 model testing and retrain/unit testing > kubernetes depolyment\\n\\n  \\n\\n  \\n\\n\\xa0Be tenacious.\\n\\n\\xa0Battle smart and fight 'til the end. Live for the underdog moments. Turn mistakes into opportunities to learn.\\n\\n\\xa0Play well with others.\\n\\n\\xa0Treat others with respect. Value diversity in viewpoints. Bring a positive attitude to the table.\\n\\n\\xa0Be unboring.\\n\\n\\xa0Never settle for standard. Creativity over conformity. Be your remarkable self.\\n\\n\\xa0Protect the source.\\n\\n\\xa0Community and consumers come first. If we don't maintain consumer trust, we won't have anything to offer local businesses.\\n\\n\\xa0Authenticity.\\n\\n\\xa0Tell the truth. Be straightforward\\n\\n\\xa0and over-communicate.\\n\\n\\xa0No need to spin things.\\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\nWe’re passionate about connecting people with great local businesses.\\n\\n  \\n\\nRestaurants, Retail & Other Note:\\xa0\\n\\nby Category for FY21 ~90% of Page Views & Searches ~ 40% of Ad Revenue Services ~ 10% of Page Views & Searches ~ 60% of Ad Revenue\\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\nLet’s generalize what we have seen so far. An agent interacts with an environment through actions, these actions change the state of the environment. The goal of the model is to determine what actions will lead to the maximum reward.\\n\\nTo determine the best action, reinforcement learning works by estimating the value of actions. The value of an action indicates how good an action is, e.g. how good a chess move is. All of reinforcement learning revolves around this idea of estimating the optimal value function.\\n\\nValue: The value of an action is defined as the sum of the immediate reward received by taking an action plus the expected value of the resulting state multiplied by a scaling term. In other words, the value of an action is how good the next state will be after taking that action, plus the expected future reward from that new state.\\n\\nReinforcement learning models update their value function by interacting with the environment, choosing an action, looking at the new state, looking at the reward then updating.\\n\\nAside from the value function, the model needs to learn a policy.\\n\\nPolicy: The policy of the algorithm is how it chooses what action to take based on the value of the current state.\\n\\nReinforcement learning algorithms want to evaluate states as best as possible (value function) to help them make decisions (policy) that lead to the maximum reward.\\n\\n  \\n\\n  \\n\\n  \\n\\n即便是coding题目也要确保自己多问几个问题\\n\\nwhat are the constaint of the input? 最大值最小值\\n\\nhow many calls will we made for each function? proportion?\\n\\n  \\n\\n让我自己显得更加DS一点；不要太过NLP；人家介绍项目的时候主动提问；\\n\\n  \\n\\nwhat funciton to add on yelp?\\n\\n信息流太辣鸡了；整页都是sponsorship，或者整页都是’s photo is popular near u;\\xa0\\n\\n  \\n\\n  \\n\\n  \\n\\ni have been working about a year; i really like my work content in ML DS in general. I completed several projects from the very begeining to product deployment; I can get things done with little support in a early stage startup environment; And I felt I am ready to move a step forward to a more challenge place and better oppportunities;\\xa0\\n\\n  \\n\\n  \\n\\nWhy yelp\\n\\ni heard of this opportunity from my friend Palermo;\\xa0\\n\\n  \\n\\nMy friend Palermo said many good words about the ML team at yelp; He shared similar acadamic background with me we grad from same school with a PhD. We took many similar courses.\\xa0\\n\\n  \\n\\nwant to be surronded by talented people;\\xa0\\n\\nI am interested in Yelp because I am\\xa0\\n\\n  \\n\\nI am a foodie; pretty good cook and I love to experiment new restaurant;\\xa0\\n\\n  \\n\\nexcited about the problem;\\xa0\\n\\n  \\n\\n  \\n\\nlots of review ; NLP element in it; location data etc;\\xa0\\n\\n  \\n\\nIn fact I have used Yelp Kaggle competition data before in one of my academic research; Kaggle competition;\\xa0 how to be a good influencer; exteme attitudes brings u more follower than neutral attitudes, contolling other factors;\\xa0\\n\\n  \\n\\n  \\n\\n  \\n\\nYelp Uses ML Everywhere:  \\nContent models: photo classification, popular dishes identification, fake/biased review detection, ...\\xa0\\n\\nStrategic models: ad selection/pricing, sales lead targeting, store visit detection, ...\\xa0\\n\\nConsumer models: search results, wait time estimation, personalized push notifications, ...\\xa0\\n\\n  \\n\\n  \\n\\nfake/biased review detection; inference needs to be fast;\\xa0\\n\\n  \\n\\npersonalized push notifications\\n\\n  \\n\\n  \\n\\npopular dishes identification;\\xa0\\n\\n  \\n\\n  \\n\\nhttps://engineeringblog.yelp.com/2022/04/beyond-matrix-factorization-using-hybrid-features-for-user-business-recommendations.html\\n\\n  \\n\\n  \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fc9c5f39-cb02-4de1-8973-abc3a770c512', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n\\n\\nonline metrics:\\xa0\\n\\noffline metrics:\\xa0\\n\\n  \\n\\nmodel architect:\\xa0\\n\\n  \\n\\n  \\n\\nif a customer comes to a branch, they don’t come very often; how would be able to determine if u should spend less or more time to mkt this customer;\\xa0\\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\nDesign a method to dynamically update the categories showed on Yelp homepage to improve user engagement\\n\\n  \\n\\nWhat new feature would you add to Yelp?\\n\\nCan you tell me about something you learned the hard way?\\n\\nHow would you test that the samples obtained with the function implemented are coming from the original distribution?\\n\\n  \\n\\nDesign a recommendation system from scratch. No coding but the interviewer gave very specific scenarios.\\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\n1. Tell me one machine learning model that you had to implement and the main difficulties you faced\\n\\n2. Imagine you have a feature from yelp, like the waitlist functionality. How can you measure the waiting time?\\n\\na. What variables do you get?\\n\\nb. What model do you try?\\n\\nc. How do you see if the model is working?\\n\\nd. What are the pros and cons of using indicator variables\\n\\ne. How do you assign a monetary benefit?\\n\\nf. What happens when a subsegment of the data is not performing as well as the rest of the data?\\n\\n  \\n\\n  \\n\\nIt's a coding problem. find the unpopular merchants based on the requirements described in the problem. No fancy algorithm is required. Using a dictionary can solve the problem.\\n\\n  \\n\\n  \\n\\nHow to evaluate the features classifying spams and emails.\\n\\n  \\n\\n  \\n\\nHow to swap two elements of a list\\n\\n  \\n\\n  \\n\\nList the strings that are anagrams from a set of strings\\n\\n  \\n\\n  \\n\\n3 Sum\\n\\n  \\n\\nDetermine if a word has a palindrome anagram\\n\\n  \\n\\n  \\n\\nI should've asked more questions before solving problems in the technical rounds. I did ask some questions before starting solving problems, but I think they REALLY value this, and I made (and corrected) a mistake because I didn't clarify something in sys design round\\n\\n  \\n\\n  \\n\\n- Get users opted in/out after processing an opt in/out log\\n\\n- Bigram tokenization\\n\\n- Random weighted sample of a list\\n\\n- Design a recommendation system\\n\\n- Time you dealt with failure\\n\\n- Time you dealt with a conflict at work\\n\\n- Time you dealt with an uncomfortable situation at work\\n\\n- Tell me about yourself\\n\\n- Tell me about time you worked on a team, what were your responsibilities, did you lead?\\n\\n- Tell me about your work experience\\n\\n- Why Yelp\\n\\n  \\n\\n  \\n\\n- Building a tokenizer\\n\\n- Given a set of items, create pairs given certain conditions.\\n\\n  \\n\\nProject most proud of ?\\n\\nTeam project, challenges\\n\\n  \\n\\n  \\n\\nmin_frequency， special tokens, lowercase, clean_text, stemming;\\xa0\\n\\ntokenizer.train(files=paths, vocab_size=30_000, min_frequency=2,\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 limit_alphabet=1000, wordpieces_prefix='##',\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 special_tokens=[\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 '[PAD', '[UNK]', '[CLS]', '[SEP]', '[MASK]'])\\n\\n  \\n\\n  \\n\\n  \\n\\nAfter considering the challenges in infrastructure, ML, and the adversarial space we settled for a\\xa0multi-stage multi-model approach\\xa0where there are two stages and different models for each stage and type of spam. The first stage is used to identify the subset of photos that are most likely to contain spam; the models in this stage are tuned to maximize spam\\xa0recall\\xa0while filtering out most of the safe photos. Essentially, this step changes the label distribution of the data fed into the second stage and in doing so it significantly reduces\\xa0ham/spam class imbalance and removes many potential false positives (consider the following: we do not perform inference on a large subset of photos in the second stage, and the final set of false positive is only limited to the false positives generated by the second stage, which may or may not intersect with the false positives generated in the first stage). The second stage is where the actual classification of the content happens; the models in this stage are tuned for\\xa0precision\\xa0because we aimed to send only a small amount of content to the manual moderation queue and we wanted to keep false positives to a minimum. Moreover, we have a set of heuristics playing alongside ML models which speed up the whole pipeline and are quickly tunable so that we can react in a small amount of time to a new threat our models are not capable of handling which give us the time to update our models while keeping users protected. Finally, we created a Review Then Publish (RTP) moderation workflow UI where images that are identified as spam are hidden from the users and sent to our\\xa0content moderation team\\xa0for manual review. Yelp’s content moderation team then can decide to either restore a photo if it is a false positive or allow the photo to remain hidden if it’s malicious.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7407e1c5-d0b7-4a87-bfac-b7500cf31c29', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n自我介绍 tell me about urself\\n\\npast half year I worked as a DS at this consulting firm named tiger analytics; I work for a fortune500 client in the retail industry. I work with a team of around 10 DS on demand forecasting tasks. Basically we build models to predict how many items would be sold for each product in each store. Its very complex time series models, we use very SOTA model that's published like 2-3 yrs ago. And our model performance was more than 10% better than models from our competing consulting firm. Personally I was in charge of several parts of code scripts of this huge projects and i am leading two new team members on their work \\n\\nin my previous job I worked at a startup called dealmaker; \\nI was the first hire of the data science team, so I had the opportunity to complete several projects from the very beggining, defining business problem, data collection to final product deployment; I also need to work on different data analytics and visualization needs from the company, like the product lead want me to; \\n\\n~~In that company I had experience with typical ML project like recommender system and more advanced ML products built on nlp models like transformer based models \\n\\nIn summary I feel pretty confident that I can get things done whether its in an big established data team; or in a messy environment with early stage data infrastructure and little resources; \\n\\n**why i am interested in betterhelp**\\nhonestly i was not considering switching job right now; but your HR ashley reached out to me and i was quite interested in this opp, because i am very familiar with psycho therapy. I used lots of mental help services when I was doing my phd. I also tried out a Canadian online platform before called Inkblot. So I have a general idea of this business \\nNow I dont need to use any help from mental health. I want to help more people to have a happier life. I believe thats the mission & vision of betterhelp\\n\\n**a little bit about my edu background:** I did my undergrad in engineering and found myself more interested in social science. So I had a double major in economics and continued to marketing programs in graduate school. In my phd, I studied consumer behaviour using data to answer marketing questions, my work involves a lot of data analysis and I do coding everyday in my five years during the phd.\\n\\n\\n\\n\\n**三个项目：raise prediction; investor insight score; nlp tool**\\n\\nbesides the three projects, I also did a bunch of analytics and visualization projects for mkt and finance, strategy team for instance; since I am the only data guy in the company. Those things are not deployed model into the products, more like reports & documents as the output that provides insights; \\n\\n!Untitled\\n\\n\\nthe first one that i saw asking which youtube channel brings u here\\ncreate a pin没有确认环节\\n付费环节来得好突然，没有体现online therapy可能的价值；直接就让我付费；对目标明确的用户有用，对探索的用户没用; maybe show me pictures of several good therapist; \\n\\n\\nWhat metrics would you use to track whether Uber's strategy of using paid advertising to acquire customers works?\\n\\n广告就是三类metric: **ads metrics**: download, sign up and the ads we paid (conversion rate and churn rate), **user level metrics**, LTV, CSC; \\n\\nCSC: marketing costs metrics, Cost per thousand impressions, Cost per click, Cost per acquisition, Cost per ride\\n\\nMetrics of Successful Paid Advertising Success Metric: \\n- app downloads/sign-ups traced from advertisement link\\n- Conversion Rate: # of app downloads or sign ups / total paid advertising impressions\\n\\nGuardrail Metric:\\n- Customer Acquisition Cost (CAC): how much does it cost to acquire a new customer?\\n- LTV / CAC: determines how much a new customer will cover the expenses needed to acquire them (ideally have this ratio over 1)\\n\\n\\n\\n1. 先明确定义；business problem \\n2. 宏观环境和基线；了解这个波动是否正常\\n3. 拆解问题，从设备，人群，地域等角度提出一些可能性；不同情况下，分子，分母分别会怎么变化\\n4. 根据上面的答案，提出自己最有可能的hypothesis来解释这个变动\\n5. 提出要验证这个hypothesis，需要什么data；\\n6. 提出metrics，如果这个metrics多大，就证明或者证伪了我的hypothesis\\nIs this coming from a particular region? Does this hold constant across all devices/platforms\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d340bcc4-4d02-4a00-aa3e-205aff5c9eea', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Project Workflow**\\n1. define business need, target user/scope\\n2. translate to DS problem; \\ngoal of the project: building a product, or gain insight? requires explanbility?\\ndefine a baseline and successful criteria, the criteria needs to be translate to DS metric; \\n3. determine if u can get proper data\\ngo through the lit to find the best methods; \\nRun EDA \\nclean data, feature engineering; train the model and evaluate; exp with more models and features \\nesemble model, deploy model by region and keep monitoring it \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='10b78d49-e997-43c7-aa14-d9f6d2168674', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\ncareer goals\\n****Define your goals**** I enjoy ML/DS very much; I am sure this field will be growing; to be a senior manager under the current path; 现阶段还不知道自己要成为技术专家还是管理者；两方面都努力进步because growth opportunity is huge in both direction for me;\\n- ****Summarize your plan for achieving your goals****\\n1. hard skills, become a tech expert: two side: one side is to get deeper, better understanding of SOTA in nlp via reading and coding; besides work, I also started work on my personal side project on nlp \\n\\nanother side is to digest existing knowledge and skills better by knowing their limitations and potentials in practice; I have gained some exp in the startup environment; those things can only be achieved by experiences; \\n\\n1. soft skill: how to get things done in DS in general; everything is a collaborative work in the modern society; 两方面任务：how to discover and define needs that can be resolved by ML and contribute to the organization; how to leverage resources to get things done; no concrete plan for this; all I did is keep reminding myself to be open minded and be proactive in communication; \\n2. \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='361cd1bd-0789-42af-bbac-beac9f42c9d2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nabout collaboration\\n驱动了 survey generation project; 先和cto确认初始目标不行；然后找到了mininal viable product; 然后说服team给我整理数据，aws上得到了其他engineer的帮助\\n学到了啥： get everyone on board; predetermined checkup every 3 days; overcommunicate; weekly project update cc all team members; use CTO's authority  \\n\\ntiger的项目：its mainly about how to lead: two goals: deliver result + motivation of the team and personal growth \\n了解每个人的特点，刚开始足够细化的任务，主动性强的人就给更开放的任务；保证能干成事so ppl see progress+让手下的人觉得有成长so they can be motivated\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='07a4d34d-dfb0-4f48-9bbd-e248e8366e13', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nhard to deal colleague\\nslack and then jira to make it public, specific time deadline; i included CTO in the meeting and confirmed the task timeline and he had no objection;  \\nquick 15min 1on1 meeting; turns out he had issues with exp design; i convinced him by data and knowledge \\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f0b0c610-78f6-4d71-9fda-d54d1f63d336', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n优点缺点\\nfast learner: django, phd learn new methods; very good product/business sense, talkative in product meetings; good training in social science ; perripheral knowledge \\n缺点：太注重细节，说话废话多，比如我怎么处理missing value; ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fd09ca7f-b241-4027-98fd-624548670061', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c30749c3-366b-481b-b08e-5d8f8d47e753', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nTime you dealt with failure; mistake 失败\\n\\n**why failed**:  I created the ticket, CTO told me let him organize it and assign tasks to different people; \\n\\nnot enough engineering resources for ab testing; no need for other projects for now\\n\\n**responsible:**\\n\\n**what I did to make it happen?**\\n\\nim partially responsible \\n\\nI did not insist to push and follow the projects frequently\\n\\nI failed to connect with other stakholders and engineers about the needs and importance of the ab testing; just let CTO handle it but its not his priority; \\n\\n**what I learned/make sure it won’t happen again:** \\n\\n- need to better understand and manage engineering resources beforehand; also need to know  that resources can be more if I fight for it and align with other people’s interest; for instance, find other needs for ab testing so people will be more interested to develop a good tool for ab testing\\n- many people don’t appreciate the value of data science, they have their lay believes that new feature is always better without testing;\\n- I organized data science workshops and brain storming session to help ppl understand importance of data science; translated into several projects; 主动找finance, sales people 聊how data science can help them; even with simple ways, like I generated some table with descriptive statistics for them\\n- 反思我自己，也存在disconnection with user；I asked them to involve me with meetings with a long term client in another project and learned a lot\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='db2ef8c7-e45d-483e-af9b-5c977cd81969', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n**问我希望工作环境是什么样子**\\n\\ntransparent; 小团队；low ego; 坦诚高效沟通（教授不回邮件）；目标一致；有一些有趣的人\\nif possible, can see the result of DS; \\n\\n软硬件上的支持，我相信XX比学校给的多; tech mentor; a little bit nudge is enough; When I first started to learn web scrapping, bug I coundn't understand, I go to TA session pretend that I am a student; \\n\\nThe reason I turn down the offer from tencent is that the team; \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f8f2b5c2-5413-419e-9ed3-252393db0159', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**你觉得自己是什么性格的人？**\\n\\n比较安静，认真，努力，自我驱动力比较强的人；\\n\\n1. can fight and get through rough time; have my way of dealing with difficulties; \\n\\n2. interesting and high curiosity, self motivated fast learner; geek in many different things; read a lot; my friends say they can often got inspirations from me; \\n3. a nice guy; pursuasive guy 有能力影响他人; Many ppl think I am an influencial person but I honest think I am a introvert guy \\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='52f909b5-b514-4df5-a47e-7e967e8bfb30', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n**如何说服别人**\\n\\nmy business and product sense is strong; compared to other engineers; besides the product manager, it is usually me who talks most; I often make product suggestions during meetings;\\n\\nI persuaded the product manager to change free tags to tags system; \\n\\n**2 possible solutions: iteration tag system or combine similar tags manually for analysis;**  \\n\\nwe first launched the free input tagging system\\nturns out people use tags that only themselves understand; invalid for analysis; we had a meeting with a long term client and asked their idea; client said it won't hurt if the preset tags are good \\n\\nshe created a set of tags and asked me opinion about it; In the end we still allow free input but will iterate over time; most people use default tagging \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='79c7e598-d71b-4bab-bbd6-d66ae6a10e6c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n为何离开上家公司 why left\\ndidnt plan to leave; ur hr reached out to me on linkedin; 1. i want to have impact on product or ads strategy etc. current consulting company can't see any impact; 2. i really like psycho therapy in general; im not just saying it because im interviewing with ur guys. i used many help in mental health during my phd; i had an article;  the mission and vission of help ppl live a happier life really resonate to me;  \\n\\n\\nlast round of presentaiont, it will be mostly DS people; i need to pretend that i am presenting to both non tech and tech ppl; \\n\\nthey want to see how u approach the problem; how u prioritize, what resource do u need; whats ur recommendation etc \\n\\n\\n\\nWe are passionate and compassionate professionals, driven by the mission of helping more people live a better and happier life every day. We are growing fast and always looking for new talent!\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='97b7d47d-02bb-4f3d-82ea-7e5ae0b9e6b8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n问他的问题：\\n\\ncompared to other companies in tech, what do u think its the unique point of betterhelp? \\n\\n\\nthe JD looks very generic; ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='836adc18-1aef-43df-ae9b-3d461cf8a004', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nwhat does the data team look like?\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3e4ad525-6e54-4b22-a7fe-f0d2c34b9fc3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nwhat does the typical task i will be work on? the JD is too generic; opp to work on NLP?\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0ff7ede7-30c6-40e0-b60c-9060bbcaf091', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nwhat are the challenges that ur team is facing?\\n\\nwho are the clients? \\nwhat are our advantage over their platform?\\n\\nneed to work with clients?\\n\\n- *How do you guys decide what to focus on?*\\xa0- This will give you another glimpse into how things are structured and the relative autonomy you can expect.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8dbe5da1-ae99-4144-9dc2-a6d147c359d8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\ncanada team; 每三个月去一次公司是干啥？\\n  \\nwhat is the biggest challenge u guys are facing; the industry is facing?\\n\\n• What’s ur exp in this company? Has your role changed since you’ve been here?\\n\\n• How has the company changed since you joined?\\n\\n• What’s one challenge you occasionally or regularly face in your job?\\n\\n1. What are the team’s plans for growth and development? in terms of tech stack; future projects, team growth \\n\\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f1e26727-98b4-43d9-91ac-54f302785ae3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nhedonic project\\n\\nhard skill: No\\nsoft/businenss sense: No\\ndifficulty; hardwork: No\\nteamwork: No\\n\\n1. experimental methods: lab and field studies; feel more \\n\\n1. **big data results to validate the finding from experiments** \\n\\nI found some weibo data; twitter in China; \\n\\nmap the posts to its geographic location and date; \\n\\nselect all posts that ;', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b3940c40-4b7c-4666-9e40-b80777305017', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='|Name|hard skill|soft/businenss sense|teamwork|difficulty; hardwork|\\n|---|---|---|---|---|\\n|Tencent nlp|Yes|Yes|Yes|No|\\n|Tencent user research|No|No|No|No|\\n|My wechat project|No|No|No|No|\\n|hedonic project|No|No|No|No|', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='119c4e4f-b347-412f-8a20-ce289d40a57a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nMy wechat project\\n\\nhard skill: No\\nsoft/businenss sense: No\\ndifficulty; hardwork: No\\nteamwork: No\\n\\n**situation**: wechat is similar to facebook in the sense that ppl can connect with their friends and also follow influencers; Information flow design; \\n\\n**task**:  I scrapped more than 5 million articles from influencer accounts from wechat; There is a lot of data so I used my school server to do that; \\n\\nit is supposed to be part of my academic research; \\n\\n**action**: I ran different models to predict likes on the articles (reg) and ads type classification problem;  For the classification label, I hired a  Chinese agency to do the manual label of 10,000 articles;  $300 CAD\\n\\n**result**: I mainly compared the effect of different models; I showed that the LSTM performs better than benchmark models like linear regression; among three tyeps of LSTM, bidirectional is the best; and showed that BERT performs better than LSTM even with only 3 epochs; In general the model works ok, classification problem 80%+ accuracy. \\n\\n我在文本上做的优化\\n\\n标题乘以三\\n\\nbert只能取512，实际是510；我从paper上看到的，前后都取，在中间部分删去是最高效的，因为前后更能表示这个文章的主旨\\n\\n修改stopword list提升很显著\\n\\n我没有用segment or type id to separate sentences between [sep], 对应是bert model里的segment embedding; if i do, there might be more info\\n\\n\\n\\n\\n**content based features; user features; average posts the user posts per day; average likes for poster;** \\n\\n- average likes for poster\\n- Last 3 post like counts\\n- Average posts per day\\n- number of followers\\n- delta followers since last post (if you have that info); how many new\\n- moving average of every week to see how the popularity is going\\n\\naverage likes; \\n\\naccount age; \\n\\nif it's not causal, it's hard to explain in the interview; \\n\\nthey might be a weak link between the posts; \\n\\ncoherent story on the results I have; \\n\\nwhat caused my X and Y; \\n\\nretrain  \\n\\n**focus on the technical details; try to say it more technically; read the interviewer and** \\n\\noutput of the last layer; \\n\\nnsp variable into similarity variable; \\n\\nyesterday's post count predict tomorrow's post count\\n\\n \\n\\n#关于wechat项目\\n\\nwechat has a updates section which is quite similar to instagram, where you can see updates and news from your friends and influencer accounts\\n\\nwe scrapped data from a third-party webisites which collect influencer articles;\\n\\nI don't have info from the profile page, so I engineered a list of features myself, like how many average user based features to do tabular analysis, like how many; I also tried time series models like ARIMA to improve the prediction\\n\\nI used bert and topic modelling with LDA to extract content based features;  some interesting insights, like topic should be focused to establish a fan base; at the beggining and become more diverse\\n\\ncombined the content and user based features  \\n\\n有什么困难、尝试和贡献：not much findings to explore what content is better for influencer account;  \\n\\n**multiple identities: 说得尽量有趣点有关business一些**\\n\\nhow do people present themselves on SNS to generate more likes; I tried several different packages like ntlk and stanford corenlp to measure the language similarities and NER to define multiple identities; I found that more identity (i.e., disimilarity) generally predicts better social image\\n\\n有什么困难、尝试和贡献：well-established theory or knowledge from psychology and sosiology and show it with big data\\n\\n**Bad Things Escalate and Good Things Satiate? how consumers respond to repeated experiences, negative exp; big dataset to show it; we found that** \\n\\n有什么困难、尝试和贡献：find data to confirm a hypothesis; I explored and scrapped lots of data, like reddit and twitter data about ppl's reaction to teams; \\n\\n\\n\\n\\n\\n\\nintroduce dataset; what is wechat; \\n\\nwhat you are trying to predict; what is your general approach; use text content \\n\\nbert has been show to have state-of-the art results; it's great point to start;  \\n\\ni use bert to embed the get embedding 768; I can use it to train other models; and I can add another layer to fine tuning the model by retraining part of the model; \\n\\nstatus label: 768 dimensions; \\n\\nimportant to not saying the wrong thing; good communicator; \\n\\n**go through the technical details of every projects; how did I think of the projects;** \\n\\ncreated the ARIMA model; \\n\\nwhat cause those one post has ; \\n\\nyesterday's post\\n\\nN of likes; \\n\\ncorrelation than causation; \\n\\ncontent based method; then it would be causation;\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aa34fd32-f936-48ce-803f-88c541463fc1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nTencent nlp\\n\\nhard skill: Yes\\nsoft/businenss sense: Yes\\ndifficulty; hardwork: No\\nteamwork: Yes\\n\\nsituation: user research dept for the whole group; need an internal survey database for people to reuse previous consumer surveys\\n\\ntask: took me a while to contact each dept in the company to share the surveys; more than a thousand surveys; a few thousand unique questions; need label the questions, classification problem; \\n\\nasked many people and tried out what's the best way to label the questions to be 1. mutually exclusive and collectively exhaustive  (hardest part). 2. easy for the to classify; 3. did manual label several times; \\n\\n**action**: \\n\\nInitially, my colleagues tried to add rules; it works with few data but can't work with more data\\n\\n machine learning models; employed many models; including some deep learning models provided by the internal open source platform; \\n\\namong machine learning models, SVM performed well with existing embeddings created by tencent; Best model is bidirectional LSTM; \\n\\n**result**: achieved pretty high accuracy; possibly because the questions are well-structed in the first place: what is your address, what is your age; demographics; price sensitivity must includes the word price; I did not deploy the model to the final product when I was there because the engineering team has their own schedule; but I heard from my collegue later that they did deploy my model to the product\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c7aa0f38-75c6-4de2-81a1-390b3d2529d9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nhard skill\\n\\n文本上的改动：自己手动改分词规则（因为data里很多产品和公司名字）比如腾讯视频，都有明显提升\\n我为何选择single 不选择multiple labels\\nwe care more about false negative (recall，即本该分类为demogra都词没有被分类为这个) instead of false positive (precision), 一定程度上是倾向multi标签都，因为只要一个标签分类是对的就会返回这个item\\n\\nmulti-label classification vs. allow ppl to choose multiple labels when filtering\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b75632e7-9ee4-4752-92ad-df308b89d629', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nSoft skill\\n\\n我的优势：well-rounded person; I am strong in exp design because I used this method often for consumer research; For perferiferal skills outside the technical part NLP, I am also experienced. I hav**e recruited labelling workers in China to label the ads in my wechat data**. \\n\\nWhen I am in Tencent and train the classification model, I spend lots of time to select labels that are both statistically robust and easy to use and understand for the users. I am extremely sensitive to potential bias or false definition in data and varaibles when I read business news and reports. Guess u can call it an expertise in data collection and control data quality. My professor told me that it’s the tradition of the chicago school of economics where we emphasize a lot on data collection quality to avoid garbage in and garbage out.\\n\\n`Correctly Predicted`\\xa0is the intersection between the set of suggested labels and the set expected one.\\xa0`Total Instances`\\xa0is the union of the sets above (no duplicate count).\\n\\nSo given a single example where you predict classes\\xa0`A, G, E`\\xa0and the test case has\\xa0`E, A, H, P`\\xa0as the correct ones you end up with\\xa0`Accuracy = Intersection{(A,G,E), (E,A,H,P)} / Union{(A,G,E), (E,A,H,P)} = 2 / 5`\\n\\n!Untitled\\n\\n!Untitled\\n\\n我们会让用户打标签，如果用multi的话未来接手的人更可能打上错的标签，会越来越混乱。用单标签的话用户会自己学习，我们也会给一个指南说哪种题目放到了哪个标签下\\n\\n文本上的改动：自己手动改分词规则（因为data里很多产品和公司名字）比如腾讯视频，都有明显提升\\n\\n我为何选择single 不选择multiple labels\\n\\nwe care more about false negative (recall，即本该分类为demogra都词没有被分类为这个) instead of false positive (precision),  一定程度上是倾向multi标签都，因为只要一个标签分类是对的就会返回这个item\\n\\n文本上的改动：自己手动改分词规则（因为data里很多产品和公司名字）比如腾讯视频，都有明显提升\\n\\n我为何选择single 不选择multiple labels\\n\\nwe care more about false negative (recall，即本该分类为demogra都词没有被分类为这个) instead of false positive (precision),  一定程度上是倾向multi标签都，因为只要一个标签分类是对的就会返回这个item\\n\\n文本上的改动：自己手动改分词规则（因为data里很多产品和公司名字）比如腾讯视频，都有明显提升\\n\\n我为何选择single 不选择multiple labels\\n\\nwe care more about false negative (recall，即本该分类为demogra都词没有被分类为这个) instead of false positive (precision),  一定程度上是倾向multi标签都，因为只要一个标签分类是对的就会返回这个item', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='287dc55a-b39b-494b-ae9a-f22fb98ddee7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nCareer advice\\n\\n**【1】事事有回应，件件有着落。**\\n\\n保持反馈，让领导知道咱们每天都在做什么，做到了什么地步，出现了什么状况。不要害怕领导会烦，相反，这会让他们有一种安全感和掌控感\\n\\n对于领导而言，我知道把事情交给你，哪怕是随手交代的一件小事，你都会去做，不论事情办到什么程度或者中间出了啥问题，你也会及时跟我同步，这就很让人放心。\\n\\n当咱们能给领导这种感觉时，他们也会把更多机会给到我们啦，毕竟，谁不喜欢有安全感的人？\\n\\n**【2】让领导做选择题，而不是问答题**\\n\\n跟老板沟通、汇报时，我们要带着方案开口，而不是做伸手党。不要让老板做解答题，正确的做法是给到背景信息，并提供已经想好的解决方案，让老板做判断题和选择题。\\n\\n比如，可以这么说，“老板，这件事是这样的……我有两个想法你看下，第一个是……第二个是这么考虑的原因是……你看看有什么建议？\\n\\n既减少了领导的思考成本，还可以展示我们的业务能力和积极靠谱的一面。\\n\\n**【3】必要时，恰当地自我暴露**\\n\\n可以拉近彼此关系。就像我们身边的好朋友好闺蜜，都是知道自己秘密和糗事的人。\\n\\n所以，向领导说说“心里话”，说说自己的小秘密。同时，也要引导领导自我暴露。我们可以主动向领导请教，主动提及自己工作生活当中的迷茫，请对方指点一二。\\n\\n比如，“老板，我最近好烦啊，你像我这个年纪的时候，是怎么处理这种事情的呢？”\\n\\n**【4】关键时刻，挺身而出**\\n\\n杨天真说，你真正能跟同事建立信任，是在关键时刻你站出来了，你承担了。\\n\\n所谓关键时刻，不是能出风头的机会，而是危机时刻，一个你去做了，可能没有好结果，甚至会被人骂的时候。但如果你去了，你顶上了，就能获得更多信任感。\\n\\n比如，当领导在会议上出现口误，咱们反应要机灵一些，帮领导从尴尬中解围。当领导工作失误时能主动站出来替领导背锅。\\n\\n相信我，当你挺身而出的那一刻，你在领导心里印象分直接拉满。\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bb7a0e35-9f5e-463b-a420-af90ece3a7a0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**和领导这样沟通，你的靠谱形象就更会坐实：**\\n\\n**【1】沟通要主动，让领导省心。**\\n\\n×：从不汇报，每次都是等到领导问起才说。\\n\\n✔：主动汇报可以让领导知道你的工作进度及离目标的距离。让领导知道你的困难以及你的能力，为实现目标以及升职加薪做铺垫。\\n\\n**【2】少讲困难，多“要支持”。**\\n\\n错误做法：直说困难，领导会认为你借口多\\n\\n正确做法：通过要支持，让领导知道你积极的态度和面临问题的难度；要资源支持，人力支持(跨部门)，你在帮领导干活，领导的资源就是你的资源，不要白不要。而且要资源后本质上是在扩大自己的影响力和权力范围，为自己晋升做准备。\\n\\n**【3】面对领导的不同意见，先别全面否定。**\\n\\n由于高度和角度不同，领导的建议有时是大方向上的，而忽略具体操作性。\\n\\n一棍子打死说法：领导，这个方案不切实际，没法做。\\n\\n更好说法：领导，您提出的这个方案方向上我们都非常认同，具体执行细节上我们有几个成本更低的方法，想请领导给指点一下。\\n\\n**【4】通过FORM法则，全面“聊”解领导信息**\\n\\nF:Family，家庭，你孩子多大了？\\n\\nO：Occupation，工作，最近工作顺利吗？\\n\\nR:Recreation，娱乐爱好，下班后喜欢做什么？\\n\\nM:Money，金钱跟梦想，退休后有何打算？\\n\\n**【5】学会预判。**\\n\\n提前预判，帮领导说不方便说的话、做领导不方便做的事。\\n\\n举个栗子：有一次，领导要请刚入职的10个新人吃饭，小李作为新人，知道领导很难这么短时间内记住每个人的名字，于是把名字做成桌牌，悄悄地放在圆桌上.事后领导问起，小李也就很自然进入领导的视野。\\n\\n**【6】巧用参照物，让领导有感触。**\\n\\n由于上下级关注的视野不同，所以在你眼里的大事，可能在领导眼里是小事。\\n\\n举个栗子：小王有一个客户订单，60万金额，在领导看来金额不大。小王想签单。于是说，60万在公司里不算是大订单，却是我们海南省一年的业绩，小王用一个业绩比较小的省份进行比较，引起了领导的重视。\\n\\n因为管得太细。管得太细就不适用于 OKR，这不是中国特有的问题，美国同样有这样的公司，跟着潮流搞 OKR 结果就是瞎搞。我之前看过一篇文章，作者研究了很多公司 OKR 失败的经历，最后总结出来不适用于 OKR 的三种体质，其中一种就是目标分解到人的团队不适用于 OKR。\\n\\n如果目标最多分解到团队，相信团队成员的主观能动性，让他们自由组织和发挥自己的才能，把团队目标给完成，这时候 OKR 是有可能实现的。例如说，团队目标是提升这个应用的用户留存率，团队成员自己想办法解决。\\n\\n这时候 O 可以是「用户爱用我们的应用」，KR 可以是「留存率提升到 X」、「发布 Y 个提升留存率的产品实验」等等。然后团队里的每一个成员都会去思考，自己能做什么能够达成 OKR。数据分析可能就跑去研究数据了，看看什么用户行为跟留存率有比较高的相关性，发现缺数据就找程序员帮忙加。用户调研可能就跑去组织用户访谈了，了解不同用户使用这个应用时的心路历程。\\n\\n一旦管得太细，「做一个这样的功能实验，要在这一天发布，程序员 A 和 B 负责后端，C 赶紧放下手里的另一个项目来帮忙做前端，那根据倒排时间表设计师 D 应该在那一天把设计做好……」无论这是谁做的安排，进行了这样的安排还想 OKR 就做梦吧。\\n\\n我也见过美国公司这样做的，这不是中国公司特有的问题。不过，中国公司基本上都这样，我只能解释为「饭都还没吃饱就别追求什么自我实现了」。员工要有很强的主观能动性，能力也要非常的强，能够自己找到什么值得做。老板自己要有安全感，没有微操团队的强迫症，知道如何提供关键信息让团队自己找方向。\\n\\n意思是说只改 bug 对你的职业晋升几乎没有任何帮助。就像普通的 IC 去做技术面试，除了自己爽、coasting 之外，对自己的职业生涯没有非常大的贡献一样。\\n\\n那什么是职业晋升的黄金通道呢？\\n\\n以我浅显的理解大致的思想是抱对、抱紧”大腿“，然后认真理解并实践自己组的 vision 和 mission 才能进入职业晋升的快车道超车。总结起来就是跟对人、做对事，升职加薪就会很容易。很可惜，最重要的这两点和改 bug 没有一毛钱关系。\\n\\n很多回答提到这个是*暗示你做更重要的事*，但我觉得还不全面。因为”更重要的事“实在是太主观，非常难落实和操作。所以我根据自己晋升 staff 时的经验提出来”跟对人，做对事“的方法供你参考。\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c6857b68-ace6-4932-b025-cc9ca3d48141', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**跟对人**\\n\\n确保那个人就是能左右你的职业生涯发展的人。一般来讲中小企业里的 VP，大企业里的 director 是能做这个主的。一个比较靠谱的方法就是保持跨级 one on one 会议，频率大概 1 个月一次。主要作用有两个：\\n\\n1）看看大佬最近的着力点在哪里，哪些是自己力所能及能帮得上忙的\\n\\n2）向大佬汇报自己当前的学习、工作状态。像这种跨级 one on one 一般大佬都不会很了解自己的项目，所以你需要主动汇报，并且旁侧敲击地看看这个项目值不值得做，是不是当前的热点等等\\n\\n所以我会经常以这样的问题开头：\\n\\n> What keeps\\n> \\n> \\n> you awake at night\\n> \\n\\n这样我就知道大致哪些问题是大佬操心的，以及当前的工作重心在哪里。\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='45db540a-9443-4979-95f1-19d90d43b268', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n**做对事**\\n\\n这个是比较难的。尤其找到适合并且自己也感兴趣的项目更是难上加难。通常一个靠谱的做法是和自己的 manager 过一遍组的 vision 和 mission，然后再来审视可选的项目。最好只选择大佬和自己老板都很看重的项目。哪怕你现在有其他的事情在手，但凡是他们看重的项目，你也要尽早参与。因为说不定明天你手头的项目就被砍掉，那时你再进入就稍微有些晚了。\\n\\n最后还是用一句老话来结束这个回答：\\n\\n> 选择比改变（努力）更重要。\\n> \\n\\n工作中 bug 可以改，但不是重点。正常情况下，你的 OKR 上甚至不应该列出来“改多少 bug”这个项目。更多的是那些和 vision & mission 相关的新功能以及从 0 到 1 的（程序）设计等。\\n\\nMaybe make a demo of the issue by comparing different models on historic/generated/bootstrapped data. I've found that running high numbers of simulations tends to impress the execs. They're probably not going to take you seriously if you talk theory with them - show them what the practical impacts of what you're describing will look like.\\n\\n管理层会认为印度人不会实干，只是嘴炮吗？当然不会。**怎么样判断你是实干家还是嘴炮？最直接的方法就是问细节，并且根据这个细节再次追问出举一反三的一串问题。**\\xa0这是职场沟通交流技巧的关键所在。 之前我已经说过，印度人自己干的活好不好不重要，他们会探头探脑的和团队里面，以及其他相关团队进行交流，不仅是知道你做了什么，还要知道其他人做了什么，并且知道你们工作之间的联系，而且进行充分的消化理解。理解了，就是你自己肚子里面的货，然后再通过自己组织的语言说出来。理解万岁！ 所以就算在开会的时候谈到了很多细节问题，他们也可以回答得从容不迫，并且有很强的举一反三的能力。而相反，中国人如果被问道一两句话说不清原理的问题，脑子里面已经过了好几遍了，但是嘴上也只有嗯嗯啊啊。\\n\\n还有一点中国人的表达方式也很有问题。不知道怎么说话。**会说话的一个核心要素是，首先要看你的听众是谁。**如果是普通小组讨论，大家把当周的工作掰开讲就好了。但是遇到和其他部门的领导开会，每个人对项目的理解层度不一样的时候，就需要首先从战略层面上面入手，把“是什么”，“为什么”讲透才能进行下一步讨论。不要让大家一开始就云里雾里，开了半小时会了，还有人会问一些基本概念，一些名词缩写之类的问题。 因为很多中国人只关注自己的那一亩三分地，所以基本上上来就讲，“上个星期，我对这个问题做了研究，然后给出了这个解决方案，实验结果是什么。” 而好的做法是，首先介绍项目，然后告诉大家项目分几个阶段，当前进行的这个阶段是处于什么，目前是项目有没有落后进度，目前解决的问题的难点，关键点在哪里。 而印度人根本不用教 ，自己就知道察言观色，知道怎么根据不同的听众说不同的话。而很多中国人就算教了都不懂。\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8a1b31fe-1409-4256-b479-a66b777eb5b8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nHow to do  research for cold email\\n\\n1. Research them, starting with their LinkedIn account. Most people have a lot of information on their LinkedIn pages. For people who don't, you can Google them - LinkedIn and Google together will cover 80% of hiring managers.\\n\\nYour goal is to get an idea of\\xa0**who they are, what they do, and the things they've done in the past that they're most proud of.**\\xa0👨\\u200d🔬\\n\\n1. Make sure you understand what your target's company and team actually do. 🧠\\n2. If it's possible to do quickly, try out the company's product. 👀\\n3. If it's possible to do quickly, write down 1-2 ideas of how you'd use data science to improve the company's product. This is less important if the company is huge, but really helpful when you're targeting a startup. 💡\\n\\n1. I already know what Uber Eats does. Chen also tells me what the data science team at Uber Eats is working on: recommender systems, routing problems, demand forecasting, and different kinds of analytics and optimization.\\n2. I order from Uber Eats every couple of weeks, so I've already tried the product.\\n3. On coming up with ideas: the Uber Eats data team is unusually sophisticated, and Chen's description already includes most of the low-hanging fruit I'd normally suggest. Off the top of my head, all I can think of here is 1) experimenting with different promotions to try to attract users from other delivery services onto Uber Eats; and 2) attracting more restaurants to Uber Eats by forecasting the extra revenue they can expect to earn by joining.\\n\\nBut Chen's team is probably doing versions of both of these things already, so I probably wouldn't include a specific suggestion in my email to him if I was sending one.\\n\\nNow that you've done your research, it's time to write the email.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9e8f3a41-6031-45f8-810d-b3288336774c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nPlease join me in welcoming John Bears, who joins us as Principal Data Engineer. He is a seasoned professional with experience as Data Solution Architect and Data Engineer primarily focused on the HealthCare and Life Sciences domain.\\n\\xa0\\nMost recently, John worked at Axtria on creating MDM, DWH, and BI Reporting solutions for SMB clients. He led multiple parallel engagements with a global delivery team to successfully deliver end-to-end data management solutions on cloud platforms.\\n\\nJohn holds a Bachelor’s Honors degree in Information Technology from India.\\n\\nDuring his leisure time, he likes\\nto drive around the bay area and try the Mediterranean and South Asian\\ncuisines.\\n\\n**Welcome to the team,\\xa0John!\\xa0We\\nwish you great success in the new role.**\\n\\nPlease join me in welcoming Kailuo (Cairo) Liu, who joins us as Data Scientist. \\n\\nCairo worked as a machine learning engineer at Dealmaker, a fintech company that builds capital raising tools for startups. He built several machine learning tools to predict investor behaviors, evaluate companies, and process financial documents.\\n\\nCairo holds a PhD in marketing from the University of Toronto, where he researched on consumer behavior with big data, such as how consumers present themselves on social media. He earned his Bachelor’s degree in engineering from China and Master’s degree from the US. \\n\\nDuring his leisure time, he likes to cook and read. He is also trying hard to learn ski. \\n\\nWelcome to the team,\\xa0Cairo!\\xa0We wish you great success in the new role.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='58ee0500-cd92-46f7-a912-6e7d70bad596', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\nHi Saeed, \\n\\nThank you for connecting. I saw that S&P Global is recruiting a number of NLP data scientists in Canada. Since it seems you’d work directly with this team, it would be wonderful to hear your thoughts on the roles. I’m looking to get some clarity on the responsibilities before I apply. Would it be possible to have a quick call (< 15 min) with you this week to learn about your work? \\n\\nThank you very much \\nCairo\\n\\n\\n\\n\\n\\n\\n> Hi {NAME},\\n> \\n\\n> I wanted to follow up with you in case my previous email got buried. Did you have any thoughts or feedback on whether my skills and interests might fit into your team?\\n> \\n\\n> If you're too busy to respond, no problem. I want you to know in any case that what {YOUR COMPANY / GROUP} are working on has been a real source of inspiration for me. If I can be of any help whatsoever going forward, please don't hesitate to reach out!\\n> \\n\\n> Bob Mentee\\n> \\n\\nim circlying back to ;\\n\\nhope its a better time connect to\\n\\nI understand you're probably busy reviewing applicants, but I wanted to reach out to see if you had any updates on the hiring timeline. I'm very excited about the opportunity to work for rbi and I am confident about my skillset. If it makes sense to talk, I would love to chat further about how my skills and interests might fit into your team.\\n\\n*Hey $NAME,*\\n\\n*I just wanted to update you on my own process. I've just received a job offer that's quite strong. That said, I really enjoyed getting to know the [$COMPANY] team and the set of projects available here really appeal to me. So I really want to see if we can make it work! Since my timeline is now compressed, is there anything you can do to expedite the process?*\\n\\n*Cheers,*\\n\\n*$YOURNAME*\\n\\n*Please let me know if there’s any other information I can provide as you move to the next stage in the hiring process.*\\n\\nI recently applied for the position of [position title]. I understand you're probably busy reviewing applicants, but I wanted to reach out to see if you had any updates on your decision timeline. \\n\\nI'm very excited about the opportunity to work at [company] and I believe that my skills, specifically [name a skill that's highly relevant to the position for which you applied], would make me an ideal match for the position.\\n\\nFor your convenience, I've attached my application materials. Please let me know if you need any additional information. I can be reached at [phone] or [email]. I look forward to hearing from you.\\n\\nDear Mr./Ms. Last Name, [if you have a name, otherwise omit this line]\\n\\nI hope you are well. I submitted a resume earlier this month for the programmer position advertised in the Times Union.\\n\\nI am very interested in working at XYZ Company and I believe that my skills, especially my extensive C++ experience at ABC Company, would be an excellent match for this position.\\n\\nIf necessary, I would be glad to resend my resume or to provide any further information you might need regarding my candidacy. I can be reached at 555-555-5555 or [[jdoe@abcd.com]]. I look forward to hearing from you.\\n\\nThank you for your consideration.\\n\\nBest regards,\\n\\nJane Doe\\n\\n############\\n\\n1. Thank again;\\n2. Asking what is the timeline looks like;\\n3. What do you recommend to me to do while I was waiting;\\n\\nHi Steph,\\n\\nThank you so much for the time you spent with me today. I really enjoyed meeting you and learned a lot about Asana. After our conversation, I am confident that I would enjoy the job and make a valuable contribution to the team.Attached please find my resume. \\n\\nThank you again!\\n\\nThank you so very much for referring me for the data scientist position at Asana. I am grateful for the time you spent reviewing my application and recommending me for the job.\\n\\nI still haven't receive any notice from the hiring team. May I ask what does the hiring timeline typically look like for data scientists? I am really excited about this opportunity and I believe my extensive experience in experimental design and analysis would be a great match for this position. \\n\\nWhile I am waiting in the hiring process, what do you recommend to me to do to improve myself to  prepare for the interviews and future work as a data scientist?\\n\\nFor your convenience, I've attached my application materials again with an extra cover letter.  Please let me know if I can provide any additional information.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='66e2c470-20f2-4049-a450-f0dc73580578', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\nAs many of you know, today marks my last day at Gap Inc after 6 years. I am profoundly grateful for the opportunity to have worked alongside many of you.  \\n  \\n\\nDespite the challenges we faced in transitioning the company toward a more analytical approach, I have complete trust in our primary objective: increased adoption, improved, and sustainable analytics. This belief stems from the exceptional talent within our organization.  \\n  \\n\\nI have gleaned a wealth of knowledge from many of you throughout this journey, so I want to express my heartfelt gratitude for your positivity, kindness, patience, and your pivotal role in my professional (and personal!) growth.  \\n  \\n\\nThere are too many individuals to thank individually, but you know who you are\\xa0😊\\n\\n  \\nPlease stay in touch. Don't be a stranger!\\n\\nhttps://www.linkedin.com/in/canozkan/\\n\\ncan-ozkan@outlook.com\\n\\nCan\\n\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5d11674a-5850-4506-a268-0d4be33f5d66', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n[[linkedin coffee chat & relationship build message]] \\n\\nto recruiter \\n\\n*I’m a social media strategist with six years of experience and currently seeking new opportunities. I’d love to chat about whether my background might be a fit for any of your openings, and I’d also be happy to connect you with other professionals in my field.*\\n\\n*Looking forward to hearing from you,*\\n\\n*Kendra Holloway*\\n\\n*Hi Peter. I am a Commercial Finance Manager with years of experience in the e-commerce industry. I’ve been interested in working for Expedia for a long time, so when I saw that you were looking for a Sr. Commercial Finance Manager, I applied right away.\\xa0I haven’t yet heard back on my application. However, I thought I’d reach out to introduce myself directly because I believe my experience is very relevant to what you’re looking for in your job ad.*\\n\\nHi Ram,\\n\\nI am an experienced data scientist and PhD from the University of Toronto. I saw that Narvar is recruiting a Senior Data Scientist in Canada and I am very interested in this position.\\xa0I have experience working in startup environment and I am very familiar with the retail industry with a background in marketing. I have submitted my application but haven’t yet heard back. However, I thought I’d reach out to introduce myself directly because I believe my experience is very relevant to what you’re looking for in your job post.\\n\\nThank you \\n\\nCairo \\n\\n*“Hi [insert recruiter name],My name is [insert name] and I recently applied for [insert job title.] I want to reaffirm my interest in being considered for the role, and confidence in my ability to bring value to your team. I look forward to the next step – is there any additional information I can provide on my end to help move the process forward?*\\n\\n*Thank you for your time,[insert name/signature]*\\n\\n*“Hello [insert recruiter name ], hope you had a great weekend. My name is [insert your name] and I recently applied for the [insert job title]. Since it has been more than a week and I haven’t had any response, I was just wondering if there’s anything else left to be shared or sent across. I am willing to disclose additional information, which you may find helpful in processing my application further.*\\n\\n*Thank you and have a good day.*\\n\\n*Sincerely,[insert name]”*\\n\\n*Hi [insert recruiter name],*\\n\\n*My name is [insert name] and I recently applied for [insert job title.] I want to reaffirm my interest in being considered for the role, and confidence in my ability to bring value to your team. I look forward to the next step – is there any additional information I can provide on my end to help move the process forward?*\\n\\nHi Jacob, \\n\\nHope you had a great holiday. \\n\\nHi Jacob,\\n\\nI am an experienced data scientist and PhD from the University of Toronto. I saw that Narvar is recruiting a Senior Data Scientist in Canada and I am very interested in this position. I have experience working in startup environment and I am very familiar with the retail industry. If it makes sense to talk, I would love to chat further about how my skills and interests might fit into your team.\\n\\nThank you\\nCairo\\n\\nHi $NAME,\\n\\nI'm interested in working at $COMPANY as a (data scientist | data analyst). Would you be able to give me a few minutes of your time to chat about your company? I know your time is valuable, so I would appreciate any amount of time you could give me.\\n\\nBest,\\n\\nNicholas\\n\\nHi Nakisa,\\nIt was a pleasure meeting you on the Toronto Machine Learning Summit. I am really interested in the data scientist position at Overbond. Is it possible to connect to learn more about your company?\\n\\nThank you\\nCairo\\n\\nto ppl \\n\\nHi Farnoush,\\n\\nI am an experienced data scientist and PhD from the University of Toronto. I saw that Flipp is recruiting a Machine Learning Scientist and I am very interested in this position. Is it possible to connect to learn more about your work?\\n\\nThank you\\n\\nCairo\\n\\nI have  worked on both traditional ML problems like recommender systems and STOA models like BERT. \\n\\nHi Ping,\\n\\nI am an experienced data scientist and PhD from the University of Toronto. I saw that Inworld AI is recruiting a Senior Data Scientist and I am very interested in this position. I have experience working in startup environment and I am passionate about blockchain and  Metaverse.\\xa0If it makes sense to talk, I would love to chat further about how my skills and interests might fit into your company.\\n\\nThank you\\n\\nCairo\\n\\nHello Atkins,\\n\\nI am an experienced fintech data scientist and PhD from the University of Toronto. I saw that Mercury is recruiting an\\xa0Data Scientist\\xa0and I am very interested in this position.\\n\\nI’m particularly excited about Mercury because my work experiences focus heavily on analyzing financial data and text. Most recently, I trained a XGBoost + BERT model to predict the success rate of startup capital raises on data extracted from SEC documents, and deployed the model as an internal tool for the sales team.\\n\\nIf it makes sense to talk, I would love to chat further about how my skills and interests might fit into your company.\\n\\nThank you\\n\\nCairo\\n\\nto unrelated ppl in the same company  \\n\\nHi Ping, \\n\\nI am a PhD from  with an undergraduate degree in materials chemistry. I saw that Spectra Plasmonics is recruiting a Data Scientist\\xa0and I am very interested in this position. Is it possible to connect to learn more about your work?\\n\\nThank you\\n\\nCairo\\n\\nHi Daniel,\\n\\nI am an experienced machine learning engineer and PhD from the University of Toronto. I saw that Lemay AI is recruiting a consultant and I am very interested in this position. I have experience building traditional ML product like recommender system and worked with SOTA models like transformers.  Most recently, I trained a XGBoost + BERT model to predict the success rate of startup capital raises on data extracted from SEC documents, and deployed the model as an internal tool for the sales team.\\n\\nIf it makes sense to talk, I would love to chat further about how my skills and interests might fit into your company.\\n\\nThank you\\n\\nCairo\\n\\nHi Mahsa, \\n\\nI am an experienced data scientist and PhD from the University of Toronto. I saw that Extreme Networks is recruiting a Data Scientist in Canada and I am very interested in this position. Is it possible to connect to learn more about your work?\\n\\nThank you\\n\\nCairo\\n\\nHi Owen,\\n\\nI am an experienced data scientist and marketing PhD from the University of Toronto. I am really interested in the data scientist position at Thumbtack. Is it possible to connect to learn more about your work?\\n\\nThank you\\nCairo\\n\\nHi Samuel,\\n\\nI am an experienced nlp engineer and PhD from the University of Toronto. As a long-time Grammarly user, I am really interested in the machine learning positions at your company. Is it possible to connect to learn more about your work?\\n\\nThank you\\nCairo\\n\\nHi Jeeyoung,\\n\\nI am an experienced data scientist and PhD from the University of Toronto. I am really interested in the data scientist position at Sanofi. Is it possible to connect to learn more about your work?\\n\\nThank you\\nCairo\\n\\nHi Hamoon,\\n\\nI am an experienced NLP data scientist and I am really interested in the scientist position at FutureFit AI. I share the vision of helping people better understand themselves with technology. Is it possible to connect to learn more about your company?\\n\\nThank you\\nCairo\\n\\nHi name,\\nI saw a DE posrting at Charle Sschwab that I'm really interested in. Would love to learn more about what it's like working there. Would you be open to\\na quick call in the next week or so?\\n\\nto regular people\\n\\n*Dear Ron,*\\n\\n*I’m also in the Society of Professional Journalists, and I’ve really enjoyed reading your posts. The piece you shared a week or two ago about the future of data journalism was pretty thought-provoking. I’d love to keep in touch and learn more about your work.*\\n\\nI’ll be in your area in a few weeks for vacation; if you have any free time, I’d love to meet up for coffee.\\n\\nIf you ever have 20 or so minutes, I’d love to hear more about how you started working in the field and what skills you believe are most relevant to the profession.\\n\\nHi Heindrik,\\n\\nI am an experienced data scientist and PhD from UofT. I am really interested in the data scientist position at GoBolt. Is it possible to connect to learn more about your team?\\n\\nThank you\\nCairo\\n\\nHi Yan,\\n\\nI am an experienced machine learning engineer and marketing PhD. I am really interested in the MLE position at Criteo. Is it possible to connect to learn more about your work?\\n\\nThank you very much\\nCairo\\n\\n\\nHi Monica,\\n\\nI am an experienced data scientist and PhD from the University of Toronto. I am really interested in the DS/MLE positions at Affirm. Is it possible to connect to learn more about your work?\\n\\nThank you very much\\nCairo\\n\\n\\n\\nHi Da,\\n\\nI am an experienced data scientist and PhD from the University of Toronto. I am really interested in the senior DS positions at Samsara in Canada. Is it possible to connect to learn more about your company?\\n\\nThank you very much\\nCairo\\n\\n\\n\\nHi James,\\n\\nI am an experienced data scientist and PhD from the University of Toronto. I am really interested in the DS positions at Jungle Scout in Canada. Is it possible to connect to learn more about your work?\\n\\nThank you very much\\nCairo\\n\\n\\nHi Saeed,\\nI am an experienced data scientist and PhD from the University of Toronto. I am really interested in the data science positions at S&P Global in Canada. Is it possible to connect to learn more about your work?\\n\\nThank you very much\\nCairo\\n\\n\\n\\nHi Molley,\\nI am an experienced data scientist and PhD from the University of Toronto. I am really interested in the data science positions at FlyWheel in Canada. Is it possible to connect to learn more about your work?\\n\\nThank you very much\\nCairo\\n\\n\\n\\nHi Dan,\\nI am an experienced data scientist and PhD from the University of Toronto. I did my dissertation on consumer behavior on social media and I am really interested in the DS positions at Sprout Social. Is it possible to connect to learn more about your work?\\n\\nThank you very much\\nCairo\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7089f344-f580-4ff4-b4d9-71b5e7b6a745', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nturn down reject offer\\n\\n“*Thank you very much for offering me the opportunity to work at [Company] as [Job Title].”*\\n\\n1. “*After careful consideration, I’ve accepted a position at another company.”*\\n\\n1. “*I sincerely appreciate you taking the time to discuss salary expectations with me. Ultimately, I will have to decline this role/job offer as the salary is too far outside my expectations to leave my current position.”*\\n\\n*After much deliberation, I will not be accepting the job offer, as it is unfortunately not the right fit for my career goals/interests at this time.*\\n\\n*Dear [Name of Hiring Manager],*\\n\\n*Thank you very much for offering me the position of [Job Title] with [Company]. I sincerely appreciate the offer and your interest in hiring me.*\\n\\n*After much consideration, I have decided to accept another role that will offer me more opportunities to pursue my interests/grow my skills in [area] and [area].*\\n\\n*Again, I would like to express my gratitude for the opportunity to interview and the offer. I wish you and [Company] all the best in finding someone suitable for this position.*\\n\\n*Kind regards,*\\n\\n*[Name]*\\n\\nHello Latasha, \\n\\nThank you very much for offering me the interview opportunity at Big Fish Games. After much consideration, I have decided to accept another role that will offer me more opportunities to grow my skills in deep learning. \\n\\nAgain, I would like to express my gratitude for the opportunity to interview. I wish you and Big Fish Games all the best in finding someone suitable for this position.\\n\\nHi Prakriti,\\n\\nHope everything is going well. Thank you very much for offering me the interview opportunity at Scotiabank. After the info session, I did not pursue further because I felt that it’s not the best fit for me. I got several offers this month and have decided to join a data consulting firm named Tiger Analytics  for more opportunities to grow my skills in deep learning. \\n\\nAgain, thank you very much for your help! I would love to stay in touch and hope we cross paths in the future.\\n\\nBest,\\n\\nCairo\\n\\n• *It’s been a pleasure getting to know you, and I hope that we cross paths in the future.*\\n\\nThank you very much for your help! Are you in Toronto recently? Let’s get coffee together sometime.\\n\\nHi Yvette,\\n\\nI am excited to share the good news that I have decided to join a fintech startup (\\n\\nhttp://dealmaker.tech\\n\\n) as a machine learning engineer. I really appreciate your referral and help with my job hunting. It's a pity that I failed to get a chance with Walmart :(\\n\\nI look forward to connecting with you again soon. Maybe we can grab a coffee some time when the pandemic slows down. For now, wishing you and your family wonder, joy, and prosperity.\\n\\nHi Yan,\\n\\nUnfortunately, my resume got screened out by the system immediately after applying to Criteo.\\n\\nGood news is that I got several offers this month and decided to join a data consulting firm named Tiger Analytics and work on NLP/deep learning.\\n\\nThank you very much for your help! I would love to stay in touch via LinkedIn and hope we cross paths in the future.\\n\\nBest,\\n\\nCairo\\n\\nHi Viacheslav, \\n\\nJust sharing an update. I went back to China, relaxed a bit in the winter, practiced my skill in  coding, and got several offers when I came back to Toronto. I have decided to join a data consulting firm named Tiger Analytics and work on deep learning. I really appreciate your help and suggestions during our interview discussion. Hope you found good candidates suitable for your team. \\n\\nBest regards,\\n\\nCairo \\n\\nHi Najeeb,\\n\\nUnfortunately, my resume got screened out by the system immediately after applying to Global Relay. Good news is that I got several offers this month and decided to join a data consulting firm named Tiger Analytics.\\n\\nThank you very much for your help! I would love to stay in touch via LinkedIn and hope we cross paths in the future.\\n\\nBest,\\n\\nCairo\\n\\nHi Krithika,\\n\\nUnfortunately, I did not hear back from Kinaxis after application. Good news is that I got several offers this month and decided to join a data consulting firm named Tiger Analytics and work on deep learning.\\n\\nThank you very much for your help! I would love to stay in touch and hope we cross paths in the future.\\n\\nBest,\\n\\nCairo\\n\\nHi Cam,\\n\\nUnfortunately, I did not pass the interviews of Mercury. I felt they are looking for someone more experienced in data analytics instead of machine learning. Good news is that I got several offers this month and decided to join a consulting firm named Tiger Analytics.\\n\\nThank you very much for referring me to Mercury! I can certainly tell it’s a great place to work from the interviews. I would love to stay in touch via LinkedIn and hope we cross paths in the future.\\n\\nBest,\\n\\nCairo\\n\\nHi Vinit,\\n\\nThank you so much for the interview opportunity to join your team. As we discussed, I’ve admired the Moz marketing community for a number of years. After much consideration, I’ve decided to accept an offer at a consulting company for a broader exposure to different machine learning problems.\\n\\nIt’s been a pleasure getting to know you, Corbin, and Andre. I would love to stay in touch via LinkedIn and I hope that we cross paths in the future. Again, thank you so much for your time and consideration.\\n\\nHowever, after much consideration, I felt a better fit to a role at another company and have decided to accept their offer\\n\\nHi Chris,\\n\\nThank you so much for the interview opportunity. I really liked the take-home problem and have prepared my detailed answer sheet. However, I’ve decided to accept an offer at a consulting company for a broader exposure to different machine learning problems. Although I hope to discuss my technical solutions with the hiring team, I don’t want to take up their valuable time for interviewing other candidates.  \\n\\nAgain, thank you so much for your time and consideration. I would love to stay in touch via LinkedIn and I hope that we cross paths in the future. \\n\\nHello Bertrand,\\n\\nI would like to express my gratitude for the opportunity to interview at TD SYNNEX.\\xa0After much consideration, I have decided to accept a role at another company.\\xa0I wish you and your team all the best in finding someone suitable for this position.\\n\\nBest regards,\\n\\nCairo\\n\\nHi Amy,\\n\\nThank you so much for the interview opportunity at Flipp. I really liked the take-home problem and have prepared my detailed answer sheet. However, I’ve decided to accept an offer at a consulting company for a broader exposure to different machine learning problems. Although I hoped to discuss my technical solutions with the hiring team, I didn’t want to take up their valuable time for interviewing other candidates.  \\n\\nAgain, thank you so much for your time and consideration. I would love to stay in touch via LinkedIn and I hope that we cross paths in the future. \\n\\nBest regards,\\n\\nCairo \\n\\nHi Palermo,\\n\\nThank you so much for referring me to Yelp. Unfortunately, I made to the final round interview but failed to get the offer. It’s certainly a great company and I learned a lot along the process. Good news is that I got several offers this month and decided to join a data consulting firm named Tiger Analytics and work on deep learning.\\n\\nAgain, thank you very much for your help! Are you in Toronto recently? Maybe we can get dinner or coffee together.\\n\\nBest regards,\\n\\nCairo\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='05e731c5-1601-47db-bafe-0e2ab6853b49', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nmy cold email finance\\n\\nHello Rich,\\n\\nI am an experienced data scientist and PhD from the University of Toronto. I saw that ASSURANCE IQ is recruiting a Data Scientist\\xa0and I am very interested in this position.\\n\\nI’m experienced in developing both traditional ML models like recommender systems and advanced ones like transformer-based nlp tools. My previous work focuses heavily on analyzing financial dataset. Most recently, I trained a XGBoost + BERT model to predict the success rate of startup capital raises on data extracted from SEC documents, and deployed the model as an internal tool for the sales team to filter clients.\\n\\nI’m particularly excited about ASSURANCE IQ because I am a good individual investor (10% + average annual return on the stock market for 10+ years) and love to help people make better financial decisions. If it makes sense to talk, I would love to chat further about how my skills and interests might fit into your company. Attached please find my resume. Thank you very much.\\n\\nBest regards,\\n\\nCairo Liu\\n\\nLinkedin: https://www.linkedin.com/in/cairoliu/\\n\\nGithub: https://github.com/lkl2050\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='916e282b-f476-432b-b320-4a252c2b5921', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nmy cold email finance2\\n\\nHello Akram,\\n\\nI came across Sagen on Linkedin and saw that you’re looking to hire a senior data scientist. I’m a PhD and data scientist with experience building data products from start to finish. I’m very impressed with the work from the data team of Sagen, and would love to talk to you about how my skill set would fit for the data scientist role.\\n\\nI have experience building sales leads classifier, recommender systems, and time-series prediction tools. My previous work focuses heavily on analyzing finance data and creating financial analytics tools. Most recently, I built a LightGBM model to predict investors’ likelihood to invest in crowdfunding projects, and deployed the model as an analytics feature to help client companies convert promising investor leads (see details at https://www.dealmaker.tech/compass ). \\n\\nI’m particularly excited about Sagen because it intersects with several areas I find incredibly interesting: finding insights from data, personal finance, and risk management. If it makes sense to talk, I would love to chat further about how my skills and interests might fit into your team. Attached please find my resume, which provides a more in-depth view of my qualification and experience. \\n\\nBest regards,\\n\\nCairo Liu\\n\\nLinkedin: https://www.linkedin.com/in/cairoliu/\\n\\nGithub: https://github.com/lkl2050\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5b85e001-4f17-4b9d-9f36-fbe21815ffbe', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nmy cold email robinhood\\n\\nHi Zhexuan,\\n\\n  \\nThank you for connecting. I’m a PhD and data scientist with experience building data products from start to finish. I’m very impressed with the work from Robinhood and really glad to see it's hiring in Canada.\\n\\n  \\n\\nMy previous work focuses heavily on modelling finance and retail problems. In my previous role at a Fintech company, I built a LightGBM model to predict investors’ likelihood to invest in projects, and deployed the model as an analytics feature to help client companies convert promising investor leads (see details at https://www.dealmaker.tech/compass)\\n\\n  \\nI’m particularly excited about Robinhood because I’m deeply engrossed in investing(8% + average annual return on the stock market for 10+ years) and am one of the earliest users of Robinhood. If it makes sense to talk, I would love to chat further about how my skills and interests might fit into your team. Attached please find my resume. Thank you very much.\\n\\n  \\n\\nBest regards,\\n\\nCairo Liu\\n\\n\\n\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0541cf8a-bf85-4b30-b27b-e7fac4142654', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nmy cold email marketing\\n\\nHello Kavitha,\\n\\nI came across Moz on LinkedIn and saw that you’re looking to hire a senior data scientist. I’m a marketing PhD and data scientist with experience building data products from start to finish. I’m very impressed with the work you’re doing at Moz, and would love to talk to you about how my skill set would fit for the data scientist role.\\n\\nI have experience building customer review classifier, recommender systems, and time-series prediction tools. My previous work focuses heavily on creating SaaS data analytics tools for enterprise clients. Most recently, I built a LightGBM model to predict investors’ likelihood to invest in crowdfunding projects, and deployed the model as an analytics function to help client companies convert promising investor leads (see details at https://www.dealmaker.tech/compass ). \\n\\nI’m particularly excited about Moz because I came from an academic background in marketing and am passionate about marketing tech in SEO. \\n\\nIf it makes sense to talk, I would love to chat further about how my skills and interests might fit into your company.Attached please find my resume, which provides a more in-depth view of my qualification and experience. \\n\\nBest regards,\\n\\nCairo Liu\\n\\nLinkedin: https://www.linkedin.com/in/cairoliu/\\n\\nGithub: https://github.com/lkl2050\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1ba4869c-79dc-4895-92ca-e7d703f1b647', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nmy cold email marketing 2\\n\\nHello Amy,\\n\\nThank you for connecting. I’m a marketing PhD and data scientist with experience building data products from start to finish. I’m very impressed with the work you’re doing at Flipp, and would love to talk to you about how my skill set would fit for the machine learning scientist role.\\n\\nI have experience building customer classifier, recommender systems, and time-series prediction tools. My previous work focuses heavily on analyzing consumer behavior data and creating analytics tools. Most recently, I built a LightGBM model to predict investors’ likelihood to invest in crowdfunding projects, and deployed the model as an analytics function to help client companies convert promising investor leads (see details at https://www.dealmaker.tech/compass ). \\n\\nI’m particularly excited about Flipp because I came from an academic background in marketing and am passionate about leading technologies in retail and advertising. If it makes sense to talk, I would love to chat further about how my skills and interests might fit into your team. Attached please find my resume, which provides a more in-depth view of my qualification and experience. \\n\\nBest regards,\\n\\nCairo Liu\\n\\nLinkedin: https://www.linkedin.com/in/cairoliu/\\n\\nGithub: https://github.com/lkl2050\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='41a08fd8-764a-4d2a-8db5-7abe1fb2c9b0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nmy cold email recommender system/ecommerce\\n\\nHello Ziqi, \\n\\nThank you so much referring me to the senior data scientist role. I’m particularly excited about Faire because it empowers offline shops instead of replaces them. In the past year, I am been building forecasting and recommender models for a Fortune 500 company in the retail industry, so I am very familiar with data & problems in Ecommerce. \\n\\nAttached please find my resume. I would love to chat further about how my skills and interests might fit into the Faire data team. \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ae6943bc-52f8-457e-b766-7da0e99b3c89', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nmy cold email gaming\\n\\nHello Jennifer,\\n\\nI came across Big Fish Gaming on Linkedin and saw that you’re looking to hire a senior data scientist in Canada. I’m a PhD and data scientist with experience building data products from start to finish. I’m very impressed with your work at BFG, and would love to talk to you about how my skill set would fit for the data scientist role.\\n\\nI have experience building sales leads classifier, recommender systems, and time-series prediction tools. My previous work focuses heavily on analyzing user behavior data and creating analytics tools. Most recently, I built a LightGBM model to predict investors’ likelihood to invest in crowdfunding projects, and deployed the model as an analytics feature to help client companies convert promising investor leads (see details at https://www.dealmaker.tech/compass). \\n\\nI’m particularly excited about Big Fish Gaming because it intersects with several areas I find incredibly interesting: finding insights from data, gaming, and gamers’ social behaviors. If it makes sense to talk, I would love to chat further about how my skills and interests might fit into your team. Attached please find my resume, which provides a more in-depth view of my qualification and experience. \\n\\nBest regards,\\n\\nCairo Liu\\n\\nLinkedin: https://www.linkedin.com/in/cairoliu/\\n\\nGithub: https://github.com/lkl2050\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4d47b399-36df-44b9-bee7-132fb5a77889', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nmy cold email dating app\\n\\nHello Michael,\\n\\nHope everything is going well. I saw that you are still hiring the senior data scientist and I want to give another try. I have experience building data products from start to finish and feel my skill set would fit for the data scientist role at PlentyOfFish.\\n\\nI have experience building sales leads classifier, recommender systems, and time-series prediction tools. My previous work focuses heavily on analyzing user behavior data and creating analytics tools. Most recently, I built a LightGBM model to predict investors’ likelihood to invest in crowdfunding projects, and deployed the model as an analytics feature to help client companies convert promising investor leads (see details at https://www.dealmaker.tech/compass). \\n\\nI’m particularly excited about PlentyOfFish because it intersects with several areas I find incredibly interesting: finding insights from data, online social behavior, and social networks. In fact, my PhD dissertation studies user behaviors on social media using experimental and NLP techniques. If it makes sense to talk, I would love to chat further about how my skills and interests might fit into your team. Attached please find my resume, which provides a more in-depth view of my qualification and experience. \\n\\nBest regards,\\n\\nCairo Liu\\n\\nLinkedin: https://www.linkedin.com/in/cairoliu/\\n\\nGithub: https://github.com/lkl2050\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='69c7dff2-0322-457b-896d-cdcaf39ddd57', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nmy cold email data consulting\\n\\nHello Bertrand,\\n\\nThank you for the invitation. I just submitted my application via Linkedin.\\n\\nI have experience building sales leads classifier, recommender systems, and time-series prediction tools. My previous work focuses heavily on data modelling and creating analytics tools. Most recently, I built a LightGBM model to predict investors’ likelihood to invest in crowdfunding projects, and deployed the model as an analytics feature to help client companies convert promising investor leads (see details at https://www.dealmaker.tech/compass). \\n\\nIf it makes sense to talk, I would love to chat further about how my skills and interests might fit into your team. Attached please find my resume, which provides a more in-depth view of my qualification and experience. \\n\\nBest regards,\\n\\nCairo Liu\\n\\nGithub: https://github.com/lkl2050\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fba68933-2615-4957-944b-a2d94b8eb29f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nmy cold email data consulting2\\n\\nHello Sreelatha,\\n\\n  \\n\\nI came across Softchoice on Linkedin and saw that you’re looking to hire a senior data scientist in Canada. I’m a PhD and data scientist with experience delivering AI products from start to finish.\\n\\n  \\n\\nI have experience making recommender systems, time-series predictors and sales management tools. My previous projects focused heavily on NLP, finance, and retail data with Fortune 500 clients.\\n\\n  \\n\\nIf it makes sense to talk, I would love to chat further about how my skills and interests might fit into your team. Attached please find my resume, which provides a more in-depth view of my qualification and experience.\\n\\n  \\n\\nBest regards,\\n\\nCairo Liu\\n\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3aa9631b-a2e5-4692-bea2-93ccbf1c03f5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nuseful sentences\\n\\nI’m particularly interested in Muse because it intersects with so many areas I find incredibly interesting: finding insights from data, Buddhism/mindful meditation, and neuroscience.\\n\\nI follow the fintech space closely and love what you're building towards with Debtsy. I've spent time working with the complaints data provided by the Consumer Financial Protection Bureau and am convinced it's possible to significantly improve the consumer loan experience for both lenders and borrowers.\\n\\nI've attached my resume, which provides a more in-depth view of my qualification and experience.\\n\\nGiven my background and skillset, I thought I would be a good fit for the various data problems Debtsy faces, inlcuding estimating a borrower's capacity and willingness to resolve outstanding balances. If it makes sense to talk, I would love to chat further about how my skills and interests might fit into your team\\n\\nI follow the fintech space closely and love what you're building towards with PayJoy. I'm a strong believer in the power extending credit can have for people in developing regions and am fascinated by the challenge of building novel credit models to assess the creditworthiness of underserved groups.\\n\\nI have experience building fraud detection systems, image classification + localization, and conducting independent research.\\n\\nI’m experienced in developing both traditional ML models like recommender systems and advanced ones like transformer-based nlp tools. \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2263ef6a-5738-4c2c-b90f-60c2f7d5ba75', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nfollow up\\n\\n> Hi {NAME},\\n> \\n\\n> I wanted to follow up with you in case my previous email got buried. Did you have any thoughts or feedback on whether my skills and interests might fit into your team?\\n> \\n\\n> If you\\'re too busy to respond, no problem. I want you to know in any case that what {YOUR COMPANY / GROUP} are working on has been a real source of inspiration for me. If I can be of any help whatsoever going forward, please don\\'t hesitate to reach out!\\n> \\n\\n> Bob Mentee\\n> \\n\\n> SUBJECT:\\n> \\n\\n> Data Science role at XYZ Robotics\\n> \\n\\n> MESSAGE:\\n> \\n\\n> Hi Ron,\\n> \\n\\n> I\\'ve been following XYZ Robotics for a while and I\\'ve found your Medium articles very interesting. They provide a clear understanding of how XYZ Robotics is tackling the problems that the autonomous vehicle industry is currently facing. Particularly your explanation for not including LIDAR in your systems is a good reminder of why you select the tool based on the problem you\\'re solving, rather than the other way around.\\n> \\n\\n> I\\'m fascinated by your idea of \"one step at a time\" problem solving, and I would love to talk about what I can offer to your mission.\\n> \\n\\n> For some context on myself, I\\'m a machine learning researcher with experience in signal and image processing. I\\'ve been working on the problem of brainwave classification using convolutional neural networks where I achieved a 124% performance improvement over traditional methods. This research has allowed me to keep myself updated on the CNN and computer vision literature. I\\'m also part of a data science mentorship program in which I\\'ve been working on achieving state-of-the-art performance for music genre classification with a Machine Learning Scientist at Amazon.\\n> \\n\\n> I\\'ve attached my resume, which provides a more in-depth view of my qualification and experience.\\n> \\n\\n> I\\'m excited to take the skills and knowledge I have and apply them to help make safe autonomous trucks a reality. I\\'d love to schedule a phone call to chat a bit about what I can offer you and to see if I will be a good fit on your team.\\n> \\n\\n> Best,\\n> \\n\\n> Bob Mentee\\n> \\n\\n> bobmentee.com\\n> \\n\\n> LinkedIn\\n> \\n\\n**Let\\'s break down this email:**\\n\\n- The mentee highlights the experience he has that\\'s 1)\\xa0**most relevant**\\xa0(computer vision, because the company is building autonomous trucks) and 2)\\xa0**most impressive**\\xa0(beating state of the art in brainwave and music classification).\\n- The email opens with a personal compliment to the recipient. We all love getting personal compliments.\\n- The email also quotes one of the recipient\\'s own insights, from Medium. This shows the mentee took the time to read something the recipient wrote.\\n- The company\\'s name was spelled correctly in the original email!! (I\\'ve anonymized it in this one.) Nothing destroys your chances faster than spelling a company\\'s name wrong. It seems insane to make this mistake, but it happens\\xa0**frighteningly**\\xa0often. 🤦\\u200d♂️\\n- Most sentences are short and crisp. The mentee used\\xa0Sapling\\xa0to proofread his email before he sent it.\\n- The mentee\\'s personal website is in his email signature. That means he doesn\\'t need to attach his resume to the email he sends, although he did that too. If the recipient is curious, he can just go to the website and find links to everything he needs.\\n- The mentee used\\xa0**Streak**\\xa0to confirm that the recipient opened the email.\\n\\nI saw [on LinkedIn / on the company’s job page / wherever] that [COMPANY] is looking to hire [ROLE]. I am a [data scientist / data analyst / data engineer / whatever you are] [plus one brief identifying detail]. I am excited about the work you’re doing at [COMPANY] and I would love to talk to you about how my skill set might be a fit for the [SPECIFIC ROLE FROM THEIR JOB WEBSITE] role.\\n\\nI just completed my [DEGREE] in [SUBJECT] at [SCHOOL], where I [name the most impressive thing you did in school or an award you won].\\n\\nI have experience [building/doing/using] [list of 3-ish things you’ve done or skills you have that are relevant to this job]. Most recently, I [built a specific project] that [name 2-3 impressive details about what the result of the project did or how the project worked] (which you can [check out on my Github (LINK IT) / read more about in this Medium post (LINK IT)]).\\n\\nI’m particularly excited about [COMPANY] because [compelling reason! are you passionate about the mission, curious about the data, interested in a specific challenge you think they might have, etc], and based on my skillset and background I thought I would be a good fit for [ROLE].\\n\\nIf it makes sense to talk, I would love to chat further about how my skills and interests might fit into your team.\\n\\nYour signature should include four things:\\n\\n1. Your full name\\n2. A link to your LinkedIn profile (make the text say “LinkedIn” and put the link underneath that text—don’t just put the raw link)\\n3. A link to your portfolio—either Github or a personal website\\n4. Contact info—email address & phone number\\n\\n> Hi Chris,\\n> \\n\\n> I saw a LinkedIn post that Muse is looking for a BioSignal Research Engineer. I’m a researcher with experience in machine learning, mindful meditation, and EEG. I\\'m very excited about the work you\\'re doing with neurofeedback and I would love to talk to you about how my skill set might be a fit for this role.\\n> \\n\\n> For some context, I’m about to complete my MSc in Medical Biophysics at the University of Toronto, where I developed methods for automated brain tumor detection and prediction of patient response to radiotherapy. I have experience in building machine learning models in Python (you can check out my GitHub: github.com/xxxxxx/OCT-Image-Classification), working with time-series EEG data from my undergraduate work in physics, and leading mindful meditation sessions from volunteer work during my BSc. I’m particularly interested in Muse because it intersects with so many areas I find incredibly interesting: finding insights from data, Buddhism/mindful meditation, and neuroscience. If it makes sense to talk, I would love to chat further about how my skills and interests might fit into your team. Would you be free anytime in the next week or two for a quick call on Hangouts or Skype?\\n> \\n\\n> I\\'ve got time between X-X pm next X, X and X. Please let me know if any of those time slots would work. (If none of them do, please feel free to book me on [Calendly] - might save us a bit of back and forth in scheduling.)\\n> \\n\\n> Best Regards,\\n> \\n\\n> Andrei Mouraviev\\n> \\n\\n> LinkedIn: linkedin.com/in/xxxxx/\\n> \\n\\n> GitHub: github.com/xxxxxx\\n> \\n\\n> Website: xxxxx.github.io/\\n> \\n\\n> Contact: xxxxx@example.com – xxx xxx xxxx\\n> \\n\\n> Hi Ryan,\\n> \\n\\n> I follow the fintech space closely and love what you\\'re building towards with Borrowell. I\\'m acutely aware of the various issues that have surface among the legacy players in space recently (e.g., Equifax, etc.) and I\\'m convinced it\\'s possible to significantly improve the credit monitoring and scoring experience for consumers.\\n> \\n\\n> I am an experienced data scientist with sound knowledge of data analytics and agile framework. I have provided effective solutions in line with business needs by successfully building models using advanced machine learning algorithms. I also worked as a developer (C#.NET) which compliments my work as a data scientist to understand the product implementation of results which comes out from complex data analysis.\\n> \\n\\n> I am currently working as a data scientist for a retail startup in Canada, building a regressor model using Python and scikit-learn for sales prediction and optimizing the inventory allocation to different stores by calculating the sales probability of every bought item. I\\'m also building dashboards using Tableau for the operations and marketing teams to reduce the time that\\'s needed for data preparation. Prior to this role, I was working in the manufacturing industry building ML models to predict product flaws based on data collected at sample points throughout our assembly lines. I also developed a model (R, C#) for a pharmaceutical company to predict the probability of patients adhering to the prescribed medications, during the industry project of data science professional course. My GitHub will give an overview of the work I have done -\\xa0https://github.com/xxxxxxx\\n> \\n\\n> I\\'ve attached my resume, which provides a more in-depth view of my qualification and experience.\\n> \\n\\n> I\\'d love to schedule a phone call to chat a bit about what I can offer you and to see if I would be a good fit for your team.\\n> \\n\\n> Best,\\n> \\n\\n> Shivam Kanoria\\n> \\n\\n> Data Scientist\\n> \\n\\n> https://www.linkedin.com/in/xxxxxxx/\\n> \\n\\n> Hi Alison,\\n> \\n\\n> I came across RepairSmith on LinkedIn and saw that you’re looking to hire a data scientist. I’m a data scientist with experience building data pipelines from start to finish. I’m very impressed with the work you’re doing at RepairSmith, and would love to talk to you about how my skill set would fit for the data scientist role.\\n> \\n\\n> I have 2 years of industry experience in data science, after completing my Master’s degree in Statistics. I have experience building fraud detection systems, image classification + localization, and conducting independent research. Most recently, I’ve built a deep neural network using transfer learning to detect out of stock items on supermarket shelves, without bounding boxes to train on. You can check it out here:\\xa0https://example.com/blog/shelf-detection/. I’m particularly excited about RepairSmith because of its journey into machine learning as a startup, and based on my skillset and background I thought I’d be a good fit for the data scientist role.\\n> \\n\\n> If it makes sense to talk, I would love to chat further about how my skills and interests might fit into your team.\\n> \\n\\n> Jamel M. Thomas,\\n> \\n\\n> LinkedIn:\\xa0https://www.linkedin.com/in/xxxxx/\\n> \\n\\n> Personal Blog:\\xa0https://example.com/\\n> \\n\\n> Contact Info: contact@example.com - (xxx) xxx - xxxx\\n> \\n\\n> Hi Kurt,\\n> \\n\\n> I follow the fintech space closely and love what you\\'re building towards with Debtsy. I\\'ve spent time working with the complaints data provided by the Consumer Financial Protection Bureau and am convinced it\\'s possible to significantly improve the consumer loan experience for both lenders and borrowers.\\n> \\n\\n> A bit about myself: After studying quantitative economics at UC Berkeley, along with coursework in computer science, I spent time working at an investment firm evaluating early-stage technology companies. Most recently, I was a researcher at a start-up asset manager where I twice presented research to the U.S. Securities and Exchange Commission (SEC) at its headquarters in Washington, D.C. I have extensive experience conducting data analysis (Python, Pandas, SQL) and building predictive models (Python, scikit-learn) and enjoy tacking business problems involving tabular and time series data. Most recently, I build a series of models to predict changes in the Zillow Home Value Index by ZIP Code which you can read more about in this write-up [actual email has link].\\n> \\n\\n> Given my background and skillset, I thought I would be a good fit for the various data problems Debtsy faces, inlcuding estimating a borrower\\'s capacity and willingness to resolve outstanding balances. If it makes sense to talk, I would love to chat further about how my skills and interests might fit into your team.\\n> \\n\\n> Best,\\n> \\n\\n> Phil\\n> \\n\\n> --\\n> \\n\\n> Phil Glazer\\n> \\n\\n> example.com | LinkedIn\\n> \\n\\n> phil@example.com\\n> \\n\\n> xxx-xxx-xxxx\\n> \\n\\n> Hi Cynthia,\\n> \\n\\n> I follow the fintech space closely and love what you\\'re building towards with PayJoy. I\\'m a strong believer in the power extending credit can have for people in developing regions and am fascinated by the challenge of building novel credit models to assess the creditworthiness of underserved groups.\\n> \\n\\n> A bit about myself: After studying quantitative economics at UC Berkeley, along with coursework in computer science, I spent time working at an investment firm evaluating early-stage technology companies. Most recently, I was a researcher at Bitwise, a start-up asset manager, where I twice presented research to the U.S. Securities and Exchange Commission (SEC) at its headquarters in Washington, D.C. I have extensive experience conducting data analysis (Python, Pandas, SQL) and building predictive models (Python, scikit-learn) and enjoy tacking business problems involving tabular and time series data. For example, I recently built a series of models to predict changes in the Zillow Home Value Index by ZIP Code, which you can read more about [[in this write-up] hyperlinked project write-up on Medium here].\\n> \\n\\n> Given my background and skillset, I thought I would be a good fit for the various data problems PayJoy faces, including developing credit and fraud prevention models. If it makes sense to talk, I would love to chat further about how my skills and interests might fit into your team.\\n> \\n\\n> Best,\\n> \\n\\n> Phil\\n> \\n\\n> --\\n> \\n\\n> Phil Glazer\\n> \\n\\n> example.com | LinkedIn\\n> \\n\\n> Dear Kate,\\n> \\n\\n> Thanks for adding me on LinkedIn. I saw in Key Values\\'s newsletter that Asana is looking to hire a Full Stack engineer. I am a data driven Full Stack engineer currently working on a fun end-to-end engineering product that involves OpenCV, People\\'s Bookshelves, and the Dewey Decimal System. I am excited about the work you’re doing at Asana and I would love to talk to you about how my skill set might be a fit for the Full Stack Engineering role.\\n> \\n\\n> I am transitioning into engineering from education, and my most recent role was as a high school and middle school math teacher at Hill Learning Center which is a school for kids with dyslexia, ADHD, and processing disabilities. Part of my job was to teach Executive Functioning skills -- in layman\\'s terms that meant I helped highly distractible kids learn to manage their workflows and build skills that helped them achieve success -- much like Asana\\'s mission. I used technology and other means to help my students learn and part of my teaching philosophy included structuring the environment so that the brain could get into deep thought and that state of psychological \\'flow\\'. On the technical side, I\\'m currently integrating APIs, computer vision, and recommender systems into my bookshelf project that I hope to launch soon. If you would like to see this in action, it\\'s on my GitHub\\xa0https://github.com/xxxxx. For me, I would love to know how Asana leverages computer vision in its work flow processes.\\n> \\n\\n> This is all to say, I think Asana may be a good fit for me as I\\'d like to stay in a space that encourages empowerment and success as part of their product and I think I have some unique insights and skills to offer.\\n> \\n\\n> If it makes sense to talk, I would love to chat further about how my skills and interests might fit into your team.\\n> \\n\\n> Thank you for reading and enjoy your week.\\n> \\n\\n> Best,\\n> \\n\\n> Sarah\\n>', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4cb60ebd-8169-46cf-a1a5-3f08bede77cf', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\ntake home exam email\\n\\n*Hi Recruiter’s Name,*\\n\\n*Thanks for sending over the take-home assignment. I’m excited to start it and will be sure to send it back in X days with my completed solution.*\\n\\n*Additionally, I was wondering if I could be provided with a set of general guidelines on how the assignment will be graded. I definitely want to be sure I’m focusing and demonstrating the correct skillset for the take-home and not accidentally going down a rabbit hole.*\\n\\n*Lastly, I would really appreciate it if after I send in my take-home assignment that I could get some feedback on it, regardless of whether or not I move on in the interview process. It would really mean a lot to understand what I did wrong or where I excelled for my own technical growth.*\\n\\n*Thanks!*', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9763e727-a463-4b8c-9a5d-be62cfe10a18', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHi Raymond, I am a friend of Prof. Guojun Liu. He mentioned your interesting work at Tiktok to me recently. I am currently interviewing with companies to enter the tech industry. Would it be possible to connect and learn more about your company? Thank you Cairo\\n\\nHi Michael,\\n\\nThank you for connecting. I saw that ClearBlue Markets is recruiting a Sr Data Scientist and I am very interested in this position.\\n\\nI’m experienced in developing both traditional ML products like recommender system and advanced models like transformer-based NLP tools. If it makes sense to talk, I would love to chat further about how my skills and interests might fit into your company.\\n\\nThank you\\nCairo\\n\\nHi Michael,\\n\\nThank you for connecting. I saw that ClearBlue Markets is recruiting a Sr Data Scientist and I am very interested in this position.\\n\\nI’m experienced in developing both traditional machine learning models like price predictors and advanced models involving NLP techniques. I am particularly interested in ClearBlue Markets because I have been closely following the development of carbon markets after taking a carbon economics course in college. \\n\\nIf it makes sense to talk, I would love to chat further about how my skills and interests might fit into your company.\\n\\nThank you very much\\nCairo\\n\\nHi Adnan,\\n\\nI am an experienced data scientist and PhD from the University of Toronto. I saw that OMERS is recruiting a data scientist and I am very interested in this position.\\n\\nI’m experienced in developing both traditional machine learning models like recommender systems and advanced models involving transformer nlp techniques. My work experiences also focus heavily on analyzing financial data and text. Most recently, I trained a XGBoost + BERT model to predict the success rate of startup capital raises on data extracted from SEC documents, and deployed the model as an internal tool for the sales team.\\n\\nI’m particularly excited about OMERS because I am a good individual investor (10% + average return on the stock market for 10+ years) and love to help people make better financial decisions. If it makes sense to talk, I would love to chat further about how my skills and interests might fit into your team.\\n\\nThank you very much\\nCairo\\n\\nHi Simran,\\n\\nI am an experienced data scientist and PhD from the University of Toronto. I saw that Tide is recruiting a data scientist in Canada and I am very interested in this position.\\n\\nAre you open to a quick chat to discuss the position? I’d love to learn more about it, and share more about my own qualifications. I look forward to hearing from you.\\n\\nThank you\\n\\nCairo\\n\\nHi Qiong,\\n\\nI am an experienced data scientist and PhD from the University of Toronto. I saw that Big Fish Games is recruiting a Data Scientist in Canada and I am very interested in this position. Is it possible to connect to learn more about your work?\\n\\nThank you\\nCairo\\n\\nHi Cam, \\n\\nThank you for connecting. I saw that Definity is looking to hire a data scientist and I am really interested in the position. Would it be possible to have a quick call (< 15 min) with you this weekend to learn about your work? \\n\\nThank you very much\\n\\nCairo \\n\\nHi Xiliang, \\n\\nThank you for adding me on Linkedin. I saw that Dialpad is recruiting a data scientist. Since it seems you’d work directly with this person, it would be wonderful to hear your thoughts on the role. I’m looking to get some clarity on the role and responsibilities before I apply. Do you have time for a quick call about it this week or the next?\\n\\nThank you very much\\n\\nCairo \\n\\nI am a NLP engineer and PhD from the University of Toronto and I am really interested in Ada.\\n\\nHi Mark, \\n\\nI am a data scientist and marketing PhD from the University of Toronto. I saw that Granify is recruiting a data scientist and I am really interested in the position. Since it seems you’d work directly with this person, it would be wonderful to hear your thoughts on the role. I’m looking to get some clarity on the role and responsibilities before I apply. Do you have time for a quick call about it this week or the next?\\n\\nThank you very much\\n\\nCairo \\n\\nHi Tom,\\n\\nHope all is well with you. (Insert something personal, such as a reference to how you know him)\\n\\nI already submitted my resume and application to the company and was curious about whether you are in contact with your company’s hiring manager. If so, it would be a tremendous help if you were able to introduce me to her. I’m confident that your referral would go a long way toward helping me land the interview I need to get the job.\\n\\nAfter reviewing your recent accomplishments for your company, I am excited at the prospect of contributing to the firm’s success myself. My resume has been included with this letter. I would appreciate it if you could review it briefly. I am sure that you will find that I would be a valuable member of your company’s team. If so, then I would also appreciate any help you can offer by way of introducing me to your company’s hiring personnel.\\n\\nHi Hamoon,\\n\\nThank you for adding me. I am really interested in the scientist position at FutureFit AI and excited at the prospect of contributing to the firm’s success myself. \\n\\nMy resume has been included below. I would appreciate it if you could review it briefly. I am sure that you will find that I would be a valuable member of your company’s team. If so, then I would also appreciate any help you can offer by way of introducing me to your company’s hiring manager.\\n\\nThank you very much\\n\\nCairo\\n\\nand was curious about whether you can  \\n\\nIf so, it would be a tremendous help if you were able to introduce me to her.\\n\\nWhen writing to a prospective mentor, make sure you’ve done your homework. Here’s an example of a message you could send:\\n\\n*Divya, your posts on edtech in the STEM education forum have been really thought-provoking! I’ve interned for a few startups in this space and am excited about my own next steps — but I definitely could use some guidance from an experienced pro like you. Would you be open to chatting about this?*\\n\\nHi Palermo,\\n\\nI am excited to share the good news that I have decided to join a fintech startup (http://dealmaker.tech) as a machine learning engineer. I really appreciate your guidance and help with my career transition from academia to the industry. \\n\\n*I look forward to connecting with you again soon. Maybe we can grab a coffee some time when the pandemic slows down. For now, wishing you and your family wonder, joy, and prosperity.*\\n\\nCheers~\\n\\nCairo\\n\\nHi Mustafa,\\n\\nI am excited to share the good news that I have decided to join a fintech startup (http://dealmaker.tech) as a machine learning engineer. I really appreciate your guidance and help with my job hunting.\\n\\nI look forward to connecting with you again soon. Maybe we can grab a coffee some time when the pandemic slows down. For now, wishing you and your family wonder, joy, and prosperity.\\n\\nCheers~\\n\\nCairo\\n\\nHi Suhas, \\n\\nI am excited to share the good news that I have decided to join a fintech startup (http://dealmaker.tech) as a machine learning engineer. I really appreciate the lessons I learned from the interviews with you.  \\n\\nI look forward to connecting with you again soon. For now, wishing you and your family wonder, joy, and prosperity. \\n\\nCheers~ \\n\\nCairo\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e9e3318f-f961-4ae5-92a2-8ada47c0d448', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Do:**\\xa0Keep the focus on relationship building.\\n\\nExample:\\n\\n> Subject: Barbara, Let’s Make Magic in the New Year!\\n> \\n> \\n> *Hi, Barbara:*\\n> \\n> *The holidays are a time of wonder for young and old, but the magic doesn’t have to end when we flip the calendar to January.*\\n> \\n> *I hope you are as excited about 2015 as I am, and I hope you felt the same optimism after our conversation regarding production process innovation. I am extremely excited for what we can achieve together in the New Year.*\\n> \\n> *I look forward to connecting with you again soon. For now, wishing you and your family wonder, joy, and prosperity.*\\n> \\n> *Cheers!*\\n> \\n> *Michael*\\n> \\n> ## Get back in touch with a connection that’s grown cold\\n> \\n> ### **Don’t:**\\xa0Ask for outlandish favors before building the relationship.\\n> \\n> Example:\\n> \\n> > Subject: I Know You Can Help Me!\\n> > \\n> > \\n> > *Hi, Ted:*\\n> > \\n> > *It’s been a long time! You remember me from last year’s MCV conference in Tallahassee, right? We talked about how the cocktail wieners at the hotel bar tasted like shrimp. That sure was weird.*\\n> > \\n> > *Anyway, the reason I’m emailing today is because I could really use a small favor. More like a huge favor, I guess. I know you’re friends with Jacqueline Allen, head of that great startup that’s blown up. I’d love to pursue OD consulting opportunities there - could you put me in touch?*\\n> > \\n> > *I kind of worried at first this was a big ask, but I know that you’ll come through for me in the spirit of the holiday.*\\n> > \\n> > *Thanks in advance!*\\n> > \\n> > *Mel*\\n> >', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='424a2818-192f-4279-b23d-162fc78a7525', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nGPA\\n\\n!Toronto\\n\\nToronto\\n\\n!哥大的\\n\\n哥大的\\n\\n!Untitled\\n\\n!北大的\\n\\n北大的', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='999ec2f6-4a77-4d9f-81db-501066809d54', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nHi Alexei,\\n\\nThank you so much for chatting with me today. It was a great pleasure to learn more about the unique role of the digital accelerator unit, and I’m very excited about the opportunity to join the team to solve data problems with scientists in different areas.\\n\\nI look forward to hearing from you about next steps. Please don’t hesitate to contact me if I can provide any additional information.\\n\\nHi XXX, \\n\\nThank you for updating me on my candidacy. I truly appreciate the opportunity and It's been a pleasure getting to know the growth operations\\nteam and learning more about Gorgias.\\nWhile this role did not work out, I hope it is okay we can stay connected. I would like to check in with you in the next two months or so and see there's another opportunity that might better match my skill set and qualifications.\\nAgain, thanks for everything and I wish you and the team all the best during this busy recruitment time.\\n\\nHi Kris,\\n\\nThank you for adding me on LinkedIn. I just finished my last round of interviews with Suhas today. I love the open culture of Bedrock and  I’m excited about the opportunity to leverage nlp techniques to improve financial transparency. \\n\\nI look forward to hearing from your team about next steps. Please don’t hesitate to contact me if I can provide any additional information.\\n\\nBest,\\n\\nCairo\\n\\nHi Suhas,\\n\\nThank you so much for the time you spent with me today. I really enjoyed meeting you and learned many new stuff about nlp (e.g., the Zipf's law).  After our conversations, I am more confident that I would enjoy the job and make a valuable contribution to the team. \\n\\nI look forward to hearing from your team about next steps. Please don’t hesitate to contact me if I can provide any additional information.\\n\\nHi Gerónimo,\\n\\nThank you so much for the time you spent with me today. I really enjoyed meeting your team and learned a lot about Dealmaker.  After our conversations, I am more confident that I would enjoy the job and make a valuable contribution to the team. \\n\\nI look forward to hearing from you about next steps. Please don’t hesitate to contact me if I can provide any additional information.\\n\\nCairo\\n\\nsowmya.murali@samsara.com\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4338efd2-1d07-4e0e-b70e-b1210c96c8bd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nlinkedin strategy\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5ca365b8-192c-40e1-ba3c-a7f0e5faf794', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nApplication strategy: LinkedIn outreach\\n\\nThis playbook for LinkedIn outreach was first perfected by\\xa0Nicholas Broad, one of our mentees, who\\'s found it\\'s an effective way to connect with local hiring managers during the pandemic. We think it will work pretty well in normal times too.\\n\\n**Here\\'s Nicholas\\'s advice for reaching out on LinkedIn (lightly edited, with my comments throughout):**\\n\\n*My approach was to get the recipient to accept a connection request so that I can send a longer message later. I\\'d start by contacting people in manager/director positions, but sometimes I found success by going directly to data scientists. If the regular data scientist likes you, they\\'ll put you in touch with their boss. With LinkedIn, all they have to do is click one button to accept whereas by email they have to write a whole response. Moreover, they can easily look at your background with one click to your profile. You\\'ll also increase your chance of connecting if you have mutual connections in the data science community.*\\n\\n**Comment:**\\xa0Before connecting, Nicholas made sure that the person he was reaching out to worked at a company that was hiring for a role he was interested in. He did that by first\\xa0looking for job leads, and then\\xa0identifying key people who might be hiring managers\\xa0at the companies he found.\\n\\n**Nicholas continues:**\\n\\n*My connection message was:*\\n\\n> Hi $NAME,\\n> \\n\\n> I\\'m interested in working at $COMPANY as a (data scientist | data analyst). Would you be able to give me a few minutes of your time to chat about your company? I know your time is valuable, so I would appreciate any amount of time you could give me.\\n> \\n\\n> Best,\\n> \\n\\n> Nicholas\\n> \\n\\n*After they connected, I\\'d usually just ask for 5-10 minutes of their time on a phone call. I figured that a single 5-10 minute phone call is probably easier than multiple typed responses over the course of days. They get a better sense of who you are, and hopefully you can do a good job selling yourself.*\\n\\n**Comment:**\\xa0I suspect the reason asking for a phone call up front worked is that it filtered out hiring managers who weren\\'t interested. Anyone who agreed to a phone call after Nicholas said he was interested in working at their company, was probably quite interested in hiring someone. So by asking for a call right away, Nicholas ensured he wouldn\\'t be wasting his time with people who weren\\'t open to hiring him.\\n\\n**Nicholas continues:**\\n\\n*To find people, I usually looked for job listings to see which companies are in my area. I then\\xa0did a \"People\" search on LinkedIn, limiting it to people currently at that company, at that specific location, and with a search query of \"Data Science\". I usually got 3-10 results that mattered, so I targeted the managers/directors first and then the ones further down the seniority chain. If they have LinkedIn Premium, the odds of a response increase.*\\n\\n**Comment:**\\xa0You can tell if someone has LinkedIn Premium by the gold \"in\" logo on their profile. It looks like this:\\n\\n!https://yazabi-grads.s3.amazonaws.com/sm-web-app/static-page-images/linkedin-premium-profile.jpg\\n\\n(The red arrow is pointing to the\\xa0**gold LinkedIn logo**\\xa0that appears if the profile is Premium.)\\n\\n**⭐️Update:**\\xa0We\\'ve been getting data from mentees that folks at the individual contributor level (people who are Data Scientists, DS Leads, ML Engineers, Data Analysts, etc.) seem to be much more receptive to LinkedIn outreach than people at the Manager / Director levels. We\\'re not sure why yet, but we suggest focusing your outreach more on the individual contributors, and less on the managers. If a data scientist likes you, they\\'ll recommend you to their manager anyway. ⭐️\\n\\n**Nicholas continues:**\\n\\n*Once on the phone, I framed it as:*\\n\\n> I\\'m interested in working at your company because $REASON. I see there is an opening for this position and I was hoping we could chat to see if I am the type of candidate you are looking for and if this is the type of position I am looking for.\\n> \\n\\n*When the phone call happened, I had a 30 second and 1.5 minute \"about me\" depending on the scenario. I also had my reasons why I wanted to work at that company and what I am looking for. And I had a cheatsheet about the caller\\'s background and current position, and some generic questions about the type of work being done, the team structure, and what they are looking for.*\\n\\n*One advantage of this is that the more people you connect with, the more it looks like you are engaged with that community. At the start I didn\\'t have any mutual 1st degree connections with the people I was targeting. Now I usually have around 3-4, all of which are people in the data science community. It doesn\\'t sound like much, but I think it is definitely better than 0.*\\n\\n**Comment:**\\xa0Nicholas was specifically reaching out to data scientists in his local area (Durham, North Carolina). People in the same area are more likely to be connected to each other. So if you use this LinkedIn strategy, you\\'ll probably be more successful if you\\xa0*concentrate your search in one city*. The more new connections you make\\xa0*in that city*, the more likely you are to have a connection in common with the\\xa0*next*\\xa0person you reach out to, and the more likely they are to respond.\\n\\nIn terms of the call itself: towards the end of the call, it\\'s perfectly okay to ask them whether they think it would be a good idea for you to apply to their company. If they think it is, they\\'ll tell you so enthusiastically. If not, you can use the opportunity to get feedback on why they think the fit might not be there.\\n\\n**Nicholas continues:**\\n\\n*I think having 500+ connections is also useful because it looks like you are actively networking and staying in-the-loop. I didn\\'t, though. It might be a good idea for mentees to add each other on LinkedIn so that their numbers look better.*\\n\\n*I think it\\'s better to wait until you talk to them before applying because sometimes they can refer you to the position. There are many companies that cannot retroactively refer a candidate once they have applied. As long as they know who you are, they will probably look for your application, though. I thought of the phone call as an opportunity for me to learn more about the role, so I wouldn\\'t waste my time applying for something I didn\\'t want.*\\n\\n*Just getting a phone call is a huge advantage. At that point, applying or not applying is not as important, in my opinion.*\\n\\n*To escalate to the next stage during the call, I’d try and get as much detail about the role as possible. And then as I\\'d hear more, I\\'d mention how this is exactly the type of role I\\'m looking for and I will be applying. You might be able to flat-out ask for a referral, or if not, at least let them know you hope to hear from them soon. You could also use the opportunity to talk about yourself to see if you are the type of candidate they are looking for.*\\n\\n***In terms of my stats:**\\xa0I sent 35 LinkedIn messages, got 12 responses, 4 interviews and 1 offer. A few of those responses said the position is open, but there is a hiring freeze but they\\'d like to stay in touch.*\\n\\n*I checked my numbers, and email actually had a slightly higher response rate than LinkedIn, but LinkedIn felt easier so I sent 2X the number of messages.*\\n\\n**Comment:**\\xa0Our own experiments consistently show that email has a higher response rate than LinkedIn\\xa0**per message**, but what matters more is your response rate\\xa0**per unit of effort**. Nicholas\\'s strategy is interesting because even though his LinkedIn response rate was a bit lower than email, sending messages was easy enough that he was able to send many more messages than customized cold emails.\\n\\n**One more important thing to note here:**\\xa0If the person you\\'re talking to tells you \"Oh we have a /jobs page, you can apply there\", that\\'s not necessarily best answer. If they offer that, feel free to ask them, \"Can I send you my resume and a one-paragraph email to forward to the right person?\" (Sometimes people forget they can do that!)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='25cb5358-f5d6-4aa1-b026-ee6e49543486', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nweb 3\\n\\nHowever, if they had built a platform to buy and sell images that wasn’t nominally based on crypto, I don’t think it would have taken off. Not because it isn’t distributed, because as we’ve seen so much of what’s required to make it work is already not distributed. I don’t think it would have taken off\\xa0*because this is a gold rush*. People have made money through cryptocurrency speculation, those people are interested in spending that cryptocurrency in ways that support their investment while offering additional returns, and so that defines the setting for the market of transfer of wealth.\\n\\nThe people at the end of the line who are flipping NFTs do not fundamentally care about distributed trust models or payment mechanics, but they care about where the money is. So the money draws people into OpenSea, they improve the experience by building a platform that iterates on the underlying web3 protocols in web2 space, they eventually offer the ability to “mint” NFTs through OpenSea itself instead of through your own smart contract, and eventually this all opens the door for Coinbase to offer access to the validated NFT market with their own platform via your debit card. That opens the door to Coinbase managing the tokens themselves through dark pools that Coinbase holds, which helpfully eliminates the transaction fees and makes it possible to avoid having to interact with smart contracts at all. Eventually, all the web3 parts\\xa0*are*\\xa0gone, and you have a website for buying and selling JPEGS with your debit card. The project can’t start as a web2 platform because of the market dynamics, but the same market dynamics and the fundamental forces of centralization will likely drive it to end up there.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='92791b9d-cdf9-4999-85df-7c3fb6efddac', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n投资相关\\n\\n方三文+李蓓：我的认知是相对稳定的波动小，市场的共识波动是比较大的；当我的认知和市场差别很大的时候，就是交易的机会；我是对的而市场是错的时候，我才能赚大钱；我和市场都对的时候，只能赚到市场平均收益\\n\\n犯错是好事，如果我的判断全都是对的，就没可学的人生就没意思了\\n\\n风控：持仓分散；止损；足够的反省\\n\\n除了盈利水平和资金成本（利率）之外，另一个重要因素是投资者的风险偏好，这决定了有多少资金会投入市场\\n\\n市场情绪我该如何判断？有客观的指标的比如其他人的仓位，趋同性等\\n\\n书末有大奖章1988-2018每年的收益率，费前平均66%，夏普2.08；费后年均39%，夏普1.92。我没有减无风险利率。但由于他收益率比较高，减2.5%的无风险利率影响不大。其实夏普比总具体数字看不是很高，但它胜在持续时间长，资金规模大，收益率很高。\\n\\nbabyquant总结\\n\\n西蒙斯很牛，但也不是一做交易就牛逼的，前10年很苦逼；\\n后来牛逼了但策略也不是他做的，招了很多其他人来做；\\n他的企业文化确实比其他公司好一些，但也不是一片祥和，内部政治斗争也很激烈；\\n公司人才济济，水平最差的小码农也是宾夕法尼亚本科、斯坦福计算机博士自然语言方向，进文艺复兴的时候已经工作多年；其他很多人进文艺复兴的时候已经当教授多年；\\n他的模型虽然牛逼，但市场剧烈动荡（2000、2008）的时候也忍不住去干预的；\\n当然了，更重要的，可能他的对手没他那么能折腾。索普应该比他聪明，但1988年就金盆洗手了；\\n当然，我觉得西蒙斯胆子也比较大，因很多自营都是8倍的夏普，做高频的；很多人2倍夏普的策略一般是做资管，不会自营。所以也说明西蒙斯不是做高频。\\n\\n有本经典的书，叫 Campbell, J. Y., A. W. Lo, and C. MacKinlay (1996).*The econometrics of financial markets.*\\xa0Princeton University Press.\\n\\n书上开篇有句经典的话：\\n\\n> 不要把时间序列中的长期漂移率项当成可预测性。\\n> \\n\\n个人感受，不是 time series analysis 不重要，**是收益率排除了长期漂移率之后剩下的能被挖掘出来的东西太少了。**\\n\\n!Untitled\\n\\n上面是一个标准的随机游走，使用常用的随机游走检验（比如 sequences and reversals test 或 runs test）能都检验。但是，哪怕是一个随机性很高的时间序列在其**局部**也会因为**随机趋势**给我们造成一种**错觉**\\xa0—— 它的随机性很弱、是可以预测的、是可以用 time series analysis 挖掘其自相关性的。\\n\\n**根据这个错觉来构建策略是非常危险的。**这是因为任何资产的实际价格走势都是某个未知分布的一个 realization（实现）而已。如果抓住这个错觉、认为该资产的价格走势有一定的预测性（即收益率有预测性），并针对它开发了一个策略，我们根本无法预期该策略在样本外有同样的表现。\\n\\n**由于仅有一个实现（过去这段时间的价格走势只发生一遍），我们无法在统计上正确的评价该策略的参数对这个未知收益率分布是否有效，正如我们不知道在样本外，随机趋势有多大以及它什么时候出现。**策略在样本外的表现很有可能和其在样本内的表现大相径庭。\\n\\n**所以核心不是这些技术，而是市场到底反映出了哪种现象，然后是从这个现象能够引出哪种合理的假设，最后才是使用哪种最佳的量化手段针对这种假设来构建策略、交易这个市场的现象。**\\n\\n比如趋势追踪策略，那针对的是局部的低频分量 ——\\xa0漂移率（A 股大牛市的时候那低频分量就是牛逼；大熊市的时候就是不行），而不是抛去漂移率之后的那点缥缈的时序上的自相关性\\n\\n但是这里有一个BUG在哪呢？就是银行冲着规避风险的方向赚钱去了，国家放出去的水，银行全给买城投债、平台债、国企债，平台和国企做大做强，民企和个体雪上加霜。这违背了放水的初衷啊，否则你看央妈天天吆喝支持民企支持小微，甚至各种助农、小微、个体任务下发是干什么。\\n\\n因为国家需要民企，需要一个良性的金融系统孵化出更多的优秀民营企业，成为经济体的重要组成部分。\\n\\n然而现在部分银行选择了无脑投回国企，钱等于从央妈手里出来，转了一圈又回到了政府平台手里，银行在中间吃了个无风险套利。——这就是空转。\\n\\n李自然说的电商逻辑：供应链管理能力作为基础保障，店铺运营能力 * 资金能力 = 利润\\n\\n 资金能力包括自己的钱和能借来的钱，包括供应链金融；店铺运营能力可以加快资金周转速度，比如2千万的货，好的店铺一个月可以出十轮差的店只能出5轮；供应链管理能力包括供应商的价格、货的质量、和货源稳定性三个部分\\n\\n零售的本质是一个公式：流量x转化率x利润率 * 资金周转率？\\n\\nsome companies makes 12% profit and take it home; some company makes the same amount but has to turn it into factories and tell the investors this is my profit \\n\\n方三文：有效率没效益的企业，给股东创造的价值应该非常有限吧？比如很多企业正处于技术升级换代需要不停创新中，这种创新最后是不是一定能给股东带来好的回报？\\n\\n王国斌：它只给消费者带来好处，对投资者是最糟糕的。\\n\\n方三文：那是不是可以倒着推过来，也许一个不需要什么技术创新的行业反而给投资者创造的回报更高？\\n\\n王国斌：那当然了，产品生命周期越长越好。我们现在的市场为什么过多用噪音和叙事在交易，背后一个很大的因素是产品要不断创新不断奔跑，投入就越来越大，产生的ROE根本看不到。\\n\\n方三文：产品周期长、对技术创新依赖不高的行当，反而可能给投资者创造好的回报是吧？\\n\\n王国斌：那当然了，所以产品生命周期越长越好。\\n\\n方三文：比如什么产品或者行业？\\n\\n王国斌：可口可乐就是最典型的，它的产品创新不需要那么频繁。\\n\\n一个行业技术创新最终带来的回报都是给消费者了，这是市场经济在推动人类社会进步。\\n\\n王国斌：现在这部分高新技术企业有部分可能是自由现金流本身带来的，也许有一部分来自博弈，但是我刚才说了，我不基于博弈做事。\\n\\n方三文：但是有可能收益是来自于博弈？比如将来有人出更高价钱买这个资产。\\n\\n王国斌：对。就像巴菲特买中石油，他是基于价值投资买的，但他挣的钱是博弈的钱，因为他当年就卖掉了。\\n\\n方三文：如果他持有到现在，其实回报也非常一般吧？\\n\\n王国斌：对。\\n\\n方三文：也许可以用博弈来增强收益？\\n\\n王国斌：不是用于增强收益，而是非常重要。巴菲特选择接班人的第一条就是对市场规则要有清晰的了解，也就是对博弈规则要非常清楚的人。\\n\\n王的观点是技术不断迭代的企业，它们三五年内有可能有投资价值，长期来说回报都给了社会而不是投资者\\n\\n“低估、高估时很容易判断的，只是低估就一定会涨吗？这里需要催化剂。噪音和叙事某种程度上就是这种催化剂，所以它并不完全是坏事”\\n\\n一个上市公司的市净率低于1的时候，意味着资本市场认为这个所谓的资产表里的东西根本不值那么多钱，要么是假账要么是垃圾。\\n\\n大家不妨去看看国内各大商业银行的市净率。\\n\\n嘴巴上怎么吹不要紧，涉及到钱的时候，大家都诚实。资本市场有话语权的人觉得中国银行业报的资产有多大水分，一目了然。\\n\\nbtc 虽然在原理上是抗通胀的，但他本身完全没有表现出抗通胀的特性，他表现的特性就是风险资产，像纳指一样是很怕通胀的\\n\\nbtc is not a productive asset; it does not even have any transaction value like gold; gold can buy stuff; \\n\\nu will things u won’t believe in your lifetime in securities mkt. You will see huge waves. If you can stay objective throughout that and detach urself temperamentally from the crowd, you get very rich. \\n\\nit doesnt take brains, it takes temperament. \\n\\n通胀时期的好公司（W Buffett）\\n\\nThe business has enough power to raise prices to mitigate the effects of inflation; The business doesnt require a huge amount of capital to support inflationary growth i.e., enough prior investment done \\n\\n茅台？no need to borrow expensive $; room to raise price \\n\\nsmall luxury bands like netflix will suffer\\n\\npricing power: an ability to increase prices rather easily even when product demand is flat and capacity is not fully utilized without fear of significant loss of either market share or unit volume.\\n\\n现在，美联储抑制通胀的办法是停止印钱并回收流动性。让市场上的美元变少，通货膨胀就停止了。但你不能抢美国人手里的美元，不能从每个人的账户里直接扣钱，所以最好的办法是制造紧张气氛，让欧洲人贱卖资产换成美元润到美国来接股市和债市的盘，美联储则是通过抛售持有国债、偿还到期国债不发行新国债等方法来缩表。如果没有这些外来资金的支撑，美联储只能选择强行加息\\n，强行加息会使部分资金退出市场投资转向储蓄，会戳破股市和楼市的泡沫。但戳破泡沫会让所有人都停止花钱，市场反而缺少了流动性，美联储反而需要继续印钱，所以这个办法不可行，美联储必须找到外来资金接盘。但你把利率涨到0.25就指望中国资本去接盘不现实，所以美国才制造了这次乌克兰危机，逼迫欧洲资本去接盘。\\n\\n**一般我们根据换手率从小到大来判断股票活跃性的强弱，今天给大家分享换手率的5种表现**\\n\\n1.换手率低小于1%，说明个股活跃度不够，流动性非常差，基本上没有什么行情。2.换手率在1 %到3%以内，那么说明个股活跃度比较适中，属于正常的波动范围。3.换手率在3%到8%之内，说明各个相对活跃，市场参与的人气还是比较旺。4.换手率在8%到15%，说明个股，活跃度非常高，-般是有主力资金介入，像大多是题材股，属于热门的股票，赚钱效应也比较高 15%。5.换手率大于15%，那么说明，个股活跃度过高，常常代表主力有出货的存在。如果股价连续上升，并且在局位横盘时，出现换手率大十25%，那么往往标志看主力在出货。\\n\\n接下来通过k线图，教会大家如何使用换手率识别主力资金的意图，每一种情况我都会配上图片,大家-定要认真看，反复学习\\n\\n1.当天换手率超过70%，就是毁灭性的换手率，这种个股当天的换手率超过70%，基本上代表了这个个股的股东，全部换了-遍。如果出现这种情况，那么次日不是涨停就是大跌，或者直接跌停。所以遇到了这种情况，不管什么原因我们都放弃操作.走为上计，安全第一\\n\\n2高位换手率达到25%，就是出货换手率。股价经过大幅度拉升之后快速脱离了主力的成本区域,那么主力的低位筹码，就会进行兑现如果前期你无法看清主力的意图，那么此时主力的出货，就会暴露出狐狸的尾巴，通常情况下伴随着高换手率配合k线形态呈现出了一根阴线，说明筹码在短时间内由追高的散户来接盘了，所以,高位换手率大于25%而且收的是阴线，那么这类个股建议不要去碰。\\n\\n3.换手率低，洗盘，后拉升.说明主力在吸筹的时候为了防止散户进场跟风，需要通过打压洗盘来操作那么在洗盘过程中，-定是散户大量的抛售，主力在吸筹时，换手率一定是很低的，但在拉升的过程中，主力往往是筹码已经足够了，这个时候拉升是非常轻松的，就会表现在换手率上,散户是逐渐在卖，越卖越少，换手率也是越来越萋缩\\n\\n4.新股巨量换手在新股刚刚上市不久,开板之后，换手率高达52%以上。如此高的换手率说明主力几乎抢到了足够多的筹码，接下来,换手率-直在维持20%以上，但股价并没有出现新低，反而连续几天的光头阳线，说明新股上市阶段，换手率是主力建仓的行为，盯准筹码，后市即将大涨。以上就是换手率由小到大的5种表现，和4种实际用法.看懂了之后，以后炒股只需要通过看换手率就可以轻松看透主力资金的意图\\n\\n预测的准确率要超过1/2才有意义，否则还不如抛硬币去决定\\n\\n其实准确率并不需要高到99%的地步，哪怕是确定高于50%，比如51%，那么长期来看也一定很赚钱。\\n\\nbuy the dip: every time an etf drops >5%, buy some\\n\\n明显的利空消息还不跌，说明是底部了\\n\\n对于这个世界的主要长线大型买方而言——主权基金、养老基金、社保基金、大学捐赠基金以及大型对冲基金——**高成长高波动的资产永远都不是核心资产，而是用来帮基金拉一拉超额收益，是用来锦上添花的核心资产旁边的小卫星。**\\n但在市场整体风险偏好降低，不确定性上升的情况下，他们也会优先被抛售——估值虽然不高，但性价比也很低，要操心的事情一大堆惶惶不可终日，比如货币政策啦汇率啦监管政策啦不确定性啦。那我自然在市场走弱的时候，先抛掉卫星资产。这就是长线基金们正在做（或者说已经做）的事情。\\n\\n凯利判据Kelly Criterion\\n\\n最优单次下注占比（相对于总赌本）f=[p(a+b)-a]/b\\n\\n长期价格预测是很可能的; 预测时间期限越长，预测难度越低\\n\\nGoodhart and Pradhan:\\n\\n> The great demographic reversal and the retreat from globalization will bring back stronger inflationary pressures – this is our highest conviction view. Worsening dependency ratios [the number of dependents vs. the number of workers] naturally raise inflation. The lesser availability of labor at home and abroad will serve to restore the (previously diminished) bargaining power of labor. It will also end up raising the equilibrium natural rate of unemployment. Households will save less, and invest more in housing, than some mainstream models suggest. The non-financial corporate sector may have to invest more to hold down unit labor costs, though we are agnostic about the various causes for recent low investment rates. But we doubt that politicians, facing rising health and pension costs, will be prepared or able to raise taxes enough to equilibrate the economy via fiscal policy. … Hence inflation will rise. That will lead central banks to raise nominal interest rates in pursuit of their inflation targets. In turn, that will put them at loggerheads with ministers of finance and prime ministers/presidents, especially those of a populist inclination. In any such conflict between politicians and central bankers, we would back the former to win – that conflict has already begun.\\n> \\n\\n2025年之后的全球GDP增速会进一步降低\\n\\n!Untitled\\n\\nMMT下主要考虑的是利率成本和gdp的比重，不考虑债务和gdp比重\\n\\n!Untitled\\n\\n!Untitled\\n\\nWSHR etf index\\n\\ngroup RSP money can come directly from payroll, does not come any other sources\\n\\nGenerally speaking, because credit creates both spending power and debt, whether or not more credit is desirable depends on whether the borrowed money is used productively enough to generate sufficient income to service the debt.\\n\\nTo give you an idea of what that might mean for an economy as a whole, really bad debt losses have been when roughly 40 percent of a loan’s value couldn’t be paid back. Those bad loans amount to about 20 percent of all the outstanding loans, so the losses are equal to about 8 percent of total debt. That total debt, in turn, is equal to about 200 percent of income (e.g., GDP), so the shortfall is roughly equal to 16 percent of GDP. If that cost is “socialized” (i.e., borne by the society as a whole via fiscal and/or monetary policies) and spread over 15 years, it would amount to about 1 percent per year, which is tolerable. Of course, if not spread out, the costs would be intolerable. For that reason, I am asserting that the downside risks of having a significant amount of debt depends a lot on the willingness and the ability of policy makers to spread out the losses arising from bad debts. I have seen this in all the cases I have lived through and studied. Whether policy makers can do this depends on two factors: 1) whether the debt is denominated in the currency that they control and 2) whether they have influence over how creditors and debtors behave with each other.\\n\\nLending naturally creates self-reinforcing upward movements that eventually reverse to create self-reinforcing downward movements that must reverse in turn. During the upswings, lending supports spending and investment, which in turn supports incomes and asset prices; increased incomes and asset prices support further borrowing and spending on goods and financial assets. The borrowing essentially lifts spending and incomes above the consistent productivity growth of the economy. Near the peak of the upward cycle, lending is based on the expectation that the above-trend growth will continue indefinitely. But, of course, that can’t happen; eventually income will fall below the cost of the loan\\n\\n[[web 3]]', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='005e0da2-610c-4e55-b937-486b6205a222', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nassistant\\n```lookup\\n{\"hypotheticals\":[\"```markdown\\\\nPersonal > Reflections > Self-Exploration > note_for_future.md:\\\\n\\\\nI\\'ve been pondering over my notes and trying to make sense of who I am. Here are some thoughts:\\\\n\\\\n- I am a curious and creative individual, always seeking new experiences and knowledge.\\\\n- I value my relationships with family and friends, and strive to be a good listener and supportive companion.\\\\n- I have a strong work ethic and am dedicated to my career, but also prioritize my mental and physical health.\\\\n- I am an introvert by nature, but enjoy socializing and connecting with others in small groups.\\\\n- I have a sense of humor and try to find joy in the little things in life.\\\\n- I am a continuous learner, always looking for ways to improve myself and grow as a person.\\\\n- I am passionate about the environment and try to live sustainably.\\\\n- I have a strong moral compass and strive to do what is right, even when it is difficult.\\\\n- I am resilient and adaptable, able to bounce back from setbacks and challenges.\\\\n```\"]}\\n```\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bbb0b7ef-a9b1-4c66-9e6b-33be358767f0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nsystem\\n```sc-context\\nLife thoughts/读书笔记 read notes/成功学简史 draft.md\\nLife thoughts/杂 all/杂 86 nice guy.md\\n日记/April notes.md\\nLife thoughts/j2.md\\nLife thoughts/杂 all/杂 101.md\\nLife thoughts/杂 all/杂 90.md\\nOld stuff/Thesis project/social perception theories.md\\nOld stuff/Thesis project/motives.md\\nCareer/求职流程相关/离职邮件.md\\nOld stuff/Thesis project/论文写作 杂/metacognition.md\\n```\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='18ef3cdf-a24e-4919-a0fe-d35a96314180', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nuser\\nbased on my notes, who am I?\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='968a8244-7651-44cc-8b3d-0683a694cca2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nassistant\\nBased on your notes, you are a person who has written or read notes on various topics, including self-help books, social perception theories, motives, leaving a job, and metacognition. You have expressed thoughts on the importance of personal growth, the role of social relationships, and the challenges of making decisions and judgments. You have also shown an interest in psychology and philosophy.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='74de20f1-23d3-4260-a5eb-dc0a0a373605', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nuser\\nHi there\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4f1e8f8c-4585-4d9b-8d9d-35735068c2b0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nuser\\nbased on my notes, who am I\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='58b3f569-dd41-4af2-8b19-96cb51564788', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nuser\\nhi\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5b346bab-6fc2-42dd-b55c-103e084e00b1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nassistant\\nBased on your notes, you are someone who has written or collected information on various topics. It's difficult to determine specific details about you, such as your age, gender, or occupation, from the provided notes alone. However, you seem to have an interest in a wide range of subjects, including self-help, social perception, motives, and metacognition. You may be a student, researcher, or someone who enjoys learning new things.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='65699b7c-7f11-4703-8c94-e76ad61dff0f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nassistant\\n```lookup\\n{\"hypotheticals\":[\"Personal > Journal > DailyReflections > # Monday, October 11, 2021:\\\\n\\\\nToday was a day filled with both challenges and accomplishments. I faced several difficult situations at work, but I managed to find creative solutions and come out on top. I\\'m proud of my ability to adapt and problem-solve under pressure.\\\\n\\\\nOn a personal note, I made some progress on my fitness goals. I went for a run in the morning and hit my target number of steps for the day. I\\'m feeling more energized and motivated to keep up with my health routine.\\\\n\\\\nI also took some time to reflect on my relationships. I had a heartfelt conversation with a friend and we were able to clear up some misunderstandings. I\\'m grateful for the strong bonds in my life and the opportunity to deepen my connections with those around me.\\\\n\\\\nTomorrow, I\\'ll tackle a new project at work and continue to focus on my personal goals. I\\'m excited for what the future holds and feel confident in my ability to handle whatever comes my way.\\\\n\\\\nTasks for tomorrow:\\\\n- Finish research for new work project\\\\n- Schedule follow-up call with friend\\\\n- Go for a run in the morning\\\\n- Hit 10,000 steps by end of day\\\\n- Cook healthy dinner at home\"]}\\n```\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='39e96141-4e18-4f1f-8252-8880155b9206', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nsystem\\n```sc-context\\nTemplates/daily_note_template.md\\nCareer/求职流程相关/离职邮件.md\\n日记/April notes.md\\n日记/2023-10-29.md\\n日记/2023-10-28.md\\n日记/2023-11-01.md\\n日记/2023-10-01.md\\n日记/2023-11-04.md\\n日记/2023-10-22.md\\n日记/2023-09-20.md\\n```\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c0649454-4a90-4fc2-b2a4-63827dfda50c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nuser\\nbased on my note, who am I\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d1e162e4-9702-4fe8-b3d5-8b0af4c35aa8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nassistant\\nBased on your notes, you are someone who values happiness, progress, gratitude, and health. You practice habits such as dedicating time each day to deep learning and avoiding using your phone. You have a history of working at Gap Inc for six years and have expressed gratitude for the experiences and growth you gained there. You also prioritize developing daily friendships to reduce feelings of loneliness and wasting time on your phone. Additionally, you have a background in using journaling as a means to reflect on your experiences and personal growth.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='60bb4de4-c094-46de-9d56-822c528491a6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nassistant\\n```lookup\\n{\"hypotheticals\":[\"```markdown\\\\nPersonal > Reflections > Self-Exploration > note_for_future.md:\\\\n\\\\nI\\'ve been pondering over my notes and trying to make sense of who I am. Here are some thoughts:\\\\n\\\\n- I am a curious and creative individual, always seeking new experiences and knowledge.\\\\n- I value my relationships with family and friends, and strive to be a good listener and supportive companion.\\\\n- I have a strong work ethic and am dedicated to my career, but also prioritize my mental and physical health.\\\\n- I am an introvert by nature, but enjoy socializing and connecting with others in small groups.\\\\n- I have a sense of humor and try to find joy in the little things in life.\\\\n- I am a continuous learner, always looking for ways to improve myself and grow as a person.\\\\n- I am passionate about the environment and try to live sustainably.\\\\n- I have a strong moral compass and strive to do what is right, even when it is difficult.\\\\n- I am resilient and adaptable, able to bounce back from setbacks and challenges.\\\\n- I am a work in progress, always striving to be the best version of myself.\\\\n```\"]}\\n```\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='eec39781-bb2d-4dfe-a3f8-bef341546da6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nsystem\\n```sc-context\\nLife thoughts/读书笔记 read notes/成功学简史 draft.md\\nLife thoughts/j2.md\\nOld stuff/Thesis project/self concept 相关理论.md\\nLife thoughts/杂 all/杂 90.md\\nLife thoughts/杂 all/杂 69 安兰德.md\\nOld stuff/Thesis project/inconsistency为何会有正负两种effect.md\\nLife thoughts/杂 all/杂 86 nice guy.md\\nLife thoughts/杂 all/杂 6.md\\nOld stuff/Thesis project/motives.md\\nCareer/求职流程相关/离职邮件.md\\n```\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ec039391-9e6c-436a-8b69-ec59a4fb50ac', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nuser\\nbased on my notes, who am I?\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8461d574-c8fa-4747-9d36-f653e05df991', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nassistant\\nBased on your notes, you are a person who has written several drafts for articles, including one about the history of self-help books. You have a strong interest in ancient Egyptian instructional literature, Stoic philosophy, and the history of conduct books. You have also read and been influenced by works by Horatio Alger Jr. You have a desire to understand the longing for the purpose or meaning of life and the role that self-help books play in satisfying this need. You have categorized self-help books into three main categories: motivational stories, naive theories of success, and Judeo-Christian ethics. You have a critical yet respectful view of self-help books and acknowledge their appeal despite your skepticism. You have also read and been influenced by works on meditation and the religious roots of self-help books. Additionally, you have written about the importance of self-growth as a long process and the role of meditation in connecting better with the body and cleansing the mind. You have also written about the importance of radical self-acceptance and the need for radical self-honesty in order to change. You have also written about the influence of social motives on consumer behavior and the desire to belong and express oneself through products and brands. Lastly, you have written a leaving email for your job at Gap Inc.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b490a71d-2e48-42d0-b032-62486ddb99a7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nassistant\\n```lookup\\n{\"hypotheticals\":[\"```markdown\\\\nPersonal > Projects > NewProject > ToDoList:\\\\n- Research potential collaborators\\\\n- Contact top 3 candidates for partnership\\\\n- Schedule initial meetings\\\\n- Discuss project goals and expectations\\\\n- Agree on roles and responsibilities\\\\n- Establish communication channels\\\\n- Set project timeline and milestones\\\\n- Sign partnership agreement\\\\n- Begin project development\\\\n```\"]}\\n```\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='18b5b5ab-a3ad-4905-8803-f62c23d08881', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nsystem\\n```sc-context\\nCareer/面试相关/聊天获得的信息.md\\nCareer/求职流程相关/after interview email.md\\nCareer/求职流程相关/follow up message.md\\nCareer/面试相关/questions to the company.md\\nCareer/面试公司记录/Betterhelp.md\\nCareer/求职流程相关/linkedin connect message.md\\nCareer/求职流程相关/linkedin coffee chat & relationship build message.md\\nCareer/Experience vs Question table/Experience vs Question table.md\\nCareer/求职流程相关/linkedin strategy.md\\nOld stuff/Thesis project/一些social perception theories.md\\n```\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8884998e-8e0b-4469-9199-982e780a1d2a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nuser\\ntell me more\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2b1f9c17-9c2b-4f5f-a9de-dc0b31023ec8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nassistant\\nThe Social Penetration Theory (SPT) is a social psychological theory that explains how interpersonal relationships develop from superficial to intimate levels. It was proposed by Irwin Altman and Dalmas Taylor in 1973. According to SPT, relationships involve different levels of intimacy, or degree of social penetration. The theory is based on the assumption that relationship development is systematic and predictable, and it primarily occurs through self-disclosure.\\n\\nSelf-disclosure refers to the intentional revealing of personal information, such as personal motives, desires, feelings, thoughts, and experiences, to others. As relationships develop, individuals engage in deeper levels of self-disclosure, which allows them to bring the relationship to a new level of intimacy. The theory also assumes that relationship development is not always linear and can move backward, resulting in de-penetration and dissolution.\\n\\nThe theory is based on four assumptions:\\n\\n1. Relationship development moves from superficial layers to intimate ones.\\n2. Interpersonal relationships develop in a generally systematic and predictable manner.\\n3. Relational development could move backward, resulting in de-penetration and dissolution.\\n4. Self-disclosure is the key to facilitate relationship development.\\n\\nThe theory has been influential in understanding relationship development and has been applied to various contexts, including romantic relationships, friendships, and work relationships. It is considered an objective theory as it is based on data drawn from experiments and not from conclusions based on individuals' specific experiences.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d4efe540-d41b-4bf2-8f6b-39d49262f552', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nmy interview feedback\\n\\nvkmk1226@gmail.com\\n\\n###########\\n\\nadam; SGD;\\n\\nRNN LSTM;\\n\\nLSTM what makes it special;\\n\\nsiamese network;\\n\\nhow 3 head are doing;\\n\\ncos function of postional encoding;\\n\\nfrequency of positional information;\\n\\nseq-seq + simple attention;\\n\\ntransfer learning vs. fine tuning;\\n\\nprepare data for BERT;\\n\\nGPT2; decoder based;\\n\\nelectra;\\n\\nmanually look for category;\\n\\n10,000;\\n\\nproduct; images;\\n\\nhow would u tokenization; data preprocessing;\\n\\nwho do we do data cleaning in text;\\n\\nskewed images;\\n\\nevaluation metrics;\\n\\nbottleneck;\\n\\nONNX framework;\\n\\ntensorRT; bert inference 36 times faster;\\n\\nlower cases; lammatization; remove stop words;\\n\\nhow do we do image processing;\\n\\nRESNET; ALEXNET; GOOGLENET\\n\\nhiretical multi classifaction;\\n\\ncontent moderation:\\n\\nmeme images and text\\n\\nclassify\\n\\nmeme-> OCR-> text\\n\\nimage ; text will be similar; image will be different;\\n\\nAdvice for future interviews:\\n\\nBrush classical Machine learning technique skills: Regression, Classification, Clustering, and dimensionality Reduction techniques.\\n\\nPrepare and learn about the latest NLP models.\\n\\nPractice 2-3 case studies every day.\\n\\nCreate a list of questions about the case study problem statement.\\n\\nExplain every step clearly, and don't go directly to the model part of the problem statement.\\n\\nExplain the e-2-e solution and estimate your time accordingly.\\n\\nStudy basic Computer Vision models, such as CNN, Resnet, and Googlenet.\\n\\nAs we discussed, I have created a guide for the NLP case study and added a couple of important questions related to different parts of the case study.\\n\\nNLP Case Study guide:\\n\\nAsk relevant questions to the interviewer; what is dataset size? How do we access the dataset? Do we care if the model adds some latency in inference or not?\\n\\nSteps:\\n\\na.Text preprocessing:\\n\\n1. Convert to lowercase\\n\\n2. Remove punctuations and stopwords\\n\\n3. Lemmatization or Stemming\\n\\nb. Train, Test, and Validation split (usually ratio 80:10:10)\\n\\nc. Tokenization\\n\\nd.\\xa0 (optional) Embedding matrix (word2vec and Glove)\\n\\ne. Model RNN, LSTM, GRU, and Transformer pre-trained models such as Bert, XLM Roberta, and GPT.\\n\\nTransformer tutorials: https://www.youtube.com/watch?v=dichIcUZfOw\\n\\nLSTM: https://www.youtube.com/watch?v=QciIcRxJvsM\\n\\nBERT: https://www.youtube.com/watch?v=xI0HHN5XKDo\\n\\nEmbedding: https://www.youtube.com/watch?v=UqRCEmrv1gQ&t=325s\\n\\nQuestion regarding the model:\\n\\nExplain the LSTM model and Cell state.\\n\\nWhy LSTM is better than a simple neural network for text classification?\\n\\nWhy is the Transformer better than the LSTM model?\\n\\nWhat makes the transformer so unique for Text classification?\\n\\nExplain the BERT model. Some basic understanding.\\n\\nWhat is positional Encoding?\\n\\nWhat does it mean by multi-headed attention?\\n\\nIf we don't have enough data, we should use a pre-trained language model to get good accuracy.\\n\\nAlso, learn about loss function and optimizer\\n\\n1.\\tSparse categorical cross-entropy, categorical cross-entropy, and Binary Cross entropy\\n\\n2.\\tAdam, Momentum, and RMSProp\\n\\n3.\\tGradient Descent and different types of GD.\\n\\nf. Evaluation matrices, e.g., Accuracy, Precision, and Recall. For language translation, then use Rogue and BLEU score\\n\\ng: How to reduce Latency in a big deep learning model?\\n\\nJust familiarize yourself with these concepts; you don’t need to be an expert in this area. Understanding these skills will be a big plus for you in the interview.\\n\\n- Distillation: https://towardsdatascience.com/distillation-of-bert-like-models-the-theory-32e19a02641f\\n- Pruning: https://medium.com/@souvik.paul01/pruning-in-deep-learning-models-1067a19acd89#:~:text=Pruning%20in%20deep%20learning%20basically%20used%20so%20that%20we%20can,values%20of%20the%20weight%20tensors.\\n- Quantization: https://medium.com/@joel_34050/quantization-in-deep-learning-478417eab72b\\n- XLA compiler: https://www.tensorflow.org/xla\\n\\nONNX framework:\\n\\nONNX Runtime applies several graph optimizations on the model graph and then partitions it into subgraphs based on available hardware-specific accelerators. Optimized computation kernels in core ONNX Runtime provide performance improvements and assigned subgraphs to benefit from further acceleration from each Execution Provider.\\n\\nhttps://towardsdatascience.com/onnx-easily-exchange-deep-learning-models-f3c42100fd77\\n\\n- TensorRT: https://www.seeedstudio.com/blog/2022/08/23/faster-inference-with-tensorrt-on-nvidia-jetson-run-yolov5-at-27-fps-on-jetson-nano/\\n\\nOverall excellent. Make sure to cover modeling, distribution, and testing. Think of bottle necks, how to handle drift, how to update your models etc.\\n\\nBe sure to describe your assumptions about the data eg is it imbalanced? In which case how do you handle for it? You did a good job here discussing encoding for continuous data.\\n\\n- *General Structure to keep in mind:**\\n- **Here is a simple, time-broken example I have seen work well for most problems.**\\n\\n- Requirements 5 mins\\n\\n- Functional [5 reqs]\\n\\n- Non Functional [3 reqs]\\n\\n- Apis/ Requests/ Data Models, Feature Eng [7 mins]\\n\\n- Don’t forget your protocols in the API conversation.\\n\\n- Don’t forget the API methods,\\n\\n- Talk about traffic considerations by endpoint, stratify endpoints by traffic and data type. If we supported live streams for instance, web hooks would be better compared to https.\\n\\n- Estimates [5mins, Generate Read/ Write Throughput]\\n\\n- Schemas [5 mins]\\n\\n- High Level Design [7 mins]\\n\\n- Deep Dive [15-20 mins]\\n\\nSection by section flow:\\n\\nfunc -> api endpoints\\n\\nnon func -> protocol\\n\\napi-endpoints -> data models\\n\\ndata models + func -> estimates\\n\\nestimates -> high level\\n\\nhigh level -> deep dive\\n\\n- *Resources**\\n- Would recommend following a standard structure for flow.\\n\\n- Alex Xu has an excellent flow structure that work well for most interviews. Follow his channel for examples of flows:\\n\\n- https://blog.bytebytego.com/\\n\\n- https://www.linkedin.com/in/alex-xu-a8131b11/\\n\\n- Here are some resources on API design and Data Modeling:\\n\\n- https://restfulapi.net/rest-api-design-tutorial-with-example/\\n\\n- https://www.tutorialspoint.com/dbms/dbms_data_models.htm\\n\\n- Data Pipelining Tutorial:\\n\\n- https://www.simplilearn.com/what-is-data-pipelining-article\\n\\n- Check out some tech blogs to build familiarity common tools tools you can cite when selecting components:\\n\\n- https://netflixtechblog.com/\\n\\n- https://developers.googleblog.com/\\n\\n- https://techcommunity.microsoft.com/t5/custom/page/page-id/Blogs\\n\\n- Check out these system design interviews to get a sense of the flow:\\n\\n- https://www.youtube.com/c/ExponentTV\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f23d0efa-bd4f-45dc-85c9-9a944c8b02c9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nExamples Non Functional Requirements\\n\\n- Speed. Speed determines how fast an application responds to commands. ...\\n- Security. ...\\n- Portability. ...\\n- Compatibility. ...\\n- Capacity. ...\\n- Reliability. ...\\n- Environment. ...\\n- Localization/ Data Nationalization eg GDPR, California Data Laws.\\n- Performance – for example Response Time, Throughput, Utilization, Static Volumetric\\n- Scalability\\n- Capacity\\n- Availability\\n- Reliability\\n- Recoverability\\n- Maintainability\\n- Serviceability\\n- Security\\n- Regulatory\\n- Manageability\\n- Environmental\\n- Data Integrity\\n- Usability\\n- Interoperability\\n- *Other Resources**\\n- Here are some resources on API design and Data Modeling:\\n\\n- https://restfulapi.net/rest-api-design-tutorial-with-example/\\n\\n- https://www.tutorialspoint.com/dbms/dbms_data_models.htm\\n\\n- Data Pipelining Tutorial:\\n\\n- https://www.simplilearn.com/what-is-data-pipelining-article\\n\\n- Check out some tech blogs to build familiarity common tools tools you can cite when selecting components:\\n\\n- https://netflixtechblog.com/\\n\\n- https://developers.googleblog.com/\\n\\n- https://techcommunity.microsoft.com/t5/custom/page/page-id/Blogs\\n\\n- Check out these system design interviews to get a sense of the flow:\\n\\n- https://www.youtube.com/c/ExponentTV\\n\\nData Engineer study guide\\n\\nhttps://docs.google.com/spreadsheets/d/1GOO4s1NcxCR8a44F0XnsErz5rYDxNbHAHznu4pJMRkw/edit#gid=0\\n\\nSoftware Engineer Study Guide:https://docs.google.com/spreadsheets/d/19hSRrL4l3gRiJ5ucH9q4iwFo2QHgic9gGMNUrcn1mm0/edit#gid=0\\n\\nStandard components:\\n\\nhttps://completedesigninterviewcourse.com/system-design-concepts-components/\\n\\n- *Modeling and API:**\\n\\nhttps://restfulapi.net/rest-api-design-tutorial-with-example/\\n\\nhttps://www.tutorialspoint.com/dbms/dbms_data_models.htm\\n\\nData Pipelining Tutorial\\n\\nhttps://www.simplilearn.com/what-is-data-pipelining-article\\n\\nExamples:\\n\\nhttps://medium.com/must-know-computer-science/system-design-message-queues-245612428a22#:~:text=Examples%20of%20queues%3A%20Kafka%2C%20Heron,%2C%20Amazon%20SQS%2C%20and%20RabbitMQ.\\n\\n[Check his collection of resources, it includes a lot of real life examples of each]\\n\\n- *Company tech blogs for system Review**\\n\\nAirbnb:\\xa0https://medium.com/airbnb-engineering\\n\\nAmazon:\\xa0https://developer.amazon.com/blogs/appstore\\n\\nAsana:\\xa0https://blog.asana.com/category/eng/\\n\\nAtlassian:\\xa0https://blog.developer.atlassian.com/\\n\\nBittorrent:\\xa0https://engineering.bittorrent.com/\\n\\nCloudera:\\xa0https://blog.cloudera.com\\n\\nDocker:\\xa0https://blog.docker.com\\n\\nDropbox:\\xa0https://dropbox.tech/\\n\\neBay: https://tech.ebayinc.com/\\n\\nFacebook:\\xa0https://engineering.fb.com/\\n\\nGitHub:\\xa0https://githubengineering.com/\\n\\nGoogle:\\xa0https://lnkd.in/ddPVy6Zj\\n\\nGroupon:\\xa0https://lnkd.in/dsyGvUWF\\n\\nHighscalability:\\xa0http://highscalability.com\\n\\nInstacart:\\xa0https://tech.instacart.com\\n\\nInstagram:\\xa0https://lnkd.in/dEs6FyGn\\n\\nLinkedin:\\xa0https://lnkd.in/d_yQe9g6\\n\\nMixpanel:\\xa0https://mixpanel.com/blog\\n\\nNetflix:\\xa0https://lnkd.in/dKhbQqxd\\n\\nNextdoor:\\xa0https://lnkd.in/dDdGPQgR\\n\\nPayPal:\\xa0https://lnkd.in/d9YkeE_h\\n\\nPinterest:\\xa0https://lnkd.in/duz8a8vq\\n\\nQuora:\\xa0https://lnkd.in/d-iuzYZq\\n\\nReddit:\\xa0https://redditblog.com\\n\\nSalesforce:\\xa0https://lnkd.in/dV9unb47\\n\\nShopify:\\xa0https://lnkd.in/dQtK4TME\\n\\nSlack:\\xa0https://slack.engineering\\n\\nSoundcloud:\\xa0https://lnkd.in/dgWK_v4h\\n\\nSpotify:\\xa0https://labs.spotify.com\\n\\nStripe:\\xa0https://lnkd.in/dm-WBTgr\\n\\nStripe:\\xa0https://lnkd.in/dm-WBTgr\\n\\nSystem design primer:\\xa0https://lnkd.in/dnUnsQE9\\n\\nTwitter:\\xa0https://lnkd.in/d9tmm5wj\\n\\nThumbtack:\\xa0https://lnkd.in/d6QTWF_p\\n\\nUber:\\xa0http://eng.uber.com\\n\\nYahoo:\\xa0https://lnkd.in/dKgyhbNE\\n\\nYelp:\\xa0https://lnkd.in/d_6hhMS4\\n\\nZoom:\\xa0https://lnkd.in/dquH3cKY', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c6665e78-6af2-44aa-94b1-71706b4efd80', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nsearch engine design\\n\\n!Untitled\\n\\n!Untitled\\n\\nsemantic search指，词汇很不同但意思差不多\\n\\n!Untitled\\n\\nBERT for create dense vector representation for every question and answer;\\n\\nkeyword based elastic search + bert based vector similarity search  \\n\\n!Untitled\\n\\n!Untitled\\n\\nk = 2; 在每个trie node里存两个最可能的值，这样就不用搜索整个树\\n\\n!Untitled\\n\\nprefix matching \\n\\ninput stream; minimal viable product; \\n\\ncontext matching; if the user u know is a big fan of asia food, ur output list will be different; \\n\\nconvert the user profile into \\n\\nIt’s time to preprocess the text data to make it feedable to our neural network. As introduced in this previous\\xa0post\\xa0on recurrent neural networks, the smart way to deal with text preprocessing is typically to use an embedding layer that translates words into vectors. However, text embedding is insuitable for this task since our goal is to build a character-level text generation model. In other words, our model is not going to generate word predictions; instead, it will spit out a character each prediction cycle. Therefore, we will use an alternative technique, namely mapping each character to an integer value. This isn’t as elegant as text embedding or even one-hot encoding but for a character-level analysis, it should work fine. The\\xa0preprocess_split\\xa0function takes a string text data as input and returns a list of training data, each of length\\xa0max_len, sampled every\\xa0step\\xa0characters. It also returns the training labels and a hash table mapping characters to their respective integer encodings.\\n\\nLet’s now design our model. Because there is obviously going to be sequential, temporal structure underlying the training data, we will use an LSTM layer, a type of advanced recurrent neural network we saw in the previous post. In fact, this is all we need, unless we want to create a deep neural network spanning multiple layers. However, training such a model would cost a lot of time and computational resource. For the sake of simplicity, we will build a simple model with a single LSTM layer. The output layer is going to be a dense layer with\\xa0vocab_size\\xa0number of neurons, activated with a softmax function. We can thus interpret the index of the biggest value of the final array to correspond to the most likely character.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='804fe614-28ba-4472-b978-e84c80f3c6ef', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\npartial dependence plots PDP\\n\\n**The computation of partial dependence plots is\\xa0intuitive: The partial dependence function at a particular feature value represents the average prediction if we force all data points to assume that feature value. In my experience, lay people usually understand the idea of PDPs quickly.**\\n\\n**If the feature for which you computed the PDP is not correlated with the other features, then the PDPs perfectly represent how the feature influences the prediction on average. In the uncorrelated case, the\\xa0interpretation is clear: The partial dependence plot shows how the average prediction in your dataset changes when the j-th feature is changed. It is more complicated when features are correlated, see also disadvantages.**\\n\\n**Partial dependence plots are\\xa0easy to implement.**\\n\\n!Untitled\\n\\n!Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='733d729d-5c21-42b3-be8b-1462e66c9b22', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nUCB upper confidence bound\\n\\nHomepage Recommendation with Exploitation and Exploration\\n\\nFood Discovery with Uber Eats: Recommending for the Marketplace\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d19fc9c0-fe4e-4873-a590-635a27e33997', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**How we combined exploitation and exploration**\\n\\nIt is neither feasible nor optimal to directly implement the UCB in its original form because:\\n\\n- Each consumer can choose from thousands of stores and hundreds of thousands of items at a certain location and time, making it impossible to try each of them and hampering collection of cumulative conversion data to estimate the pConv.\\n- Estimating the expected pConv for entities with mixed types adds further complexity\\n- The recommendation engine produces a ranked list of entities for each consumer. But because a single app window can only show a few entities and because each consumer can choose how deep they wish to browse/scroll, there’s uncertainty around how many of the recommended entities will receive effective consumer feedback.\\n- When introducing fresh options for a consumer, we need to control uncertainty carefully so that they do not add confusion or interrupt the consumer experience.\\n\\nGiven these considerations, our final solution was to integrate the UR into the UCB algorithm. This allows us to make scalable and smart trade-offs between exploitation and exploration, allowing fresh choices for consumers without disturbing their routing experience.\\n\\n!Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c423669d-53cd-4283-ba48-d28080c18fb0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nlogistic regression\\n\\nhttps://madewithml.com/courses/foundations/logistic-regression/\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n**Another disadvantage of the logistic regression model is that the interpretation is more difficult because the interpretation of the weights is multiplicative and not additive.**\\n\\n**Logistic regression can suffer from\\xa0complete separation. If there is a feature that would perfectly separate the two classes, the logistic regression model can no longer be trained. This is because the weight for that feature would not converge, because the optimal weight would be infinite. This is really a bit unfortunate, because such a feature is really useful. But you do not need machine learning if you have a simple rule that separates both classes. The problem of complete separation can be solved by introducing penalization of the weights or defining a prior probability distribution of weights.**\\n\\n**On the good side, the logistic regression model is not only a classification model, but also gives you probabilities. This is a big advantage over models that can only provide the final classification. Knowing that an instance has a 99% probability for a class compared to 51% makes a big difference.**\\n\\n[[07_Logistic_Regression.ipynb]]\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\nIn logistic regression, the dependent variable is a\\xa0*logit*\\n, which is the natural log of the odds, that is,\\n\\n!Untitled\\n\\nlogit function是softmax即logistic function的反函数\\n\\nthe logit is a type of function that maps probability values from\\xa0{\\\\displaystyle (0,1)}\\n\\xa0to real numbers in\\xa0{\\\\displaystyle (-\\\\infty ,+\\\\infty )}\\n\\n!Untitled\\n\\n**Why use logistic regression rather than ordinary linear regression?**\\n\\nWhen I was in graduate school, people didn't use logistic regression with a binary DV. They just used ordinary linear regression instead. Statisticians won the day, however, and now most psychologists use logistic regression with a binary DV for the following reasons:\\n\\n1. If you use linear regression, the predicted values will become greater than one and less than zero if you move far enough on the X-axis. Such values are theoretically inadmissible.\\n2. One of the assumptions of regression is that the variance of Y is constant across values of X (homoscedasticity). This cannot be the case with a binary variable, because the variance is PQ. When 50 percent of the people are 1s, then the variance is .25, its maximum value. As we move to more extreme values, the variance decreases. When P=.10, the variance is .1*.9 = .09, so as P approaches 1 or zero, the variance approaches zero.\\n3. The significance testing of the\\xa0*b*\\xa0weights rest upon the assumption that errors of prediction (Y-Y') are normally distributed. Because Y only takes the values 0 and 1, this assumption is pretty hard to justify, even approximately. Therefore, the tests of the regression weights are suspect if you use linear regression with a binary DV.\\n\\n!Untitled\\n\\n!Untitled\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2259a107-e0c7-4546-b7af-0504c134c040', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nxgboost vs. random forrest\\n\\nIn other words, random forests assume that on average trees make good predictions and it just some random noise that prevents them from achieving good performance. Therefore, ensembles are used to reduce this noise. Gradient boosting, on the other hand, assume that a single tree may consistently miss the target and hence uses ensembles to direct it to the right target.\\n\\nBoosting happens to be iterative learning which means the model will predict something initially and self analyses its mistakes as a predictive toiler and give more weightage to the data points in which it made a wrong prediction in the next iteration. After the second iteration, it again self analyses its wrong predictions and gives more weightage to the data points which are predicted as wrong in the next iteration. This process continues as a cycle. Hence technically, if a prediction has been done, there is an at most surety that it did not happen as a random chance but with a thorough understanding and patterns in the data. Such a model that prevents the occurrences of predictions with a random chance is trustable most of the time.\\n\\nRandom forest is just a collection of trees in which each of them gives a prediction and finally, we collect the outputs from all the trees and considers the mean, median, or mode of this collection as the prediction of this forest depending upon the nature of data (either continues or categorical). At a high level, this seems to be fine but there are high chances that most of the trees could have made predictions with some random chances since each of the trees had their own circumstances like class imbalance, sample duplication, overfitting, inappropriate node splitting, etc.\\n\\n1. **XG Boost straight away prunes the tree with a score called “Similarity score” before entering into the actual modeling purposes.**\\xa0It considers the “Gain” of a node as the difference between the similarity score of the node and the similarity score of the children. If the gain from a node is found to be minimal then it just stops constructing the tree to a greater depth which can overcome the challenge of overfitting to a great extend. Meanwhile, the Random forest might probably overfit the data if the majority of the trees in the forest are provided with similar samples. If the trees are completely grown ones then the model will collapse once the test data is introduced. Therefore major consideration should be given to distributing all the elementary units of the sample with approximately equal participation to all trees.\\n2. **XG Boost is a good option for unbalanced datasets but we cannot trust random forest in these types of cases.**\\xa0In applications like forgery or fraud detection, the classes will be almost certainly imbalanced where the number of authentic transactions will be huge when compared with unauthentic transactions. In XG Boost, when the model fails to predict the anomaly for the first time, it gives more preferences and weightage to it in the upcoming iterations thereby increasing its ability to predict the class with low participation but we cannot assure that random forest will treat the class imbalance with a proper process.\\n3. **One of the most important differences between XG Boost and Random forest is that the XG boost always gives more importance to functional space when reducing the cost of a model while Random Forest tries to give more preferences to hyperparameters to optimize the model.**\\xa0A small change in the hyperparameter will affect almost all trees in the forest which can alter the prediction. Also, this is not a good approach when we expect test data with so many variations in real-time with a pre-defined mindset of hyperparameters for the whole forest but XG boost hyperparameters are applied to only one tree at the beginning which is expected to adjust itself in an efficient manner when iterations progress. Also, the XG boost needs only a very low number of initial hyperparameters (shrinkage parameter, depth of the tree, number of trees) when compared with the Random forest.\\n4. When the\\xa0**model is encountered with a categorical variable**\\xa0with a different number of classes then there lies a possibility that Random forest may give more preferences to the class with more participation. You can relate this point with point 3.\\n5. XG Boost may more preferable in situations like Poisson regression, rank regression, etc. This is because trees are derived by optimizing an objective function.\\n\\nBy reading this answer, please don’t think that I am an ardent fan of boosting and an enemy bagging. It all depends upon the use of cases and data. There are certain generic situations where random forests perform better than the XG boost like-\\n\\n1. Random forests are easier to tune than Boosting algorithms.\\n2. Random forests easily adapt to distributed computing\\n3. Random forests will not overfit almost certainly if the data is neatly pre-processed and cleaned unless similar samples are repeatedly given to the majority of trees.\\n\\nFor most reasonable cases, xgboost will be significantly slower than a properly parallelized random forest.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fa8b45bb-d49d-4f38-8dcf-4fe27b45622b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**XGBoost (1) & Random Forest (0):**\\n\\n**XGBoost straight away prunes the tree with a score called “Similarity score” before entering into the actual modeling purposes.**\\xa0It considers the “Gain” of a node as the difference between the similarity score of the node and the similarity score of the children. If the gain from a node is found to be minimal then it just stops constructing the tree to a greater depth which can overcome the challenge of overfitting to a great extend. Meanwhile, the Random forest might probably overfit the data if the majority of the trees in the forest are provided with similar samples. If the trees are completely grown ones then the model will collapse once the test data is introduced. Therefore, major consideration is given to distributing all the elementary units of the sample with approximately equal participation to all trees.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='307edf70-017b-4b70-b7e4-6b991906678d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**XGBoost (2) & Random Forest (0):**\\n\\n**XGBoost is a good option for unbalanced datasets but we cannot trust random forest in these types of cases.**\\xa0In applications like forgery or fraud detection, the classes will be almost certainly imbalanced where the number of authentic transactions will be huge when compared with unauthentic transactions. In XGBoost, when the model fails to predict the anomaly for the first time, it gives more preferences and weightage to it in the upcoming iterations thereby increasing its ability to predict the class with low participation; but we cannot assure that random forest will treat the class imbalance with a proper process.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dd358975-2a4f-42c1-a2ff-4d854095dfca', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**XGBoost (3) & Random Forest (0):**\\n\\n**One of the most important differences between XG Boost and Random forest is that the XGBoost always gives more importance to functional space when reducing the cost of a model while Random Forest tries to give more preferences to hyperparameters to optimize the model.**\\xa0A small change in the hyperparameter will affect almost all trees in the forest which can alter the prediction. Also, this is not a good approach when we expect test data with so many variations in real-time with a pre-defined mindset of hyperparameters for the whole forest but XG boost hyperparameters are applied to only one tree at the beginning which is expected to adjust itself in an efficient manner when iterations progress. Also, the XGBoost needs only a very low number of initial hyperparameters (shrinkage parameter, depth of the tree, number of trees) when compared with the Random forest.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='05523d3f-7d97-49e7-b51a-4d2ce3b9329a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**XGBoost (4) & Random Forest (0):**\\n\\n**When the model is encountered with a categorical variable**\\xa0with a different number of classes then there lies a possibility that Random forest may give more preferences to the class with more participation.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3c337855-eff7-48bd-bcb0-2d903d26787e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**XGBoost (5) & Random Forest (0):**\\n\\n**XGBoost may more preferable in situations**\\xa0like Poisson regression, rank regression, etc. This is because trees are derived by optimizing an objective function.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='11b17fa2-2940-4c28-a532-c0f7b1f5ff6f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**XGBoost (5) & Random Forest (1):**\\n\\nRandom forests are\\xa0**easier to tune**\\xa0than Boosting algorithms.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='30a9d076-ac0c-404a-a563-9131b5bc900e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**XGBoost (5) & Random Forest (2):**\\n\\nRandom forests\\xa0**easily adapt to distributed computing**\\xa0than Boosting algorithms.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b89f67a6-d1f8-4e84-814c-2d181d1639e6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**XGBoost (5) & Random Forest (3):**\\n\\nRandom forests will not overfit almost certainly if\\xa0**the data is neatly pre-processed and cleaned**\\xa0unless similar samples are repeatedly given to the majority of trees.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='945c96ce-3ffc-4448-a1f0-d47b9f41aec8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n[[video recommender]]\\n\\n[[feed ranking]]\\n\\n[[Ad click prediction]]\\n\\n[[rental search ranking]]\\n\\n[[estimate delivery time]]\\n\\n[[  Design A Machine Learning Platform  ]]\\n\\n[[  Design Facebook Photo Tagging  ]]\\n\\n[[  Design A Fake News Detector  ]]\\n\\n[[  Design YouTube's Recommendation System  ]]\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b917d16e-ae04-4083-8075-e1ad8783bf19', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nFactorization Machines and matrix factorization\\n\\n[[Collaborative filtering & Recommender system basics]] \\n\\nGBDT+LR排序模型中输入的特征都是稠密的通用特征,记忆能力较差,所以引入支持高稀疏特征的FM模型。\\n计算复杂度为O(N).\\nGBDT+FM上线后,各项效果均比 GBDT+LR提升了4%~6%\\n\\n\"Follow The Regularized Leader\" (FTRL) is an optimization algorithm developed at Google for click-through rate prediction in the early 2010s. It is most suitable for shallow models with large and sparse feature spaces. \\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\nUnlike the classic MF model discussed above which inputs a user-item interaction matrix,\\xa0FM models represent user-item interactions as tuples of real-valued feature vectors and numeric target variables\\n\\n!Untitled\\n\\n!Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='83b24f1e-c942-433d-bcde-19a62305d2cf', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nThere are two kinds of sparsity: data sparsity and model sparsity. Model sparsity can be good because it means that there is a concise explanation for the effect that we are modeling. Data sparsity is usually bad because it means that we are missing information that might be important. That slide is talking about data sparsity.\\n\\nUsing the SVD to compress the matrix gives us dense low-dimensional vectors for each word. This is a way of sharing information between similar words to help deal with the data sparsity. Another thing that people sometimes do to deal with sparsity is to use sub-word units instead of words or to use stemming or lexicalization to reduce the vocabulary size.\\n\\nData sparsity is more of an issue in NLP than in other machine learning fields because we typically deal with large vocabularies where it is impossible to have enough data to actually observe examples of all the things that people can say. There will be many real phrases that we will just never see in the training data.\\n\\nUsing something like word2vec or glove, you solve both of these problems. Word2vec uses a feed forward neural network with a single hidden layer. We will end up using the weights from the hidden layer as the word vector representations. The length of vectors is a hyperparameter, (100, 300, etc.). This way words such as hotel and motel can do vector operations to find the similarity or closeness to another vector. Also, this reduces the size of each word vector from 100,000 from the example above to 300 or 100, depending on the dimension of the word vector set.\\n\\nData sparsity is mostly a\\xa0*computational*\\xa0problem. Think of a recommender system that recommends thousands of products to hundreds of thousands of users, if you stored the data about user-product interaction in a matrix, it would be a huge amount of data consisting of lots of zeros (most users are interested just in a selected subset of products). More wisely, you would store such data using a sparse matrix representation (that records only the non-zeros). This would help with a storage, but still you would need an algorithm that can interact with such sparsely represented data and in many cases this is not offered off-the-shelf.\\n\\nMoreover, since most of the values are zeros, for most samples, they bring relatively little information and if you train your model on such data, then you end up with a huge number of parameters that are useless most of the time. Models with huge number of parameters are problematic on their own, because to estimate the parameters you need huge amounts of data and the optimization algorithms that work well in such settings. So in cases where sparse data is common, like language data where the distribution of words is very skewed, we\\xa0*usually*\\xa0use some kind of dimensionality reduction, like embeddings (this is what word2vec does).\\n\\nData sparsity is one of the most important challenges in data in which each user only rates a small set of items. This problem is critical with increasing dimensions of data. We present an idea based on linear algebra and machine learning to solve this problem. This research applies a framework to cluster users and items in similar groups simultaneously. This method imputes appropriate values for missing data based on similar ratings in each cluster. This has the advantages of more accurate process results in each cluster due to users’ similarity of interests, and the reduction of sparsity negative effect. This approach is represented on 3dimensional data of users, items and times. The experimental results on MovieLense datasets, show that the method can help to overcome data sparsity, and increase the accuracy of prediction.\\n\\nIn this work, we address this by (i) detecting underlying user communities that aggregate similar tastes and (ii) predicting new rela- tions within communities.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1b5cc663-f6df-4190-8b19-d2820e89b8f7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n[[033a autoencoder]] \\n\\n[[034 autoencoder]] \\n\\nLinear feature extraction techniques:\\n\\n- Principal component analysis (PCA)\\n- Non-negative matrix factorization (NMF)\\n- Linear discriminant analysis (LDA)\\n\\nNon-linear feature extraction techniques:\\n\\n- T-distributed stochastic neighbor embedding (t-SNE)\\n- Generalized discriminant analysis (GDA)\\n- Autoencoder\\n- Kernel PCA\\n\\n!Untitled\\n\\nLDA is typically used for multi-class classification. It can also be used as a dimensionality reduction technique. LDA best separates or discriminates (hence the name LDA) training instances by their classes. The major difference between LDA and PCA is that LDA finds a linear combination of input features that optimizes class separability while PCA attempts to find a set of uncorrelated components of maximum variance in a dataset. Another key difference between the two is that PCA is an unsupervised algorithm whereas LDA is a supervised algorithm where it takes class labels into account.\\n\\nThere are some limitations of LDA. To apply LDA, the data should be normally distributed. The dataset should also contain known class labels. The maximum number of components that LDA can find is the number of classes minus 1. If there are only 3 class labels in your dataset, LDA can find only 2 (3–1) components in dimensionality reduction\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cef1c8cc-e63d-4b4b-9efc-a15daf26d7c6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Truncated Singular Value Decomposition (SVD)**\\n\\nThis method performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). It works well with sparse data in which many of the row values are zero. In contrast, PCA works well with dense data. Truncated SVD can also be used with dense data. Another key difference between truncated SVD and PCA is that factorization for SVD is done on the data matrix while factorization for PCA is done on the covariance matrix.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='24cf0c35-b5da-4015-8216-8d25c81e1db1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Feature Selection Methods**\\n\\nPerhaps the most common are so-called feature selection techniques that use scoring or statistical methods to select which features to keep and which features to delete.\\n\\n> … perform feature selection, to remove “irrelevant” features that do not help much with the classification problem.\\n> \\n\\n— Page 86,\\xa0Machine Learning: A Probabilistic Perspective, 2012.\\n\\nTwo main classes of feature selection techniques include wrapper methods and filter methods.\\n\\nFor more on feature selection in general, see the tutorial:\\n\\n- An Introduction to Feature Selection\\n\\nWrapper methods, as the name suggests, wrap a machine learning model, fitting and evaluating the model with different subsets of input features and selecting the subset the results in the best model performance. RFE is an example of a wrapper feature selection method.\\n\\nFilter methods use scoring methods, like correlation between the feature and the target variable, to select a subset of input features that are most predictive. Examples include Pearson’s correlation and Chi-Squared test.\\n\\nFor more on filter-based feature selection methods, see the tutorial:\\n\\n- How to Choose a Feature Selection Method for Machine Learning\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4da5bd76-5c6f-4343-aa9c-61731a2ff2b9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Matrix Factorization**\\n\\nTechniques from linear algebra can be used for dimensionality reduction.\\n\\nSpecifically, matrix factorization methods can be used to reduce a dataset matrix into its constituent parts.\\n\\nExamples include the eigendecomposition and singular value decomposition.\\n\\nFor more on matrix factorization, see the tutorial:\\n\\n- A Gentle Introduction to Matrix Factorization for Machine Learning\\n\\nThe parts can then be ranked and a subset of those parts can be selected that best captures the salient structure of the matrix that can be used to represent the dataset.\\n\\nThe most common method for ranking the components is principal components analysis, or PCA for short.\\n\\n> The most common approach to dimensionality reduction is called principal components analysis or PCA.\\n> \\n\\n— Page 11,\\xa0Machine Learning: A Probabilistic Perspective, 2012.\\n\\nFor more on PCA, see the tutorial:\\n\\n- How to Calculate Principal Component Analysis (PCA) From Scratch in Python\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='62395bd4-6266-4c7f-9033-06b52d684bcf', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Manifold Learning**\\n\\nTechniques from high-dimensionality statistics can also be used for dimensionality reduction.\\n\\n> In mathematics, a projection is a kind of function or mapping that transforms data in some way.\\n> \\n\\n— Page 304,\\xa0Data Mining: Practical Machine Learning Tools and Techniques, 4th edition, 2016.\\n\\nThese techniques are sometimes referred to as “*manifold learning*” and are used to create a low-dimensional projection of high-dimensional data, often for the purposes of data visualization.\\n\\nThe projection is designed to both create a low-dimensional representation of the dataset whilst best preserving the salient structure or relationships in the data.\\n\\nExamples of manifold learning techniques include:\\n\\n- Kohonen Self-Organizing Map (SOM).\\n- Sammons Mapping\\n- Multidimensional Scaling (MDS)\\n- t-distributed Stochastic Neighbor Embedding (t-SNE).\\n\\nThe features in the projection often have little relationship with the original columns, e.g. they do not have column names, which can be confusing to beginners.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fbaa79c6-d886-4c52-bee0-abd47fccfdad', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Autoencoder Methods**\\n\\nDeep learning neural networks can be constructed to perform dimensionality reduction.\\n\\nA popular approach is called autoencoders. This involves framing a self-supervised learning problem where a model must reproduce the input correctly.\\n\\nFor more on self-supervised learning, see the tutorial:\\n\\n- 14 Different Types of Learning in Machine Learning\\n\\nA network model is used that seeks to compress the data flow to a bottleneck layer with far fewer dimensions than the original input data. The part of the model prior to and including the bottleneck is referred to as the encoder, and the part of the model that reads the bottleneck output and reconstructs the input is called the decoder.\\n\\n> An auto-encoder is a kind of unsupervised neural network that is used for dimensionality reduction and feature discovery. More precisely, an auto-encoder is a feedforward neural network that is trained to predict the input itself.\\n> \\n\\n— Page 1000,\\xa0Machine Learning: A Probabilistic Perspective, 2012.\\n\\nAfter training, the decoder is discarded and the output from the bottleneck is used directly as the reduced dimensionality of the input. Inputs transformed by this encoder can then be fed into another model, not necessarily a neural network model.\\n\\n> Deep autoencoders are an effective framework for nonlinear dimensionality reduction. Once such a network has been built, the top-most layer of the encoder, the code layer hc, can be input to a supervised classification procedure.\\n> \\n\\n— Page 448,\\xa0Data Mining: Practical Machine Learning Tools and Techniques, 4th edition, 2016.\\n\\nThe output of the encoder is a type of projection, and like other projection methods, there is no direct relationship to the bottleneck output back to the original input variables, making them challenging to interpret.\\n\\nFor an example of an autoencoder, see the tutorial:\\n\\n- A Gentle Introduction to LSTM Autoencoders', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='59f442e0-2536-4ca7-a36c-19f2646223d5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nTwo stage recommender system\\n\\nf its large scale, funnel approach; reoommendation system: u want to suggest top 10 out of 1000; select top 500 first; maximize recall in this step; next stage want to select top 10; focus on precision, user will definitely select; \\n\\n拿YouTube视频推荐系统举例，一般推荐系统中有两个流程：\\n\\n- 第一步是召回模型，主要是进行初筛操作，从海量视频资源池中初步选择一部分用户可能感兴趣的视频数据子集，从数量上看可能是从千万级别筛选出百级别；\\n- 第二步是精排模型，主要作用是对上面找到的百级别的视频子集进一步精筛，从数量上看可能是从百级别筛选出几十级别。然后根据得分高低排序，生成一个排序列表作为用户的候选播放列表从而完成视频推荐任务。\\n\\n双塔模型的优势，总结如下:\\n\\n- 可以离线计算item的embedding\\n- 线上计算user(&context)的embedding\\n- 线上计算相似度\\n- 实时性好\\n\\n说来说去，主要就是实时性好，cos的表达是有限的，很难提取交叉特征，所以双塔还是比较适用于召回场景。\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n一个依据是精排分数的大小，另一个是多样性；即使用户是篮球迷，也不能全给他放篮球内容\\n\\n!Untitled\\n\\nFor this problem, we have to understand what our dataset consists of before being able to build a model for recommendations. More so we need to understand what a recommendation feed might look like for the user. For example, what we are expecting is that the user could go to a tab or open up a mobile app and then view a list of recommended jobs sorted by the highest recommended at the top.\\n\\nWe can either use an unsupervised or supervised model. For an unsupervised model, we could use the nearest neighbors or a collaborative filtering algorithm based on features from users and jobs. But if we want more accuracy, we would likely go with a supervised classification algorithm.\\n\\nYouTube [9, 40, 42], Linkedin [4], Pinterest [12]. T\\n\\n!Untitled\\n\\n!Untitled\\n\\nIf you’re discussing a recommendation system, the first stage is looking at a large set of items to recommend and narrowing this down. If the universe of possible items to recommend is very large, then it’s not feasible to evaluate each one in real time. You need a heuristic to generate an initial list of candidates. What are some common heuristics?\\n\\n- Items that are ‘close’ to the user in an embedded space\\n- Items that are similar to items the user has interacted with\\n- Items that are globally popular across a large population\\n- Items that friends of friends like\\n- Items that match some type of search criteria\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='28c7897b-e29f-49fa-80ed-422e68f68d5a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nxhs 小红书推荐系统课程\\n\\n矩阵补充（matrix completion），它是一种向量召回通道。矩阵补充的本质是对用户 ID 和物品 ID 做 embedding，并用两个 embedding 向量的內积预估用户对物品的兴趣。值得注意的是，矩阵补充存在诸多缺点，在实践中效果远不及双塔模型\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n如果有几百万内容，都要把这些物品和这个用户做内积，时间太长，所以要用approximate nearest neighbor search的方法\\n\\n!Untitled\\n\\n把物品的数据点划分为不同的区域。每个区域的向量作为key，输入这个key就可以找到该区域内的所有点。\\n\\n这就可以快速做线上召回了。因为把用户的emb 和这些区域向量做相似度对比，就可以确定在啥区域最接近这个用户的偏好；这个区域内比如有几万个物品，计算量就小很多\\n\\nfacebook出的Fiass, pysparnn 可以对高温空间中的相似vector进行相似邻近搜索\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n[[015 entity embedding]] \\n\\n[[sentence embedding]] \\n\\n[[word embedding]] \\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\ncos similarity 就是内积除以他们各自的长度，其实就是对两个向量先做normalization之后再内积\\n\\n!Untitled\\n\\n!Untitled\\n\\n训练的时候用softmax activation and cross entropy loss function; 鼓励正样本cos similarity 尽量大，负样本cos similarity尽量小\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n这种在进入NN之前就把user, product emb融合起来的，叫前期融合结构，和双塔模型的结构有很大区别；召回只能用双塔这样的后期融合模型\\n\\n如果把这种结构用于召回，就要把所有物品的emb都挨个输入模型，预估用户对所有物品的兴趣，计算量不可行，关键是无法用最近邻近似查找来加速计算；所以只能用户排序，从几百个物品里挑几十个来推荐\\n\\n!Untitled\\n\\n物品emb是存储在数据库的；用户emb不存，是在需要用的时候，现计算一个user emb，把他当做query去物品数据库里找数据\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n只根据用户近几小时行为更新用户embedding layer\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n几个常用召回通道\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n双塔模型这类推荐系统，数据来源都很倾向头部商品；长尾商品被选的数量很少，这类商品的物品表征就学得不好，用他来算物品相似度效果不好\\n\\n!Untitled\\n\\n图文内容预训练最牛逼的方法是CLIP\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n[[冷启动 cold start 推荐系统]]\\n\\n****行为序列01：用户历史行为序列建模****\\n\\n!Untitled\\n\\n!Untitled\\n\\n比平均更好的方法是attention，但计算量比较大\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n[[self attention]]', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5cbb58b9-2aaf-444c-a952-328096c9f4c7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n\\n\\n!Untitled\\n\\nearly stopping\\n\\n!Untitled\\n\\nIn general, regularization is adopted universally as simple data models generalize better and are less prone to overfitting. Examples of regularization, included;\\n\\n- K-means: Restricting the segments for avoiding redundant groups.\\n- Neural networks: Confining the complexity (weights) of a model.\\n- Random Forest: Reducing the depth of tree and branches (new features)\\n\\nIn this context, L1 regularization can be helpful in features selection by eradicating the unimportant features, whereas, L2 regularization is not recommended for feature selection.\\n\\nTraditional methods like cross-validation, stepwise regression to handle overfitting and perform feature selection work well with a small set of features but these techniques are a great alternative when we are dealing with a large set of features.\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\nstandardization normalization的区别\\n\\n!Untitled\\n\\n!Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d9f8a121-88fa-4f6d-a918-94baec6184bc', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nVIP Learning to Rank, in Hotel Travel e-commerce space | by Narasimha M | MakeMyTrip-Engineering\\n\\nPersonalized Ranking Model for Lodging | by Debraj G. | Expedia Group Technology | Medium\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n*MRR: Mean Reciprocal Rank； 专注在第一个；*\\n\\n*MAP: Mean Average Precision；给前排的更大权重*\\n\\n*NDCG: Normalized Discounted Cumulative Gain*\\n\\nTo understand NDCG, we need to understand its predecessors:\\xa0**Cumulative Gain(CG)**\\xa0and\\xa0**Discounted Cumulative Gain(DCG).**\\xa0Also, we need to keep the following assumption in mind:\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c8e85770-b17b-4d64-be6b-628250ddbbdc', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**NDCG@k**\\n\\nThis metric asserts the following:\\n\\n1. Very relevant results > somewhat relevant results > irrelevant results (**C**umulative\\xa0**G**ain).\\n2. Relevant results are more useful when they appear earlier in the set of results (**D**iscounting).\\n3. The result of the ranking should be irrelevant to the query performed (**N**ormalization).\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f90489fa-f1fb-4a77-a727-046c921bdd71', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nThe highly relevant documents are more useful than moderately relevant documents, which are in turn more useful than irrelevant documents.\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='45b804b1-d099-4785-b5b6-79fdfca3ca31', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**MRR Pros**\\n\\n- This method is simple to compute and is easy to interpret.\\n- This method puts a high focus on the first relevant element of the list. It is best suited for targeted searches such as users asking for the “best item for me”.\\n- Good for known-item search such as navigational queries or looking for a fact.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='427e5fbd-54b8-4971-8d6d-8c73e7ad6e00', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**MRR Cons**\\n\\n- The MRR metric does not evaluate the rest of the list of recommended items. It focuses on a single item from the list.\\n- It gives a list with a single relevant item just a much weight as a list with many relevant items. It is fine if that is the target of the evaluation.\\n- This might not be a good evaluation metric for users that want a list of related items to browse. The goal of the users might be to compare multiple related items.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f112ecac-e8b8-4194-843d-c95eb5e54351', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**MAP: Average Precision and Mean Average Precision**\\n\\nNext is the MAP metric. Let’s say we have a binary relevance data set. We want to evaluate the whole list of recommended items up to a specific cut-off N. This cut-off was previously incorporated using the Precision@N metric. The P@N decision support metric calculates the fraction of n recommendations that are good. The drawback of this metric is that it does not consider the recommended list as an\\xa0**ordered list**. P@N considers the whole list as a set of items, and treats all the errors in the recommended list equally.\\n\\nThe goal is to cut the error in the first few elements rather than much later in the list. For this, we need a metric that weights the errors accordingly. The goal is to weight heavily the errors at the top of the list. Then gradually decrease the significance of the errors as we go down the lower items in a list.\\n\\nThe Average Prediction (AP) metric tries to approximate this weighting sliding scale. It uses a combination of the precision at successive sub-lists, combined with the change in recall in these sub-lists. The calculation goes as follows:\\n\\n!Untitled\\n\\nFrom the figure above, we see that the Average Precision metric is at the single recommendation list, i.e. user level. Computing the precision through this item means sub-dividing the recommendation list. We examine a new sub-list every time we get a relevant item. Then we calculate the precision on this current sublist. We do this for every sublist until we reach the end of our recommendations. Now that we have a set of precisions, we average them to get the average precision for a single user. Then we get the AP for all users and get the mean average precision.\\n\\nThis is primarily an approximation of the original goal of the AP metric. The AP metric represents the area under the precision-recall curve. We get the precision-recall curve by computing the precision as a function of recall values. In this\\xa0excellent lecture, the concept is expanded in great detail. Very recommended. The overall process is to generate a PR curve for every user recommended list. Then generate an interpolated PR curve, and finally average the interpolated PR curves. This is the process visually:\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0020f708-e05c-4f56-abe8-d8e0df17aafc', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**MAP Pros**\\n\\n- Gives a single metric that represents the complex Area under the Precision-Recall curve. This provides the average precision per list.\\n- Handles the\\xa0**ranking of lists**\\xa0recommended items naturally. This is in contrast to metrics that considering the retrieved items as\\xa0**sets.**\\n- This metric is able to give more weight to errors that happen high up in the recommended lists. Conversely, it gives less weight to errors that happens deeper in the recommended lists. This matches the need to show as many relevant items as possible high up the recommended list.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5aab936f-3f23-45bd-8cad-d9fdacb9c1ed', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**MAP Cons**\\n\\n- This metrics shines for binary (relevant/non-relevant) ratings. However, it is not fit for fine-grained numerical ratings. This metric is unable to extract an error measure from this information.\\n- With fine-grained ratings, for example on a scale from 1 to 5 stars, the evaluation would need first to threshold the ratings to make binary relevancies. One option is to consider only ratings bigger than 4 as relevant. This introduces bias in the evaluation metric because of the manual threshold. Besides, we are throwing away the fine-grained information. This information is in the difference between a 4 and 5 stars ratings, as well as the information in the non-relevant items. Is a 1 star rating really the same as a 3 stars rating?\\n\\nTo deal with these issues the recsys community has come up with another more recent metric. This metric takes into account the fined grained information included in the ratings. Let’s take a look at the Normalized Discounted Cumulative Gain (NDCG) metric.\\n\\n!Untitled\\n\\nBefore the NDCG we had the cumulative gain CG. This represented a basic measure to accumulate the graded relevances. This metric does not take into account the position of the elements in the ranked list. For ranking tasks, we need to increase the relative impact of the position of elements in the ranked list. The standard Discounted Cumulative Gain, DCG, adds a logarithmic reduction factor to penalize the relevance score proportionally to the position of the item. Furthermore, in industrial applications, it is common to see that the relevance scores get a boost to emphasis retrieving relevant documents. This appears in the industrial DCG formula.\\n\\nWe are dealing with dynamic systems. Users will get a variable number of relevant items recommended. This makes the DCG measure not comparable across users. We need to normalize the metric to be between 0 and 1. To this effect, we determine the ideal ranking for a user. Then we use that ranking as the Ideal Discounted Cumulative Gain IDCG. This provides a nice normalization factor. It helps compute the Normalized Discounted Cumulative Gain. As this is a per-user metric, we need to calculate this metric for all users in the test set. Then we average across users to get a single number. This average is then used for comparing recsys systems to each other. To visualize this process, we go through the calculation in the figure below with the predicted and ideal ranking for a single user.\\n\\n!Untitled\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2bcca697-61f5-4650-bf02-e5bd39394eb1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**NDCG Pros**\\n\\n- The primary advantage of the NDCG is that it takes into account the graded relevance values. When they are available in the dataset, the NDCG is a good fit.\\n- Compared to the MAP metric it does a good job at evaluating the position of ranked items. It operates beyond the binary relevant/non-relevant scenario.\\n- The smooth logarithmic discounting factor has a good theoretical basis discussed\\xa0here.\\xa0The authors of that work show that for every pair of substantially different ranking recommender, the NDCG metric is consistently able to determine the better one.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2d08335d-4b0b-4bc6-83f1-bfed48d38208', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**NDCG Cons**\\n\\n- The NDCG has some issues with partial feedback. This happens when we have incomplete ratings. This is case for the majority of recommender systems situations. If we had complete ratings there would be no real task to achieve! In this case, the recsys system owner needs to decide how to impute the missing ratings. Setting the missing values to 0 would mark them as irrelevant items. Other calculated value such as the mean/median rating for a user can also help with this drawback.\\n- Next, the user needs to manually handle the case where the IDCG is equal to zero. This occurs when users have no relevant documents. A strategy here is to set the NDCG to 0 as well.\\n- Another issue is handling NDCG@K. The size of the ranked list returned by the recsys system can be less than K. To handle this we can consider fixed-size result sets and pad the smaller sets with minimum scores.\\n\\nAs I said the primary advantage of the NDCG is that it takes into account the graded relevance values. If your dataset has the right form and you are dealing with graded relevance, then NDCG measure is your go-to metric.\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='47322032-2c2d-48f7-a17d-ee9441ff0fbf', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nHow do machine learning models optimise for learning to rank problems?\\n\\nOptimization functions are important in establishing the relevance scores of items in a ranked list when learning to rank. Optimization functions employed in\\xa0**learning to rank fall into three categories: pointwise, pairwise, and listwise**.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3b2ff817-41e0-4093-b982-0069ed4aac4c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Pointwise**\\n\\nPointwise optimization functions are used to learn the relevance scores of individual items based on a set of attributes.\\xa0**These functions treat each item as an independent instance and do not take into account the order of the items in the list.**\\xa0Pointwise optimization functions are best suited for cases where the ordering of items in a list has little influence on the relevance scores.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0d5ea456-5b0c-4dd8-adb0-7c41469d3721', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Pairwise**\\n\\nPairwise optimization functions learn item relevance scores by taking into account the order of items in the list.\\xa0**The goal of pairwise optimization is to maximise the scores of pairs of items rather than individual items.**\\xa0This enables the model to learn the relative relevance of the list’s contents. RankNet and LambdaRank are two common pairwise optimization methods.\\n\\n**RankNet:**\\xa0RankNet is a neural network-based pairwise optimization function that employs backpropagation to learn the relevance ratings of items. RankNet learns the relevance scores by analysing the relative order of pairs of items and updating the scores as necessary.\\n\\n**LambdaRank:**\\xa0LambdaRank is a pairwise optimization function that learns item relevance ratings using gradient descent.\\xa0**It is based on the concept of maximising the change in the list’s NDCG scores after exchanging the locations of two items.**\\xa0LambdaRank is frequently used in large-scale learning to rank issues where RankNet’s computing cost is prohibitive.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0ede9f96-d8bf-40af-947e-a3242bdc541b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Listwise**\\n\\nListwise optimization functions learn the relevance scores of items by considering the complete list of items.\\xa0**The primary principle underlying listwise optimization is to optimise the scores of the list as a whole, rather than individual items or pairs of items.**\\xa0A popular example of a listwise optimization function is LambdaMart.\\n\\n**LambdaMart:**\\xa0LambdaMart is a listwise optimization function that combines the advantages of pointwise and pairwise optimization.\\xa0**It employs gradient descent to learn item relevance ratings by taking into account both individual relevance scores and the relative ordering of items in the list.**\\xa0Unlike RankNet and LambdaRank, which only analyse the relative order of pairs of items, LambdaMart takes into account the complete list and optimises the NDCG scores of the entire list. As a result, LambdaMart is well suited for large-scale learning to rank situations where the order of items in the list is critical.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='14d1dd4d-663c-4e95-bd68-1da5aa228fd6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\ndiversity recommender 多样化问题\\n\\n[[冷启动 cold start 推荐系统]] \\n\\nThis problem motivated us to think about another ranking objective: diversity. More specifically, a good recommendation should be both relevant (so that eaters can easily find relevant restaurants) and diverse (so that eaters can explore different types of food as well). To this end, we developed a personalized diversification algorithm that takes the eater’s taste profile and restaurant cuisine profile as vector representations of the eater and the restaurant, and optimizes a combined objective of relevance and diversity. Optimization is done in a\\xa0greedy\\n\\xa0way in that the restaurant at each position is determined sequentially. The launch of personalized diversification for Uber Eats leads to a significant lift in business metrics.\\n\\nThe relevance ranking model for a plain list of restaurants outlined above relies heavily on historic data. Without special treatment, a new restaurant on our platform may have a low ranking since it has no impressions and no historic orders. To give new restaurants a fair opportunity to rank high and gather exposure, we used the\\xa0multi-armed bandit\\xa0(MAB) framework.\\n\\nIn this case, we applied the\\xa0upper confidence bound\\xa0(UCB, one of the methodologies of MAB) approach to facilitate the exploration of new restaurants or restaurants with low impressions. We calculate a UCB score for each restaurant based on metrics such as its historic impression number, total click number, and boosting factor. The UCB score, along with other objectives discussed above, decides the ranking order among restaurants. A new restaurant will have a relatively high UCB score initially and hence rank highly, increasing its exposure. As the new restaurant gathers more impressions, the UCB score will smoothly decrease and gradually transfer ranking weight back to other objective scores such as relevance.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='26d8d383-5e9d-4fb7-9e6e-1e4f60fd1be8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nDesign Facebook Photo Tagging\\n\\n***Q:\\xa0Do I need to design the ingestion of these photos into some analysis system?***\\n\\nA:\\xa0Assume that the photos you have access to come into an HDFS cluster from a batch ingestion each day.\\n\\n***Q:\\xa0Do I have to design the system which serves the tag suggestions, or just the model(s)?***\\n\\nA:\\xa0Let's not focus on serving the predictions.\\n\\n***Q:\\xa0Okay, do I need to design the ingestion of these photos into some analytics system?***\\n\\nA:\\xa0Assume that the photos you have access to come into an HDFS cluster from a batch ingestion each day from our PostgreSQL cluster.\\n\\n***Q:\\xa0Can I use pre-trained models?***\\n\\nA:\\xa0Yes, but you have to explain how you'd train and use them, and how they work.\\n\\n***Q:\\xa0Can I assume that we have access to a workforce which can label images?***\\n\\nA:\\xa0Yes, but go into detail on how you'd use this workforce to label images.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2bf8ec25-b8d3-425b-a964-7ed82a90f2cd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\ngather requirements\\n\\n!Untitled\\n\\nAs with any ML design interview question, the first thing that we want to do is gather requirements. We need to figure out what we need to build and what we don't need to build.\\n\\nWe're designing a system which allows users to more easily tag other users in photos they upload by presenting suggestions for which users are in the photo.\\n\\nTo accomplish this, we'll need to do 5 ML-related tasks to implement tag suggestions:\\n\\n- Image and Label Collection\\n- Image Pre-processing\\n- Face Detection\\n- Face Recognition\\n- Performance Measurement\\n\\nWe'll also need to rely on systems which we won't touch on or design:\\n\\n- The website's UI displaying the suggestions\\n- The inference service providing the suggestions to the UI\\n- The HDFS cluster and underlying processing cluster(s)\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d4a79ab2-0531-4926-9148-4eb32ff4fc13', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nImage and Label Collection\\n\\nAssuming we have access to images, we'll need two roughly equal sized samples of labeled images. One sample where each image contains one or more faces and another sample where each image contains no faces. The images with faces in them should contain faces at different scales, contrasts, poses, and facial expressions. To start it should include frontal faces with no occlusions or illumination problems. We'll start with roughly 5000 samples of each image class, faces and no faces.\\n\\nTo get the images labeled we'll need some annotation software which provides images and to a workforce so that the images can be labeled appropriately. The annotation software should provide examples of what frontal face images are and are not. This will provide labels for the images so that we can train a model which detects faces in images.\\n\\nlabeled whether they contain a face\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fabbb6bf-abf5-445c-8d16-448a5bbf01f8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nImage Pre-processing\\n\\nThis stage is relatively simple. We can start by standardizing the images. Here, that means finding the mean and standard deviation of all the pixel values across the entire training set. For each pixel of each image we will subtract the mean and divide by the standard deviation.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='115e945f-b21b-4f58-ad7c-89a267f3a77f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nFace Detection\\n\\nBefore we can perform facial recognition which will enable us to provide tag suggestions for photos, we need to detect the faces in an image.\\n\\nGenerally, we would use a pre-trained model for this. Since we're dealing with only frontal faces, we can use OpenCV's implementation of a cascade classifier. It's a model which uses the Viola-Jones algorithm to detect objects. The algorithm, based on the Viola-Jones framework, uses a cascade of Haar-based features to determine if a subsection of an image contains a face or not.\\n\\nHaar-based features are filters placed on top of an image where the sum of some parts of the image subsection are subtracted from another sum of parts in the subsection. This is done in an effort to detect areas of adjacent darkness and brightness. An example of why this is helpful in detecting faces is that, typically, the area below a face's eyes is brighter than the eyes themselves. Haar features are created from subsections of an image, or window. This window will have effectively every possible Haar feature tried on it of all possible sizes. This includes line features, edge features, and four rectangle features. To speed up the calculations of all these Haar-based features, Viola-Jones works by creating an integral image which stores the cumulative sums of pixel values in the image. This allows for efficient Haar feature calculations. To narrow down to the large number of features generated from the exhaustive search for all Haar features, each independent Haar-based feature will be used in a weak classifier. The weak classifier takes the difference between the sum of the pixels in each Haar subregion and learns the best threshold of that difference according to which threshold produces the best classification rate. A weighted sum of these weak classifiers will form a strong classifier. Adaboost is used to assign weights to each of the weak classifiers based on their ability to independently classify subsections (face or no face) of images correctly. This results in only a small subset of the Haar-based features being used.\\n\\nNext, we will use these selected Haar-based features to detect faces in an image. Each image will be examined with a 24x24 pixel sliding window. The window evaluates each Haar-based feature stage in descending order by the weights determined by Adaboost. A stage contains one or more weak classifiers. If the sliding window does not pass the threshold of a stage, which is determined in training, the sliding window moves on to the next subsection to avoid performing unnecessary evaluations of subsequent stages. If the sliding window passes the threshold of each stage, then a bounding box can be placed around the subsection to indicate a face is in the subsection of the image. A bounding box is a series of points, typically four, to make up the corners of a square which forms a box around the object we're attempting to detect. In our case, this box will surround a face. To handle different scales, or sizes, of faces within an image we'll use an image pyramid. An image pyramid starts with the original image and down samples that image according to neighboring pixels. This downsampled image is then run through the same process as the original image.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5b428406-9689-4457-9e1d-3c693e1791ee', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nFace Recognition\\n\\nNow that we have detected the faces, we want to be able to recognize the faces. For this scenario, we generally want to represent the faces in an embedding space because we likely won't have labeled identities of the faces in the photos when they're uploaded. When someone uploads a picture to Facebook, the photo could have the uploader, friends, family, or even strangers in the photo. If we represent faces in an embedding space then we can eventually label these embeddings with identities as they are given. Here, that means waiting until users manually tag on another their photos or themselves in others' photos.\\n\\nThese embeddings can be obtained from a pre-trained model such as SphereFace. SphereFace is a convolutional neural network with a ResNet architecture. After several convolutions and some max poolings, images of faces are represented as embeddings. These embeddings are optimized with a softmax loss to have more distance between unlike faces and less distance between like faces. After the model is trained, the model can produce embeddings for unseen faces. To differentiate faces, a standard softmax isn't the greatest loss to use because there's a low inter-class variance and high intra-class variance. All that means is that two unlike faces may be smiling, have two eyes, a nose, and a mouth (low variance). However, two like faces, belonging to the same user, could be in different poses, lighting, contrasts, and be making different facial expressions (high variance). We need a loss function that fights this tendency in the data and attempts to explicitly separate unlike faces and compress the distances of like faces. The loss function which SphereFace uses is called angular margin softmax. The general idea of angular margin softmax is to project the embeddings on a sphere (hence SphereFace) and introduce a tunable margin which encourages large angular distances for inter-class faces and smaller angular distances for intra-class faces.\\n\\nWe'd use this trained model to recognize faces from the bounding boxes produced by the face detection algorithm. Let's say someone uploads a new photo to their account. The image would start by going through the face detection process we talked about. Then the user will have the opportunity to click on those bounding boxes produced by the face detection algorithm. Meanwhile, the face subsection will be sent through the pre-trained SphereFace model which will produce a face embedding. This face embedding will be used in a nearest neighbor algorithm. Then, there's a few things which can happen\\n\\n- We haven't seen the face yet, based on a threshold of distance or similarity in reference to faces we've already seen. So in our case, if the cosine similarity is below some threshold, then we won't supply a suggested tag for that bounding box.\\n- We have seen the face one or more times, meaning the threshold is not breached for an unseen face, but the other similar faces we've seen do not yet have an identity. In this case, we also won't return a suggested tag for that bounding box.\\n- The happy case where the KNN returns a series of identified faces. Some predefined rule, for instance the top m items found by the KNN algorithm match, then we'll return the suggested tag. To tune m, we'd do hyperparameter tuning through offline evaluation. If the m threshold isn't met, we can return more than one tag suggestion or none if there is generally no consensus around the top m.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='07bf2743-281a-4dad-92bc-5596b8bf0f3d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nPerformance Measurement\\n\\nAt the model level, it's good to measure the accuracy, precision, recall, and f1 score at ranks 1 through k. Here, the k-th rank is the k-th face in the k-nearest neighbors of face embeddings. At the business level, we're assuming the feature lowers the friction of a user wanting to tag people in their photos. If we can enable more users to be tagged in more photos, it will then show up on their friends' timelines providing more content to users' feeds which encourages users to stay scrolling longer which can likely be related back to a lift in ad revenue. We could verify this through A/B testing.\\n\\nIf we assume there are non-frontal faces, we need a model to perform alignment. This process is sometimes called 'frontalization'. Generally, the goal with side faces, some small occlusions, and maybe images with illumination problems is to find landmarks on the face. We can use a pre-trained Multi-task Cascade Convolution Network (MTCNN). It works by generating and refining candidate bounding boxes with 3 stages of CNNs. It outputs 5 landmarks. The benefit with MTCNN is that it frames face detection and alignment as a joint problem.\\n\\n!Untitled\\n\\n!Untitled\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4b5c9314-71ae-41bf-86a2-afc14db7871c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nDesign A Machine Learning Platform\\n\\n***Q:\\xa0Are we building this mainly for a website and/or app that millions of users interact with daily?***\\n\\nA:\\xa0Yes, there's a website and a corresponding mobile version that millions of people visit every day.\\n\\n***Q:\\xa0I'm assuming that we'll be supporting dozens of teams, tens of thousands of features, and potentially hundreds of models. Is that a correct assumption?***\\n\\nA:\\xa0Yes, that's the right scale.\\n\\n***Q:\\xa0Do we have access to a data lake of all the clickstreams and logs required to create relevant features?***\\n\\nA:\\xa0Yes, there's an HDFS data lake.\\n\\n***Q:\\xa0Along with that sized data, is there also the need for thousands of daily data jobs to create the tens of thousands of features and labels?***\\n\\nA:\\xa0Yes, that's reasonable.\\n\\n***Q:\\xa0Does the platform need to support deep learning models?***\\n\\nA:\\xa0For now, we'll stick to basic models.\\n\\nTo accomplish this we'll need to design 5 components:\\n\\n- Managed Data\\n- Managed Models\\n- Managed Model Hosting\\n- Managed Experiments\\n- Managed Monitoring\\n\\nWe'll need to rely on systems which we won't touch on or design:\\n\\n- HDFS data lake which contains ingested clickstreams and logs\\n- The UI which millions of users interact with daily\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='547a2023-4aeb-4cf9-907d-b2bf6b66763c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nmanaged data\\n\\nWe need to:\\n\\n- Provide a way to create, find, and share features and labels for machine learning\\n- Have close integration with HDFS data lake\\n- Scale to thousands of daily data jobs on petabytes of data which will create features and labels\\n- Provide a way create, view, and update data metadata\\n- Incorporate data quality monitoring\\n- Provide online and offline data access\\n- Enable custom data transforms specific to models\\n\\nData metadata refers to information about the data itself. This can include the data size, the number of rows and columns, who consumes the data, data description, and data schema. As well, metadata can include metrics about the quality of the data including how often the data is updated, the number of missing values and the variance of each column, and finally a total percentage of missing data across all of the columns.\\n\\nArchitecturally, we'll need a dedicated Feature Store which will support Hive queries to transform the relevant raw clickstream/log data ingested from the data lake into features and labels we can use for our models. \\n\\nThe Feature Store Hive queries will also compute the values for the data metadata such as the number or rows and columns for a particular data set. These computed values will be used for data quality monitoring. As well, these hive jobs will join features and labels together to create training examples used to train models. These will be placed in a dedicated Examples Store. The Example Store can be an HDFS cluster or S3 and will be used during model exploration, training, and retraining. \\n\\nAlong with the Examples Store, we'll need a way to serve features in a low-latency way such that predictions can be obtained for models when in production serving real-time traffic. This can be a distributed cache such as Redis or a low-latency database such as Cassandra. We'll call this Online Serving. This setup ensures that the exact same features we train on will be available in production while serving inferences to users. \\n\\nWe also need to provide access points to the Feature Store so users can create, view, and share features and labels. The access points will include programmatic access as well as a simplified UI experience. These access points will provide users the ability to set the frequency at which the features are updated. As well, within the data metadata, users must specify the features and label columns so that the appropriate routing can be done downstream for the Examples Store and the Online Serving of features. \\n\\nThe API access point allows users to explore the data more in depth than what the UI allows. It also enables users to create more complex jobs than the simple joins and aggregations supported in the UI. It's important to note that the Example Store and Online Serving will more than likely not be in sync all of the time. This is because labels will be obtained later based the user behavior which resulted from the features and a model. As soon as those labels are obtained, they will be joined to the features and made available in the Examples Store but there will probably be more up-to-date features since then. \\n\\nFinally, since features and labels may need to be tweaked for some particular model, we need to provide the ability to transform the features within the Example Store and Online Serving. These transforms could include feature scaling, subselections of features, or filling in missing values. We'll call this the Transformer. A transform can be implemented as user-defined functions or within Docker containers which implement a 'transform' function. Since the transformer is associated per model, the specific transformation will be referenced in the model configuration which we'll cover later.\\n\\nWe'll orchestrate the daily jobs of transferring relevant raw data from the data lake to the Feature Store by using Airflow. The periodic Hive jobs which create features and labels from the raw data will also be managed with Airflow. Teams will access the Example Store and Online Serving with an HDFS location and an HTTP endpoint respectively. The Transformer can transform all of the features in the Example Store and Online Serving in batches. We can have manual and algorithmic systems in place to check that the same features and labels aren't being duplicated across different teams. This will reduce redundant storage and processing.\\n\\nhive queries are similar to sql query\\n\\n!Untitled\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a87b72fa-2188-426b-a2cb-8419ad1aa978', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nmanaged models\\n\\nWe need to have:\\n\\n- Consistent representation of models\\n- Guaranteed Model Pipeline consistency between exploration and serving predictions in production\\n- Ways to explore data and model combinations\\n- Support for basic machine learning models\\n- Resources for training, validation, and evaluation\\n\\n!Untitled\\n\\nfeatures in each session won’t be stored in feature store; \\n\\n!Untitled\\n\\n!Untitled\\n\\nautoml can provide feature selection, model selection and hyperparameter tuning automatically; \\n\\n!Untitled\\n\\nTo create a consistent representation of models we'll use the concept of Model Pipelines. The Model Pipeline will contain stages. The primary stage will be model inference. This is the stage in which features are provided to the model in order to get a prediction. Another stage, before the model inference, will provide the method of deserializing the features obtained from the Feature Store into something that the model can actually use. As well, this stage will apply the features not present in the feature store such as country, website path, device, or search terms. These features are based on the session of the user and not kept in the Feature Store. The final stage of the Model Pipeline will execute after the inference is complete and will transform the output of the model into something that is usable. For instance, the model may output a softmax of probabilities and we need to transform that into an actual prediction such as a product ID or video ID.\\n\\nNext, we need to guarantee consistency of the Model Pipeline between model exploration and when moving a Model Pipeline into production. We'll create a Model Repository which will store serialized Model Pipelines. The Model Repository will also manage Model Pipeline metadata which contains:\\n\\n- Example Store location\\n- Feature Store ID\\n- Team Owner and Contact Info\\n- Environment Details\\n- Online Serving Endpoint\\n- Transformer ID\\n- Training configuration\\n- Model ID, description, and performance\\n\\nAs well as Model Pipeline serialization, the Model Repository will provide libraries for deserialization so that the Model Pipelines can be loaded for hosting in production so we can serve predictions. We'll go over hosting in more detail later.\\n\\nNow that we can represent and store models, we need to provide a way for teams to explore data and model combinations. We're going to offer two ways to do this. First, we'll host a simplified point-and-click UI which allows users to select Example Store data and apply it to an assortment of pre-defined models. As a step further, users will also have the option of using AutoML which will perform feature selection, model selection, and hyperparameter without anymore user input. For more control over the modeling experience, we'll provide an advanced interface. This will include an API for the Feature Store allowing users to view and create features in terms of custom Hive queries. The newly created features will be available in the Example Store and Online Serving. As well, a library will be provided so users can create custom Model Pipelines. To take full advantage of the advanced interface, we'll want to provide a common workspace such as Jupyter Notebooks through a JupyterHub server managed by the platform. This will support user-level environments and container images as well as data governance.\\n\\nNext, we'll need to provide teams the ability to incorporate the following machine learning models into their Model Pipelines:\\n\\n- Linear/Logistic Regression\\n- Gradient Boosted Trees\\n- Random Forests\\n- K-means\\n\\nAs well, we need to support both gridsearch and Bayesian hyperparameter tuning. Lastly, we want to provide teams access to the compute resources required for training, validation, and evaluation.\\n\\nArchitecturally, we'll need a dedicated Model Repository can be a implemented with a NoSQL database such as DynamoDB. We also need to provide access points to the Model Repository so teams can create and view Model Pipelines. The access points will include programmatic access as well as a simplified UI experience. As well, we'll need a JupyterHub server which create Jupyter notebook servers for each user. These notebooks will need to interact with the Feature Store, Example Store, and Model Repository. Finally, we'll need a Training Cluster which provides the compute resources for training, validating, and evaluating Model Pipelines. This cluster will be a Hadoop cluster using YARN and Spark.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='62a1a6d0-66fd-4b80-ba09-53aca6d609ee', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nmodel hosting\\n\\n!Untitled\\n\\nTo serve predictions to users in production we need to provide a way to host a team's model. We'll need to support:\\n\\n- Online model hosting\\n- Batch inferences\\n- Model retraining with most recent features and labels\\n- Potentially tens of thousands of requests per second\\n\\nOnline model hosting will enable teams to call an HTTP endpoint to obtain a prediction with low-latency. Batch inferences will enable teams to perform inferences across entire sets of users or products. One example where batch inference makes sense is if a client needed to generate personalized email templates for all users. It wouldn't be practical for that client to call the HTTP endpoint hundreds of millions of times. However, batch inferences can also be useful in cases where clients have strict latency requirements. Since batch inferences pre-compute all of the inferences offline before hand, we can eliminate the latency incurred when executing the Model Pipeline at runtime. We also need to support model retraining to avoid models becoming stale. This happens when the current model parameters are not optimized with respect to the most current features and labels being provided by Online Serving.\\n\\nArchitecturally, we'll need a dedicated Inference Service which can be implemented with several Docker containers behind a load balancer. Upon startup, the containers will call the Model Repository and the load the appropriate models. The container environments will match the environments used during model exploration. For online model hosting, clients will call the Inference Service and provide some identifier for which features to retrieve from Online Serving (such as a user ID). The retrieved features will then be run through the Model Pipeline specified in the request. The endpoint will return the result of the Model Pipeline. In the case that a feature is not available in the Feature Store, such as a the device type, the request must contain the feature. Deployments will be initiated through the Model Repository interface. When a request to promote a Model Pipeline to production, the Deployment Configuration service will be responsible for creating containers with the correct environments. It will then sanity test that the requested Model Pipeline can be executed on the container. As well, it will ensure that the the Online Serving endpoint specified in the Model Pipeline metadata is up and running. This ensures synchronization of the deployment of the Model Pipeline, the container environment in the Inference Service, and the online feature store. Separately, batch inferences will be created with Spark on a Hadoop cluster. The batch inferences will be made available in two ways.\\n\\n- A distributed file system such as HDFS or S3 for scenarios which are not latency sensitive, such as generating personalized email templates\\n- An HTTP endpoint, just as online model hosting, for scenarios with strict latency requirements\\n\\nModel retraining and batch inference calculations can be managed with Airflow.\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='899251bc-e608-4d4f-aa44-dfc888dffa0f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nmanage experiments\\n\\n!Untitled\\n\\nTo enable teams to experiment with different models, we need to cover 4 things:\\n\\n- Support experimentation creation\\n- Support viewing in progress and historical experiments and their results\\n- Enable frequentist/Bayesian A/B testing as well as multi-armed bandits\\n- Support experiment stoppage within minutes\\n\\nExperiments will be created in the Model Repository interface. Past and in-progress experiments can be viewed along with their results in the Model Repository interface as well. Finally, this is also where experiments can be stopped.\\n\\nArchitecturally, we'll need a dedicated Experiment Manager service. This service will be called by the Inference Service when a client requests an inference along with an experiment ID. The Inference Service will call the experiment manager with a user ID and experiment ID to get the treatment allocation for that user in that particular experiment. This treatment will indicate which Model Pipeline should be used when generating a prediction for a user. Now to obtain experiment results, we need to know how the users behaved when given a particular treatment. Based on the users' behavior for each treatment, we'll be able to make a decision on whether to launch a treatment or to keep the current experience in place. To get the user behavior we need the same resources that Managed Monitoring will require. Let's move on to Managed Monitoring and we'll see how they relate.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='758cc675-8100-4b44-8bbd-a52f0051b6b1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nmanage monitoring\\n\\nTo enable teams to monitor the results of their models that are in production, we need to provide 3 things:\\n\\n- Interface to see history of model outputs\\n- Interface to see how customers have responded to the model outputs\\n- Drift Detection\\n\\nArchitecturally, we'll need a dedicated Monitoring Service. This service will be asynchronously called by the Inference Service with the predictions that the Inference Service has been making per Model Pipeline. To see how users behave given a particular prediction, we'll need a access to the data lake which contains the clickstream and service logs. This raw data will include the action a customer took of the customers given some prediction. To relate a prediction with a user response, we can supply a unique identifier for each prediction generated which can then be associated with a user's session. Both the Experiment Manager and the Monitoring Service will use a similar Spark or Hive job to join the user session actions with the predictions generated for that user. Given this joined data, the Experiment Manager will be able to calculate p-values in the case of frequentist A/B testing or update the Beta distributions in the case of multi-armed bandits. The Monitoring Service will be able to display to teams the progress or regressions that the model is having on any metric of interest including business metrics as well as model metrics.\\n\\nSince we have time, how would you incorporate features that need to be updated more frequently than once a day? For instance, let's say we want a feature representing the number of units we have left in stock for a particular product.\\n\\nGenerally, the idea is to use design a streaming architecture for those features such that we can have near real-time values. For our architecture, we can use Spark Streaming. That means that when something comes in from Kafka, Spark Streaming immediately processes it with our specified transformations and then it sends it off to a destination. One of the destinations in our case would be Online Serving. It would update the feature value immediately. Online Serving would also need to make sure the value doesn't get overwritten by an older value when the Feature store attempts to update Online Serving with a potentially less recent value. We could so enforce this by checking timestamps during the ingestion from Feature Store to Online Serving. The other destination would be the Examples Store. A similar process would be followed.\\n\\nWhat if we wanted to add deep learning capabilities to this platform?\\n\\nWe would add whatever libraries we want to support (Tensorflow, PyTorch, or MXNet) to the Jupyter Notebook container images by default. We would also ensure those packages are included in the Training Cluster as well as the Inference Service. For the Training Cluster it will likely be beneficial to add support for training models on GPUs or even multiple GPUs. In the case that we have very large neural networks, we should consider using parameter server topology to implement large-scale distributed training. The Inference Service's online model hosting will likely still use CPUs but will benefit from a larger amount of RAM in the case of large models. Batch inferences will benefit from GPUs.\\n\\n!Untitled\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b6bf87a6-00fd-4f54-b00b-dc5f998b2b49', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\nLearning to Rank, in Hotel Travel e-commerce space | by Narasimha M | MakeMyTrip-Engineering\\n\\n\\n\\nPersonalized Ranking Model for Lodging | by Debraj G. | Expedia Group Technology | Medium\\n\\n\\nBroadly, the ranking scenarios can be grouped into three types.\\n\\n1. New users and logged-off/guest users\\n\\n2. Users who have booked/interacted with MakeMyTrip in previous trip searches.\\n\\n3. Recently interacted/clicked/reviewed hotels.\\n\\nFor the 2nd scenario and 3rd scenarios, multiple AI/ML algorithms are in production which have yielded a 10% to 15% lift in conversion for domestic and international hotel categories, compared to respective baselines.\\n\\nIn scenario 2, for example, collaborate filtering/matrix factorization methods performed quite well (70–80% cases) compared to a similar simpler baseline.\\n\\nIn scenario 3, international hotels, an ensemble of content embedding and click sequence embeddings (graph network model) performed better than collaborative filtering by 10% and 12% (conversion ratio) in android and iOS respectively. We will explain these embedding and graph network models in another blog.\\n\\n\\n\\nFor this topic, consider the 1st scenario. It is a cold start problem when viewed from the perspective of matrix factorization (MF or Neural MF) or collaborative filtering “solutions”. But there is merit in improving ranking as many users come to the platform solely for a single session or browse without logging in. The ranking objective for scenario 1 now translates to, how to leverage user search context, seasonal popularity of hotels, hotel metadata, hotel’s recent performance data (CTR, conversion ratios, number of bookings and clicks), and improve the ranking for new users and logged-off/guest users.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='19c49522-eeac-4a46-a509-35fc795ed74c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**How to define relevance?**\\n\\nThe key purpose of L2R algorithms is to understand “relevance”. The user’s behavior within the app informs us how relevant is a recommendation in the item list (implicit feedback). For example, in hotel LOB, funnel depth indicates user-item interaction quality. We can consider that the deeper user goes into the funnel for a hotel, the more relevant such hotels are for the user’s trip. And, hotel funnel depth has broadly following list of key sections.\\n\\n1. Hotel listing\\n\\n2. Hotel detail\\n\\n3. Detail page, location map view\\n\\n4. Detail page, hotel photos\\n\\n5. Select room and rate plan code, for the hotel\\n\\n6. Review\\n\\n7. Payment page review\\n\\n8. Book the hotel\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ab03c12c-4abe-4c5c-942a-36c8705b3217', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Model Algorithm types**\\n\\nL2R algorithms can be categorized into three classes, based on function approximator “objective” or “likelihood”/“loss function.\\n\\n**1.Pointwise**\\n\\n— — Treat each row in the training dataset as “independent”. Assume error is I.I.D distributed (independent and identically distributed).\\n\\n— — Basic version, could easily capture “importance” based ranking when appropriate features are present.\\n\\n**2.Pair-wise**\\n\\n— — Learn relative importance of hotels/items within each pair, from pairs of hotels as input.\\n\\n— — Each informative pair would contribute equally towards the loss function.\\n\\n**3.List-wise**\\n\\n— — Learn relevance across the entire list of items for a given user-trip-query.\\n\\n— — One of the approaches is to modify loss/objective function to treat pairs as “not equal”. Weight for a pair could be proportional to the expected drop in model metrics if the model gets the order of items wrong in the pair.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fc5b4fe8-7980-462d-8baf-3ea92398a360', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**What offline metrics are appropriate to track?**\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8e4413c0-ca58-4ab0-ac0f-f13d6ed3e48a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Normalized Discounted Cumulative Gain (NDCG)**\\n\\nNDCG summarizes the cumulative effect of the items which were “relevant” in a list of recommendations while discounting for the rank/position in the items in the list. In an ideal world, all relevant items will be positioned at the top of the list, in descending order of relevance.\\n\\nSince the “discounted cumulative” score/gain can be different for one user to another user, based on the number of items interacted and the relevance/depth of interaction, such score should be “normalized with respect to “ideal discounted cumulative” score, which leads to NDCG.\\n\\n**EXAMPLE:**\\xa0Consider a ranking system that returns a hotels list for a given search context in a city. Let’s assume there are only 7 hotels in the city and for now and we will look at the top 5 hotels only. So, p is 5. Result list = [H1, H2, H3 ,H4, H5, H6, H7]. With historical clickstream data we know that relevance of these hotels for current search context are : relevance = [2, 1, 0, 1, 0, 0, 1]\\n\\n**Discounted Cumulative Gain:**\\n\\n!\\n\\n**Ideal Discounted Cumulative Gain:**\\n\\n!\\n\\n!\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='42b8ba76-beea-4162-bd9a-badbf232e6a3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Mean reciprocal rate (MRR)**\\n\\nThe Reciprocal Rank (RR) measure calculates the inverse of the rank at which the most relevant item was retrieved. RR is 1 if a relevant item was retrieved at rank 1, if not it is 0.5 if a relevant document was retrieved at rank 2, and so on. When averaged across search queries, the measure is called the Mean Reciprocal Rank (MRR)\\n\\n!\\n\\nA few other metrics explored by researchers are MAP, Average Precision, Expected Reciprocal Rate, Yandex P-found et. Some have also recommended metrics for dispersion, diversity, and degree of novelty recommendations.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1f1bf386-382c-4b82-a85a-3f416df14207', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Simulated ranking stats**\\n\\nWhile we emphasize NDCG, MRR, and a measure of dispersion as primary metrics, at MakeMyTrip, before deploying a model we also simulate and assess whether key feature distributions among the top 30 recommended items are satisfactory or not (“sanity check”). The key features to check for example could be the ranks of the recommended hotels on booking count, clicks context-specific rank, CTR etc., within the city OR within a category of hotels.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='55032094-f0cb-4548-8755-132d60907764', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Assortment of Model types**\\n\\n!\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ac3954eb-5f5b-4590-bef4-78519ede3b75', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**How to choose ideal relevance values?**\\n\\nWe experimented with multiple relevance sets and assessed which set maximizes product metrics (CTR) and business metrics (conversion ratio). While it is trivial to define monotonically increasing relevance scores with funnel depth, the gap between funnel steps doesn’t have to be uniform.\\n\\nFor example, the gap between booking and review funnel depth could have a huge non-linear contribution to the objective function (2^booking_relevance — 2^review_relevance), which may or may not be ideal.\\n\\nBased on relevance structure, the contribution of features and feature interactions in the function approximator could vary, leading to a very different set of hotel recommendations.\\n\\nFor example, price/discount is a primary driver for converting users from review to book step. If the relevance (modeling assumption) gap is too big between these two, the function approximator learns to be biased to ranking cheaper or hotels with steep discount % in the top, sacrificing specific relevance of hotels for the search context.\\n\\nIn applied data science, NDCG alone will not be sufficient to decide which set or model is appropriate. On many occasions, we observed different performance rank order of models/sets based on offline metrics (such as NDCG, MRR) vs based on online metric impact (%lift in CTR or %lift in conversion ratio). One must be prepared to test multiple models in production and learn which model/relevance-set is appropriate, from offline AND online metric(s) improvement.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a67066c7-ad8a-4484-9657-a5b11bb9f8f7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Model form and loss functions**\\n\\nWe have experimented with pair-wise, list-wise loss functions, with xgBoost, lightGBM, TensorFlow, TFranking library, and custom list MLE losses ( PyTorch).\\n\\nIn addition to testing a few loss functions, we have also experimented with MLP NN, Dense Net, AutoINT, and deep/medium depth neural nets with residual connections.\\n\\n!\\n\\nOf all neural network architectures, NN with residual connections gave better offline results (mild improvement in NDCG in 3rd decimal, substantive lift in MRR metric) compared to xgBoost implementation of listwise learners.\\n\\nHowever, the simulated rank stats were only comparable for lambdaMART/GBDT LambdaRank. As NN structures warranted further iterations, GBDT LambdaRank based function approximator was productized for domestic hotels.\\n\\nA few experiments with the NN model structure were conducted for International hotels, in early 2020.\\n\\n!\\n\\nAutoINT (Automatic feature interaction with self-attention network)\\n\\nWith AutoINT preliminary iterations, the NDCG hardly showed improvement, with higher inference latency. We deprioritized further optimizing/fine-tuning this architecture. Note that, unlike other datasets where AutoINT structure could be more “effective”, out datasets had fewer categorical variables; most of the variables had continuous value distributions.\\n\\nTFranking library group-wise loss yielded comparable NDCG score, but simulated rank statistics were poor compared with GBDT LambdaMART/LambdaRank function approximators.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='05a9f199-5438-425c-b2a7-b5c6112ee17f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**NN parameter initialization experiments**\\n\\nOrthogonal initializations helped stabilize the model iterations and converge with fewer epochs, with no substantive impact on offline metrics in the test dataset (NDCG, MRR).\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='642a02c3-49cc-4c41-8f8f-4375d5e8d8c4', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Feature transformations**\\n\\nWhile using continuous value features, the exact numerical values are slightly less important in tree-based function approximators, as long as their relative ordering is meaningful. But for neural network models, the relative ordering of values alone doesn’t suffice, extreme values in features could cause large gradients shutting down activation functions like ReLU.\\n\\nTo avoid these pitfalls long-tailed features can be transformed and restricted/clipped.\\n\\nMost of the continuous variables took skewed power-law or non-normal gamma or log-normal distributions. For such features, median value is mapped closer to 0 — log(1+value/ (1+median))\\n\\nAlso, to make the inference results be more sensitive to the important features, we apply rank indexing. Bin or Rank each feature (for ex., booking counts, click counts, P15day CTR, conversion ratio, etc) of a hotel w.r.t. all other hotels within the city.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='161b35d8-2685-47b5-9990-da0f16523127', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Selection bias correction (negative sample insertion)**\\n\\nEvery ranking system primes users to engage more with items at the top of the list. Options to sort or filter mitigate this problem but not entirely.\\n\\nHotel impressions will systematically be higher for popular hotels than others. Hence training data has substantial “selection bias”. Training a model with such data creates a “selection bias” gap between “training” and “inference”. The parameters may not generalize enough to score all items/hotels in the city.\\n\\nTo reduce such bias in cold start L2R ranking problems, we generate negative samples/items per QID and inserted them in training data.\\n\\nIn many situations, one cannot generate appropriate “negative samples”. Instead, the selection bias correction techniques such as “Heckman_rank” or “propensity_SVM_rank” or counterfactual methods, as mentioned in the literature, could be explored.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7a901a2c-c554-434d-9191-680507e61584', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Position bias correction**\\n\\nIn addition to selection bias, the training data has click bias in “relevance”/implicit feedback. Users tend to interact/click/review top-ranked hotels though some of them may not be relevant to them.\\n\\nTo weed out position bias correction, we are experimenting, currently, with inverse propensity score weighting and other debiasing methods.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='545a523d-190c-4b60-a17b-397314c89e6e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Business Impact**\\n\\nOverall, we have found ~5% to ~14% lift in conversion ratio (varies across projects, device types, compared to dynamic-yet-rule-based-engine as baseline), CTR, and dispersion of hotel preference clicks.\\n\\nAcross the spectrum of ML projects at MakeMyTrip & GoIbibo, there are a few where online metric improvement did not tally with offline metric improvement. A few such experiments were in learning to rank (L2r) charter too.\\n\\nFor example, L2R experiments with similarity scores (based on user-hotel recent interaction — scenario 3) gave +ve impact on conversion ratio in iOS but not in android traffic (w.r.t. baseline collaborative filtering algorithm). Note that separate models are trained & tuned for iOS and android with respective consumer behavior data.\\n\\nAt the same time, L2R experiments for cold start problem in Goibibo, gave substantive positive impact on conversion ratio (7.5% — 15%), improved CTR in top ranks, and dispersion metric in both android & iOS device users.\\n\\nAlso, the preliminary results of personalized L2R experiments for Domestic Flights have been positive.\\n\\nBased on these insights, the data science teams at MakeMyTrip (& GoIbibo) are working on both model and data-driven ML tasks to bolster learning to rank model performances for all devices users(android, iOS, desktop), for both brands.\\n\\n\\n\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7aa9a511-04d9-419c-9d87-17280681c3c2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Problem formulation**\\n\\nThere are multiple ways to formulate the problem and apply ML algorithms to rank hotels retrieved in a search, for example:\\n\\n1. We can formulate it as a\\xa0_binary classification problem_\\xa0(booked vs. not-booked being the positive and negative classes) and apply ML algorithms for classification to predict a booking score or probability for each hotel retrieved in a search. Then the hotels can be sorted by decreased booking score, ranking the most preferred hotels at the top.\\n\\n2. We can apply\\xa0_learning to rank_\\xa0algorithms where ranks of the relevant hotels are optimized directly based on information retrieval measures, such as the proportion of bookings at the top ranks or\\xa0nDCG\\xa0(normalized discounted cumulative gain).\\n\\n3. We can apply a\\xa0_collaborative filtering_\\xa0approach using users’ hotel ratings or bookings (implicit ratings), then rank the hotels based on the predicted ratings.\\n\\nOf these, we describe the application of the classification approach in this article. The application and results of other methods will be described in future articles.\\n\\nWe’ve developed and deployed a model to predict if a hotel will be booked or not (_binary classification_) based on the historical data features of users, hotels, and current search context. This is a\\xa0point-wise\\xa0ranking, where the final ranking is based on decreasing order of the predicted booking score (a higher score indicates a higher probability of booking). We developed and evaluated using tree-based models, such as RandomForest and Gradient Boosting Trees (GBTs), and based on online A/B testing performance (below), we deployed a RandomForest model.\\n\\nFor model training, we use AWS EMR Spark clusters.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d4b15e09-c303-435f-bf04-8b407265014a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nModel performance: How do we measure performance?\\n\\nSince repeat booking patterns are frequent in corporate travel, the shopping experience of corporate travelers is in general less exploratory and more focused on finding their preferred hotels with efficiency. Therefore, our objective was to improve customer experience (efficiency). The metrics we measure are related to shopping efficiency include (but not limited to):\\n\\n1. Proportions of hotels booked within top 1, 3, or 10 ranks: More selected hotels from the top ranks are more convenient for users.\\n\\n2. Search conversion (Proportion of searches converted to bookings): Fewer searches taken to make a transaction (or higher proportion of searches converting to bookings) is more efficient for users.\\n\\n3. Time needed to make a booking: Less time needed is more efficient for customers.\\n\\nWe’ve tested the RandomForest model online using A/B testing and\\xa0**compared it to a previously used model without personalization**. The model with personalization showed a significant improvement in the above user metrics relative to a gradient boosting tree-based model that was previously developed without personalization using data from Expedia Group’s leisure brands.\\n\\nBookings within top 1 displayed rank: Increase of 7.4% (p < 10e-10)\\n\\nSearch conversion (Proportion of searches converted to bookings): Increase of 1.3% (p < 10e-10)\\n\\nProportion of bookings made within 5 mins: Increase of 1.3% (p < 10e-10).\\n\\nCurrently, about 90% of users’ selections are made within the top 10 ranked properties, and about 75% of selections are made from top 3 ranked properties (see model monitoring section below).\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8e3255dc-d2c3-4865-bc03-7753db3a5fc0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nModel interpretation: Relative importance of features\\n\\n**The importance of the features in the model are discussed below. The five most important features**\\xa0for this model were (in decreasing order):\\n\\n(i) historical self- booking frequency of traveler at the hotel,\\n\\n(ii) historical same-company traveler booking frequency (incl. coworkers),\\n\\n(iii) distance of the hotel from search location,\\n\\n(iv) if the user’s company had negotiated rates at the hotel, and\\n\\n(v) historical overall booking frequency of the hotel.\\n\\nTogether, these accounted for ~90% of the total feature importance. The traveler and company-based personalization features — such as self and co-worker historical booking percentage at the same hotel, as well as the loyalty of the user for specific brands, account for about 80% of the feature importance.\\n\\n!The figure above shows relative importance (scale 0 to 1) of the data features that are used in the RandomForest model. Features are organized according to decreasing importance.\\n\\nHotel sort RandomForest model feature importance plot\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1fe190f6-f935-46e4-a011-be986c297030', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nModel deployment\\n\\nAfter training, we ‘bundle’ the model using the\\xa0MLeap\\xa0library using Scala, which can then be put in deployment in JVMs on AWS VMs running Kubernetes. During online prediction, the model utilizes both live and cached features. Cached features are refreshed on a daily basis.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='987a5efe-64ae-437b-bdad-2a05730e4a6c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Model monitoring**\\n\\nWe have a data pipeline to monitor the performance metrics of the model. An example figure is given below.\\n\\n!The figure shows the percentage of all bookings made within the top 10 displayed ranks for a representative period of 6 months.\\n\\nThe percentage of bookings at top 10 ranks are consistent at >90%.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0d592f15-87ae-486a-ba99-9e55ba9f6338', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nSummary\\n\\nCurrently, Egencia enables the personalized lodging ranking model for all Egencia customers. In cases where there are client negotiated or preferred hotels, we rank the remaining hotels retrieved in a search.\\n\\nPersonalized search ranking with users’ search and booking history at Egencia translates to significant improvement of users’ hotel shopping efficiency metrics.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4f5a790b-e8ea-412d-914d-e9ce09d0fc79', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nDesign YouTube's Recommendation System\\n\\n***Q:\\xa0Do we have to design a way to train the model that we'll use?***\\n\\nA:\\xa0Yes, you should detail how the model that you'll use will be trained.\\n\\n***Q:\\xa0Do we have to design a way for scientists and ML engineers to do model development?***\\n\\nA:\\xa0No, that's out of the scope of this question.\\n\\n***Q:\\xa0Do we need to design the way that our model will be hosted for real-time users?***\\n\\nA:\\xa0Yes, you'll need to cover how live users will receive recommendations from your model.\\n\\n***Q:\\xa0Do we need to have our system perform online evaluations such as A/B tests?***\\n\\nA:\\xa0No, we're only concerned with designing a model—not measuring the model's performance against an existing model.\\n\\n***Q:\\xa0Do we have to support model retraining or online learning?***\\n\\nA:\\xa0No, you can assume that the model doesn't need retraining or online learning.\\n\\n***Q:\\xa0Can we assume that we have access to a cloud-compute-infrastructure platform such as Google Cloud Platform?***\\n\\nA:\\xa0Yes, that's fine to use here.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7a6f906c-6cd5-413f-aeeb-73512d0540a5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nget requirements\\n\\nAs with any ML design interview question, the first thing that we want to do is to gather requirements; we need to figure out what we need to build and what we don't need to build.\\n\\nWe're designing YouTube's recommendation system.\\n\\nTo accomplish this we'll need to design 3 components:\\n\\n- Personalized Recommendation Model\\n- Distributed Training, Tuning, and Evaluation\\n- Model Hosting\\n\\nWe'll need to rely on systems which we won't touch on or design:\\n\\n- Cloud Compute and Storage infrastructure (AWS or GCP)\\n- Retraining or Online Learning\\n- User Interface\\n- Model Development\\n- Online experimentation such as A/B Testing\\n- The transformation of raw data into useable features and labels\\n\\n!Untitled\\n\\nget last 50 videos the user watched; and last 50 historical search queries; \\n\\ntraining into dense emb by the weight matrix; and take average of the 50, into one dense vector; \\n\\n \\n\\ncandidate generaten model 的metrics is MAP\\n\\n!Untitled\\n\\nranking model的其中一个feature可以是上一轮 candidate generate recall funnel 出来的score \\n\\n!Untitled\\n\\nranking model should be evaluated by recall; ???\\n\\n!Untitled\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ffef819b-0a04-41b8-aa6b-30fa59564243', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nmodel architec\\n\\nThe problem we're trying to solve is video recommendations. However, there's typically a surrogate problem that we optimize ML models for. Our surrogate problem here will be trying to predict the next video that a user will watch. Framing the problem this way allows us to use features which include historical interactions a user has had with videos. Labels will then be assigned based on subsequent videos that a user has watched. This way, a model will predict the next video that a user will watch and that will be presented to the user as a recommendation. To do this, we'll need the following:\\n\\n- Candidate Generators\\n- A Ranking Model\\n- Bias Mitigators\\n- Cold-start Mitigators\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9da16219-e4ab-4bba-b92d-3dd8a54169da', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Bias**\\n\\nOne challenge in inferring guest preference from past bookings is that the booking decisions are not solely a function of guest preference. They are also influenced by the position in which the listings are shown in the search results. Attention of users drops monotonically as we go down the list of results, so we can infer that higher ranked listings have a better chance of getting booked solely due to their position. This creates a feedback loop, where listings ranked highly by previous models continue to maintain higher positions in the future, even when they could be misaligned with guest preferences.\\n\\nTo address this bias, we add position as a feature in the DNN. To avoid over reliance on the position feature, we introduce it along with a dropout rate. In this case, we set the position feature to 0 probabilistically 15% of the time during training. This additional information lets the DNN learn the influence of both the position and the quality of the listing on the booking decision of a user. While ranking listings for future users, we then set the input position to 0, effectively leveling the playing field for all listings. Correcting for positional bias led to an increase of +0.7% in bookings in an online A/B test.\\n\\nThe candidate generator is responsible for narrowing millions of possible video recommendations down to hundreds of recommendation candidates. It aims to provide a relevant candidate subset with a high degree of precision (as opposed to recall). This allows us to later use a model to score these candidates with a more detailed representation of users and videos without having to consider millions of videos. This two-stage approach allows us to provide inferences with reasonable latency also allows us to use more than a single candidate generator. The features we use for the candidate generator should include at least the video watch history. Here, we consider a video as watched if the user watched an entire video to completion. To represent the historical watches, we\\'ll use a continuous bag of words (CBOW) embedding. For the input to the CBOW each video will be represented as a one-hot encoding of the million most popular videos and the embedding layer creates a dense representation of the video. The CBOW will consider the previous 50 video watches as input. Since each of these one-hot encoded videos will produce a dense vector after the embedding layer, we\\'ll average the 50 embeddings together to get a single, fixed-length representation of a user\\'s watch history. If a watched video is not present in the list of the one million most popular videos, that video will be assigned to a zero vector instead of being one-hot encoded. In addition to historical watches, we can include historical search queries the user made. We can use a similar approach to historical watches with another CBOW embedding. Each of the previous 50 search queries will be tokenized into unigrams and bigrams. Bigrams will be selected by how frequently unigrams are adjacent to each other. These tokens will be one-hot encoded among the most frequent one million tokens. If a token a user searched is not in the set of the one million most frequent tokens, a vector of zeros will be used to represent that query. Similarly, the 50 search query embeddings will be averaged together to obtain one fixed-length embedding of the user search history. \\n\\nThe label for the model will be the subsequent video watched by the user. Here, that means the label will be the one-hot encoded 51st video the user watched. Since the output of the model is also a one-hot encoded video, it means that we have a multiclass prediction across one million classes. This is called an extreme multiclass classification problem. Calculating these million class probabilities for hundreds of billions of training examples is intractable so we\\'ll have to use candidate sampling. This just means we\\'ll sample a few thousand of the incorrect class outputs as well as the correct class output and only consider the gradient calculation among the sampled classes (instead of all one million classes). Classes will be sampled in proportion to the frequency that the label appears in the training set. As well, we\\'ll need to use importance sampling to account for only updating samples of the class parameters and not all of them. This just means we\\'ll scale the impact each class has on a parameter update in proportion to how much our sampling technique differs from updating the class parameters without sampling. This allows us to speed up training significantly without heavily influencing or some cases, improving the model performance. Architecturally, a maximum of 50 one-hot encoded vectors for each the video and search histories will be fed into separate CBOW embedding layers to produce a 256 dimension embedding. This will feed into fully-connected layer of 2048 units with the ReLU activation function. Each successive layer will halve the number of units until a 256 unit layer remains. This layer represents an embedding for the user. A 256 unit embedding is appended to the network which feeds into a million unit softmax layer which predicts the next video a user will watch. The 256 unit embedding directly preceding the softmax layer represents an unnormalized distribution of the one million video in a 256 dimensional space. This means that the softmax layer just decodes the video embedding into a normalized probability distribution across all one million videos. Once trained, an unseen example is provided to the model. The model will produce a 256 dimension user embedding representing a single user. The dot product of the user embedding and any of the million video embeddings represents the similarity between the user embedding and the video embedding. Theoretically, we can find the dot product for all one million videos and the user embedding. The videos producing the highest resulting dot products can be used as recommendation candidates. We\\'ll talk more about how to do this practically in the Model Hosting section to avoid one million dot product calculations during inference. The offline evaluation metric we\\'ll use for the candidate generator is the mean average precision for the top k candidates (MAP@k) produced by the test set. We can improve the MAP by rejecting candidates based on explicit feedback such as dislikes or implicit feedback like partial watches.\\n\\nThe ranking model is used to score each of the recommendation candidates returned by the candidate generator. Since the candidate generator only provides a few hundred videos to the ranking model, we can use a richer feature set than the candidate generator and still maintain a reasonable amount of latency and cost per recommendation. Finally, the ranking model is useful in the case that we want to use more than one candidate generator. Some of the ranking features will include the same features used by the candidate generator such as a user\\'s previously watched videos. We can still average the CBOW embeddings for the videos watched (and searches) by the user to summarize the user\\'s preference. We can also include specific features of how long it\\'s been since a user watched a video from a particular channel, how long it\\'s been since a user watched a video about a certain topic, how many videos have been impressed on the user, and finally we can even provide the candidate generators score of the item (in this case the distance from the user/video embedding dot product to the candidate video). For these continuous features, scaling significantly impacts model performance. Using normalization ensures each feature lies between 0 and 1. As well, we can provide transformations of the continuous features to the model including the squared feature and the square root of the feature. It\\'s important to note that features obtained from videos (last video watched, last video shown to the user) all use the same embedding to learn a generalized representation of videos. The same is true for search terms. Since we\\'re using far more features in the ranking model than the candidate generator, we should use a fewer number of embedding units such as 32 instead of 256 to speed up the latency of inferences. In addition, we\\'ll also want to incorporate the impressed videos that a user was shown for that particular training example. This impressed video is also sent through the CBOW layer but instead of averaging the resulting embedding along with the video watch history, it will be concatenated alongside the other features we provide to the ranking model. The label will inform the network whether the user clicked on that impression. Labels will simply be binary based on whether or not a user clicked on the impressed video. This impressed video is included in the features as an input to the model. Architecturally, the ranking model will be similar to the candidate generator such that it will also take on a \"tower\" deep neural network. The final layer of the network will use a sigmoid function to output the probability of the user clicking on the candidate video. Each of the candidates will then be sorted based on these click probabilities. A dozen or so videos at the top of this list will be shown to the user as recommendations. The ranker should be evaluated in terms of its recall.\\n\\nAs our model currently stands, it won\\'t account for several biases. We need to make sure that our model mitigates the following:\\n\\n- Model the freshness of videos\\n- Discount popular videos\\n- Limit the positive feedback loop created by the model\\n- Prevent exploitation of the site structure\\n- Prevent highly active users from overinfluencing the loss\\n- Discouraging click-bait\\n\\nThe first bias we should consider is the fact that users favor videos that are new (or fresh). We should add a feature to both the candidate generator as well as the ranking model which represents the days since the video was posted. This allows the model to build an awareness of how fresh a video is. We can even make the feature value negative in the case that an uploader scheduled a post through the UI. This feature will have to be normalized just as the other features are. As well, our model will likely favor more popular videos since most of the features are derived from video watches. If we don\\'t account for some videos being more popular than others, then our model could over exploit popular videos instead of exploring more relevant videos that are less popular. We can mitigate this bias by downsampling videos in the training examples in proportion to their popularity. In line with the trade-off of exploration vs. exploitation, we should be sure to include video watches that weren\\'t a direct result of our recommendation model. We can accomplish this by providing training examples to the network that came from views of videos embedded on other websites. \\n\\nAs well, the site structure can be learned and exploited by the model if search queries are sequentially tied to video views, which we don\\'t want. For instance, if a user searches for something then watches the top video of the search results, then the model would learn that a particular search history is associated strongly with a particular video watch. This will result in the model recommending videos which user has already searched for - which is not a meaningful recommendation. Fortunately, our model already mitigates this by having no sequential relationships between search histories and videos views due to separately averaging each of their embeddings. \\n\\nWe also need to prevent highly active users from over influencing the loss during training. This naturally happens because the training examples will include far more instances from users who use the platform often. If we ensure that each user has the same number of associated training examples in the training set, then each user will be represented the same number of times which will prevent the overinflunce of highly active users. \\n\\nLastly, the ranking model is trained on click through rate (which is whether a user clicked on a recommended video impression). This can lead to biasing the model toward click-bait. Click-bait is defined as a video which is good at attracting clicks (by means of an attractive thumbnail or title) but which doesn\\'t retain the user once they begin watching the video. This often is the result of the video not delivering on the promise impliplied by the title or thumbnail. To reduce the ranking model\\'s bias for favoring click-bait, we can instead weight the loss in terms of how long that particular user watched the video supplied in the input of the model (and therefore impressed on the user). Videos which were impressed on the user but not clicked will receive a unit weight of one to indicate zero watch time for that video. Framing the loss in this way means that the longer a user watched a video, the more influence that example will have on the model. This will correct for the click-bait bias.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d971740d-f9ca-4df7-a9b0-a41af8129dfe', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\ncold start mitigators\\n\\nOne shortcoming of the current model is its inability to deal with new users or and newly posted videos. This is often referred to as the cold-start problem. To support recommendations for new users, we can add features to both the candidate generator and the ranking model which don't depend on previous watches or search history. Typically what's used is user demographics such as the user's age, device, gender identity, and nationality. We'll use those features here which can be required during signup. \\n\\nFor new videos, we can obtain watches as implicit features and dislikes as explicit features by recommending the videos to an uploader's subscribers. This allows the model to incorporate impression and watch features in future training examples and for immediate inference as well.\\n\\n[[冷启动 cold start 推荐系统]] \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cf19d65c-4a9b-4620-80bc-b02cf9d19b6c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\ntraining tuning and evaluation\\n\\nWe need to train, tune, and evaluate models with hundreds of billions of examples. As well, the models will have billions of parameters. To ensure that our system can properly handle models of this caliber, we'll need to implement:\\n\\n- Data Parallelism\\n- Model Parallelism\\n- Checkpointing\\n\\nWe will need to use a distributed training strategy which can also be leveraged for model tuning. We can use asynchronous stochastic gradient descent or a variant optimization technique thereof. We'll use a parameter server architecture here. This means a parameter server cluster will be responsible for maintaining billions of model parameters. Worker nodes will be responsible for fetching parameters from the parameter server as well as obtaining the location of a relevant subset of training examples from an example queue. The example queue maintains a queue of mini- batches to be trained on. To produce these random mini-batches, we need a randomly shuffled view of the training set can and this can be handled by a dedicated service which we'll refer to as the Training Manager. The worker nodes will train the parameters fetched from the parameter server on the training examples referenced by the polled message from the example queue. This architecture can also be used for distributed model tuning and evaluation. Since we selected an asynchronous distributed training architecture, we can trade off parameter staleness with the bandwidth requirements between the parameter server and worker nodes. For example, the workers can train on several mini-batches retrieved from the example queue before communicating with the parameter server to update the global parameter set.\\n\\nAs we mentioned earlier, the model parameters can't fit on a single machine so we'll need each worker node to actually be a cluster of GPU-equipped machines which partition the model across each of the machine's GPUs. This can include techniques like pipelining model parallelism and model partitioning.\\n\\nA machine in a worker cluster can become unresponsive at any time so it will benefit us to have some mechanism of model checkpointing. This means that gradient calculations will be stored outside of the clusters. This way, in the case that a worker machine goes down before it can communicate the parameter update with the parameter server, then a new machine can be spun up and the cluster can pick up where it left off in terms of training.\\n\\n[[model and data parallel training]] \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='df177043-2310-4a08-a053-e9a08c301e8c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nmodel hosting\\n\\nYouTube has over 100MM daily active users each of which can refresh the recommendation home page any number of times. This means we need an efficient to provide roughly one trillion total recommendations per day. This assumes each user on average will look at the recommendation home page consisting of about 10 videos. Each request should be on the order of tens of milliseconds of latency. As well, if a user refreshes the recommendation page, we should ensure that the same videos are not recommended again. The final recommendations will be the result of inferences from two models:\\n\\n- Candidate Generator\\n- Ranker\\n\\nFor candidate generation, we'll take advantage of two strategies. The first is that we can use the dot product of the user embedding and video embedding obtained from the candidate generator network. The higher this dot product, the more similarity there is between the video and the user. Effectively, we want to find the k nearest neighboring videos given a user in terms of their dot product. These k nearest neighbors will be the candidates used for ranking. The problem is that finding the nearest neighbors this way, referred to as maximum inner product search (MIPS) across one million videos can take prohibitively long when serving candidates in real time to users. So, we'll need a way to trade off the accuracy of the nearest neighbors search in favor of a reduced latency. This is commonly referred to as approximate nearest neighbor search. \\n\\n!Untitled\\n\\nThe method we can use here is a variation of locality sensitive hashing called asymmetric locality sensitive hashing (ALSH). The main idea of ALSH is to partition the embeddings by their similarities and then when searching for nearest neighbors, look only at the partition that the input example falls into. This prevents the need to search through all of the embeddings and instead isolates the search to a partition which will contain similar embeddings. There's often a reduction in accuracy compared to an exhaustive search but that tradeoff is accepted because the speed up makes the model usable for real time use. Instead of implementing this from scratch we can use ScaNN which comes from Google. ScaNN is trained by learning partitions for the existing embeddings. To search for the nearest neighbors of a provided input, the embeddings in the top N closest partitions are sent to be scored in terms of an estimated dot product. Finally, the top of those candidates are then rescored by their exact dot products. The top K of those candidates are selected and sent off to be ranked by the ranking model. Here our K can be a few hundred video embeddings. This approximate nearest neighbors search is tuned for a desired MAP@k. We can do that by adjusting the hyperparameters ScaNN. One hyperparameter is the number of partitions to create during training. As the number of partitions grow, the precision improves at the expense of speed. Another influential hyperparameter is the number of partitions to consider for a provided input. For instance, if we only consider the single closest partition to the input embedding, then we only need to evaluate a fraction of the total embeddings. However, since these partitions are not exact representations of locality, it could be the case that other adjacent partitions contain even closer embeddings (and therefore more relevant candidate videos). \\n\\nFinally, the last hyperparameter is how many embeddings we want to rescore based on their estimated distances from the input embedding. If we chose to only rescore 10 embeddings, then the calculation will be far faster than if we choose to rescore the approximate closest 100 embeddings. \\n\\nAfter tuning, we should be able to achieve a precision of roughly 95% all while supporting a few thousand queries per second on a moderately sized machine with 16 cores and 32 GB of memory. We'd maintain several servers behind a load balancer to manage almost 100MM requests per day. We can ensure the same videos are not recommended when a user refreshes the recommendation page by rejecting candidates based on explicit feedback such as dislikes or implicit feedback like not clicking on a particular video impression.\\n\\n!Untitled\\n\\nFor ranking, we'll simply have several machines behind a load balancer which will parallely rank video candidates received from the candidate generator. The candidates will be sorted by their scores according to the ranker's inference and the subset of the highest scoring candidates will be shown to the user in the form of a recommendation on the IU. To speed up inferences, we can purchase machines equipped with enough RAM to fit our entire model and enough cores to run hundreds of inferences in parallel.\\n\\nOne benefit of using a two-stage modelling approach - one for candidate generation and the other for ranking - is that we can use more than one candidate generator. For instance, another way we can generate relevant candidates with high precision is by using something called a co-visitation graph. In this graph, each node represents a video. An edge between two nodes is number of times those videos are viewed within the same user session over the past 24 hours. As well, the edges are normalized by the product of total views of the connected video nodes. To generate candidates, we can select a video node that a user has interacted with either implicitly (watch time) or explicitly (liking or adding to playlist) and finding the adjacent video nodes with the largest edge values. \\n\\nTo avoid recommending videos that are too similar to the original video, we can create a spanning tree around the node which at least some number of edges away from the original video node. These candidates can be included alongside the other candidate generator's candidates to be ranked by the ranking model.\\n\\n!Untitled\\n\\n[[ranking metrics  MAP MRR NDCG]]\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b848d7d3-3e6c-4692-80d2-b2d5f00b9719', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nrental search ranking\\n\\n!Untitled\\n\\n!Untitled\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='22b87bf9-a01d-4d10-bd6c-27ffecfc2700', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Requirements**\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ee69ef47-050c-4d76-9149-427e7252db6a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Training**\\n\\n- Imbalanced data and clear-cut session: An average user might do extensive research before deciding on a booking. As a result, the number of non-booking labels has a higher magnitude than booking labels.\\n- Train/validation data split: Split data by time to mimic production traffic, for example, we can select one specific date to split training and validation data. We then select a few weeks of data before that date as training data and a few days of data after that date as validation data.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d0f8d8de-8987-4d92-aebb-04f1992c7877', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Inference**\\n\\n- Serving: Low latency (50ms - 100ms) for search ranking\\n- Under-predicting for new listings: Brand new listings might not have enough data for the model to estimate likelihood. As a result, the model might end up under-predicting for new listings.\\n\\n!Untitled\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9a1d0833-272d-4ffd-93df-6a209aa69842', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**3. Model**\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7d78107d-b7c9-4293-b77f-7cd2160e73cc', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Feature engineering**\\n\\n- Geolocation of listing (latitude/longitude): Taking raw latitude and raw longitude features is very tough to model as feature distribution is not smooth. One way around this is to take a log of distance from the center of the map for latitude and longitude separately.\\n- Favourite place: store user’s favorite neighborhood place in 2 dimensions grid. For example, users add Pier 39 as their favorite place, we encode this place into a specific cell, then use embedding before training/serving.\\n\\n!Untitled\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5170f401-e4b0-428d-b888-571c7ed50d67', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Training data**\\n\\n- User search history, view history, and bookings. We can start by selecting a period of data: last month, last 6 months, etc., to find the balance between training time and model accuracy.\\n\\n> In practice, we decide the length of training data by running multiple experiments. Each experiment will pick a certain time period to train data. We then compare model accuracy and training time across different experimentations.\\n> \\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n- The user searches for rentals with given queries, i.e., city and time. The Application Server receives the query and sends a request to the Search Service.\\n- Search Service looks up in the indexing database and retrieves a list of rental candidates. It then sends those candidates list to the Ranking Service.\\n- Ranking service uses the Machine Learning model to score each candidate. The score represents how likely it is a user will book a specific rental. The Ranking service returns the list of candidates with their score.\\n- Search Service receives the candidates list with booking probability and uses the probability to sort the candidates. It then returns a list of candidates to the Application server, which, in turn, returns it to the users.\\n\\nAs you can see, we started with a simple design, but this would not scale well for our demands, i.e., 150 million users and 1.25 billion searches per month. We will see how to scale the design in the next section.\\n\\n!Untitled\\n\\n!Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4262ade5-a50a-4b99-b285-c44e996f5a86', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n!Untitled\\n\\n!Untitled\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='68e26f6b-7392-416f-9968-4249be36e2fa', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Candidate generation model**\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='465eaf88-4d6a-4879-8f42-33ec623c55b9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Feature engineering**\\n\\n- Each user has a list of video watches (videos, minutes_watched).\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f949c11a-9963-4b6f-8c50-f7dddc152791', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Training data**\\n\\n- For generating training data, we can make a user-video watch space. We can start by selecting a period of data like last month, last 6 months, etc. This should find a balance between training time and model accuracy.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e9099f3e-b398-4853-b949-693c3977fdb4', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Model**\\n\\n- The candidate generation can be done by\\xa0Matrix factorization#:~:text=Matrix%20factorization%20is%20a%20class,two%20lower%20dimensionality%20rectangular%20matrices.). The purpose of candidate generation is to generate “somewhat” relevant content to users based on their watched history. The candidate list needs to be big enough to capture potential matches for the model to perform well with desired latency.\\n- One solution is to use\\xa0collaborative algorithms\\xa0because the inference time is fast, and it can capture the similarity between user taste in the user-video space.\\n\\n> In practice, for large scale system (Facebook, Google), we don’t use Collaborative Filtering and prefer low latency method to get candidate. One example is to leverage Inverted Index (commonly used in Lucene, Elastic Search). Another powerful technique can be found\\xa0FAISS\\xa0or Google\\xa0ScaNN.\\n> \\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='66685b16-26d2-44b5-b95f-97c4212b55ed', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Challenges**\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0a59ea6a-1ae6-4da5-a193-c922cbb78cee', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Huge data size**\\n\\n- Solution: Pick 1 month or 6 months of recent data.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2eb919f2-22f2-44e2-ad14-c161206537e7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Imbalance data**\\n\\n- Solution: Perform random negative down-sampling.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8f8e48af-cf79-4b6d-9d9a-a614b048f560', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**High availability**\\n\\n- Solution 1: Use model-as-a-service, each model will run in Docker containers.\\n- Solution 2: We can use Kubernetes to auto-scale the number of pods.\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6096fef1-7bd3-4f2f-b492-a455932a1a36', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nDesign A Fake News Detector\\n\\n***Q:\\xa0How are we defining fake news?***\\n\\nA:\\xa0Let's define fake news as news articles that contain intentionally misleading info from what appears to be a reliable source.\\n\\n***Q:\\xa0Who are we designing the system for?***\\n\\nA:\\xa0Social media platforms. They're our primary customers.\\n\\n***Q:\\xa0Why are these social media platforms paying us?***\\n\\nA:\\xa0Our service aims to limit the negative financial, health, and social impacts of misinformation on the platforms.\\n\\n***Q:\\xa0For clarification, is this a reasonable way that one of these platforms would use our service: they call our service with a news article that's been posted on their platform, and depending on our response, they have logic around allowing the post, limiting exposure of the post, or disallowing the post?***\\n\\nA:\\xa0Yes, that sounds reasonable, but our service doesn't tell the platform how they should handle unreliable news articles. In other words, you shouldn't be concerned with that.\\n\\n***Q:\\xa0What do our clients expect us to provide to them through our service?***\\n\\nA:\\xa0Our platform returns a response of either 'reliable' or 'unreliable' depending on the provided news article.\\n\\n***Q:\\xa0I'm assuming that we need to be able to explain why our service returns 'reliable' or 'unreliable'. Is this reasonable?***\\n\\nA:\\xa0Yes, being able to explain our decision is important to both the model development and our clients if they have an inquiry.\\n\\n***Q:\\xa0How will these social media platforms interact with our service?***\\n\\nA:\\xa0Our service is API-based.\\n\\n***Q:\\xa0Do clients call our service with the raw text of articles?***\\n\\nA:\\xa0They can, but they can also just send us the URL of a news article.\\n\\n***Q:\\xa0Do we have to implement a way to extract articles from provided URLs, or can we assume that we're provided with that functionality?***\\n\\nA:\\xa0Yes, we have to implement that extraction.\\n\\n***Q:\\xa0Do clients expect higher latencies if they send us just the URL of an article?***\\n\\nA:\\xa0Yes, and we can also reject requests when we can't extract an article from the provided URL.\\n\\n***Q:\\xa0Do we allow clients to bring their own models or data for our system to operate with?***\\n\\nA:\\xa0No, we only use our proprietary models and data.\\n\\n***Q:\\xa0Do we need to build an ML platform supporting dozens of scientists to rapidly iterate our model development?***\\n\\nA:\\xa0No, I'm more concerned with how your system handles data and hosts a single model.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='106c2ec6-0d9d-4489-a407-0063c43b8292', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nget requirments\\n\\nAs with any ML design interview question, the first thing that we want to do is to gather requirements; we need to figure out what we need to build and what we don't need to build.\\n\\nWe're designing a system which detects fake news for social media platforms.\\n\\nThe system should provide each of the following functionalities:\\n\\n- Scrape news articles and fake news datasets from the web\\n- Wrangle data from various sources into a unified structure\\n- Labelling interface to enable a workforce to label newly scraped articles\\n- Extract linguistic features from the scraped articles\\n- Model support including training, validation, hyper-parameter tuning, and hosting\\n- Online model experimentation\\n- Extensive Logging\\n\\nWe'll need to rely on systems which we won't touch on or design:\\n\\n- Authentication and authorization of our system's API\\n- Cloud infrastructure provider\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='85e7c733-c8a8-422b-987c-4369033d933f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nweb scraper\\n\\nWe need to scrape news articles from the web. This involves handling web pages with:\\n\\n- HTTPS\\n- Javascript\\n- Login prompts\\n- Paywalls\\n- CAPTCHAs\\n\\nThe Web Scraper can be created from scratch with libraries such as Beautiful Soup but that's not recommended. We can instead use 3rd party services like scrapestack or the dedicated article scraper plugin for ScrapeBox. However, there's also open source web scraping libraries available such as Scrapy. Most of these tools will cover our requirements and we'll be able to avoid creating our own custom web scraper. The Web Scaper will be responsible for polling the top 100 news sites in the US (CNN, New York Times, Huffington Post, etc). We need to establish contractual agreements with news sources that don't allow web scraping. The newly scraped articles will be sent to the Data Wrangler.\\n\\nFor the fake news datasets, we can download them directly from their sources (with permission for commercial use) including the Information Security and Object Technology (ISOT) Fake News dataset as well as the Kaggle Fake News dataset. These datasets can be placed in an object store (such as S3) which will be periodically polled by the Data Wrangler to be processed.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e5df73b1-e5af-4b61-a4c1-c8a5edf0e57e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nWrangle data\\n\\nBecause we're using multiple data sources, we need to wrangle the data so that the data is structured in a consistent way for creating features. This includes:\\n\\n- Date and time format manipulations\\n- Changing data types to match a consistent schema\\n- Replacing missing data with NAs\\n- Joining data\\n- Exporting data to a storage layer\\n\\nThese functionalities can be developed from scratch using Pyspark libraries on a Spark cluster. As well, Spark allows us to perform stream processing so we don't have to temporarily store the raw data before we wrangle it. After processing, the articles will be stored. Our storage layer will be a PostgreSQL database where each row will be keyed on an article ID. The rows will contain columns including the article contents and article metadata such as the publication date, publisher, article length, and perhaps the news article author. We'll partition the database tables on the article publication date so we can use date ranges to create data sets to train on. After the Data Wrangler has finished, the articles need to be labeled as either reliable or unreliable.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='19525820-b328-470d-a064-fa2b6648d75c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nLabelling interface to enable a workforce to label newly scraped articles\\n\\nWe'll need to implement a way for a workforce to label the newly scraped articles as either reliable or unreliable. Labelling must be done manually with either an internal workforce or we can outsource the labelling task to a 3rd party service such as Mechanical Turk. Comprehensive label guidelines will have to be defined so that articles are correctly and consistently labeled. An interface will need to be built to display news articles to the labelers along with the labelling guidelines and buttons to allow labelers to label an article as reliable or unreliable. Architecturally, when the Data Wrangler adds an unlabeled article to the database we can have a Change Data Capture mechanism on the database (for PostgreSQL the write-ahead logs are used to track changes to a table) which will add a task to a queue (AWS SQS). When labelers log in to the Labelling Interface, the queue will be polled for an unlabeled article and displayed to the labeler for labelling. The interface can be a simple web app hosted on a lightweight framework such as Flask. The Flask server can directly add labels to the label column in the same PostgreSQL table used by the Data Wrangler for each article.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='46d895f9-53e0-4ab3-b32d-5262a9643a4d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nfeature extractor\\n\\n[[bag of words; tranditional nlp]] \\n\\nIn addition to labelling potentially hundreds of thousands of news articles, we need to extract linguistic features from them so that we can train our model.\\n\\nOur primary source of features will be generated with Linguistic Inquiry and Word Count (LIWC pronounced 'Luke'). We can also use additional features such as the news article publisher or whether or not the article is featured on a site that requires HTTPS.\\n\\nLIWC extracts over 90 features for a provided text. It works by calculating to which degree various categories of features are observed within the given text including:\\n\\n- Summary Language Variables (words per sentence, tone, words with more than 6 letters)\\n- Linguistic Dimensions (total pronouns, prepositions, conjugations)\\n- Psychological Processes (positive/negative emotions, social references, informal language)\\n\\nLIWC generates features by maintaining dictionaries, or lexicons, pertaining to each feature category and counting the occurrences of words in each respective dictionary. Numerical features are then calculated to represent the percentage of the words which were allocated to each category. For instance, if there are 5 conjugations within a 100 word text then the conjugation feature would be 0.05 (or 5%).\\n\\nArchitecturally, when the Data Wrangler adds an article to the database we can have a Change Data Capture component on the database which will add a task to a queue (here RabbitMQ). A cluster of Celery workers will poll tasks from the queue and perform the LIWC feature extraction. Since the lexicons of LIWC are proprietary, we'll need to integrate with either the Receptiviti API or work to get a custom solution of a local LIWC library so we don't have to make an API call for each news article. When a Celery worker is finished extracting features from an article, the features are added to the feature store (here FEAST). FEAST has the ability to generate statistics about the features such as number of missing features, minimum/max/mean/mode/standard deviation of feature values, and also the number of unique values across features. This will allow the system to produce data quality metrics which we can create alarms around so we can readily discover anomalies and data drift. The statistics generated by FEAST are fully compatible with Tensorflow Data Validation (TFDV) which provides libraries to identify anomalies and data drift. The logging and alarms will be implemented in the same way that we implement logging in the Extensive Logging section (LogStash, Elasticsearch, and Kibana).\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cdfb4605-b1fc-4cec-9257-3755dbb6bf45', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nmodel support\\n\\nIn terms of our model, we'll need to support:\\n\\n- Training\\n- Tuning\\n- Evaluation\\n- Hosting\\n- Explainability\\n\\nFor model training, we'll need to split a labeled feature set into a test set and a training set. The training set will be used to train and tune the model (with a subset assigned as the validation set for tuning). The test set will be used to perform offline model evaluation. We'll be using a random forest as our model. We'll start with 128 trees each with a max depth of six. This will make the model small enough to fit in a single machine and the model training should take under 1 hour to train on modern dual core processors which means we don't necessarily need implement model training checkpoints. The average news article length is under 800 words and the average word length in English is 4.7 characters. A UTF-8 character requires at most 4 bytes of memory which means we can assume each article is on average under 15 kilobytes (KB). This means a dataset of two hundred thousand news articles will likely take up 3 gigabytes (GB) of memory. This data can fit in the RAM of commodity hardware which means we don't need to partition our data set into chunks for incremental training. This means we don't have to implement an online learning process. Since the proportion of news articles that are unreliable is so few compared to those that are reliable, we'll need to account for this in our training process so we avoid degraded model performance. There's commonly two techniques used in the case of random forests. \\n\\n[[deal with unbalanced data]] \\n\\nThe first approach to mitigating unbalanced data sets is to use class weighting. To implement class weighting, we simply add a stronger weight for the minority class when calculating the Gini impurity for a split - this usually results in a higher rate of false positives. \\n\\nThe second approach (and a more appropriate method for our use case) is to use Synthetic Minority Oversampling Technique (SMOTE). SMOTE generates synthetic examples within the minority class to be used for training. It does this by randomly selecting a minority example and drawing connecting lines between it and its nearest minority neighbors in the feature space. It then generates minority examples by creating examples which lie on the connecting lines. SMOTE generally produces less false positives than class weighting but models generally benefit from separately trying both methods and picking whichever one performs better.\\n\\nFor model tuning we'll need to take a subset of the training set and create a validation set. This validation set will be used to tune the hyperparameters of the random forest model. These hyperparameters can include max tree depth, number of trees, max number of leaf nodes, minimum impurity decrease for a split, minimum samples in a leaf, minimum samples required to perform a split, and the max number of features to evaluate at each split. To search for the optimal hyperparameters in an efficient way, we'll use bayesian hyperparameter tuning. After tuning is complete, we need to serialize the model and save it to a model store for hosting or for later reference.\\n\\nEvaluating the performance of our random forest model should incorporate metrics in terms of the business as well as the model itself. For business metrics, we can measure the number of complaints clients reach out to us about and perhaps the number of defamation suits filed against news articles our system deems reliable.\\n\\n For model metrics, we need to be careful that we don't rely heavily on binary accuracy. Since our data set is unbalanced, accuracy can present a deceiving degree of high performance. Better metrics like the F1 score and perhaps Cohen's kappa should be used. Cohen's kappa calculates the model performance in comparison to a model which randomly predicts reliable or unreliable in direct proportion to the class imbalance within the dataset. Finally, when evaluating our model, it's important to recognize how much the model helps identify fake news compared to not having the model. To determine this, we can use Bayes’ theorem. Assuming that the prior of fake news is 2% and that our model's true positive rate is 92% and that the false positive rate is 3%, we get a 38% probability of a news article being unreliable given that our model predicts an article as unreliable. Compared to the 18% probability of an article being unreliable given that a human predicts it to be unreliable, our model is roughly twice as good as a human. We can appropriately say that this is a useful model.\\n\\nIn terms of model explainability, we'll use Shapley values. Shapley values estimate the impact that each individual feature has on the overall prediction from our model. This way, when we're asked for an explanation, we can look at the Shapley values of the model and the feature values for the sample in question and see how strongly each feature influenced the prediction.\\n\\nFor model hosting, we'll need to load the tuned model into the hosting service on startup from the model store (S3). As well, since clients are able to send request with just URLs, we'll need to call on the Web Scraper, the Data Wrangler, and the Feature Extractor in order to obtain features from the URL. This means APIs need to be created around these services. Architecturally, we can use a production Flask server to host our API. A cluster of these servers will sit behind a load balancer and be equipped with an NGINX reverse proxy which will send requests to a Gunicorn WSGI which forwards the requests to our Flask server. As well, we'll also need monitoring around our model to detect model drift. Model drift occurs when articles that were at one time correctly classified are now being incorrectly classified due to the changing nature of how news articles evolve. The monitoring and alarms will be implemented in the same way that we implement logging in the Extensive Logging section (LogStash, Elasticsearch, and Kibana). Calling the APIs for the Web Scraper, the Data Wrangler and the Feature Extractor could add significant latency for the client when they provide only a URL.\\n\\nFinally, we'll want to implement a mechanism (guardrails) to override the model behavior in the case that we know things that the model does not. For instance, The Onion is a popular satirical news publisher. With guardrails we can return to the client that articles published by The Onion should not be taken down and therefore should be deemed reliable.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='65417acb-d970-47ff-8bf8-6efb0e50a26b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nonline model experimentation\\n\\nEven though we're only expected to build a single model, we still need to support online model experimentation so we can build our confidence when replacing existing logic. We can build this element ourselves from scratch. This should include p-value calculations for A/B tests or expected improvement calculations for multi-armed bandits. These calculations should be based on a relevant business metric such as client complaints and at least one model metric such as the F1 score. These calculations can be done once we learn the true label of each article in the experiment. For A/B testing, each article within a request will be assigned to a particular treatment of either the model or the existing logic. \\n\\nTo avoid assigning the same article to more than one treatment - which can happen in the case that the same article gets sent to our service more than once - we'll need to generate a unique identifier for each article content. In this case, we can use a checksum. Each request from the client will be accompanied by a request ID which will be used to eventually associate online predictions with true labels for the prediction. This will enable us to perform online evaluations of the model which can be used to compare against the existing logic. True labels will be generated either by a client complaint or by the guidelines established in the Labelling Interface. This means that each article requested by our clients will also have to be enqueued in the Labelling Interface. We can accomplish by using the same Change Data Capture used by Labelling Service except we'll now need to monitor the request ID table. \\n\\nArchitecturally, we will create another table in the PostgreSQL database where each row is keyed on the request ID. Each row will have columns indicating what our model predicted as well as what the true label is eventually determined to be.\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='74d8fdfb-8570-4c3f-bfe9-9bc12987d0fd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n!Untitled\\n\\n!Untitled\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0be30414-8ef9-4b5f-af8d-730919701349', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Requirements**\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='324973c9-90c6-491b-88c5-172e93cfc729', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Training**\\n\\n- During training, we need to handle a large amount of data. For this, the training pipeline should have a high throughput. To achieve this purpose, data can be organized in\\xa0Parquet files\\n- The model should undergo retraining every few hours. Delivery operations are under a dynamic environment with a lot of external factors: traffic, weather conditions, etc. So, it is important for the model to learn and adapt to the new environment. For example, on game day, traffic conditions can get worse in certain areas. Without a retraining model, the current model will consistently underestimate delivery time. Schedulers are responsible for retraining models many times throughout the day.\\n- Balance between overestimation and under-estimation. To help with this, retrain multiple times per day to adapt to market dynamic and traffic conditions.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='606b16e0-54a4-47dc-9a76-d7f8d3ef2f6c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Inference**\\n\\n- For every delivery, the system needs to make real-time estimations as frequently as possible. For simplicity, we can assume we need to make 30 predictions per delivery.\\n- Near real-time update, any changes on status need to go through model scoring as fast as possible, i.e., the restaurant starts preparing meals, the driver starts driving to customers.\\n- Whenever there are changes in delivery, the model runs a new estimate and sends an update to the customer.\\n- Capture near real-time aggregated statistics, i.e., feature pipeline aggregates data from multiple sources (Kafka, database) to reduce latency.\\n- Latency from 100ms to 200ms\\n\\n!Untitled\\n\\n!Untitled\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='90cba9b9-04bd-426a-bca4-d1c66a7439b6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Training data**\\n\\n- We can use historical deliveries for the last 6 months as training data. Historical deliveries include delivery data and actual total delivery time, store data, order data, customers data, location, and parking data.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4e9a1dcc-1b84-415f-a595-64e3525595ad', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Model**\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dd37d740-9936-4d74-9198-afc85569e112', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Gradient Boosted Decision Tree**\\n\\n- Gradient Boosted Decision Tree sample\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n- Feature Store: Provides fast lookup for low latency. A feature store with any key-value storage with high availability like Amazon DynamoDB is a good choice.\\n- Feature pipeline: Reads from Kafka, transforms, and aggregates near real-time statistics. Then, it stores them in feature storage.\\n- Database: Delivery Order database stores historical Orders and Delivery. Data prep is a process to create training data from a database. We can store training data in cloud storage, for example, S3.\\n- We have three services: Status Service, Notification Service, and Estimate Delivery Time service. The first two services handle real-time updates and the Estimate Delivery Time service uses our Machine Learning Model to estimate delivery time.\\n- We have a scheduler that handles and coordinates retraining models multiple times per day. After training, we store the Model in Model Storage.\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='060606cf-38d1-438f-9986-506b5ab033f7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n!Untitled\\n\\n!Untitled\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='eaaa680e-3bd1-408e-abc8-e858d791bbfb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Online metrics**\\n\\n- For non-stationary data, offline metrics are not usually a good indicator of performance. Online metrics need to reflect the level of engagement from users once the model has deployed, i.e.,\\xa0**Conversion rate**\\xa0(ratio of clicks with number of feeds).\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ee443adc-dd2d-403c-8571-cb3e6ba2c3a4', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Requirements**\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0b9a21f0-2453-4a43-b722-7c3d095a4ad8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Training**\\n\\n- We need to handle large volumes of data during training. Ideally, the models are trained in distributed settings. In social network settings, it’s common to have online data distribution shift from offline training data distribution. One way to address this issue is to retrain the models (incrementally) multiple times per day.\\n- Personalization: Support is needed for a high level of personalization since different users have different tastes and styles for consuming their feed.\\n- Data freshness: Avoid showing repetitive feed on the user’s home feed.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d3524352-7497-4e77-9d7a-53676d99ee61', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Inference**\\n\\n- Scalability: The volume of users’ activities are large and the LinkedIn system needs to handle 300 million users.\\n- Latency: When a user goes to LinkedIn, there are multiple pipelines and services that will pull data from multiple sources before feeding activities into the ranking model. All of these steps need to be done within 200ms. As a result, the Feed Ranking needs to return within\\xa0**50ms**.\\n- Data freshness:**Feed Ranking needs to be fully aware of whether or not a user has already seen any particular activity.** Otherwise, seeing repetitive activity will compromise the user experience. Therefore, data pipelines need to run really fast.\\n\\n!Untitled\\n\\n!Untitled\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7f6f17e2-97c4-401e-9f5e-98e556aa974c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Training data**\\n\\nBefore building any ML models we need to collect training data. The goal is to collect data across different types of posts, while simultaneously improving user experience. Below are some of the ways we can collect training data:\\n\\n- Rank by chronicle order: This approach ranks each post in chronological order. Use this approach to collect click/not-click data. The trade-off here is serving bias because of the user’s attention on the first few posts. Also, there is a data sparsity problem because different activities, such as job changes, rarely happen compared to other activities on LinkedIn.\\n- Random serving: This approach ranks post by random order. This may lead to a bad user experience. It also does not help with sparsity, as there is a lack of training data about rare activities.\\n- Use a Feed Ranking algorithm: This would rank the top feeds. Within the top feeds, you would permute randomly. Then, use the clicks for data collection. This approach provides some randomness and is helpful for models to learn and explore more activities.\\n\\nBased on this analysis, we will use an algorithm to generate training data so that we can later train a machine learning model.\\n\\nWe can start to use data for training by selecting a period of data: last month, last 6 months, etc. In practice, we want to find a balance between training time and model accuracy. We also downsample the negative data to handle the imbalanced data.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7cc96d4a-3cc8-47ff-a5a1-0a3ec95e5020', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Model**\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a2a990e1-5b21-4dc9-af04-a94987e9a38e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Selection**\\n\\n- We can use a probabilistic sparse linear classifier (logistic regression). This is a popular method because of the computation efficiency that allows it to work well with sparse features.\\n- With the large volume of data, we need to use distributed training: Logistic Regression in Spark or Alternating Direction Method of Multipliers.\\n- We can also use deep learning in distributed settings. We can start with the fully connected layers with the Sigmoid activation function applied to the final layer. Because the CTR is usually very small (less than 1%), we would need to resample the training data set to make the data less imbalanced. It’s important to leave the validation set and test set intact to have accurate estimations about model performance.\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n- A user visits the LinkedIn homepage and requests an Application Server for feeds. The Application Server sends feed requests to the Feed Service.\\n- Feed Service gets the latest model from Model Repos, gets the correct features from the Feature Store, and all the feeds from the ItemStore. Feed Service will provide features for the Model to get predictions.\\n- The Model returns recommended feeds sorted by click through rate likelihood.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7d46dded-ee6c-49d8-bc73-d321284e3a7c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**6. Scale the design**\\n\\n- Scale out the Feed Service module as it represents both Retrieval Service and Ranking Service. This provides better visualization.\\n- Scale out the Application Server and put the Load Balancer in front of the Application Server to balance load.\\n\\n!Untitled\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5e1a2bd1-705f-4b75-bbb7-0da1451b6064', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**7. Summary**\\n\\n- We learned how to build Machine Learning models to rank feeds. The binary classification model with custom loss function helps the model be less sensitive to background click through rate.\\n- We learned how to create the process to generate training data for the Machine Learning Model.\\n- We learned how to scale training and inference by scaling out the Application Server and Feed Services.\\n- You can also learn more about how companies scale there design\\xa0here.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='40f6d60b-d576-400d-9dfe-67263718447c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n!Untitled\\n\\n!Untitled\\n\\n对广告而言，false pos means the user won’t click but we predict he will, cost is wasting $ on him; false neg means the user will click but we predict he won’t, cost is losing a highly potential buyer; \\n\\n这两者收益的对比：cost of ads vs. profit from the ads; \\n\\nfalse neg seems more costly to me so we try to have high recall and reaonsable precision\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='51671392-6c3f-46f2-b694-b27e455fe452', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Online metrics**\\n\\n- Revenue Lift: Percentage of revenue changes over a period of time. Upon deployment, a new model is deployed on a small percentage of traffic. The key decision is to balance between percentage traffic and the duration of the A/B testing phase.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8665ab83-2ee8-4225-b223-6143282478a1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Requirements**\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='278e952c-34c7-4590-a144-eb2725a751b8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Training**\\n\\n- Imbalance data: The Click Through Rate (CTR) is very small in practice (1%-2%), which makes supervised training difficult. We need a way to train the model that can handle highly imbalanced data.\\n- Retraining frequency: The ability to retrain models many times within one day to capture the data distribution shift in the production environment.\\n- Train/validation data split: To simulate a production system, the training data and validation data is partitioned by time.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='84b20443-cf3f-430d-90dc-cef1fd15dde6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Inference**\\n\\n- Serving: Low latency (50ms - 100ms) for ad prediction.\\n- Latency: Ad requests go through a waterfall model, therefore, recommendation latency for ML model needs to be fast.\\n- Overspent: If the ad serving model repeatedly serves the same ads, it might end up over-spending the campaign budget and publishers lose money.\\n- 可能可以适当重复给相同的广告，但不能给太多次\\n\\n!Untitled\\n\\n!Untitled\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='47d093c5-5f7f-448a-8b62-bf28866a23e6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Training data**\\n\\nBefore building any ML models we need to collect training data. The goal here is to collect data across different types of posts while simultaneously improving the user experience. As you recall from the previous lesson about the waterfall model, we can collect a lot of data about ad clicks. We can use this data for training the Ad Click model.\\n\\nWe can start to use data for training by selecting a period of data: last month, last six months, etc. In practice, we want to find a balance between training time and model accuracy. We also downsample the negative data to handle the imbalanced data.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='63a519ee-105e-416e-8abd-3af298e5ba9e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Model**\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7da97076-266a-45f7-a75b-cf2114f52d0f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Selection**\\n\\n- We can use deep learning in distributed settings. We can start with fully connected layers with the Sigmoid activation function applied to the final layer. Because the CTR is usually very small (less than 1%), we would need to resample the training data set to make the data less imbalanced. It’s important to leave the validation and test sets intact to have accurate estimations about model performance.\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n- Data lake: Store data that is collected from multiple sources, i.e., logs data or event-driven data (Kafka)\\n- Batch data prep: Collections of ETL (Extract, Transform, and Load) jobs that store data in Training data Store.\\n- Batch training jobs organize scheduled jobs as well as on-demand jobs to retrain new models based on training data storage.\\n- Model Store: Distributed storage like S3 to store models.\\n- Ad Candidates: Set of Ad candidates provided by upstream services (refer back to waterfall model).\\n- Stream data prep pipeline: Processes online features and stores features in key-value storage for low latency down-stream processing.\\n- Model Serving: Standalone service that loads different models and provides Ad Click probability.\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n- User visits the homepage and sends an Ad request to the Candidate Generation Service. Candidate Generation Service generates a list of Ads Candidates and sends them to the Aggregator Service.\\n- The Aggregator Service splits the list of candidates and sends it to the Ad Ranking workers to score.\\n- Ad Ranking Service gets the latest model from Model Repos, gets the correct features from the Feature Store, produces ad scores, then returns the list of ads with scores to the Aggregator Service.\\n- The Aggregator Service selects top K ads (For example, k = 10, 100, etc.) and returns to upstream services.\\n\\n!Untitled\\n\\n!Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3b44b187-b49d-4aef-9eeb-c218e10bf9ab', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n冷启动 cold start 推荐系统\\n\\nSolving Cold User problem for Recommendation system using Multi-Armed Bandit\\n\\nTo address this cold start issue, we developed a more accurate way of estimating the engagement data of a new listing rather than simply using a global default value for all new listings. This method considers\\xa0*similar listings,*\\n\\xa0as measured by geographic location and capacity, and aggregates data from those listings to produce a more accurate estimation of how a new listing would perform. These more accurate predictions for new listing engagement resulted in a +14% increase in bookings for new listings and an increase of +0.4% for overall bookings in a controlled, online A/B test.\\n\\n如何扶持新的餐馆：流量倾斜；或者在recommender model loss function里加入对popularity的惩罚项；downsample popular videos in training set; 加入popularity and ranking into the model as a feature to control its effect; \\n\\nour model will likely favor more popular videos since most of the features are derived from video watches. If we don't account for some videos being more popular than others, then our model could over exploit popular videos instead of exploring more relevant videos that are less popular. We can mitigate this bias by downsampling videos in the training examples in proportion to their popularity. In line with the trade-off of exploration vs. exploitation, we should be sure to include video watches that weren't a direct result of our recommendation model. We can accomplish this by providing training examples to the network that came from views of videos embedded on other websites. \\n\\n!Untitled\\n\\n!Untitled\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8b579281-6bfc-4859-9d6d-a90bde486f50', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Approaches (TL;DR)**\\n\\n- Representative based: use subset of items and users that represents the population\\n- Content based: use side information such as text, social networks, etc.\\n- Bandit: consider the exploration vs exploitation tradeoffs in new items.\\n- Deep learning: recent methods that tries to solve some of the issues tackled above but using a black box.\\n\\n!Untitled\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4ab37fe8-a11f-4964-9720-fcbfb62551fe', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Advantages**\\n\\n- More interpretability, because new users can be expressed in terms of few representative items.\\n- If you’re using MF methods already, this can be a simple extension to handle cold start.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8affd2a2-869d-4fa0-b59d-10a66efa407a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Disadvantages**\\n\\n- Need to change UI and front end logic to ask the users to rate the representative items.\\n\\n!Untitled\\n\\n!Untitled\\n\\n1. **User cold-start problems:**\\xa0When there is almost no information available about the user, the user cold-start problem arises.\\n2. **Product cold-start problems:**\\xa0When there is almost no information about the product, the product cold-start problem arises.\\n\\nContent-based filtering is the method that answers this question. Our system first uses the metadata of new products when creating recommendations, while visitor action is secondary for a certain period of time.\\n\\nTo address\\xa0**item\\xa0cold start**\\n, we resort to content-based filtering. This means that recommendations of new products are based on their attributes (product details) for a certain period of time. In this initial phase, user actions are taken into account with less weight.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='67d775d1-2813-4252-b9ad-920c6b43c7b6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Visitor cold start**\\n\\nThe user or visitor cold start simply means that a recommendation engine meets a new visitor for the first time. because there is no user history about her, the system doesn’t know the personal preferences of the user. Getting to know your visitors is crucial in creating a great user experience for them.\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n高曝光的笔记，根据现有推荐系统算法，就能得到曝光\\n\\n用户侧指标和作者侧指标会互相伤害，所以需要平衡\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\nembed 是用户和笔记的交互学习出来的；冷启动的时候这个emb才刚初始化，所以新笔记的id emb 没有什么内容；每篇笔记都有一个自己的id embed\\n\\n!Untitled\\n\\n!Untitled\\n\\nitemCF计算物品相似度不是通过内容，而是通过有多少重叠用户都喜欢他\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n这里的用户emb 是双塔模型训练得到的\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2159e41c-ac6c-47bf-9b93-06ed6bc9c081', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nlightGBM and XGBoost vs. catboost\\n\\n[[007 gradient boost and xgboost]] \\n\\n**LIGHTGBM**\\xa0will ignore missing values during a split, then allocate them to whichever side reduces the loss the most.\\xa0https://github.com/microsoft/LightGBM/issues/2921\\n\\nThere are some options you can set such as usemissing=false, which disables handling for missing values. You can also use the zeroas_missing option to change behavior.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='20a1b91a-1669-4611-baad-fccd09a128c2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**XGBoost and missing values: where the magic happens**\\n\\nOnce a tree structure has been trained it isn’t too hard to also consider the presence of missing values in the test set: it’s enough to attach a default direction to each decision node. If a sample’s feature is missing and a decision node splits on that feature the path takes the default direction of the branch and the path continues. But assigning a default direction to each branch is more complex and it’s probably the most interesting part of the paper.\\n\\nThe already explained\\xa0*split finding*\\xa0algorithm can be tweaked a little to return not only the best split at each step but also the default direction to be assigned to the newly inserted decision node. Given a feature set\\xa0**I,**\\xa0all the possible splits are enumerated, but now the corresponding loss isn’t computed once but twice, one for each default direction the missing values for that feature can take. The best of the two is the best default direction to assign when splitting according to the value\\xa0**j**\\xa0of the feature\\xa0**m**. The best split is still the split that maximises the computed score, but now we have attached a default direction to it.\\n\\nThis algorithm is called\\xa0*Sparsity-aware split finding*\\xa0and it is where a lot of the magic behind XGBoost lies;\\n\\nTo outperform the XGBoost built-in default strategy we need two things:\\n\\n- a distance metric that takes into account missing values (thanks to\\xa0this post\\xa0by AirBnb for the inspiration)\\n\\nThe missing value of a feature is imputed using the median value of said feature of the K closest samples, and in the very specific case of not to find at least one non-missing value in the K retrieved neighbours, the median of the whole column is used.\\n\\nPerformances have been measured via k-fold cross-validation comparing three different imputation strategies:\\n\\n- the default one built-in in the XGBoost algorithm\\n- a simple column-wise median imputation\\n- a KNN as described in the previous paragraph\\n\\n!Untitled\\n\\n!Untitled\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='086b6f89-427c-4598-a389-70668ce98119', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n3 differences:\\n\\n1. tree difference: catb is symetric trees, lgbm grow by leafs can be unbalanced, xbg grow by depth\\n2. way of handling dummy variable:\\n\\ncatboost deal with cat variable by order encoding; \\n\\nlgbm: bin or bucket based \\n\\nxgb: encode features urself and send it to model\\n\\n**Catboost**\\n\\xa0uses a combination of one-hot encoding and an advanced mean encoding. For features with low number of categories, it uses one-hot encoding.\\n\\nCatBoost proposes an inventive method for processing categorical features, based on a well-known preprocessing strategy called\\xa0**target encoding**\\n. In general, the encoded quantity is an estimation of the expected target value in each category of the feature.\\n\\n**LightGBM**\\n\\xa0splits categorical features by partitioning their categories into 2 subsets. The basic idea is to sort the categories according to the training objective at each split. From our experience, this method does not necessarily improve the LightGBM model. It has comparable (and sometimes worse) performance than other methods (for example, target or label encoding).\\n\\n**XGBoost**\\n\\xa0doesn’t have an inbuilt method for categorical features. Encoding (one-hot, target encoding, etc.) should be performed by the user.\\n\\n!Untitled\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cd55f8ec-9460-4a68-865d-1f526eeadb83', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Leaf growth**\\n\\n**Catboost**\\xa0grows a balanced tree. In each level of such a tree, the feature-split pair that brings to the lowest loss (according to a penalty function) is selected and is used for all the level’s nodes. It is possible to change its policy using the\\xa0*grow-policy*\\xa0parameter.\\n\\n**LightGBM**\\xa0uses leaf-wise (best-first) tree growth. It chooses to grow the leaf that minimizes the loss, allowing a growth of an imbalanced tree. Because it doesn’t grow level-wise, but leaf-wise, overfitting can happen when data is small. In these cases, it is important to control the tree depth.\\n\\n**XGboost**\\xa0splits up to the specified\\xa0*max_depth*\\xa0hyperparameter and then starts pruning the tree backwards and removes splits beyond which there is no positive gain. It uses this approach since sometimes a split of no loss reduction may be followed by a split with loss reduction. XGBoost can also perform leaf-wise tree growth (as LightGBM).\\n\\n1. different ways of sampling: lgbm use GOSS; xgb use simple bootstrap \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8a51fd1d-4f7e-4075-8a48-1a98c56ae25e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Catboost**\\n\\nThis is based on gradient boosting and developed by Yandex researchers in 2017. This algorithm focuses on categorical features in a data set, which gave it the name ‘CatBoost’. As categorical features have a discrete set of categories, a popular technique to handle them in boosted trees is one hot encoding, where a new binary feature is added to the existing feature in each category. However, with high cardinality features, this technique may lead to an infeasibly large number of features.\\n\\n·\\xa0**Ordered boosting and ordered TS (Target Statistics)**\\n\\nThe Catboost Algorithm has an ordering principal that stops target leakage and outperforms other gradient boosting techniques. In this algorithm, ordered target statistics encoding is developed from target mean encoding. Ordered boosting removes the prediction shift problem which is resulted from a difference in the distribution of train and test data sets.\\n\\nFeatures with fewer categories use one-hot encoding. The maximum number of categories for one-hot encoding can be controlled by the\\xa0*one_hot_max_size*parameter. For the remaining categorical columns, CatBoost uses an efficient method of encoding, similar to mean encoding but with an additional mechanism aimed at reducing overfitting.\\n\\nIf the data set is significantly biased with the number of categorical features, the ‘explainability’ of the model should also be biased with tunable categorical features relative to the numerical features, which leads to the algorithm to provide promising results.\\n\\n- **In XGBoost, trees grow depth-wise while in LightGBM, trees grow leaf-wise which is the fundamental difference between the two frameworks.**\\n- XGBoost is backed by the volume of its users that results in enriched literature in the form of documentation and resolutions to issues. While LightGBM is yet to reach such a level of documentation.\\n- Both the algorithms perform similarly in terms of model performance but LightGBM training happens within a fraction of the time required by XGBoost.\\n- Fast training in LightGBM makes it the go-to choice for machine learning experiments.\\n- XGBoost requires a lot of resources to train on large amounts of data which makes it an accessible option for most enterprises while LightGBM is lightweight and can be used on modest hardware.\\n- LightGBM provides the option for passing feature names that are to be treated as categories and handles this issue with ease by splitting on equality.\\n- H2O’s implementation of XGBoost provides the above feature as well which is not yet provided by XGBoost’s original library.\\n- Hyperparameter tuning is extremely important in both algorithms.\\n\\n!Untitled\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='df82359d-f599-4897-917c-8c79356a0643', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**XGBoost parameters**\\n\\nHere are the most important XGBoost parameters:\\n\\n1. ***n_estimators [default 100] –***\\xa0Number of trees in the ensemble. A higher value means more weak learners contribute towards the final output but increasing it significantly slows down the training time.\\xa0******\\n2. ***max_depth [default 3] –***\\xa0This parameter decides the complexity of the algorithm. The lesser the value assigned, the lower is the ability for the algorithm to pick up most patterns (underfitting). A large value can make the model too complex\\xa0******and pick patterns that do not generalize well (overfitting).\\n3. ***min_child_weight [default 1] –***\\xa0\\xa0We know that an extremely deep tree can deliver poor performance due to overfitting. The min_child_weight parameter aims to regularise by limiting the depth of a tree. So, the higher the value of this parameter, the lower are the chances of the model overfitting on the training data.\\n4. ***learning_rate/ eta [default 0.3] –***\\xa0The rate of learning of the model is inversely proportional to the accuracy of the model. Lowering the learning rate, although slower to train, improves the ability of the model to look for pattergns and learn them. If the value is too low then it raises difficulty in the model to converge.\\n5. ***gamma/ min_split_loss [default 0] –***\\xa0This is a regularization parameter that can range from 0 to infinity. Higher the value, higher is the strength of regularization, lower are the chances of overfitting (but can underfit if it’s too large). Hence, this parameter varies across all types of datasets.\\n6. ***colsample_bytree [default 1.0] –***\\xa0This parameter instructs the algorithm on the fraction of the total number of features/ predictors to be used for a tree during training. This means that every tree might use a different set of features for prediction\\xa0******and hence reduce the chances of overfitting and also improve the speed of training as not all the features are being used in every tree. The value ranges from 0 to 1.\\n7. ***subsample [default 1.0] –***\\xa0Similar to colsample_bytree, the subsample parameter instructs the algorithm on the fraction of the total number of instances to be used for a tree during training. This also reduces the chances of overfitting and improves training time.\\n\\nFind more parameters\\xa0here.\\n\\n!Untitled\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a3c72294-5aa0-422a-a133-db8bbefb57e7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**LightGBM parameters**\\n\\nHere are the most important LightGBM parameters:\\n\\n1. ***max_depth –***\\xa0Similar to XGBoost, this parameter instructs the trees to not grow beyond the specified depth. A higher value increases the chances for the model to overfit.\\n2. ***num_leaves –***\\xa0This parameter is very important in terms of controlling the complexity of the tree. The value should be less than 2^(max_depth) as a leaf-wise tree is much deeper than a depth-wise tree for a set number of leaves. Hence, a higher value can induce overfitting.\\n3. ***min_data_in_leaf –***\\xa0The parameter is used for controlling overfitting. A higher value can stop the tree from growing too deep but can also lead the algorithm to learn less (underfitting). According to the LightGBM’s official documentation, as a best practice, it should be set to the order of hundreds or thousands.\\n4. ***feature_fraction –***\\xa0Similar to\\xa0*colsample_bytree*\\xa0in XGBoost\\n5. ***bagging_fraction –***\\xa0Similar to\\xa0*subsample*\\xa0in XGBoost\\n\\n- **num_leaves**\\xa0: This parameter is used to set the number of leaves to be formed in a tree. Theoretically relation between num_leaves and max_depth is num_leaves= 2^(max_depth). However, this is not a good estimate in case of Light GBM since splitting takes place leaf wise rather than depth wise. Hence num_leaves set must be smaller than 2^(max_depth) otherwise it may lead to overfitting. Light GBM does not have a direct relation between num_leaves and max_depth and hence the two must not be linked with each other.\\n- **min_data_in_leaf**\\xa0: It is also one of the important parameters in dealing with overfitting. Setting its value smaller may cause overfitting and hence must be set accordingly. Its value should be hundreds to thousands of large datasets.\\n- **max_depth**: It specifies the maximum depth or level up to which tree can grow.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='737606b9-275c-4a5a-8949-322b50adb232', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nxhs 小红书的nlp课\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n把one hot encoding映射到低维，具体是把one hot向量e 乘以参数矩阵PT，d是词向量的维度是我们定义的，v is the N of vocab\\n\\n!Untitled\\n\\nPT的每一列，OR P的每一行就是一个词向量；这里的d = 2；具体操作的时候，d的值可以通过cross validation选出来\\n\\n!Untitled\\n\\n参数矩阵P的大小就是vocab * emb_dim，vocab越大parameter越多；每条评论有20个词。每个词用8维向量表示\\n\\n!Untitled\\n\\n!Untitled\\n\\nh0包含了the的信息，h1包含了the cat的信息，所以ht包含了整句话的信息\\n\\nA是参数矩阵，只有一个\\n\\n!Untitled\\n\\n算矩阵A和向量的乘积，用tanh激活函数，得到ht; ht 是 ht-1和xt的函数\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n这里的32word emb维度是自己设置的，可以用cross validation来确认；\\n\\n一般只需要输出ht就够了，他包含了整句话的信息\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n[[011 how to choose optimizers]] \\n\\n如果用h1 - ht所有state, 就需要加一个flattern层把状态矩阵变成一个vector\\n\\n!Untitled\\n\\n!Untitled\\n\\n500个词，所以有500个状态向量h，每个都是32维\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n理论上说改变x1, 后面的h应该都变化，但其实没啥变化，导数都接近0\\n\\n!Untitled\\n\\nRNN只有一个参数矩阵，LSTM有四个；\\n\\n!Untitled\\n\\n!Untitled\\n\\nsigmoind 把向量都压到0-1；\\n\\n!Untitled\\n\\nwf是backprop学到的；\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\nRNN模型与NLP应用(4/9)：LSTM模型\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n这里用dropout发现没有改善，是因为overfit不是由lstm层造成的，是embed层造成的，emb层参数特别多有320000 见上图, 这是因为vocab size * dim = 10000 * 32\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n第一层lstm的输出会成为第二层的输入，所以return sequence 必须为true\\n\\n!Untitled\\n\\nemb层参数太多有overfit，没有足够的数据把他训练好，再多的lstm层都没用；\\n\\nht能记住右边的词，ht’能记住左边的词，所以双向的一般效果都不错\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\npretrain task 和我的task越相近越好; pretrain data 的vocab要和我的任务相近\\n\\n!Untitled\\n\\npretrain 后上面的A ， h层全都丢掉，只保留emb层；train new task时候把emb层freeze\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n红色的有len=40, 步长=3，说明下一轮要移动3个字符\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n这里是字母层次的，所以one hot encoding之后顶多也就几十维，这里是57，不需要做embedding降维；\\n\\n每个segment成了60 * 57的矩阵；总共有大概二十万的traning data\\n\\n!Untitled\\n\\n文本生成只能有单向模型，不能用双向的\\n\\n!Untitled\\n\\n!Untitled\\n\\nx_input是上文，60个letter; \\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n大的概率会变大，小的概率会变小\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\nv 是vocab size; l is segment length; \\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\nh, c 包含了前面的英语句子的信息，\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n把英语直接依照h c输出翻译成英语，这样的话decoder train data多了一倍，encoder可以训练得更好\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n计算第i个状态 hi 和decoder当前状态s0的相关性，记为weight； \\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\nc0 对应s0; \\n\\n!Untitled\\n\\n如果不用attention, 传统rnn是如何更新s0 到s1的; attention 里就要用到c0来更新，c0是encoder里所有状态的加权平均，所以RNN遗忘的问题就解决了；\\n\\n更新c1要算h与s1的相关性；要算新一轮的alpha weight\\n\\ndecoder里的当前状态s和encoder里的所有hidden状态h做对比align 操作，计算相关性，把算出的数字alpha作为权重\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bad36fd6-3c15-4f06-98a3-bafc3f6aaf16', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nself attention\\n\\n!Untitled\\n\\nh0是全0向量可以忽略；\\n\\n先算出h，再拿当前h算出他和已有向量h做对比，算出所有权重alpha weight，对前面的所有h做掐权平均，结果就是新的context vector c\\n\\n每一轮更新状态h之前，都用context vector看一遍之前的所有状态，这样就不会遗忘之前的信息\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\nencoder里两个参数矩阵，decoder里是Wq; \\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n计算一个c，需要用到所有的V, K和一个q 向量；c2依赖于x’2, 和所有encoder里的x ，所以c2知道encoder里所有的信息\\n\\nc2通过softmax之后生成一个概率分布p2，就知道第三个词x’3，作为第三轮的输入\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\nc2 和所有x都相关\\n\\n!Untitled\\n\\n[[self attention]] \\n\\nalpha 1依赖于q1 和所有的K向量\\n\\n!Untitled\\n\\n!Untitled\\n\\nc1依赖于alpha1和所有的V\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n堆叠起来的C向量变得更大\\n\\n!Untitled\\n\\n!Untitled\\n\\n这里dense全连接层都是一样的，相同的参数Wu\\n\\n改变任何一个x, u2 都会变化\\n\\n!Untitled\\n\\n!Untitled\\n\\n每个block里有两层，包括一个self attention和一层dense layer\\n\\n!Untitled\\n\\n!Untitled\\n\\nu1 - um是encoder层的输入；\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\ndecoder有三层，self attention, attention，和最后的全连接层\\n\\n!Untitled\\n\\nm是输入句子长度，t是输入长度\\n\\n最终输入t个向量，每个都是512维度\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\nBERT本质是一种预训练transformer encoder的方法\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n训练时我们希望输出的p向量和cat的one hot向量接近\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n一方面让相邻两句话的词向量出现一些关联；另一方面让transformer attention层里的权重函数可以根据两个句子之间的关联调整\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\nThe idea behind multi-head attention is to allow the attention function to extract information from different representation subspaces, which would otherwise be impossible with a single attention head.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='efdf7c30-c8c3-4d7e-b228-eb33d9b21cab', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Transformers**\\n\\nTransformers (Attention is all you need) were introduced in the context of machine translation with the purpose to avoid recursion in order to allow parallel computation (to reduce training time) and also to reduce drops in performance due to long dependencies. The main characteristics are:\\n\\n- **Non sequential**: sentences are processed as a whole rather than word by word.\\n- **Self Attention**: this is the newly introduced \\'unit\\' used to compute similarity scores between words in a sentence.\\n- **Positional embeddings**: another innovation introduced to replace recurrence. The idea is to use fixed or learned weights which encode information related to a specific position of a token in a sentence.\\n\\nThe first point is the main reason why transformer do not suffer from long dependency issues. The original transformers do not rely on past hidden states to capture dependencies with previous words. They instead process a sentence as a whole. That is why there is no risk to lose (or \"forget\") past information. Moreover, multi-head attention and positional embeddings both provide information about the relationship between different words.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='de39c113-e553-413c-be26-cbd3db7e0289', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n**RNN / LSTM**\\n\\nRecurrent neural networks and Long-short term memory models, for what concerns this question, are almost identical in their core properties:\\n\\n- **Sequential processing**: sentences must be processed word by word.\\n- **Past information retained through past hidden states**: sequence to sequence models follow the Markov property: each state is assumed to be dependent only on the previously seen state.\\n\\nThe first property is the reason why RNN and LSTM can't be trained in parallel. In order to encode the second word in a sentence I need the previously computed hidden states of the first word, therefore I need to compute that first. The second property is a bit more subtle, but not hard to grasp conceptually. Information in RNN and LSTM are retained thanks to previously computed hidden states. The point is that the encoding of a specific word is retained only for the next time step, which means that the encoding of a word strongly affects only the representation of the next word, so its influence is quickly lost after a few time steps. LSTM (and also GruRNN) can boost a bit the dependency range they can learn thanks to a deeper processing of the hidden states through specific units (which comes with an increased number of parameters to train) but nevertheless the problem is inherently related to recursion. Another way in which people mitigated this problem is to use bi-directional models. These encode the same sentence from the start to end, and from the end to the start, allowing words at the end of a sentence to have stronger influence in the creation of the hidden representation. However, this is just a workaround rather than a real solution for very long dependencies.\\n\\nTo summarize, Transformers are better than all the other architectures because they totally avoid recursion, by processing sentences as a whole and by learning relationships between words thanks to multi-head attention mechanisms and positional embeddings. Nevertheless, it must be pointed out that also transformers can capture only dependencies within the fixed input size used to train them, i.e. if I use as a maximum sentence size 50, the model will not be able to capture dependencies between the first word of a sentence and words that occur more than 50 words later, like in another paragraph.\\n\\nOne major advantage of the transformer architecture, is that at each step we have\\xa0**direct**access to all the other steps (self-attention), which practically leaves no room for information loss, as far as message passing is concerned. On top of that, we can look at both future and past elements at the same time, which also brings the benefit of bidirectional RNNs, without the 2x computation needed. And of course, all this happens in parallel (non-recurrent), which makes both training/inference much faster.\\n\\nThe self-attention with every other token in the input means that the processing will be in the order of\\xa0\\ue23b(𝑁2)O(N2)\\xa0(glossing over details), which means that it's going to be costly to apply transformers on long sequences, compared to RNNs. That's probably one area that RNNs still have an advantage over transformers.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d52cf800-e6e0-4e14-a554-f4dfaee0a951', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nwhy transformer is better than attention\\n\\nOne reason why transformers may be considered better than attention in some cases is that they can process input sequences in parallel, allowing for more efficient computation. This is because the transformer architecture uses self-attention, which allows each element in the input sequence to attend to all the other elements at once, rather than having to attend to them one at a time like in the case of traditional attention mechanisms. This allows the transformer to process the entire input sequence simultaneously, making it more efficient.\\n\\nAnother reason why transformers may be considered superior to attention is that they can handle longer input sequences more effectively. This is because the self-attention mechanism in the transformer allows it to capture long-range dependencies in the input sequence, which can be difficult for attention mechanisms to handle. This makes the transformer well-suited for tasks like language translation, where the input sequences can be quite long.\\n\\nTransformers handle long text better than attention because they use self-attention mechanisms which allow them to weigh the importance of different parts of the input when processing it. This means that the model can selectively focus on certain parts of the input that are relevant to the task at hand, rather than being restricted to a fixed-length context window like traditional attention mechanisms. Additionally, the transformer architecture uses multi-head attention which allows the model to attend to multiple parts of the input simultaneously, further improving its ability to handle longer text.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fc5f3d9e-8b92-4794-97b5-509f38ee94b7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nyelp 题库\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4627e864-3715-40f8-83de-97eadc562c42', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nDynamic programming 1\\n\\nhttps://www.youtube.com/watch?v=vYquumk4nWw\\n\\n\\n\\nfib的另一种解法。通过recurssion直接填充这个array，就不需要记忆化的方法了\\n\\n\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='02c1b8de-4b4d-4a4a-b1f5-8fe52514402a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nWhat is Dynamic Programming\\n\\nDynamic programming divides the main problem into smaller subproblems, but it does not solve the subproblems independently. \\xa0It stores the results of the subproblems to use when solving similar subproblems. Storing the results of subproblems is called memorization. Before solving the current subproblem, it checks the results of the previous subproblems. Finally, it checks the results of all subproblems to find the best solution or the optimal solution. \\xa0This method is effective as it does not compute the answers again and again. Usually, dynamic programming is used for optimization.\\n\\nElements of dynamic programming are as follows.\\n\\n**Simple subproblems**\\xa0– Divide the original problem into small subproblems that have a similar structure\\n\\n**Optimal\\xa0substructure of the problem**\\xa0– The optimal solution to the main problem is within the optimal solution to its subproblems\\n\\n**Overlapping subproblems**\\xa0– Situations of solving the same subproblems again and again\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2755235c-3eba-4754-a833-3234cc174f9b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nWhat is Divide and Conquer\\n\\nDivide and conquer divides the main problem into small subproblems. The subproblems are divided again and again. At one point, there will be a stage where we cannot divide the subproblems further.\\xa0 Then, we can solve each subproblem independently. \\xa0Finally, we can combine the solutions of all subproblems to get the solution to the main problem.\\n\\nThere are three main steps in divide and conquer. They are as follows.\\n\\n**Divide (Break)**\\xa0– Involves splitting the main problem into a collection of subproblems\\n\\n**Conquer (Solve)**\\xa0– Involves solving each subproblem separately\\n\\n**Combine (Merge)**\\xa0– Joins the solutions of the subproblems to obtain the solution of the main problem\\n\\n**Divide and Conquer**\\n\\nDivide and Conquer works by dividing the problem into sub-problems, conquer each sub-problem recursively and combine these solutions.\\n\\n**Dynamic Programming**\\n\\nDynamic Programming is a technique for solving problems with overlapping subproblems. Each sub-problem is solved only once and the result of each sub-problem is stored in a table ( generally implemented as an array or a hash table) for future references. These sub-solutions may be used to obtain the original solution and the technique of storing the sub-problem solutions is known as memoization.\\n\\nYou may think of\\xa0`DP = recursion + re-use`', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bbc2647d-03f7-446a-994a-6fe7d2991491', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nBFS\\n\\nDFS can be implemented via recursion, it is used to go through all possible paths, complete search backtracking etc. It’s done by stack, so last in first out\\n\\nBFS can be implemented via iteration; check if paths exist between nodes, find hops or distance. 可以想象为环线。先search center node, 然后search 一环，然后二环 etc \\n\\nAlgorithms: Breadth first search - Inside code - YouTube\\n\\nBreadth First Search (BFS): Visualized and Explained - YouTube\\n\\nBreadth First Search Algorithm - YouTube\\n\\nDepth First & Breadth First Graph Search - DFS & BFS Graph Searching Algorithms - YouTube\\n\\n!Untitled\\n\\nbfs 要求我们从3走到12， 但他们并不直接相连，所以要通过10，才能找到12；所以我们traverse到10的时候，就要enque all its children ; 每次traverse到一个值，就要pop it out表明之后不会再走到他了\\n\\n!Untitled\\n\\n!Untitled\\n\\n```python\\n# three types of bfs: Binary tree, n-ary tree, graph\\n\\nclass Tree:\\n  def __init__(self, data, left = None, right = None):\\n    self.data = data\\n    self.left = left\\n    self.right = right\\n\\ndef bfs(root):\\n  if root is None:\\n    return\\n  queue = [root]\\n  i = 0\\n  while i < len(queue):\\n\\t\\t#popped = queue.pop(0) #如果用这个写法pop最左的元素，On, \\n#因为every remaining ele will move by 1 cell，所以我们用下面的写法 O1\\n    popped = queue[i]\\n    i += 1\\n    print(popped.data)\\n    if popped.left is not None:\\n      queue.append(popped.left)\\n    if popped.right is not None:\\n      queue.append(popped.right)\\n\\nclass Tree:\\n  def __init__(self, data, children = None):\\n    if children is None:\\n      children = []\\n    self.data = data\\n    self.children = children\\n\\ndef bfs(root):\\n  if root is None:\\n    return\\n  queue = [root]\\n  i = 0\\n  while i < len(queue):\\n    popped = queue[i]\\n    i += 1\\n    print(popped.data)\\n    for child in popped.children:\\n      queue.append(child)\\n\\n# graph need to track which node is visited \\nclass Graph:\\n  def __init__(self, adjList = None):\\n    if adjList is None:\\n      adjList = []\\n    self.adjList = adjList\\n    \\ndef bfs(graph, root):\\n  queue = [root]\\n  enqueued = {root}\\n  i = 0\\n  while i < len(queue):\\n    popped = queue[i]\\n    i += 1\\n    print(popped)\\n    for neighbor in graph.adjList[popped]:\\n      if neighbor not in enqueued:\\n        queue.append(neighbor)\\n        enqueued.add(neighbor)\\n```\\n\\n!Untitled\\n\\n733 flood fill \\n\\n!Untitled\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='735d6471-57d2-45a4-aabc-26bd3eab2101', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**算法流程**\\n\\n1. 首先将根节点放入队列中。\\n2. 从队列中取出第一个节点，并检验它是否为目标。\\n    - 如果找到目标，则结束搜索并回传结果。\\n    - 否则将它所有尚未检验过的直接子节点加入队列中。\\n3. 若队列为空，表示整张图都检查过了——亦即图中没有欲搜索的目标。结束搜索并回传“找不到目标”。\\n4. 重复步骤 2。\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='52bf8eb1-e5ce-4abe-ae6a-bd1f20b2c529', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**算法模板**\\n\\n```python\\nconst visited = {}\\nfunction bfs() {\\n\\tlet q = new Queue()\\n\\tq.push(初始状态)\\n\\twhile(q.length) {\\n\\t\\tlet i = q.pop()\\n        if (visited[i]) continue\\n        if (i 是我们要找的目标) return 结果\\n\\t\\tfor (i的可抵达状态j) {\\n\\t\\t\\tif (j 合法) {\\n\\t\\t\\t\\tq.push(j)\\n\\t\\t\\t}\\n\\t\\t}\\n    }\\n    return 没找到\\n}\\n```\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c40c0f5b-d8c6-4e5e-8cf7-5e31e8238d1d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**两种常见分类**\\n\\nBFS 我目前使用的模板就两种，这两个模板可以解决所有的树的 BFS 问题。\\n\\n前面我提到了“BFS 比较适合找**最短距离/路径**和**某一个距离的目标**”。 如果我需要求的是最短距离/路径，我是不关心我走到第几步的，这个时候可是用不标记层的目标。而如果我需要求距离某个节点距离等于 k 的所有节点，这个时候第几步这个信息就值得被记录了。\\n\\n> 小于 k 或者 大于 k 也是同理。\\n> \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a81ae869-62f3-4273-995c-f0e0faf9b49e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**标记层**\\n\\n一个常见的 BFS 模板，代入题目只需要根据题目微调即可。\\n\\n`class Solution:\\n    def bfs(k):\\n        # 使用双端队列，而不是数组。因为数组从头部删除元素的时间复杂度为 N，双端队列的底层实现其实是链表。\\n        queue = collections.deque([root])\\n        # 记录层数\\n        steps = 0\\n        # 需要返回的节点\\n        ans = []\\n        # 队列不空，生命不止！\\n        while queue:\\n            size = len(queue)\\n            # 遍历当前层的所有节点\\n            for _ in range(size):\\n                node = queue.popleft()\\n                if (step == k) ans.append(node)\\n                if node.right:\\n                    queue.append(node.right)\\n                if node.left:\\n                    queue.append(node.left)\\n            # 遍历完当前层所有的节点后 steps + 1\\n            steps += 1\\n        return ans`\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b5927fa2-6802-4a9b-ae1d-6c6b4ff658b3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**不标记层**\\n\\n不带层的模板更简单，因此大家其实只需要掌握带层信息的目标就够了。\\n\\n一个常见的 BFS 模板，代入题目只需要根据题目微调即可。\\n\\n`class Solution:\\n    def bfs(k):\\n        # 使用双端队列，而不是数组。因为数组从头部删除元素的时间复杂度为 N，双端队列的底层实现其实是链表。\\n        queue = collections.deque([root])\\n        # 队列不空，生命不止！\\n        while queue:\\n            node = queue.popleft()\\n            # 由于没有记录 steps，因此我们肯定是不需要根据层的信息去判断的。否则就用带层的模板了。\\n            if (node 是我们要找到的) return node\\n            if node.right:\\n                queue.append(node.right)\\n            if node.left:\\n                queue.append(node.left)\\n        return -1`\\n\\n以上就是 BFS 的两种基本方式，即带层和不带层，具体使用哪种看题目是否需要根据层信息做判断即可', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bd60f3b4-adda-459e-b635-c874d0bf7f4b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nmutable immutable class python\\n\\n!Untitled\\n\\n!Untitled\\n\\n- Immutable are quicker to access than mutable objects.\\n- Mutable objects are great to use when you need to change the size of the object, example list, dict etc.. Immutables are used when you need to ensure that the object you made will always stay the same.\\n- Immutable objects are fundamentally expensive to “change”, because doing so involves creating a copy. Changing mutable objects is cheap.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='46ea47df-efce-441a-a561-9f7f87cfd68a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n算法课 拉勾2\\n\\n递归有两层含义：\\n\\n递归问题必须可以分解为若干个规模较小、与原问题形式相同的子问题。并且这些子问题可以用完全相同的解题思路来解决；\\n\\n递归问题的演化过程是一个对原问题从大到小进行拆解的过程，并且会有一个明确的终点（临界点）。一旦原问题到达了这个临界点，就不用再往更小的问题上拆解了。最后，从这个临界点开始，把小问题的答案按照原路返回，原问题便得以解决。\\n\\n简而言之，递归的基本思想就是把规模大的问题转化为规模小的相同的子问题来解决。 在函数实现时，因为大问题和小问题是一样的问题，因此大问题的解决方法和小问题的解决方法也是同一个方法。这就产生了函数调用它自身的情况，这也正是递归的定义所在。\\n\\n格外重要的是，这个解决问题的函数必须有明确的结束条件，否则就会导致无限递归的情况。总结起来，递归的实现包含了两个部分，一个是递归主体，另一个是终止条件\\n\\n写出递归代码的关键在于，写出递推公式和找出终止条件。\\n\\n也就是说我们需要：首先找到将大问题分解成小问题的规律，并基于此写出递推公式；然后找出终止条件，就是当找到最简单的问题时，如何写出答案；最终将递推公式和终止条件翻译成实际代码\\n\\n分治法是什么？\\n计算机求解问题所需的计算时间，与其涉及的数据规模强相关。简而言之，问题所涉及的数据规模越小，它所需的计算时间也越少；反之亦然。\\n\\n我们来看一个例子：在一个包含 n 个元素的无序数组中，要求按照从小到大的顺序打印其 n 个元素。\\n\\n假设我们采用 n 个元素之间的两两比较的计算方法，去得到从小到大的序列。分析如下：\\n\\n当数据量 n = 1 时，不需任何计算，直接打印即可；\\n\\n当数据量 n = 2 时 ，那需要做 1 次比较即可达成目标；\\n\\n当数据量 n = 3 时，要对这 3 个元素进行两两比较，共计 3 次比较；\\n\\n而当数据量 n = 10 时，问题就不那么容易处理了，我们需要 45 次比较（计算方式是 0.5*n(n-1) ）。\\n\\n因此，要想通过上述方法直接解决一个规模较大的问题，其实是相当困难的。\\n\\n基于此，分治法的核心思想就是分而治之。具体来说，它先将一个难以直接解决的大问题，分割成一些可以直接解决的小问题。如果分割后的问题仍然无法直接解决，那么就继续递归地分割，直到每个小问题都可解。\\n\\n通常而言，这些子问题具备互相独立、形式相同的特点。这样，我们就可以采用同一种解法，递归地去解决这些子问题。最后，再将每个子问题的解合并，就得到了原问题的解\\n\\n例如下面这个问题，在 1000 个有序数字构成的数组 a 中，判断某个数字 c 是否出现过。\\n\\n第一种方法，全局遍历。 复杂度 O(n)。采用 for 循环，对 1000 个数字全部判断一遍。\\n\\n第二种方法，采用二分查找。 复杂度 O(logn)。递归地判断 c 与 a 的中位数的大小关系，并不断缩小范围。\\n\\n这两种方法，对时间的消耗几乎一样。那分治法的价值又是什么呢？\\n\\n其实，在小数据规模上，分治法没有什么特殊价值。无非就是让代码显得更牛一些。只有在大数据集上，分治法的价值才能显现出来\\n\\n分治法的使用方法\\n前面我们讲到分治法的核心思想是“分而治之”，当你需要采用分治法时，一般原问题都需要具备以下几个特征：\\n\\n难度在降低，即原问题的解决难度，随着数据的规模的缩小而降低。这个特征绝大多数问题都是满足的。\\n\\n问题可分，原问题可以分解为若干个规模较小的同类型问题。这是应用分治法的前提。\\n\\n解可合并，利用所有子问题的解，可合并出原问题的解。这个特征很关键，能否利用分治法完全取决于这个特征。\\n\\n相互独立，各个子问题之间相互独立，某个子问题的求解不会影响到另一个子问题。如果子问题之间不独立，则分治法需要重复地解决公共的子问题，造成效率低下的结果。\\n\\n根据前面我们对分治法的分析，你一定能迅速联想到递归。分治法需要递归地分解问题，再去解决问题。因此，分治法在每轮递归上，都包含了分解问题、解决问题和合并结果这 3 个步骤。\\n\\n为了让大家对分治法有更清晰地了解，我们以二分查找为例，看一下分治法如何使用。关于分治法在排序中的使用，我们会在第 11 课时中讲到。查找问题指的是，在一个有序的数列中，判断某个待查找的数字是否出现过。二分查找，则是利用分治法去解决查找问题。通常二分查找需要一个前提，那就是输入的数列是有序的。\\n\\n二分查找的思路比较简单，步骤如下：\\n\\n选择一个标志 i 将集合 L 分为二个子集合，一般可以使用中位数；\\n\\n判断标志 L(i) 是否能与要查找的值 des 相等，相等则直接返回结果；\\n\\n如果不相等，需要判断 L(i) 与 des 的大小；\\n\\n基于判断的结果决定下步是向左查找还是向右查找。如果向某个方向查找的空间为 0，则返回结果未查到；\\n\\n回到步骤 1。\\n\\n我们对二分查找的复杂度进行分析。二分查找的最差情况是，不断查找到最后 1 个数字才完成判断。那么此时需要的最大的复杂度就是 O(logn)\\n\\n下面我们一起来看一个例子。在数组 { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 } 中，查找 8 是否出现过。\\n\\n首先判断 8 和中位数 5 的大小关系。因为 8 更大，所以在更小的范围 6, 7, 8, 9, 10 中继续查找。此时更小的范围的中位数是 8。由于 8 等于中位数 8，所以查找到并打印查找到的 8 对应在数组中的 index 值\\n\\n从代码实现的角度来看，我们可以采用两个索引 low 和 high，确定查找范围。最初 low 为 0，high 为数组长度减 1。在一个循环体内，判断 low 到 high 的中位数与目标变量 targetNumb 的大小关系。根据结果确定向左走（high = middle - 1）或者向右走（low = middle + 1），来调整 low 和 high 的值。直到 low 反而比 high 更大时，说明查找不到并跳出循环。我们给出代码如下：\\n\\n复制代码\\npublic static void main(String[] args) {\\n// 需要查找的数字\\nint targetNumb = 8;\\n// 目标有序数组\\nint[] arr = { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 };\\nint middle = 0;\\nint low = 0;\\nint high = arr.length - 1;\\nint isfind = 0;\\n\\n```\\nwhile (low <= high) {\\n\\tmiddle = (high + low) / 2;\\n\\tif (arr[middle] == targetNumb) {\\n\\t\\tSystem.out.println(targetNumb + \" 在数组中,下标值为: \" + middle);\\n        isfind = 1;\\n\\t\\tbreak;\\n\\t} else if (arr[middle] > targetNumb) {\\n\\t\\t// 说明该数在low~middle之间\\n\\t\\thigh = middle - 1;\\n\\t} else {\\n\\t\\t// 说明该数在middle~high之间\\n\\t\\tlow = middle + 1;\\n\\t}\\n}\\nif (isfind == 0) {\\n\\t\\tSystem.out.println(\"数组不含 \" + targetNumb);\\n}\\n\\n```\\n\\n}\\n\\n什么是排序问题\\n排序，就是让一组无序数据变成有序的过程。 一般默认这里的有序都是从小到大的排列顺序。下面我们先来讲讲，如何判断不同的排序算法的优劣。\\n\\n衡量一个排序算法的优劣，我们主要会从以下 3 个角度进行分析：\\n\\n1．时间复杂度，具体包括，最好时间复杂度、最坏时间复杂度以及平均时间复杂度。\\n\\n2．空间复杂度，如果空间复杂度为 1，也叫作原地排序。\\n\\n3．稳定性，排序的稳定性是指相等的数据对象，在排序之后，顺序是否能保证不变。\\n\\n常见的排序算法及其思想\\n接下来，我们就开始详细地介绍一些经典的排序算法。\\n\\n冒泡排序\\n1、冒泡排序的原理\\n\\n从第一个数据开始，依次比较相邻元素的大小。如果前者大于后者，则进行交换操作，把大的元素往后交换。通过多轮迭代，直到没有交换操作为止。 冒泡排序就像是在一个水池中处理数据一样，每次会把最大的那个数据传递到最后\\n\\n2、冒泡排序的性能\\n\\n冒泡排序最好时间复杂度是 O(n)，也就是当输入数组刚好是顺序的时候，只需要挨个比较一遍就行了，不需要做交换操作，所以时间复杂度为 O(n)。\\n\\n冒泡排序最坏时间复杂度会比较惨，是 O(n*n)。也就是说当数组刚好是完全逆序的时候，每轮排序都需要挨个比较 n 次，并且重复 n 次，所以时间复杂度为 O(n*n)。\\n\\n很显然，当输入数组杂乱无章时，它的平均时间复杂度也是 O(n*n)。\\n\\n冒泡排序不需要额外的空间，所以空间复杂度是 O(1)。冒泡排序过程中，当元素相同时不做交换，所以冒泡排序是稳定的排序算法\\n\\npublic static void main(String[] args) {\\nint[] arr = { 1, 0, 3, 4, 5, -6, 7, 8, 9, 10 };\\nSystem.out.println(\"原始数据: \" + Arrays.toString(arr));\\nfor (int i = 1; i < arr.length; i++) {\\nfor (int j = 0; j < arr.length - i; j++) {\\nif (arr[j] > arr[j + 1]) {\\nint temp = arr[j];\\narr[j] = arr[j + 1];\\narr[j + 1] = temp;\\n}\\n}\\n}\\nSystem.out.println(\"冒泡排序: \" + Arrays.toString(arr));\\n}\\n\\n插入排序\\n1、插入排序的原理\\n\\n选取未排序的元素，插入到已排序区间的合适位置，直到未排序区间为空。插入排序顾名思义，就是从左到右维护一个已经排好序的序列。直到所有的待排数据全都完成插入的动作\\n\\n2、插入排序的性能\\n\\n插入排序最好时间复杂度是 O(n)，即当数组刚好是完全顺序时，每次只用比较一次就能找到正确的位置。这个过程重复 n 次，就可以清空未排序区间。\\n\\n插入排序最坏时间复杂度则需要 O(n*n)。即当数组刚好是完全逆序时，每次都要比较 n 次才能找到正确位置。这个过程重复 n 次，就可以清空未排序区间，所以最坏时间复杂度为 O(n*n)。\\n\\n插入排序的平均时间复杂度是 O(n*n)。这是因为往数组中插入一个元素的平均时间复杂度为 O(n)，而插入排序可以理解为重复 n 次的数组插入操作，所以平均时间复杂度为 O(n*n)。\\n\\n插入排序不需要开辟额外的空间，所以空间复杂度是 O(1)\\n\\npublic static void main(String[] args) {\\nint[] arr = { 2, 3, 5, 1, 23, 6, 78, 34 };\\nSystem.out.println(\"原始数据: \" + Arrays.toString(arr));\\nfor (int i = 1; i < arr.length; i++) {\\nint temp = arr[i];\\nint j = i - 1;\\nfor (; j >= 0; j--) {\\nif (arr[j] > temp) {\\narr[j + 1] = arr[j];\\n} else {\\nbreak;\\n}\\n}\\narr[j + 1] = temp;\\n}\\nSystem.out.println(\"插入排序: \" + Arrays.toString(arr));\\t\\n}\\n\\n相同点\\n\\n插入排序和冒泡排序的平均时间复杂度都是 O(n*n)，且都是稳定的排序算法，都属于原地排序。\\n\\n差异点\\n\\n冒泡排序每轮的交换操作是动态的，所以需要三个赋值操作才能完成；\\n\\n而插入排序每轮的交换动作会固定待插入的数据，因此只需要一步赋值操作。\\n\\n以上两种排序算法都比较简单，通过这两种算法可以帮助我们对排序的思想建立基本的了解，接下来再介绍一些时间复杂度更低的排序算法，它们的时间复杂度都可以达到 O(nlogn)。\\n\\n归并排序\\n1、归并排序的原理\\n\\n归并排序的原理其实就是我们上一课时讲的分治法。它首先将数组不断地二分，直到最后每个部分只包含 1 个数据。然后再对每个部分分别进行排序，最后将排序好的相邻的两部分合并在一起，这样整个数组就有序了。\\n\\npublic static void main(String[] args) {\\nint[] arr = { 49, 38, 65, 97, 76, 13, 27, 50 };\\nint[] tmp = new int[arr.length];\\nSystem.out.println(\"原始数据: \" + Arrays.toString(arr));\\ncustomMergeSort(arr, tmp, 0, arr.length - 1);\\nSystem.out.println(\"归并排序: \" + Arrays.toString(arr));\\n}\\n\\npublic static void customMergeSort(int[] a, int[] tmp, int start, int end) {\\nif (start < end) {\\nint mid = (start + end) / 2;\\n// 对左侧子序列进行递归排序\\ncustomMergeSort(a, tmp, start, mid);\\n// 对右侧子序列进行递归排序\\ncustomMergeSort(a, tmp,mid + 1, end);\\n// 合并\\ncustomDoubleMerge(a, tmp, start, mid, end);\\n}\\n}\\n\\npublic static void customDoubleMerge(int[] a, int[] tmp, int left, int mid, int right) {\\nint p1 = left, p2 = mid + 1, k = left;\\nwhile (p1 <= mid && p2 <= right) {\\nif (a[p1] <= a[p2])\\ntmp[k++] = a[p1++];\\nelse\\ntmp[k++] = a[p2++];\\n}\\nwhile (p1 <= mid)\\ntmp[k++] = a[p1++];\\nwhile (p2 <= right)\\ntmp[k++] = a[p2++];\\n// 复制回原素组\\nfor (int i = left; i <= right; i++)\\na[i] = tmp[i];\\n\\n2、归并排序的性能\\n\\n对于归并排序，它采用了二分的迭代方式，复杂度是 logn。\\n\\n每次的迭代，需要对两个有序数组进行合并，这样的动作在 O(n) 的时间复杂度下就可以完成。因此，**归并排序的复杂度就是二者的乘积 O(nlogn)。**同时，它的执行频次与输入序列无关，因此，归并排序最好、最坏、平均时间复杂度都是 O(nlogn)。\\n\\n空间复杂度方面，由于每次合并的操作都需要开辟基于数组的临时内存空间，所以空间复杂度为 O(n)。归并排序合并的时候，相同元素的前后顺序不变，所以归并是稳定的排序算法。\\n\\n快速排序\\n1、快速排序法的原理\\n\\n快速排序法的原理也是分治法。它的每轮迭代，会选取数组中任意一个数据作为分区点，将小于它的元素放在它的左侧，大于它的放在它的右侧。再利用分治思想，继续分别对左右两侧进行同样的操作，直至每个区间缩小为 1，则完成排序\\n\\npublic static void main(String[] args) {\\nint[] arr = { 6, 1, 2, 7, 9, 11, 4, 5, 10, 8 };\\nSystem.out.println(\"原始数据: \" + Arrays.toString(arr));\\ncustomQuickSort(arr, 0, arr.length - 1);\\nSystem.out.println(\"快速排序: \" + Arrays.toString(arr));\\n}\\n\\npublic void customQuickSort(int[] arr, int low, int high) {\\nint i, j, temp, t;\\nif (low >= high) {\\nreturn;\\n}\\n\\n```\\ni = low;\\nj = high;\\ntemp = arr[low];\\nwhile (i < j) {\\n\\t// 先看右边，依次往左递减\\n\\twhile (temp <= arr[j] && i < j) {\\n\\t\\tj--;\\n\\t}\\n\\t// 再看左边，依次往右递增\\n\\twhile (temp >= arr[i] && i < j) {\\n\\t\\ti++;\\n\\t}\\n\\tt = arr[j];\\n\\tarr[j] = arr[i];\\n\\tarr[i] = t;\\n}\\narr[low] = arr[i];\\narr[i] = temp;\\n// 递归调用左半数组\\ncustomQuickSort(arr, low, j - 1);\\n// 递归调用右半数组\\ncustomQuickSort(arr, j + 1, high);\\n\\n```\\n\\n}\\n\\n2、快速排序法的性能\\n\\n在快排的最好时间的复杂度下，如果每次选取分区点时，都能选中中位数，把数组等分成两个，那么此时的时间复杂度和归并一样，都是 O(n*logn)。\\n\\n而在最坏的时间复杂度下，也就是如果每次分区都选中了最小值或最大值，得到不均等的两组。那么就需要 n 次的分区操作，每次分区平均扫描 n / 2 个元素，此时时间复杂度就退化为 O(n*n) 了。\\n\\n快速排序法在大部分情况下，统计上是很难选到极端情况的。因此它平均的时间复杂度是 O(n*logn)。\\n\\n快速排序法的空间方面，使用了交换法，因此空间复杂度为 O(1)。\\n\\n很显然，快速排序的分区过程涉及交换操作，所以快排是不稳定的排序算法。\\n\\n排序算法的性能分析\\n我们先思考一下排序算法性能的下限，也就是最差的情况。在前面的课程中，我们写过求数组最大值的代码，它的时间复杂度是 O(n)。对于 n 个元素的数组，只要重复执行 n 次最大值的查找就能完成排序。因此排序最暴力的方法，时间复杂度是 O(n*n)。这恰如冒泡排序和插入排序。\\n\\n当我们利用算法思维去解决问题时，就会想到尝试分治法。此时，利用归并排序就能让时间复杂度降低到 O(nlogn)。然而，归并排序需要额外开辟临时空间。一方面是为了保证稳定性，另一方面则是在归并时，由于在数组中插入元素导致了数据挪移的问题。\\n\\n为了规避因此而带来的时间损耗，此时我们采用快速排序。通过交换操作，可以解决插入元素导致的数据挪移问题，而且降低了不必要的空间开销。但是由于其动态二分的交换数据，导致了由此得出的排序结果并不稳定。\\n\\n经过以上分析，我们对方法论进行提练，宏观上的步骤总结为以下 4 步：\\n\\n复杂度分析。估算问题中复杂度的上限和下限。\\n\\n定位问题。根据问题类型，确定采用何种算法思维。\\n\\n数据操作分析。根据增、删、查和数据顺序关系去选择合适的数据结构，利用空间换取时间。\\n\\n编码实现。\\n\\n这套方法适用于绝大多数的问题，在实战中需要你灵活运用\\n\\n案例\\n梳理完方法论之后，我们回过头来再看一下以前的例子，看看采用方法论是如何分析题目并找到答案的。\\n\\n例 1，在一个数组 a = [1, 3, 4, 3, 4, 1, 3] 中，找到出现次数最多的那个数字。如果并列存在多个，随机输出一个。\\n\\n我们先来分析一下复杂度。假设我们采用最暴力的方法。利用双层循环的方式计算：\\n\\n第一层循环，我们对数组中的每个元素进行遍历；\\n\\n第二层循环，对于每个元素计算出现的次数，并且通过当前元素次数 time_tmp 和全局最大次数变量 time_max 的大小关系，持续保存出现次数最多的那个元素及其出现次数。\\n\\n由于是双层循环，这段代码在时间方面的消耗就是 n*n 的复杂度，也就是 O(n²)。这段代码我们在第 1 课时中的例子里讲过，这里就不再赘述了。\\n\\n接着，我们思考一下这段代码最低的复杂度可能是多少？\\n\\n不难发现，这个问题的复杂度最低低不过 O(n)。这是因为某个数字的数值是完全有可能影响最终结果。例如，a = [1, 3, 4, 3, 4, 1]，随机输出 1、3、4 都可以。如果 a 中增加一个元素变成，a = [1, 3, 4, 3, 4, 1, 3, 1]，则结果为 1。\\n\\n由此可见，这个问题必须至少要对全部数据遍历一次，所以复杂度再低低不过 O(n)。\\n\\n显然，这个问题属于在一个数组中，根据某个条件进行查找的问题。既然复杂度低不过 O(n)，我们也不用考虑采用二分查找了。此处是用不到任何算法思维。那么如何让 O(n²) 的复杂度降低为 O(n) 呢？\\n\\n只有通过巧妙利用数据结构了。分析这个问题就可以发现，此时不需要关注数据顺序。因此，栈、队列等数据结构用到的可能性会很低。如果采用新的数据结构，增删操作肯定是少不了的。而原问题就是查找类型的问题，所以查找的动作一定是非常高频的。在我们学过的数据结构中，查找有优势，同时不需要考虑数据顺序的只有哈希表，因此可以很自然地想到用哈希表解决问题。\\n\\n哈希表的结构是“key-value”的键值对，如何设计键和值呢？哈希表查找的 key，所以 key 一定存放的是被查找的内容，也就是原数组中的元素。数组元素有重复，但哈希表中 key 不能重复，因此只能用 value 来保存频次。\\n\\n分析到这里，所有解决方案需要用到的关键因素就出来了，我们总结为以下 2 点：\\n\\n预期的时间复杂度是 O(n)，这就意味着编码采用一层的 for 循环，对原数组进行遍历。\\n\\n数据结构需要额外设计哈希表，其中 key 是数组的元素，value 是频次。这样可以支持 O(1) 时间复杂度的查找动作。\\n\\n因此，这个问题的代码就是\\n\\npublic void s2_4() {\\n\\n```\\nint a[] = { 1, 3, 4, 3, 4, 1, 3, 1 };\\n\\nMap d = new HashMap();\\n\\nfor (int i = 0; i < a.length; i++) {\\n\\n\\tif (d.containsKey(a[i])) {\\n\\n\\t\\td.put(a[i], d.get(a[i]) + 1);\\n\\n\\t} else {\\n\\n\\t\\td.put(a[i], 1);\\n\\n\\t}\\n\\n}\\n\\nint val_max = -1;\\n\\nint time_max = 0;\\n\\nfor (Integer key : d.keySet()) {\\n\\n\\tif (d.get(key) > time_max) {\\n\\n\\t\\ttime_max = d.get(key);\\n\\n\\t\\tval_max = key;\\n\\n\\t}\\n\\n}\\n\\nSystem.out.println(val_max);\\n\\n```\\n\\n}\\n\\n**例 2**，这个问题是力扣的经典问题，two sums。给定一个整数数组 arr 和一个目标值 target，请你在该数组中找出加和等于目标值的两个整数，并返回它们在原数组中的下标。\\n\\n你可以假设，原数组中没有重复元素，而且有且只有一组答案。但是，数组中的元素只能使用一次。例如，arr = [1, 2, 3, 4, 5, 6]，target = 4。因为，arr[0] + arr[2] = 1 + 3 = 4 = target，则输出 0，2。\\n\\n**首先，我们来分析一下复杂度**。假设我们采用最暴力的方法，利用双层循环的方式计算，步骤如下：\\n\\n- 第一层循环，我们对数组中的每个元素进行遍历；\\n- 第二层循环，对于第一层的元素与 target 的差值进行查找。\\n\\n例如，第一层循环遍历到了 1，第二层循环就需要查找 target - arr[0] = 4 - 1 = 3 是否在数组中。由于是双层循环，这段代码在时间方面的消耗就是 n*n 的复杂度，也就是 O(n²)。\\n\\n**接下来，我们看看下限**。很显然，某个数字是否存在于原数组对结果是有影响的。因此，复杂度再低低不过 O(n)。\\n\\n这里的问题是在数组中基于某个条件去查找数据的问题。然而可惜的是原数组并非有序，因此采用二分查找的可能性也会很低。那么如何把 O(n²) 的复杂度降低到 O(n) 呢？路径只剩下了数据结构。\\n\\n在暴力的方法中，第二层循环的目的是查找 target - arr[i] 是否出现在数组中。很自然地就会联想到可能要使用哈希表。同时，这个例子中对于数据处理的顺序并不关心，栈或者队列使用的可能性也会很低。因此，不妨试试如何用哈希表去降低复杂度。\\n\\n既然是要查找 target - arr[i] 是否出现过，因此哈希表的 key 自然就是 target - arr[i]。而 value 如何设计呢？这就要看一下结果了，最终要输出的是查找到的 arr[i] 和 target - arr[i] 在数组中的索引，因此 value 存放的必然是 index 的索引值。\\n\\n**基于上面的分析，我们就能找到解决方案，分析如下**：\\n\\n1. 预期的时间复杂度是 O(n)，这就意味着编码采用一层的 for 循环，对原数组进行遍历。\\n2. 数据结构需要额外设计哈希表，其中 key 是 target - arr[i]，value 是 index。这样可以支持 O(1) 时间复杂度的查找动作。\\n\\n因此，代码如下\\n\\nprivate static int[] twoSum(int[] arr, int target) {\\n\\n```\\nMap map = new HashMap();\\n\\nfor (int i = 0; i < arr.length; i++) {\\n\\n\\tmap.put(arr[i], i);\\n\\n}\\n\\nfor (int i = 0; i < arr.length; i++) {\\n\\n\\tint complement = target - arr[i];\\n\\n\\tif (map.containsKey(complement) && map.get(complement) != i) {\\n\\n\\t\\treturn new int[] { map.get(complement), i };\\n\\n\\t}\\n\\n}\\n\\n    return null;\\n\\n```\\n\\n}\\n\\n在这段代码中我们采用了两个 for 循环，时间复杂度就是 O(n) + O(n) = O(n)。额外使用了 map，空间复杂度也是 O(n)。第一个 for 循环，把数组转为字典，存放的是“数值 -index”的键值对。第二个 for 循环，在字典中依次判断，target - arr[i] 是否出现过。如果它出现过，且不是它自己，则打印 target - arr[i] 和 arr[i] 的索引\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='05deb691-c226-485f-81c5-0033fcece857', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**例题2：判断一个数组中是否存在某个数**\\n\\n**【题目】给定一个经过任意位数的旋转后的排序数组，判断某个数是否在里面**。\\n\\n例如，对于一个给定数组 {4, 5, 6, 7, 0, 1, 2}，它是将一个有序数组的前三位旋转地放在了数组末尾。假设输入的 target 等于 0，则输出答案是 4，即 0 所在的位置下标是 4。如果输入 3，则返回 -1。\\n\\n**【解析】**\\xa0这道题目依旧是按照解决代码问题的方法论的步骤进行分析。\\n\\n- 先做复杂度分析\\n\\n这个问题就是判断某个数字是否在数组中，因此，复杂度极限就是全部遍历地去查找，也就是 O(n) 的复杂度。\\n\\n- 接着，进入定位问题的环节中\\n\\n这个问题有很多关键字，因此能够让你立马锁定问题。例如，判断某个数是否在数组里面，这就是一个查找问题。\\n\\n- 然后，我们来做数据操作分析\\n\\n原数组是经过某些处理的排序数组，也就是说原数组是有序的。有序和查找，你就会很快地想到，这个问题极有可能用二分查找的方式去解决，时间复杂度是 O(logn)，相比上面 O(n) 的基线也是有显著的提高。\\n\\n在利用二分查找时，更多的是判断，基本没有数据的增删操作，因此不需要太多地定义复杂的数据结构。\\n\\n分析到这里，解决方案已经非常明朗了，就是采用二分查找的方法，在 O(logn) 的时间复杂度下去解决这个问题。二分查找可以通过递归来实现。**而每次递归的关键点在于，根据切分的点（最中间的那个数字），确定是向左走还是向右走。这也是这个例题中唯一的难点了**。\\n\\n试想一下，在一个旋转后的有序数组中，利用中间元素作为切分点得到的两个子数组有什么样的性质。经过枚举不难发现，这两个子数组中，一定存在一个数组是有序的。也可能出现一个极端情况，二者都是有序的。如下图所示：\\n\\n!https://s0.lgstatic.com/i/image/M00/2E/C4/CgqCHl8Fi6eAVyX3AAAnk9vJF3c337.png\\n\\n对于有序的一边，我们是很容易判断目标值，是否在这个区间内的。如果在其中，也说明了目标值不在另一边的旋转有序组里；反之亦然。\\n\\n当我们知道了目标值在左右哪边之后，就可以递归地调用旋转有序的二分查找了。之所以可以递归调用，是因为，对于旋转有序组，这个问题和原始问题完全一致，可以调用。对于有序组，它是旋转有序的特殊情况（即旋转 0 位），也一定是可以通过递归的方法去实现查找的。直到不断二分后，搜索空间只有 1 位数字，直接判断是否找到即可\\n\\n**主函数中，第 2 到 4 行。定义数组和 target，并且执行二分查找**。二分查找包括两部分，其一是二分策略，其二是终止条件。\\n\\n**二分策略在代码的 16～33 行：**\\n\\n- 16 行计算分裂点的索引值。17 到 19 行，进行目标值与分裂点的判断。\\n    - 如果相等，则查找到结果并返回；\\n    - 如果不等就要继续二分。\\n- 在二分的过程中，第 20 行进行了左右子数组哪边是有序的判断。\\n    - 如果左边有序，则进入到 21 到 25 行；\\n    - 如果右边有序，则进入到 28 到 32 行。\\n- 假设左边有序，则还需要判断 target 是否在有序区间内，这是在第 21 行。\\n    - 如果在，则继续递归的调用 bs(arr,target, begin,middle-1)；\\n    - 如果不在有序部分，则说明 target 在另一边的旋转有序中，则调用 bs(arr,target, middle+1,end)。\\n\\n下面的逻辑与此类似，不再赘述。\\n\\n**经过了层层二分，最终 begin 和 end 变成了相等的两个变量，则进入到终止条件，即 8 到 15 行**。\\n\\n- 在这里，需要判断最后剩下的 1 个元素是否与 target 相等：\\n    - 如果相等则返回索引值；\\n    - 如果不等则返回 -1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9fcd2f02-c60e-40d9-95f5-d509ec869981', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n递归2  recursion (divide and conq, backtrack)\\n\\nhttps://www.youtube.com/watch?v=AqGagBmFXgw\\n\\n%2034304900389645dc9a574bcca0de9b71/Untitled.png)\\n\\n%2034304900389645dc9a574bcca0de9b71/Untitled%201.png)\\n\\n%2034304900389645dc9a574bcca0de9b71/Untitled%202.png)\\n\\n%2034304900389645dc9a574bcca0de9b71/Untitled%203.png)\\n\\n%2034304900389645dc9a574bcca0de9b71/Untitled%204.png)\\n\\n%2034304900389645dc9a574bcca0de9b71/Untitled%205.png)\\n\\n%2034304900389645dc9a574bcca0de9b71/Untitled%206.png)\\n\\n%2034304900389645dc9a574bcca0de9b71/Untitled%207.png)\\n\\n%2034304900389645dc9a574bcca0de9b71/Untitled%208.png)\\n\\nand 代表了分治算法的combine部分，因为左子树和柚子树都要满足条件\\n\\n%2034304900389645dc9a574bcca0de9b71/Untitled%209.png)\\n\\n%2034304900389645dc9a574bcca0de9b71/Untitled%2010.png)\\n\\n%2034304900389645dc9a574bcca0de9b71/Untitled%2011.png)\\n\\n%2034304900389645dc9a574bcca0de9b71/Untitled%2012.png)\\n\\n%2034304900389645dc9a574bcca0de9b71/Untitled%2013.png)\\n\\n%2034304900389645dc9a574bcca0de9b71/Untitled%2014.png)\\n\\n%2034304900389645dc9a574bcca0de9b71/Untitled%2015.png)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='eee448e5-f4dc-41bb-9485-bf8f7199bc45', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nTUF 笔记 recur & backtrack & dfs\\n\\nGet all subsequences of [3,1,2]，思路就是在这三个位置里分别决定取还是不取\\n\\n下面是最最经典的backtracking模板，注意他用[]表示array，不止表示空array\\n\\n!Untitled\\n\\n!Untitled\\n\\n视频已下载\\n\\nL6. Recursion on Subsequences | Printing Subsequences - YouTube\\n\\nL13 就是力扣家家的三色标记法', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='96275f6a-2154-464b-bfd0-39e191e0f8ac', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n链表 code\\n\\n为何一般的题目都要在前面加一个dummy node? 因为往往在循环之后失去了最开始的head地址，所以需要一个dummy node来存下他，记住return的位置一般在dummy.next\\n\\n```python\\nclass ListNode(object):\\n    def __init__(self, x):\\n        self.val = x\\n        self.next = None\\n\\na = ListNode(2)\\na1 = ListNode(3)\\na.next = a1\\nb = ListNode(1)\\nb1 = ListNode(1)\\nb.next = b1\\n```\\n\\n```python\\n**class** **SingleLinkList**(object):\\n    \"\"\"单链表\"\"\"\\n\\n    **def** **__init__**(self):\\n        self**.**_head **=** **Nonedef** **is_empty**(self):\\n        \"\"\"判断链表是否为空\"\"\"\\n        **return** self**.**_head **is** **Nonedef** **length**(self):\\n        \"\"\"链表长度\"\"\"\\n        *# 初始指针指向head*cur **=** self**.**_head\\n        count **=** 0\\n        *# 指针指向None 表示到达尾部***while** cur **is** **not** **None**:\\n            count **+=** 1\\n            *# 指针下移*cur **=** cur**.**next\\n        **return** count\\n\\n    **def** **items**(self):\\n        \"\"\"遍历链表\"\"\"\\n        *# 获取head指针*cur **=** self**.**_head\\n        *# 循环遍历***while** cur **is** **not** **None**:\\n            *# 返回生成器***yield** cur**.**item\\n            *# 指针下移*cur **=** cur**.**next\\n\\n    **def** **add**(self, item):\\n        \"\"\"向链表头部添加元素\"\"\"\\n        node **=** Node(item)\\n        *# 新结点指针指向原头部结点*node**.**next **=** self**.**_head\\n        *# 头部结点指针修改为新结点*self**.**_head **=** node\\n\\n    **def** **append**(self, item):\\n        \"\"\"尾部添加元素\"\"\"\\n        node **=** Node(item)\\n        *# 先判断是否为空链表***if** self**.**is_empty():\\n            *# 空链表，_head 指向新结点*self**.**_head **=** node\\n        **else**:\\n            *# 不是空链表，则找到尾部，将尾部next结点指向新结点*cur **=** self**.**_head\\n            **while** cur**.**next **is** **not** **None**:\\n                cur **=** cur**.**next\\n            cur**.**next **=** node\\n\\n    **def** **insert**(self, index, item):\\n        \"\"\"指定位置插入元素\"\"\"\\n        *# 指定位置在第一个元素之前，在头部插入***if** index **<=** 0:\\n            self**.**add(item)\\n        *# 指定位置超过尾部，在尾部插入***elif** index **>** (self**.**length() **-** 1):\\n            self**.**append(item)\\n        **else**:\\n            *# 创建元素结点*node **=** Node(item)\\n            cur **=** self**.**_head\\n            *# 循环到需要插入的位置***for** i **in** range(index **-** 1):\\n                cur **=** cur**.**next  #这个cur是待插入位置的前驱节点\\n            node**.**next **=** cur**.**next\\n            cur**.**next **=** node\\n\\n    **def** **remove**(self, item):\\n        \"\"\"删除节点\"\"\"\\n        cur **=** self**.**_head\\n        pre **=** **Nonewhile** cur **is** **not** **None**:\\n            *# 找到指定元素***if** cur**.**item **==** item:\\n                *# 如果第一个就是删除的节点***if** **not** pre:\\n                    *# 将头指针指向头节点的后一个节点*self**.**_head **=** cur**.**next\\n                **else**:\\n                    *# 将删除位置前一个节点的next指向删除位置的后一个节点*pre**.**next **=** cur**.**next\\n                **return** **Trueelse**:\\n                *# 继续按链表后移节点*pre **=** cur\\n                cur **=** cur**.**next\\n#待删除位置的前驱节点.next = 待删除位置的前驱节点.next.next\\n    **def** **find**(self, item):\\n        \"\"\"查找元素是否存在\"\"\"\\n        **return** item **in** self**.**items()\\n```\\n\\n```python\\n\\n#定义循环链表\\nclass SingleCycleLinkList(object):\\n\\n    def __init__(self):\\n        self._head = None\\n\\n    def is_empty(self):\\n        \"\"\"判断链表是否为空\"\"\"\\n        return self._head is None\\n\\n    def length(self):\\n        \"\"\"链表长度\"\"\"\\n        # 链表为空\\n        if self.is_empty():\\n            return 0\\n        # 链表不为空\\n        count = 1\\n        cur = self._head\\n        while cur.next != self._head:\\n            count += 1\\n            # 指针下移\\n            cur = cur.next\\n        return count\\n\\n    def items(self):\\n        \"\"\" 遍历链表 \"\"\"\\n        # 链表为空\\n        if self.is_empty():\\n            return\\n        # 链表不为空\\n        cur = self._head\\n        while cur.next != self._head:\\n            yield cur.item\\n            cur = cur.next\\n        yield cur.item\\n\\n    def add(self, item):\\n        \"\"\" 头部添加结点\"\"\"\\n        node = Node(item)\\n        if self.is_empty():  # 为空\\n            self._head = node\\n            node.next = self._head\\n        else:\\n            # 添加结点指向head\\n            node.next = self._head\\n            cur = self._head\\n            # 移动结点，将末尾的结点指向node\\n            while cur.next != self._head:\\n                cur = cur.next\\n            cur.next = node\\n        # 修改 head 指向新结点\\n        self._head = node\\n\\n    def append(self, item):\\n        \"\"\"尾部添加结点\"\"\"\\n        node = Node(item)\\n        if self.is_empty():  # 为空\\n            self._head = node\\n            node.next = self._head\\n        else:\\n            # 寻找尾部\\n            cur = self._head\\n            while cur.next != self._head:\\n                cur = cur.next\\n            # 尾部指针指向新结点\\n            cur.next = node\\n            # 新结点指针指向head\\n            node.next = self._head\\n\\n    def insert(self, index, item):\\n        \"\"\" 指定位置添加结点\"\"\"\\n        if index <= 0:  # 指定位置小于等于0，头部添加\\n            self.add(item)\\n        # 指定位置大于链表长度，尾部添加\\n        elif index > self.length() - 1:\\n            self.append(item)\\n        else:\\n            node = Node(item)\\n            cur = self._head\\n            # 移动到添加结点位置\\n            for i in range(index - 1):\\n                cur = cur.next\\n            # 新结点指针指向旧结点\\n            node.next = cur.next\\n            # 旧结点指针 指向 新结点\\n            cur.next = node\\n\\n    def remove(self, item):\\n        \"\"\" 删除一个结点 \"\"\"\\n        if self.is_empty():\\n            return\\n        cur = self._head\\n        pre = Node\\n        # 第一个元素为需要删除的元素\\n        if cur.item == item:\\n            # 链表不止一个元素\\n            if cur.next != self._head:\\n                while cur.next != self._head:\\n                    cur = cur.next\\n                # 尾结点指向 头部结点的下一结点\\n                cur.next = self._head.next\\n                # 调整头部结点\\n                self._head = self._head.next\\n            else:\\n                # 只有一个元素\\n                self._head = None\\n        else:\\n            # 不是第一个元素\\n            pre = self._head\\n            while cur.next != self._head:\\n                if cur.item == item:\\n                    # 删除\\n                    pre.next = cur.next\\n                    return True\\n                else:\\n\\n                    pre = cur  # 记录前一个指针\\n                    cur = cur.next  # 调整指针位置\\n        # 当删除元素在末尾\\n        if cur.item == item:\\n            pre.next = self._head\\n            return True\\n\\n    def find(self, item):\\n        \"\"\" 查找元素是否存在\"\"\"\\n        return item in self.items()\\n\\nif __name__ == \\'__main__\\':\\n    link_list = SingleCycleLinkList()\\n    print(link_list.is_empty())\\n    # 头部添加元素\\n    for i in range(5):\\n        link_list.add(i)\\n    print(list(link_list.items()))\\n    # 尾部添加元素\\n    for i in range(6):\\n        link_list.append(i)\\n    print(list(link_list.items()))\\n    # 添加元素\\n    link_list.insert(3, 45)\\n    print(list(link_list.items()))\\n    # 删除元素\\n    link_list.remove(5)\\n    print(list(link_list.items()))\\n    # 元素是否存在\\n    print(4 in link_list.items())\\n```\\n\\n```python\\n#定义双向链表结点\\nclass Node(object):\\n    \"\"\"双向链表的结点\"\"\"\\n\\n    def __init__(self, item):\\n        # item存放数据元素\\n        self.item = item\\n        # next 指向下一个节点的标识\\n        self.next = None\\n        # prev 指向上一结点\\n        self.prev = None\\n\\n#定义双向链表\\nclass BilateralLinkList(object):\\n    \"\"\"双向链表\"\"\"\\n\\n    def __init__(self):\\n        self._head = None\\n\\n    def is_empty(self):\\n        \"\"\"判断链表是否为空\"\"\"\\n        return self._head is None\\n\\n    def length(self):\\n        \"\"\"链表长度\"\"\"\\n        # 初始指针指向head\\n        cur = self._head\\n        count = 0\\n        # 指针指向None 表示到达尾部\\n        while cur is not None:\\n            count += 1\\n            # 指针下移\\n            cur = cur.next\\n        return count\\n\\n    def items(self):\\n        \"\"\"遍历链表\"\"\"\\n        # 获取head指针\\n        cur = self._head\\n        # 循环遍历\\n        while cur is not None:\\n            # 返回生成器\\n            yield cur.item\\n            # 指针下移\\n            cur = cur.next\\n\\n    def add(self, item):\\n        \"\"\"向链表头部添加元素\"\"\"\\n        node = Node(item)\\n        if self.is_empty():\\n            # 头部结点指针修改为新结点\\n            self._head = node\\n        else:\\n            # 新结点指针指向原头部结点\\n            node.next = self._head\\n            # 原头部 prev 指向 新结点\\n            self._head.prev = node\\n            # head 指向新结点\\n            self._head = node\\n\\n    def append(self, item):\\n        \"\"\"尾部添加元素\"\"\"\\n        node = Node(item)\\n        if self.is_empty():  # 链表无元素\\n            # 头部结点指针修改为新结点\\n            self._head = node\\n        else:  # 链表有元素\\n            # 移动到尾部\\n            cur = self._head\\n            while cur.next is not None:\\n                cur = cur.next\\n            # 新结点上一级指针指向旧尾部\\n            node.prev = cur\\n            # 旧尾部指向新结点\\n            cur.next = node\\n\\n    def insert(self, index, item):\\n        \"\"\" 指定位置插入元素\"\"\"\\n        if index <= 0:\\n            self.add(item)\\n        elif index > self.length() - 1:\\n            self.append(item)\\n        else:\\n            node = Node(item)\\n            cur = self._head\\n            for i in range(index):\\n                cur = cur.next\\n            # 新结点的向下指针指向当前结点\\n            node.next = cur\\n            # 新结点的向上指针指向当前结点的上一结点\\n            node.prev = cur.prev\\n            # 当前上一结点的向下指针指向node\\n            cur.prev.next = node\\n            # 当前结点的向上指针指向新结点\\n            cur.prev = node\\n\\n    def remove(self, item):\\n        \"\"\" 删除结点 \"\"\"\\n        if self.is_empty():\\n            return\\n        cur = self._head\\n        # 删除元素在第一个结点\\n        if cur.item == item:\\n            # 只有一个元素\\n            if cur.next is None:\\n                self._head = None\\n                return True\\n            else:\\n                # head 指向下一结点\\n                self._head = cur.next\\n                # 下一结点的向上指针指向None\\n                cur.next.prev = None\\n                return True\\n        # 移动指针查找元素\\n        while cur.next is not None:\\n            if cur.item == item:\\n                # 上一结点向下指针指向下一结点\\n                cur.prev.next = cur.next\\n                # 下一结点向上指针指向上一结点\\n                cur.next.prev = cur.prev\\n                return True\\n            cur = cur.next\\n        # 删除元素在最后一个\\n        if cur.item == item:\\n            # 上一结点向下指针指向None\\n            cur.prev.next = None\\n            return True\\n\\n    def find(self, item):\\n        \"\"\"查找元素是否存在\"\"\"\\n        return item in self.items()\\n```\\n\\n为了能够获取和设置Node里面的信息，我们还需要定义几个方法，代码如下：\\n\\n`def __init__(self, data):\\n        self.data = data\\n        self.next = None\\n    \\n    # 获取node里面的数据\\n    def getData(self):\\n        return self.data\\n    \\n    # 获取下一个节点的引用\\n    def getNext(self):\\n        return self.next\\n    \\n    # 设置node里面的数据\\n    def setData(self, newdata):\\n        self.data = newdata\\n    \\n    # 设置下一个节点的引用\\n    def setNext(self, newnext):\\n        self.next = newnext`\\n\\n这些方法用于存取Node里面的数据，方便在链表结构里面去使用。\\n\\nNode对象定义好之后，接下来我们就可以开始定义链表对象了。我们这里讲的是单向链表，所以英文成为Single Link List。定义链表时，最主要的是定义好链表的头(head)，因为之前我们说过，我们只要找到了链表的头，就能够沿着这个头找到其他所有的node。所以，链表的初始定义很简单，我们只要定义一个head属性即可，代码如下：\\n\\n`class MySingleLinkList():\\n    \\n\\n    def __init__(self):\\n\\n        # 定义链表的头结点\\n\\n        self.head = None`\\n\\n**这里大家要注意一点，链表对象本身是不包含任何节点Node对象的，相反，它只包含对链接结构中第一个节点的单个引用(self.head)，这个head实际上永远会指向链表中的第一个节点。如果head为None，实际上意味着这是一个空的链表。链表LinkList对象和Node对象从定义上是独立的，互相并不包含对方。**这个基本思想很重要，只要大家记住这个基本原则，那么我们就可以开始接着实现链表中的其他方法。\\n\\n!https://pic2.zhimg.com/80/v2-825cceb9d3c1a11a17969465bcea06d1_720w.jpg\\n\\n图2\\n\\n一般来说，一个链表中应该包含的基本操作主要有以下几个：\\n\\n- 判断链表是否为空 isEmpty()\\n\\n`def isEmpty(self):\\n    \\'\\'\\'\\n    判断head指向的节点是否为None，如果head指向None，说明该链表为空\\n    \\'\\'\\'\\n    return self.head == None`\\n\\n- 获取链表的长度 size()\\n\\n获取链表的长度的关键是要遍历这个链表，并对节点数进行计数。遍历链表是链表操作中会被频繁使用到的基本操作，像链表节点的查询、删除等操作都会涉及到链表的遍历。我们之前说过，遍历链表时，我们必须先找到链表的head节点，从链表的头部开始不断地查找每个节点的next节点，最终直到某一个节点的next指向None，就说明遍历完成了。\\n\\n`def size(self):\\n    current = self.head  # 将链表的头节点赋值给current，代表当前节点\\n    count = 0\\n    while current != None:\\n        count += 1\\n        current = current.getNext()  # 计数后，不断把下一个节点的引用赋值给当前节点，这样我们就能不断向后面的节点移动\\n    return count`\\n\\n- 向链表中增加一个节点 add()\\n\\n向链表中增加一个节点的时候，我们要考虑两个问题。第一个问题是，新加入的节点该加到哪里？大家可以想一想，链表是一个无序结构，其实把新的节点加到哪里对链表本身来说是无所谓的。但加入到链表的不同位置，对于我们的代码操作难度是有区别的。因为我们之前定义的链表结构始终只保持对第一个节点的引用，所以从这个角度来看，最简单的方法就是把新的节点加入到链表的头部，使新节点成为链表的第一个节点，然后以前的节点依次后移。第二个问题是，加入新节点的时候，要进行哪些操作？实际上要加入一个新的节点，需要两个步骤。首先需要把之前的head节点赋值给新节点的下一个节点，也就是新节点的next，然后再把新节点赋值给head节点，让它成为新的head（如图3所示）。\\n\\n!https://pic4.zhimg.com/80/v2-c1cf651aff22ea10cc54badddcca6aeb_720w.jpg\\n\\n图3\\n\\n`def add(self, val):\\n    temp = Node(val)\\n    temp.next = self.head   # 将原来的开始节点设置为新开始节点的下一节点\\n    self.head = temp        # 将新加入节点设置为现在的第一个节点`\\n\\n必须要注意temp.next = self.head和self.head=temp这两个语句的先后顺序，如果把self.head=temp写在前面，则会使得head原来指向的下一个节点的信息全部丢失，这并不是我们想要的结果(如图4所示)。\\n\\n!https://pic3.zhimg.com/80/v2-b4d28539d2e121c81510b2e675bcaede_720w.jpg\\n\\n图4\\n\\n- 查找指定节点是否在链表中 search()\\n\\n要实现查找算法，必然也是要遍历链表的，我们可以设置一个布尔变量作为是否查找到目标元素的标志，然后通过遍历链表中的每个元素，判断该元素的值是否等于要查找的值，如果是，则将布尔值设置为True，最后返回该布尔值即可。代码如下：\\n\\n`def search(self, item):\\n    current = self.head\\n    found = False\\n    while current != None and not found:\\n        if current.getData() == item:\\n            found = True\\n        else:\\n            current = current.getNext()\\n        \\n    return found`\\n\\n- 移除指定节点 remove()\\n\\n移除指定节点也是在链表中一个常见的操作，在移除指定节点时，除了要先遍历链表找到指定元素外，还需要对这个即将被移除的节点做一些处理，以确保剩下的节点能够正常工作。在我们找到要被移除的节点时，按照之前写过的遍历方法我们知道，current应该是指向要被移除的节点。可问题是怎么才能移除掉该节点呢？为了能够移除节点，我们需要修改上一个节点中的链接，以便使其直接指向当前将被移除节点的下一个节点，使没有任何其他节点指向这个被移除的节点，以达到移除节点的目的。但这里有个问题，就是当我们循环链表到当前节点时，没法回退回去操作当前节点的前一个节点。所以，为了解决这个问题，在遍历链表时，除了需要记录当前指向的节点current外，还需要设置一个变量来记录当前节点的上一个节点previous，每次循环时，如果当前节点不是要被移除的节点，那么就将当前节点的值赋值给previous，而将下一个节点的引用赋值给当前节点，以达到向前移动的目的。同时，在找到了将被移除的节点后，我们会把found设置为true，停止遍历。\\n\\n另外，在删除节点时，可能会有三种情况：\\n\\n（1）被移除的节点就是链表中的开始节点，这时previous一定是None值，我们只需要将current.next赋值给head即可。（2）被移除的节点是链表中最后的节点。（3）被移除的节点是普通节点（即不是第一个也不是最后一个节点）。其中第（2）（3）种情况并不需要特殊处理，直接设置previous的next为current的next即可。\\n\\n`def remove(self, item):\\n    current = self.head\\n    previous = None\\n    found = False\\n    \\n    # 判断指定值是否存在于链表中 \\n    if not self.search(item):\\n        return\\n        \\n    while not found:\\n        if current.getData() == item:\\n            found = True\\n        else:\\n            previous = current\\n            current = current.getNext()\\n        \\n    if previous == None:\\n        self.head = current.getNext()\\n    else:\\n        previous.setNext(current.getNext())`\\n\\n- 获取链表中所有节点的值 getAllData()\\n\\n获取链表中的所有节点的值方便我们随时查看链表中究竟有哪些值。由于链表并不像普通的list一样可以直接打印出来看，所以一般我们需要借助于遍历链表把链表中的每个节点的值取出来放到一个列表中，然后再打印这个列表，从而取得链表中所有节点值的目的。\\n\\n`def getAllData(self):    # 得到链表中所有的值\\n    data = []\\n    current = self.head\\n    while current:\\n        data.append(current.getData())\\n        current = current.getNext()\\n    return data`\\n\\n这些就是我们在链表中常用的操作方法及对应的代码实现，接下来我们可以尝试来操作并测试一下我们写的这些方法对不对。\\n\\n`linkList = MySingleLinkList()\\nfor i in range(10, 50, 5):\\n    linkList.add(i)\\nprint(linkList.size())  # output: 8\\nprint(linkList.getAllData()) # output: [45, 40, 35, 30, 25, 20, 15, 10]\\n\\nlinkList.remove(25)\\nprint(linkList.getAllData()) # output: [45, 40, 35, 30, 20, 15, 10]\\n\\nlinkList.search(25)  # output: False\\nlinkList.isEmpyt()   # output: False`\\n\\n---\\n\\n##########################\\n\\n示例：操作链表\\n\\n```python\\n**if** __name__ **==** \\'__main__\\':\\n    link_list **=** SingleLinkList()\\n    *# 向链表尾部添加数据***for** i **in** range(5):\\n        link_list**.**append(i)\\n    *# 向头部添加数据*link_list**.**add(6)\\n    *# 遍历链表数据***for** i **in** link_list**.**items():\\n        print(i, end**=**\\'\\\\t\\')\\n    *# 链表数据插入数据*link_list**.**insert(3, 9)\\n    print(\\'\\\\n\\', list(link_list**.**items()))\\n    *# 删除链表数据*link_list**.**remove(0)\\n    *# 查找链表数据*print(link_list**.**find(4))\\n```\\n\\n#插入单链表\\n\\n\\n\\n```python\\nListValue = [1, 4, 5, 2]\\nListRight = [3, 2, -1, 1]\\nhead = 0\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000 #初始化头指针\\nnum = 3\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000#num为要插入的元素\\nnext,last = head,head\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000#初始化表示插入位置的下一个元素和上一个元素的指针\\ndef Output():\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000 #定义一个函数用于输出链表\\n    next = head\\n    while next != -1:\\n        print(ListValue[next])\\n        next = ListRight[next]\\nOutput()\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000 #输出列表查看插入前的顺序\\n    while ListValue[next] <= num and next != -1:\\u3000\\u3000 #找到适合插入元素的位置\\n        last = next\\n        next = ListRight[next]\\nListValue.append(num)\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000 #向数组末尾加上新元素的值\\nListRight.append(ListRight[last])\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000 #加上新元素指针指向的位置（下一个元素）\\nListRight[last] = len(ListValue)-1\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000#上一个元素的指针指向新元素\\nOutput()\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000 #输出列表查看结果\\n```\\n\\n节点类定义如下：\\n\\nclass Node:\\ndef **init**(self,cargo = None, next = None):\\nself.cargo = cargo\\nself.next = next\\ndef **str**(self):\\n#测试基本功能，输出字符串\\nreturn str(self.cargo)\\nprint Node(\"text\")\\n#输出text\\n\\n因为任何值都能通过str函数，且能存储下来。\\n\\n链表怎么定义呢？\\n我们可以先定义一个一个节点，如下：\\n\\nnode1 = Node(1)\\nnode2 = Node(2)\\nnode3 = Node(3)\\n1\\n2\\n3\\n然后再把每个节点的关系表示出来，就OK了\\n\\nnode1.next = node2\\nnode2.next = node3\\n————————————————\\n2.1 计算链表长度\\n\\nclass Node(object):\\n#节点类\\n#功能：输入一个值data，将值变为一个节点\\ndef **init**(self, data, next = None):\\nself.data = data\\nself.next = next\\n\\n```\\nclass Node(object):\\n#节点类\\n    #功能：输入一个值data，将值变为一个节点\\n    def __init__(self, data, next = None):\\n        self.data = data\\n        self.next = next\\n\\n    def __str__(self):\\n        return self.data\\n\\nclass LinkedList(object):\\n\\n    def __init__(self, head = None):\\n        self.head = head\\n    def __len__(self):\\n        #功能：输入头节点，返回链表长度\\n        curr = self.head\\n        counter = 0\\n        while curr is not None:\\n            counter += 1\\n            curr = curr.next\\n        return counter\\n————————————————\\n版权声明：本文为CSDN博主「黄小猿」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/qq_39422642/article/details/78988976\\n```\\n\\n> 2.2 从前，后插入\\n> \\n\\n从前插入：\\n\\n- 被插入数据为空，返回\\n- 使用该输入数据创建一个节点，并将该节点指向原来头节点\\n- 设置该节点为头节点\\n\\n\\n\\n从后：append\\n\\n- 若输入数据为空，返回None\\n- 若头节点为空，直接将输入数据作为头节点\\n- 遍历整个链表，直到当前节点的下一个节点为None时，将当前节点的下一个节点设置为输入数据\\n\\n\\n\\n**查找**\\n\\n- 若查找的数据为空，返回\\n- 设置头节点为当前节点，若当前节点不为None,遍历整个链表\\n- 若当前节点的data与输入的data相同，但会当前节点，否则轮到下一个节点\\n\\n\\n\\n删除1\\n申请两个变量，如果遇到匹配的，不用删除，直接将匹配节点的前一节点指向匹配节点的下一节点，因此需要定义一个前节点和一个当前节点，当前节点用来判断是否与输入数据匹配，前节点用来更改链表的指向。\\n\\n若输入数据为None,返回\\n将头节点设置为前节点，头节点的下一个节点设置为当前节点\\n判断前节点是否与输入数据匹配，若匹配，将头节点设置为当前节点\\n遍历整个链表，若当前节点与输入数据匹配，将前节点的指针指向当前节点的下一个节点，否则，移到下一个节点\\n\\n时间复杂度为O(n),空间复杂度为O(1).\\n————————————————\\n版权声明：本文为CSDN博主「黄小猿」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/qq_39422642/article/details/78988976\\n\\n\\n\\n**删除2**第二种解决办法就是只定义一个变量作为当前节点，使用它的下一个节点去判断是否与数据数据匹配，若匹配，直接将当前节点指向下下一个节点。\\n\\n时间复杂度为O(n),空间复杂度为O(1).\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cda18574-9986-4e41-b594-2b02b8eddc92', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n递归1\\n\\n由于递归函数调用自己，因此编写这样的函数时很容易出错，进而导致\\n无限循环。例如，假设你要编写一个像下面这样倒计时的函数\\n\\n\\n\\n编写递归函数时，必须告诉它何时停止递归。正因为如此，每个递归\\n函数都有两部分：基线条件 （base case）和递归条件 （recursive\\ncase）。递归条件指的是函数调用自己，而基线条件则指的是函数不再\\n调用自己，从而避免形成无限循环。\\n\\n\\n\\n\\n\\n\\n\\n注意，每个fact 调用都有自己的x 变量。在一个函数调用中不能访问另 一个的x 变量。\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9b0d1a24-440b-4475-8e95-2706d5b81aa6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n树 Code\\n\\n我们先来定义一下 二叉树的节点结构：\\n\\n```\\nclassBinTreeNode(object):\\ndef__init__(self, data, left=None, right=None):\\n        self.data, self.left, self.right = data, left, right\\n\\n```\\n\\n当然和链表类似，root 节点是我们的入口，于是乎定义一个 二叉树：\\n\\n```\\nclassBinTree(object):\\ndef__init__(self, root=None):\\n        self.root = root\\n\\n```\\n\\n怎么构造上图中的二叉树呢，似乎其他课本没找到啥例子(有些例子是写了一堆嵌套节点来定义，很难搞清楚层次关系)，我自己定义了一种方法，首先我们输入节点信息，仔细看下边代码，叶子节点的 left 和 right 都是 None，并且只有一个根节点 A:\\n\\n```\\nnode_list = [\\n    {\\'data\\': \\'A\\', \\'left\\': \\'B\\', \\'right\\': \\'C\\', \\'is_root\\':True},\\n    {\\'data\\': \\'B\\', \\'left\\': \\'D\\', \\'right\\': \\'E\\', \\'is_root\\':False},\\n    {\\'data\\': \\'D\\', \\'left\\':None, \\'right\\':None, \\'is_root\\':False},\\n    {\\'data\\': \\'E\\', \\'left\\': \\'H\\', \\'right\\':None, \\'is_root\\':False},\\n    {\\'data\\': \\'H\\', \\'left\\':None, \\'right\\':None, \\'is_root\\':False},\\n    {\\'data\\': \\'C\\', \\'left\\': \\'F\\', \\'right\\': \\'G\\', \\'is_root\\':False},\\n    {\\'data\\': \\'F\\', \\'left\\':None, \\'right\\':None, \\'is_root\\':False},\\n    {\\'data\\': \\'G\\', \\'left\\': \\'I\\', \\'right\\': \\'J\\', \\'is_root\\':False},\\n    {\\'data\\': \\'I\\', \\'left\\':None, \\'right\\':None, \\'is_root\\':False},\\n    {\\'data\\': \\'J\\', \\'left\\':None, \\'right\\':None, \\'is_root\\':False},\\n]\\n```\\n\\n然后我们给 BinTreeNode 定义一个 build_from 方法，当然你也可以定义一种自己的构造方法：\\n\\n```\\nclass BinTree(object):\\n    def __init__(self, root=None):\\n        self.root = root\\n\\n    @classmethod\\n    def build_from(cls, node_list):\\n        \"\"\"通过节点信息构造二叉树\\n        第一次遍历我们构造 node 节点\\n        第二次遍历我们给 root 和 孩子赋值\\n        最后我们用 root 初始化这个类并返回一个对象\\n\\n        :param node_list: {\\'data\\': \\'A\\', \\'left\\': None, \\'right\\': None, \\'is_root\\': False}\\n        \"\"\"\\n        node_dict = {}\\n        for node_data in node_list:\\n            data = node_data[\\'data\\']\\n            node_dict[data] = BinTreeNode(data)\\n        for node_data in node_list:\\n            data = node_data[\\'data\\']\\n            node = node_dict[data]\\n            if node_data[\\'is_root\\']:\\n                root = node\\n            node.left = node_dict.get(node_data[\\'left\\'])\\n            node.right = node_dict.get(node_data[\\'right\\'])\\n        return cls(root)\\nbtree = BinTree.build_from(node_list)\\n```\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e21a3c2d-d85a-4492-b894-0ae8bfaede33', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n而几乎所有的题目几乎都是二叉树，因此下面这个模板更常见\\n\\n```\\nfunction dfs(root) {\\n\\tif (满足特定条件）{\\n\\t\\t// 返回结果 or 退出搜索空间\\n\\t}\\n    dfs(root.left)\\n    dfs(root.right)\\n}\\n```\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2e132d2a-4c7e-4a2f-81e6-070c7ba882f8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**二叉树的遍历**\\n\\n不知道你有没有发现，二叉树其实是一种递归结构，因为单独拿出来一个 subtree 子树出来，其实它还是一棵树。那遍历它就很方便啦，我们可以直接用递归的方式来遍历它。但是当处理顺序不同的时候，树又分为三种遍历方式:\\n\\n- 先(根)序遍历: 先处理根，之后是左子树，然后是右子树\\n- 中(根)序遍历: 先处理左子树，之后是根，最后是右子树\\n- 后(根)序遍历: 先处理左子树，之后是右子树，最后是根\\n\\n我们来看下实现，其实算是比较直白的递归函数:\\n\\n```python\\nclass BinTreeNode(object):\\n    def __init__(self, data, left=None, right=None):\\n        self.data, self.left, self.right = data, left, right\\n\\nclass BinTree(object):\\n    def __init__(self, root=None):\\n        self.root = root\\n\\n    @classmethod\\n    def build_from(cls, node_list):\\n        \"\"\"通过节点信息构造二叉树\\n        第一次遍历我们构造 node 节点\\n        第二次遍历我们给 root 和 孩子赋值\\n\\n        :param node_list: {\\'data\\': \\'A\\', \\'left\\': None, \\'right\\': None, \\'is_root\\': False}\\n        \"\"\"\\n        node_dict = {}\\n        for node_data in node_list:\\n            data = node_data[\\'data\\']\\n            node_dict[data] = BinTreeNode(data)\\n        for node_data in node_list:\\n            data = node_data[\\'data\\']\\n            node = node_dict[data]\\n            if node_data[\\'is_root\\']:\\n                root = node\\n            node.left = node_dict.get(node_data[\\'left\\'])\\n            node.right = node_dict.get(node_data[\\'right\\'])\\n        return cls(root)\\n\\n    def preorder_trav(self, subtree):\\n        \"\"\" 先(根)序遍历\\n\\n        :param subtree:\\n        \"\"\"\\n        if subtree is not None:\\n            print(subtree.data)    # 递归函数里先处理根\\n            self.preorder_trav(subtree.left)   # 递归处理左子树\\n            self.preorder_trav(subtree.right)    # 递归处理右子树\\n\\nnode_list = [\\n    {\\'data\\': \\'A\\', \\'left\\': \\'B\\', \\'right\\': \\'C\\', \\'is_root\\': True},\\n    {\\'data\\': \\'B\\', \\'left\\': \\'D\\', \\'right\\': \\'E\\', \\'is_root\\': False},\\n    {\\'data\\': \\'D\\', \\'left\\': None, \\'right\\': None, \\'is_root\\': False},\\n    {\\'data\\': \\'E\\', \\'left\\': \\'H\\', \\'right\\': None, \\'is_root\\': False},\\n    {\\'data\\': \\'H\\', \\'left\\': None, \\'right\\': None, \\'is_root\\': False},\\n    {\\'data\\': \\'C\\', \\'left\\': \\'F\\', \\'right\\': \\'G\\', \\'is_root\\': False},\\n    {\\'data\\': \\'F\\', \\'left\\': None, \\'right\\': None, \\'is_root\\': False},\\n    {\\'data\\': \\'G\\', \\'left\\': \\'I\\', \\'right\\': \\'J\\', \\'is_root\\': False},\\n    {\\'data\\': \\'I\\', \\'left\\': None, \\'right\\': None, \\'is_root\\': False},\\n    {\\'data\\': \\'J\\', \\'left\\': None, \\'right\\': None, \\'is_root\\': False},\\n]\\n\\nbtree = BinTree.build_from(node_list)\\nbtree.preorder_trav(btree.root)    # 输出 A, B, D, E, H, C, F, G, I, J\\n```\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a738064b-deb5-4c96-a57f-0945c53adda5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n二叉树的中序遍历\\n\\n```python\\n\\n#我自己写的\\n\\ndef inorder(root):\\n    if not root:\\n        return\\n    inorder(root.left)\\n    print(root.val)\\n    inorder(root.right)\\n\\ninorder(root)\\n```\\n\\n```python\\nclass Solution:\\n    def inorderTraversal(self, root: TreeNode) -> List[int]:\\n        \"\"\"\\n        1. 递归法可以一行代码完成，无需讨论；\\n        2. 迭代法一般需要通过一个栈保存节点顺序，我们这里直接使用列表\\n          - 首先，我要按照中序遍历的顺序存入栈，这边用的逆序，方便从尾部开始处理\\n          - 在存入栈时加入一个是否需要深化的参数\\n          - 在回头取值时，这个参数应该是否，即直接取值\\n          - 简单调整顺序，即可实现前序和后序遍历\\n        \"\"\"\\n        # 递归法\\n        # if root is None:\\n        #     return []\\n        # return self.inorderTraversal(root.left)\\\\\\n        #     + [root.val]\\\\\\n        #     + self.inorderTraversal(root.right)\\n        # 迭代法\\n        result = []\\n        stack = [(1, root)]\\n        while stack:\\n            go_deeper, node = stack.pop()\\n            if node is None:\\n                continue\\n            if go_deeper:\\n                # 左右节点还需继续深化，并且入栈是先右后左\\n                stack.append((1, node.right))\\n                # 节点自身已遍历，回头可以直接取值\\n                stack.append((0, node))\\n                stack.append((1, node.left))\\n            else:\\n                result.append(node.val)\\n        return result\\n```\\n\\n```python\\n#inorder BST 的另一种写法\\n# Recursive function to insert an key into BST\\ndef insert(root, x):\\n     \\n    if (root == None):\\n        return Node(x)\\n    if (x < root.data):\\n        root.left = insert(root.left, x)\\n    elif (x > root.data):\\n        root.right = insert(root.right, x)\\n    return root\\n```\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6cc51139-3b45-4cfa-8ddb-85e601923ba8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**前序遍历**\\n\\n如果你的代码大概是这么写的（注意主要逻辑的位置）：\\n\\n```python\\nfunction dfs(root) {\\n\\tif (满足特定条件）{\\n\\t\\t// 返回结果 or 退出搜索空间\\n    }\\n    // 主要逻辑\\n    dfs(root.left)\\n    dfs(root.right)\\n}\\n```\\n\\n那么此时我们称为前序遍历。\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='279a7f6d-589e-4652-bfe6-7cb4a473c9bb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**后序遍历**\\n\\n而如果你的代码大概是这么写的（注意主要逻辑的位置）：\\n\\n```python\\nfunction dfs(root) {\\n\\tif (满足特定条件）{\\n\\t\\t// 返回结果 or 退出搜索空间\\n    }\\n    dfs(root.left)\\n    dfs(root.right)\\n    // 主要逻辑\\n}\\n\\n```\\n\\n中**序遍历**\\n\\n```python\\nfunction dfs(root) {\\n\\tif (满足特定条件）{\\n\\t\\t// 返回结果 or 退出搜索空间\\n    }\\n    dfs(root.left)\\n// 主要逻辑\\n    dfs(root.right)\\n    \\n}\\n```\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='39969cb6-154b-496c-8424-49e725f77dec', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**二叉树层序遍历**\\n\\n除了递归的方式遍历之外，我们还可以使用层序遍历的方式。层序遍历比较直白，就是从根节点开始按照一层一层的方式遍历节点。 我们可以从根节点开始，之后把所有当前层的孩子都按照从左到右的顺序放到一个列表里，下一次遍历所有这些孩子就可以了。\\n\\n```\\ndef layer_trav(self, subtree):\\n        cur_nodes = [subtree]  # current layer nodes\\n        next_nodes = []\\n        while cur_nodes or next_nodes:\\n            for node in cur_nodes:\\n                print(node.data)\\n                if node.left:\\n                    next_nodes.append(node.left)\\n                if node.right:\\n                    next_nodes.append(node.right)\\n            cur_nodes = next_nodes  # 继续遍历下一层\\n            next_nodes = []\\n```\\n\\n还有一种方式就是使用一个队列，之前我们知道队列是一个先进先出结构，如果我们按照一层一层的顺序从左往右把节点放到一个队列里， 也可以实现层序遍历\\n\\n```python\\ndef layer_trav_use_queue(self, subtree):\\n        q = Queue()\\n        q.append(subtree)\\n        while not q.empty():\\n            cur_node = q.pop()\\n            print(cur_node.data)\\n            if cur_node.left:\\n                q.append(cur_node.left)\\n            if cur_node.right:\\n                q.append(cur_node.right)\\n\\nfrom collections import deque\\nclass Queue(object):  # 借助内置的 deque 我们可以迅速实现一个 Queue\\n    def __init__(self):\\n        self._items = deque()\\n\\n    def append(self, value):\\n        return self._items.append(value)\\n\\n    def pop(self):\\n        return self._items.popleft()\\n\\n    def empty(self):\\n        return len(self._items) == 0\\n```\\n\\n二叉查找树（Binary Search Tree），又称为二叉搜索树、二叉排序树。\\n\\n链表插入数据很快，查询慢，数组查询快，插入慢，而二叉查找树则两者都比较快。\\n无特征的树结构基本没什么用。而叉查找树是一种有树结构有特征的结构，能够做到插入和查询的相对快速。\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f7fb4f39-1fc1-408d-be5d-6fdd9d67bc8f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nNode class\\n\\n要代表一个树结构，我们需要创建一个类包含三个属性\\n\\n- Left node\\n- Right node\\n- Node's data\\n\\n`class Node:\\n    def __init__(self, data):\\n        self.left = None\\n        self.right = None\\n        self.data = data`\\n\\n创建root节点：\\n\\n`root = Node(8)`\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b9bfafae-7707-46e2-8e69-871bb8bf4de9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nInsert method\\n\\n```python\\nclass Node:\\n    ......\\n    def insert(self, data):\\n        if self.data is None:\\n            self.data = data\\n            return \\n        if data < self.data:\\n            if self.left is None:\\n                self.left = Node(data)\\n            else:\\n                self.left.insert(data)  # 递归调用\\n        elif data > self.data:  # data 大于当前节点 应该放在右边\\n            if self.right is None:\\n                self.right = Node(data)\\n            else:\\n                self.right.insert(data)\\n```\\n\\n```\\nroot.insert(3)\\nroot.insert(10)\\nroot.insert(1)\\n```\\n\\n这是当添加了第二个节点（第一个是root节点，8） node(3)发生的事情:\\n\\n- root节点调用了insert(),插入一个data=3的数据\\n- 3小于8,左边的树为None，所以把3附加在了root节点的左边。\\n\\n这是当添加了第三个节点node(10)发生的事情:\\n\\n- root节点的方法调用了insert方法，传递参数data=10\\n- 10大于8 并且右子树为None，所以把8添加到root节点的右节点\\n\\n这是当添加了第四个节点node(1)发生的事情:\\n\\n- root 的节点的insert方法调用，传递参数data=1\\n- 1小于3,所以会root‘s 左child(3)的insert方法会调用，并且传递参数data=1\\n- 1 < 3,并且node(3)的左child为None, 所以node(1)添加在node(3)的左边。\\n\\n现在这个二分查找树的结构像这样：\\n\\n\\n\\n```\\nroot.insert(6)\\nroot.insert(4)\\nroot.insert(7)\\nroot.insert(14)\\nroot.insert(13)\\n```\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bee0ca6f-5404-4191-8d28-99e27478a41c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nLookup method\\n\\n我们需要一种方法在二叉查找树上查找指定的一个数，我们添加一个新的方法叫lookup，需要一个node节点的数据作为参数，返回None如果没有找到，返回节点已经父节点如果找到该数据。\\n\\n```python\\nclass Node:\\n......\\n    def lookup(self, data, parent=None):\\n        if data < self.data:\\n            if self.left is None:\\n                return None, None\\n            return self.left.lookup(data, self)\\n        elif data > self.data:\\n            if self.right is None:\\n                return None, None\\n            return self.right.lookup(data, self)\\n        else:\\n            return self, parent\\n```\\n\\n现在查找node（6）试一试\\n\\n`node, parent = root.lookup(6)`\\n\\n以下流程解释了当lookup()函数调用之后的结果:\\n\\n- root's 的lookup() 函数调用了，data参数是6, parent是默认值为None\\n- data=6小于root’s节点 8\\n- root's 的左边的child的lookup()调用，传入data=6,parent=当前节点(node(3))\\n- data=6 大于 data=3\\n- node(3)的右边的child的lookup()函数调用，传入data=6,parent=当前节点\\n- node's 的data数据相等，返回当前节点和父节点\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d59b195e-6b87-43d2-b1ca-add9ef994cfd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nDelete method\\n\\ndelete()方法需要一个被删除节点的参数\\n\\n`class Node:\\n......\\n    def delete(self, data):\\n        node, parent = self.lookup(data)\\n        if node is not None:\\n            children_count = node.children_count()`\\n\\n这里有三种可能需要处理\\n\\n- 要删除的节点没有child\\n- 要删除的节点有一个child\\n- 要删除的节点有两个child\\n\\n处理第一种情况挺容易的，我们找到要删除的数据的节点，设置它的左child或者右child为None，如果是root节点，清除它的数据。\\n\\n作者：Baloneo\\n\\n链接：https://www.jianshu.com/p/60580b496a84\\n\\n来源：简书\\n\\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\\n\\n```\\n    def delete(self, data):\\n        node, parent = self.lookup(data)\\n        if node is not None:\\n            children_count = node.children_count()\\n        if children_count == 0:\\n            if parent:\\n                if parent.left is node: # 判断是否是parent的左节点\\n                    parent.left = None\\n                else:\\n                    parent.right = None\\n                del node\\n            else:\\n                self.data = None  # root node\\n\\n```\\n\\n```\\nclass Node():\\n......\\n    def children_count(self):\\n        cnt = 0\\n        if self.left:\\n            cnt += 1\\n        if self.right:\\n            cnt += 1\\n        return cnt\\n```\\n\\nroot.delete(1)\\n\\n举一个例子，我们想删除node(1), Node（3）的左child会设置为None\\n\\n`root.delete(1)`\\n\\n\\n\\n现在，让我们处理第二种情况，删除的节点有一个child，我们将节点替换为它的child，当是root节点的时候，还做了特殊处理。\\n\\n```python\\nelif children_count == 1:\\n    if node.left:\\n        n = node.left\\n    else:\\n        n = node.right\\n    if parent:\\n        if parent.left is node:  # 判断是否是parent的左节点\\n            parent.left = n\\n        else:\\n            parent.right = n\\n        del node\\n    else:  # root node\\n        self.left = n.left\\n        self.right = n.right\\n        self.data = n.data\\n```\\n\\n例如，我们想删除node(14), Node 14 的data会设置为13(它的左child's data) ,并且它的左child会设置为None\\n\\n让我们看最后一种可能,删除的节点有2个children，替换数据为继承者的数据，然后修复继承者的parent‘s的child\\n\\n       `else:\\n            # if node has 2 children\\n            # 找到它的继承者\\n            parent = node\\n            successor = node.right\\n            while successor.left:\\n                parent = successor\\n                successor = successor.left\\n            # 替换接点数据为继承者(子节点)数据\\n            node.data = successor.data\\n            if parent.left == successor:\\n                parent.left = successor.right\\n            else:\\n                parent.right = successor.right`\\n\\n例如， 我们想删除node 3, 我们正确的查找到了叶子节点(最后一个节点)才离开循环，把node 3 替换为了node 4,node 4没有child，所以把node 6作为右child\\n\\n\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='188aa936-233e-4f41-a473-7aab5783dad2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nPrint method\\n\\n我们添加了一个方法打印树的中序(左中右)，这个方法不需要参数。我们在print_tree()里面使用递归来走到树的最深处，先贯穿整个左子树，再打印root节点，然后贯穿右子树。\\n\\n`class Node:\\n......\\n    def print_tree(self):\\n        if self.left:\\n            self.left.print_tree()\\n        print(self.data, end=' ')\\n        if self.right:\\n            self.right.print_tree()`\\n\\n输出的结果是： 1, 3, 4, 6, 7, 8, 10, 13, 14\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fa65dcb4-eba8-4049-b11d-40d1250bb10b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nComparing 2 trees\\n\\n为了比较2个树，我们添加一个方法用来递归的比较每一个子树，当有叶子节点不在相同的两个树中，返回False，包括叶子节点丢失和data不一样。需要传入一个root节点的树来作为比较的参数。\\n\\n`def compare_trees(self, node):\\n    if node is None:  # 传入的节点为None\\n        return False\\n    if self.data != node.data:  # 当前节点的数据和其它树的节点的数据不同\\n        return False\\n    res = True\\n    if self.left is None:\\n        if node.left:  # 当前节点没有右节点 但是其它树节点有\\n            return False\\n    else:\\n        res = self.left.compare_trees(node.left)  # 比较下一个左节点的数据\\n    if res is False:\\n        return False\\n    if self.right is None:\\n        if node.right:  # 逻辑同上 现在遍历右边的节点\\n            return False\\n    else:\\n        res = self.right.compare_trees(node.right)\\n    return res`\\n\\n\\n\\n当调用compare_trees()会发生以下的情况：\\n\\n- root 节点的compare_trees() 方法调用了，用来比较另一个树\\n- root 节点有左child，所以调用了左child的compare_trees()方法\\n- 左子树比较完返回True\\n- root 节点有右child，所以调用了右child的compare_trees()方法\\n- 右子树不同，返回False\\n- 整个compare_trees()返回False\\n\\n作者：Baloneo\\n\\n链接：https://www.jianshu.com/p/60580b496a84\\n\\n来源：简书\\n\\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\\n\\n```python\\n# A utility function to search a given key in BST\\ndef search(root,key):\\n     \\n    # Base Cases: root is null or key is present at root\\n    if root is None or root.val == key:\\n        return root\\n \\n    # Key is greater than root's key\\n    if root.val < key:\\n        return search(root.right,key)\\n   \\n    # Key is smaller than root's key\\n    return search(root.left,key)\\n \\n# This code is contributed by Bhavya Jain\\n```\\n\\n```python\\n# Python program to demonstrate\\n# insert operation in binary search tree\\n \\n# A utility class that represents\\n# an individual node in a BST\\n \\n \\nclass Node:\\n    def __init__(self, key):\\n        self.left = None\\n        self.right = None\\n        self.val = key\\n \\n# A utility function to insert\\n# a new node with the given key\\n \\n \\ndef insert(root, key):\\n    if root is None:\\n        return Node(key)\\n    else:\\n        if root.val == key:\\n            return root\\n        elif root.val < key:\\n            root.right = insert(root.right, key)\\n        else:\\n            root.left = insert(root.left, key)\\n    return root\\n \\n# A utility function to do inorder tree traversal\\n \\n \\ndef inorder(root):\\n    if root:\\n        inorder(root.left)\\n        print(root.val)\\n        inorder(root.right)\\n \\n \\n# Driver program to test the above functions\\n# Let us create the following BST\\n#    50\\n#  /     \\\\\\n# 30     70\\n#  / \\\\ / \\\\\\n# 20 40 60 80\\n \\nr = Node(50)\\nr = insert(r, 30)\\nr = insert(r, 20)\\nr = insert(r, 40)\\nr = insert(r, 70)\\nr = insert(r, 60)\\nr = insert(r, 80)\\n \\n# Print inoder traversal of the BST\\ninorder(r)  \\n```\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='89ff64f2-8acd-4398-959a-c75965a6b9b4', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nabout dummy node\\n\\nAdd a dummy node. Now, every node ALWAYS has a previous and a next and the list is never truly ‘empty’ because of the dummy node.\\n\\nIn algorithms questions, we always pass the head of the linked list as the argument. If you are changing the position of the head node and you need to return the new head node, the problem will be how are you gonna return the new head. That\\'s why we initially create a dummy node and dummy.next will point to the head.\\n\\n!Untitled\\n\\n!Untitled\\n\\nthroughout the while loop, the\\xa0`res`\\xa0variable actually advances, see\\xa0`res = res.next`.\\n\\nBut dummy is always pointing to the initial node. So you sill need that to be able to access to the whole ListNode, not just the last node as\\xa0`res`\\xa0is pointing to at the end of execution.\\n\\n####\\n\\nBecause what value do you put into that first node? You don\\'t know yet. You\\'d have to duplicate the logic that\\'s inside the while loop in order to figure out which value should go first.\\n\\nYou create a dummy node so that you don\\'t have to duplicate code and handle everything inside of the while loop.\\n\\n#####\\n\\nI\\'d like to explicitly highlight that assignment of objects in Python is copying a reference, not copying the object itself. Hence,\\xa0`dummy`\\n\\xa0does change as\\xa0`res`\\n\\xa0does because they refer to the same object in memory\\n\\n######\\n\\nHere the variable dummy has a listnode obj. Which is same as res before the while loop starts. During the first loop iteration res.next gets updated which means dummy.next also gets updated because both res and dummy points to the same listnode. When the loop ends res has been moved to other nodes. So we return the first node ie. Dummy.next.\\n\\nThank you so much! Can you help explain why \"res.next gets updated which means dummy.next also gets updated because both res and dummy points to the same listnode\"? My understanding is that before the loop, res and dummy are both empty listnodes, but they are different and separate objects which do not influence each other.\\n\\nUsing regular python objects say a = 3, and define b = a, if we add anything to b, it certainly won\\'t change the value of a. Why is it different for linked list?\\n\\nHere dummy and res are reference to the class objects. Check the first answer in\\xa0this\\n. For some reason we can\\'t have reference for an nteger in python, whereas classes in python are by default referenced.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e10ede71-c812-4dc2-b728-ad5028e93c56', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n刷题清单2\\n\\n1. Linked List\\n2. Stack\\n3. Queue\\n4. Binary Search Tree\\n5. Hash Table and Map\\n6. Sorting algorithms\\n7. Searching algorithms\\n8. Greedy Algorithms\\n9. Dynamic Programming basics\\n10. Graph BFS and DFS\\n\\nI am now continuing to learn by the program which I created and it helps me to learn topics faster, efficiently and on a long-term basis. I can solve the problems which I solved and similar problems in a fast way. So let's dive into the program which I created for myself and you.\\n\\nFirst of all, let me analyze all the common mistakes which coders continue to make.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='324e19c8-6282-4c67-afd4-0ec6284408d2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**1. UNDERESTIMATE TIME COMPLEXITY FINDING**\\n\\nI always see the coders who underestimate and do not study time complexities. What do you think will be the result of not knowing the time complexities? Yes, the probability that you will have Time Limit Exceeded is very high. I strongly advise firstly to learn time complexities. Before even writing your code you should be able to analyze its time complexity.You may ask how? For example, if you are going to use for the inside of other for then the time complexity of your program will be O(n^2), if you are going to use map inside for then your time complexity will probably be O(nlogn), if you are going to use bubble sort then your time complexity will be O(n^2).\\n\\n**Tips before writing a code:**\\n\\n1. Look for the input range. Is it small or too big? Calculate in such a way: 10^9 operations will take around 1 second so try to make the number of operations which your code will run under 10^9.\\n2. Try to think of all possible data structures you can apply and analyze time complexity. If you have an option between using for inside other for and hash table inside one for then the time complexity of the first solution will be O(n^2) and of the second option will be O(n). It is obvious that it is better to use a hash table to make less time complexity.\\n3. Have strong knowledge of time complexities.\\n\\n**Resources for learning time complexities:**\\n\\n1. MIT 6.001 course, Understanding Program Efficiency Part 1 and Part 2 on YouTube\\n2. MIT 6.006 course on YouTube\\n3. Abdul Bari Algorithms on YouTube\\n\\nAll of these resources helped me to master time complexity finding. They show deep analysis in finding different time complexities.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='857c082d-bda4-4c76-9d2f-94b39368c089', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n**2. DO NOT SPEND TIME FOR LEARNING THEORY**\\n\\nYou always should have a knowledge and understanding of the data structure or algorithm which you are going to use. You should know the working principle of different data structures and algorithms. For example in which cases it is better to use insertion sort rather than quicksort, how stack and queue work, or when it is useful to apply hash table for solving problems. For example, a stack can be used for counting parentheses and reversing a string, hash table for fast find the value by key, for fast access in other words, etc. I promise you to deeply learn theory but do not spend too much time on it.\\n\\n**Best resources for learning theory:**\\n\\n1. MIT Algorithms on YouTube\\n2. Jenny's Lectures on YouTube\\n3. Abdul Bari Algorithms on YouTube\\n4. Mycodeschool on YouTube\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bf12a13a-81bd-4792-9c59-81d663cad175', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n**3. NOT REVISING AND DOING A LOT OF PROCRASTINATION**\\n\\nI always heard that people cannot or spend too much time for solving problems which they solved already. Some are not sure that they can solve the problem which they already solved. That is a huge problem and I think that the possible reasons are:\\n\\n1. Not revising\\n2. Not enough learning a theory\\n3. Procrastinating by studying one day and having a rest for several days.\\n\\nI think that programming is like a sport. For example, to succeed in the sport you will always need to train, repeat your training program, again and again, add some new training methods and exercises, and never give up. The main thing is discipline. You always should revise the topics by solving several problems for them to not forget them and master them. You should apply the knowledge and techniques which you learned by studying the theory immediately to problem-solving. Learn theory and immediately go to practice. Think that theory is your personal trainer and you are the athlete who is going to practice that again and again while you do not master it.\\n\\n**Tips for learning for the long term:**\\n\\n1. Revising all topics, even two times a week\\n2. Solve some problems and revise your solved problems every week\\n3. Write contests\\n4. Learn theory and have a strong foundation\\n5. Do something each day\\n\\nYou may ask why I mentioned writing contests? The answer is that competition will give you additional motivation for learning. You will see your level and will want to have a better rating and will you will begin training by studying and practising. Also try to solve at least one problem each day for 21 days, after 21 days you will automatically have a habit to go and solve problems. Do not procrastinate. NEVER DO THAT! If you will procrastinate for 3 days you will have a habit to continue doing that and as a result, the work that you already did will have no meaning, you will just lose your time.Discipline and hard work are the keys to success.\\n\\nNow let's consider the topics which you need to learn and how. Let's discuss the program.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8e2c4675-5abd-44e2-b03b-bc88d3f97ff3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Program:**\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7bc6fd3c-3744-4255-aaa6-4b3333033cab', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**1. Time complexities**\\n\\n1. Learn mainly Big-O notation and analyzing the time complexity of the program which you wrote\\n2. Practice by considering tests and some quizzes from the internet\\n3. Advised time to spend: 1 week\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='76c46028-55db-4868-93b7-5daf98194044', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**2. Recursion**\\n\\n1. Learn how the recursion works\\n2. Try to write some basic problems by recursion and solve LeetCode problems for recursion\\n3. Advised time to spend: 1 week\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='99eaeffa-0427-46cd-9148-dffece761c7e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**3. Linked List**\\n\\n1. Learn the concept of a linked list\\n2. Insertion and deletion of nodes at different positions\\n3. Time complexity and advantages of using it\\n4. When and in what problems to apply\\n5. Linked List traversals. Both iterative and recursive ways\\n6. Solve LeetCode problems\\n7. Advised time to spend: 1-1.5 weeks\\n8. List of advised problems:\\xa0https://leetcode.com/list/50sfo32d\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d63238c9-7e37-45ec-b3b0-4980f296ab80', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**4. Stack and Queue**\\n\\n1. Learn the basics of stack and queue\\n2. How they work and how to implement them\\n3. Their basic operations: push(), pop(), top() and etc.\\n4. Time complexities and advantages of using them\\n5. Consider different standard problems such as Valid Parentheses, Reversing Linked List and String\\n6. Advised time to spend: 1-1.5 weeks\\n7. List of advised problems:\\xa0https://leetcode.com/list/504xdrcr\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1a8a0166-5263-4906-bced-49aa05859c98', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**5. Binary Search Tree**\\n\\n1. Properties, terms, and the meaning of Binary Search Trees\\n2. How to search, add and delete nodes in BST and their time complexities\\n3. Main traversals such as BFS: Level-order-traversal and DFS: Preorder, Inorder, Postorder. Think about both recursive ways and iterative ways where will need to apply the knowledge of stack and queue\\n4. Advised time to spend: 2-2.5 weeks\\n5. List of advised problems:\\xa0https://leetcode.com/list/504mfxd2\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='29984d49-9f80-4009-84c9-5807b88c45b7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**6. Hash Table and Map**\\n\\n1. The meaning and work principle of Hash Table and Map\\n2. The difference between Hash Table and Map\\n3. Their time complexities and how to apply during problem-solving\\n4. Learn the main functions and their library\\n5. Advised time to spend: 2-2.5 weeks\\n6. List of advised problems:\\xa0https://leetcode.com/list/504wrexe\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='41fd7489-7031-4e49-b0d5-f6e704e10273', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**7. Sorting and Searching Algorithms**\\n\\n1. Bubble Sort, Insertion Sort, Selection Sort, Merge Sort, Quick Sort, Count Sort, Radix Sort.\\n2. Learn the working principle and time complexity of each of the sorting algorithms and their advantages over each other\\n3. Linear Search, Binary Search, Ternary Search\\n4. Learn the working principle and time complexity of each of the searching algorithms and their advantages over each other\\n5. Advised time to spend: 2-3 weeks\\n6. List of advised problems for sorting:\\xa0https://leetcode.com/list/5047kw65\\n7. List of advised problems for searching:\\xa0https://leetcode.com/list/504ixc37\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='eb0a5c33-6552-4047-872a-f4d525e867ab', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**8. Greedy Algorithms**\\n\\n1. How to use sorting and hash table\\n2. Understand the logic of some basic problems\\n3. The main thing to master greedy algorithms is to practice a lot\\n4. Advised time to spend: 2.5-3 weeks\\n5. List of advised problems:\\xa0https://leetcode.com/list/5ik01ftj\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4ec27dd9-f4e8-46e1-b1fe-d5a3739c416f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**9. Dynamic Programming**\\n\\n1. Solutions of basic problems\\n2. Check my this post:\\xa0https://leetcode.com/discuss/general-discussion/1081421/powerful-dynamic-programming-explanation\\n3. Advised time to spend: 4-5 weeks\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='625ed842-ea9e-46fb-a811-e291d35d0f96', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**10. Graphs**\\n\\n1. Learn different graph traversals such as BFS and DFS\\n2. Learn Minimum Spanning Trees and their different algorithms\\n3. Learn the Shortest Paths and its algorithms\\n4. Learn Topological sort and priority queue\\n5. Learn the basic work principle of each algorithm and the way of applying them\\n6. Advised time to spend: 4-5 weeks\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c6ca719e-010f-46c8-b53c-bd42a355e4de', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**11. String Processing**\\n\\n1. Learn different algorithms on strings\\n2. Learn trie and solve problems\\n3. Advised time to spend: 1-2 weeks', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e0530c65-1c0a-4352-b3d2-9f3b22e54e32', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nbacktracking\\n\\nVIPVIP this explains backtrack by putting recur function before vs. after the execution of the function  \\n\\nL2. Problems on Recursion - YouTube\\n\\n[[]] at master · [[]]\\n\\n回溯法是一种复杂度很高的暴力搜索算法，实现简单且有固定模板，常被用于搜索排列组合问题的所有可行性解。不同于普通的暴力搜索，回溯法会在每一步判断状态是否合法，而不是等到状态全部生成后再进行确认。当某一步状态非法时，它将回退到上一步中正确的位置，然后继续搜索其他不同的状态。前进和后退是回溯法的关键动作，因此可以使用递归去模拟整个过程，即使用递归实现回溯法\\n\\n判断回溯很简单，拿到一个问题，你感觉如果不穷举一下就没法知道答案，那就可以开始回溯了。一般回溯的问题有三种：\\n\\n1. Find a path to success 有没有解\\n2. Find all paths to success 求所有解\\n    - 求所有解的个数\\n    - 求所有解的具体信息\\n3. Find the best path to success 求最优解\\n\\n理解回溯：给一堆选择, 必须从里面选一个. 选完之后我又有了新的一组选择. This procedure is repeated over and over until you reach a final state. If you made a good sequence of choices, your final state is a goal state; if you didn\\'t, it isn\\'t.\\n\\n回溯是一种算法思想，主要是通过递归来构建并求解问题，当发现不满足问题的条件时，就回溯寻找其他的满足条件的解。\\n\\n这类问题之所以难是因为通常运用回溯思想求解的问题，通常是递归和循环同时存在，是的思考问题和求解的过程变得比较复杂。\\n\\n通过几道典型问题的求解，大致可以将回溯问题的求解步骤分为三个：\\n\\n1. choices 即在这个问题中，我们可以做哪些事情。例如在数独空格中我们的选择有0 - 9是个数字。\\n2. constraint 即我们并不能随心所欲的选择，在做选择的同时，有一定的约束会限制我们的选择。比如数独空格中我们需要依据数独的规则，每个空格选填一个数字，并不是每个空格随心所欲的填0-9 任意一个都可以。\\n3. goals 或者我们叫做递归的停止条件。 即随着选择的进行，我们什么时候就不用再做选择了，即问题求解完成\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nhttps://www.cis.upenn.edu/~matuszek/cit594-2012/Pages/backtracking.html\\n\\nBacktracking is a form of recursion.\\n\\nThe usual scenario is that you are faced with a number of options, and you must choose one of these. After you make your choice you will get a new set of options; just what set of options you get depends on what choice you made. This procedure is repeated over and over until you reach a final state. If you made a good sequence of choices, your final state is a\\xa0goal state;\\xa0if you didn\\'t, it isn\\'t.\\n\\nConceptually, you start at the root of a tree; the tree probably has some good leaves and some bad leaves, though it may be that the leaves are all good or all bad. You want to get to a good leaf. At each node, beginning with the root, you choose one of its children to move to, and you keep this up until you get to a leaf.\\n\\nSuppose you get to a bad leaf. You can\\xa0backtrack\\xa0to continue the search for a good leaf by revoking your\\xa0most recent\\xa0choice, and trying out the next option in that set of options. If you run out of options, revoke the choice that got you here, and try another choice at that node. If you end up at the root with no options left, there are no good leaves to be found.\\n\\nThis needs an example.\\n\\n!Untitled\\n\\n1. Starting at Root, your options are A and B. You choose A.\\n2. At A, your options are C and D. You choose C.\\n3. C is bad. Go back to A.\\n4. At A, you have already tried C, and it failed. Try D.\\n5. D is bad. Go back to A.\\n6. At A, you have no options left to try. Go back to Root.\\n7. At Root, you have already tried A. Try B.\\n8. At B, your options are E and F. Try E.\\n9. E is good. Congratulations!\\n\\nIn this example we drew a picture of a tree. The tree is an abstract model of the possible sequences of choices we could make. There is also a data structure called a tree, but usually we don\\'t have a data structure to tell us what choices we have. (If we do have an actual tree data structure, backtracking on it is called\\xa0depth-first tree searching.)\\n\\n**The backtracking algorithm.**\\n\\nHere is the algorithm (in pseudocode) for doing backtracking from a given node n:\\n\\n> boolean solve(Node n) {\\n    if n is a leaf node {\\n        if the leaf is a goal node, return true\\n        else return false\\n    } else {\\n        for each child c of n {\\n            if solve(c) succeeds, return true\\n        }\\n        return false\\n    }\\n}\\n> \\n\\nNotice that the algorithm is expressed as a\\xa0boolean\\xa0function. This is essential to understanding the algorithm. If\\xa0`solve(n)`\\xa0is true, that means node\\xa0`n`\\xa0is part of a solution--that is, node\\xa0`n`\\xa0is one of the nodes on a path from the root to some goal node. We say that\\xa0`n`\\xa0is\\xa0solvable. If\\xa0`solve(n)`\\xa0is false, then there is\\xa0no\\xa0path that includes\\xa0`n`\\xa0to any goal node.\\n\\nHow does this work?\\n\\n- If any child of\\xa0`n`\\xa0is solvable, then\\xa0`n`\\xa0is solvable.\\n- If no child of\\xa0`n`\\xa0is solvable, then\\xa0`n`\\xa0is not solvable.\\n\\nHence, to decide whether any non-leaf node\\xa0`n`\\xa0is solvable (part of a path to a goal node), all you have to do is test whether any child of\\xa0`n`\\xa0is solvable. This is done recursively, on each child of\\xa0`n`. In the above code, this is done by the lines\\n\\n```\\n        for each child c of n {\\n            if solve(c) succeeds, return true\\n        }\\n        return false\\n```\\n\\nEventually the recursion will \"bottom\" out at a leaf node. If the leaf node is a goal node, it is solvable; if the leaf node is not a goal node, it is not solvable. This is our base case. In the above code, this is done by the lines\\n\\n> if n is a leaf node {\\n    if the leaf is a goal node, return true\\n    else return false\\n}\\n> \\n\\nThe backtracking algorithm is simple but important. You should understand it thoroughly. Another way of stating it is as follows:\\n\\nTo search a tree:\\n1. If the tree consists of a single leaf, test whether it is a goal node,\\n2. Otherwise, search the subtrees until you find one containing a goal node, or until you have searched them all unsuccessfully.\\n\\n**Non-recursive backtracking, using a stack**\\n\\nBacktracking is a rather typical recursive algorithm, and any recursive algorithm can be rewritten as a stack algorithm. In fact, that is how your recursive algorithms are translated into machine or assembly language.\\n\\n> boolean solve(Node n) {\\n    put node n on the stack;\\n    while the stack is not empty {\\n        if the node at the top of the stack is a leaf {\\n            if it is a goal node, return true\\n            else pop it off the stack\\n        }\\n        else {\\n            if the node at the top of the stack has untried children\\n                push the next untried child onto the stack\\n            else pop the node off the stack\\n\\n    }\\n    return false\\n}\\n> \\n\\nStarting from the root, the only nodes that can be pushed onto the stack are the children of the node currently on the top of the stack, and these are only pushed on one child at a time; hence, the nodes on the stack at all times describe a valid path in the tree. Nodes are removed from the stack only when it is known that they have no goal nodes among their descendents. Therefore, if the root node gets removed (making the stack empty), there must have been no goal nodes at all, and no solution to the problem.\\n\\nWhen the stack algorithm terminates successfully, the nodes on the stack form (in reverse order) a path from the root to a goal node.\\n\\nSimilarly, when the recursive algorithm finds a goal node, the path information is embodied (in reverse order) in the sequence of recursive calls. Thus as the recursion unwinds, the path can be recovered one node at a time, by (for instance) printing the node at the current level, or storing it in an array.\\n\\nHere is the recursive backtracking algorithm, modified slightly to print (in reverse order) the nodes along the successful path:\\n\\n> boolean solve(Node n) {\\n    if n is a leaf node {\\n        if the leaf is a goal node {\\n           print n\\n           return true\\n        }\\n        else return false\\n    } else {\\n        for each child c of n {\\n            if solve(c) succeeds {\\n                print n\\n                return true\\n            }\\n        }\\n        return false\\n    }\\n}\\n> \\n\\n**Keeping backtracking simple**\\n\\nAll of these versions of the backtracking algorithm are pretty simple, but when applied to a real problem, they can get pretty cluttered up with details. Even determining whether the node is a leaf can be complex: for example, if the path represents a series of moves in a chess endgame problem, the leaves are the checkmate and stalemate solutions.\\n\\nTo keep the program clean, therefore, tests like this should be buried in methods. In a chess game, for example, you could test whether a node is a leaf by writing a\\xa0`gameOver`\\xa0method (or you could even call it\\xa0`isLeaf`). This method would encapsulate all the ugly details of figuring out whether any possible moves remain.\\n\\nNotice that the backtracking altorithms require us to keep track, for each node on the current path, which of its children have been tried already (so we don\\'t have to try them again). In the above code we made this look simple, by just saying\\xa0`for\\xa0each\\xa0child\\xa0c\\xa0of\\xa0n`. In reality, it may be difficult to figure out what the possible children are, and there may be no obvious way to step through them. In chess, for example, a node can represent one arrangement of pieces on a chessboard, and each child of that node can represent the arrangement after some piece has made a legal move. How do you find these children, and how do you keep track of which ones you\\'ve already examined?\\n\\nThe most straightforward way to keep track of which children of the node have been tried is as follows: Upon initial entry to the node (that is, when you first get there from above), make a list of all its children. As you try each child, take it off the list. When the list is empty, there are no remaining untried children, and you can return \"failure.\" This is a simple approach, but it may require quite a lot of additional work.\\n\\nThere is an easier way to keep track of which children have been tried,\\xa0**if**\\xa0you can define an ordering on the children. If there is an ordering, and you know which child you just tried, you can determine which child to try next.\\n\\nFor example, you might be able to number the children\\xa0`1`\\xa0through\\xa0`n`, and try them in numerical order. Then, if you have just tried child\\xa0`k`, you know that you have already tried children\\xa0`1`\\xa0through\\xa0`k-1`, and you have not yet tried children\\xa0`k+1`\\xa0through\\xa0`n`. Or, if you are trying to color a map with just four colors, you can always try red first, then yellow, then green, then blue. If child yellow fails, you know to try child green next. If you are searching a maze, you can try choices in the order left, straight, right (or perhaps north, east, south, west).\\n\\nIt isn\\'t always easy to find a simple way to order the children of a node. In the chess game example, you might number your pieces (or perhaps the squares of the board) and try them in numerical order; but in addition each piece may also have several moves, and these must also be ordered.\\n\\nYou can probably find some way to order the children of a node. If the ordering scheme is simple enough, you should use it; but if it is too cumbersome, you are better off keeping a list of untried children.\\n\\n**Example: TreeSearch**\\n\\nFor starters, let\\'s do the simplest possible example of backtracking, which is searching an actual tree. We will also use the simplest kind of tree, a binary tree.\\n\\nA\\xa0**binary tree**\\xa0is a data structure composed of\\xa0**nodes**. One node is designated as the\\xa0**root node**. Each node can reference (point to) zero, one, or two other nodes, which are called its\\xa0**children**. The children are referred to as the\\xa0**left child**\\xa0and/or the\\xa0**right child**. All nodes are reachable (by one or more steps) from the root node, and there are no cycles. For our purposes, although this is not part of the definition of a binary tree, we will say that a node might or might not be a goal node, and will contain its name. The first example in this paper (which we repeat here) shows a binary tree.\\n\\n!Untitled\\n\\nHere\\'s a definition of the BinaryTree class:\\n\\n> public class BinaryTree {\\n    BinaryTree leftChild = null;\\n    BinaryTree rightChild = null;\\n    boolean isGoalNode = false;\\n    String name;\\n    \\n    BinaryTree(String name, BinaryTree left, BinaryTree right, boolean isGoalNode) {\\n        this.name = name;\\n        leftChild = left;\\n        rightChild = right;\\n        this.isGoalNode = isGoalNode;\\n    }\\n}\\n> \\n\\nNext we will create a\\xa0`TreeSearch`\\xa0class, and in it we will define a method\\xa0`makeTree()`\\xa0which constructs the above binary tree.\\n\\n> static BinaryTree makeTree() {\\n    BinaryTree root, a, b, c, d, e, f;\\n    c = new BinaryTree(\"C\", null, null, false);\\n    d = new BinaryTree(\"D\", null, null, false);\\n    e = new BinaryTree(\"E\", null, null, true);\\n    f = new BinaryTree(\"F\", null, null, false);\\n    a = new BinaryTree(\"A\", c, d, false);\\n    b = new BinaryTree(\"B\", e, f, false);\\n    root = new BinaryTree(\"Root\", a, b, false);\\n    return root;\\n}\\n> \\n\\nHere\\'s a main program to create a binary tree and try to solve it:\\n\\n> public static void main(String args[]) {\\n    BinaryTree tree = makeTree();\\n    System.out.println(solvable(tree));\\n}\\n> \\n\\nAnd finally, here\\'s the recursive backtracking routine to \"solve\" the binary tree by finding a goal node.\\n\\n> static boolean solvable(BinaryTree node) {\\n/* 1 */  if (node == null) return false;\\n/* 2 */  if (node.isGoalNode) return true;\\n/* 3 */  if (solvable(node.leftChild)) return true;\\n/* 4 */  if (solvable(node.rightChild)) return true;\\n/* 5 */  return false;\\n}\\n> \\n\\nHere\\'s what the numbered lines are doing:\\n\\n1. If we are given a null node, it\\'s not solvable. This statement is so that we can call this method with the children of a node, without first checking whether those children actually exist.\\n2. If the node we are given is a goal node, return success.\\n3. See if the left child of\\xa0`node`\\xa0is solvable, and if so, conclude that\\xa0`node`\\xa0is solvable. We will only get to this line if\\xa0`node`\\xa0is non-null and is not a goal node, says to\\n4. Do the same thing for the right child.\\n5. Since neither child of\\xa0`node`\\xa0is solvable,\\xa0`node`\\xa0itself is not solvable.\\n\\nThis program runs correctly and produces the unenlightening result\\xa0`true`.\\n\\nEach time we ask for another node, we have to check if it is\\xa0`null`. In the above we put that check as the first thing in\\xa0`solvable`. An alternative would be to check first whether each child exists, and recur only if they do. Here\\'s that alternative version:\\n\\n> static boolean solvable(BinaryTree node) {\\n    if (node.isGoalNode) return true;\\n    if (node.leftChild != null && solvable(node.leftChild)) return true;\\n    if (node.rightChild != null && solvable(node.rightChild)) return true;\\n    return false;\\n}\\n> \\n\\nI think the first version is simpler, but the second version is slightly more efficient.\\n\\n**What are the children?**\\n\\nOne of the things that simplifies the above binary tree search is that, at each choice point, you can ignore all the previous choices. Previous choices don\\'t give you any information about what you should do next; as far as you know, both the left and the right child are possible solutions. In many problems, however, you may be able to eliminate children immediately, without recursion.\\n\\nConsider, for example, the problem of four-coloring a map. It is a theorem of mathematics that any map on a plane, no matter how convoluted the countries are, can be colored with at most four colors, so that no two countries that share a border are the same color.\\n\\nTo color a map, you choose a color for the first country, then a color for the second country, and so on, until all countries are colored. There are two ways to do this:\\n\\n- **Method 1.**\\xa0Try each of the four possible colors, and recur. When you run out of countries, check whether you are at a goal node.\\n- **Method 2.**\\xa0Try only those colors that have not already been used for an adjacent country, and recur. If and when you run out of countries, you have successfully colored the map.\\n\\nLet\\'s apply each of these two methods to the problem of coloring a checkerboard. This should be easily solvable; after all, a checkerboard only needs two colors.\\n\\nIn both methods, the colors are represented by integers, from\\xa0`RED=1`\\xa0to\\xa0`BLUE=4`. We define the following helper methods. The helper method code isn\\'t displayed here because it\\'s not important for understanding the method that does the backtracking.\\n\\n`boolean mapIsOK()`Used by method 1 to check (at a leaf node) whether the entire map is colored correctly.`boolean okToColor(int row, int column, int color)`Used by method 2 to check, at every node, whether there is an adjacent node already colored with the given color.`int[] nextRowAndColumn(int row, int column)`Used by both methods to find the next \"country\" (actually, the row and column of the next square on the checkerboard).\\n\\nHere\\'s the code for method 1:\\n\\n> boolean explore1(int row, int column, int color) {\\n    if (row >= NUM_ROWS) return mapIsOK();\\n    map[row][column] = color;\\n    for (int nextColor = RED; nextColor <= BLUE; nextColor++) {\\n        int[] next = nextRowAndColumn(row, column);\\n        if (explore1(next[0], next[1], nextColor)) return true;\\n    }\\n    return false;\\n}\\n> \\n\\nAnd here\\'s the code for method 2:\\n\\n> boolean explore2(int row, int column, int color) {\\n    if (row >= NUM_ROWS) return true;\\n    if (okToColor(row, column, color)) {\\n        map[row][column] = color;\\n        for (int nextColor = RED; nextColor <= BLUE; nextColor++) {\\n            int[] next = nextRowAndColumn(row, column);\\n            if (explore2(next[0], next[1], nextColor)) return true;\\n        }\\n    }\\n    return false;\\n}\\n> \\n\\nThose appear pretty similar, and you might think they are equally good. However, the timing information suggests otherwise:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='236ed230-abc4-4734-ad5b-bbfd0b3bf764', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nbinary search 二分查找\\n\\n二分法的一个常见应用场景就是有序数组这类结构的查找。当题目涉及有序或查找时，我们可以尝试用二分法进行思考，并且思考的方向是中间元素是否和周围元素存在某种关联，以及如何利用中间元素来缩小问题规模。\\n\\n二分法的使用条件并不局限于有序数组，它只是一种特例。从本质上来说，如果每次都可以使用某种策略来验证中间元素（验证可以视为找规律），并将下次查找的范围缩小一半，那么就可以使用二分法: LC 153\\n\\n二分法的识别简单的题目能够一眼看出“查找”任务，读者也就可以联想到使用二分法；中等级别或困难级别的题目往往背景复杂，无法马上看出是“查找”任务，需要读者进一步加工并对题目进行转换，利用题目中的已知信息，构建查找的目标，以及目标所在的范围\\n\\n外，当题目的数据规模超过时，有较大的可能是二分法类型的题目，这也是一个识别二分法的小技巧。二分法的运用关注查找范围内的中间元素，挖掘背后的规律，往往中间元素和题目的目标值、左右相邻元素及左右边界元素等存在一定的关联，根据这些关联可以将查找范围缩小一半。具体的实现方法包括原始的二分查找及二分查找的变种，二者的实现难度不大，唯一需要注意的是二分查找的变种的边界问题，当更新左边界l=mid时，需要修改循环的退出条件为l+1==horl==h。\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dfca4036-2dcc-459b-910b-b99926dcde41', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**解空间**\\n\\n解空间指的是**题目所有可能的解构成的集合**。比如一个题目所有解的可能是 1,2,3,4,5，但具体在某一种情况只能是其中某一个数（即可能是 1，2，3，4，5 中的**一个数**）。那么这里的解空间就是 1,2,3,4,5 构成的集合，在某一个具体的情况下可能是其中任意一个值，**我们的目标就是在某个具体的情况判断其具体是哪个**。如果线性枚举所有的可能，就枚举这部分来说时间复杂度就是 $O(n)$。\\n\\n举了例子：\\n\\n如果让你在一个数组 nums 中查找 target，如果存在则返回对应索引，如果不存在则返回 -1。那么对于这道题来说其解空间是什么？\\n\\n很明显解空间是区间 [-1, n-1]，其中 n 为 nums 的长度。\\n\\n需要注意的是上面题目的解空间只可能是区间 [-1,n-1] 之间的整数。而诸如 1.2 这样的小数是不可能存在的。这其实也是大多数二分的情况。 但也有少部分题目解空间包括小数的。如果解空间包括小数，就可能会涉及到精度问题，这一点大家需要注意。\\n\\n比如让你求一个数 x 的平方根，答案误差在 $10^-6$ 次方都认为正确。这里容易知道其解空间大小可定义为 [1,x]（当然可以定义地更精确，之后我们再讨论这个问题），其中解空间应该包括所有区间的实数，不仅仅是整数而已。这个时候解题思路和代码都没有太大变化，唯二需要变化的是：\\n\\n1. 更新答案的步长。 比如之前的更新是\\xa0`l = mid + 1`，现在**可能**就不行了，因此这样**可能**会错过正确解，比如正确解恰好就在区间 [mid,mid+1] 内的某一个小数。\\n2. 判断条件时候需要考虑误差。由于精度的问题，判断的结束条件可能就要变成\\xa0**与答案的误差在某一个范围内**。\\n\\n对于**搜索类题目**，解空间一定是有限的，不然问题不可解。对于搜索类问题，第一步就是需要明确解空间，这样你才能够在解空间内进行搜索。这个技巧不仅适用于二分法，只要是搜索问题都可以使用，比如 DFS，BFS 以及回溯等。只不过对于二分法来说，**明确解空间显得更为重要**。如果现在还不理解这句话也没关系，看完本文或许你就理解了。\\n\\n定义解空间的时候的一个原则是： 可以大但不可以小。因为如果解空间偏大（只要不是无限大）无非就是多做几次运算，而如果解空间过小则可能**错失正确解**，导致结果错误。比如前面我提到的求 x 的平方根，我们当然可以将解空间定义的更小，比如定义为 [1,x/2]，这样可以减少运算的次数。但如果设置地太小，则可能会错过正确解\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='91fad48f-52c1-4d65-a24e-9d5d99df8144', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**极值**\\n\\n类似我在堆专题\\xa0提到的极值。只不过这里的极值是**静态的**，而不是动态的。这里的极值通常指的是**求第 k 大（或者第 k 小）的数。**\\n\\n堆的一种很重要的用法是求第 k 大的数，而二分法也可以求第 k 大的数，只不过**二者的思路完全不同**\\n\\n`Given a list of integers nums and an integer k, return the k-th (0-indexed) smallest abs(x - y) for every pair of elements (x, y) in nums. Note that (x, y) and (y, x) are considered the same pair.\\n\\nConstraints:n ≤ 100,000 where n is the length of nums\\nExample 1\\nInput:\\nnums = [1, 5, 3, 2]\\nk = 3\\nOutput:\\n2\\nExplanation:\\n\\nHere are all the pair distances:\\n\\nabs(1 - 5) = 4\\nabs(1 - 3) = 2\\nabs(1 - 2) = 1\\nabs(5 - 3) = 2\\nabs(5 - 2) = 3\\nabs(3 - 2) = 1\\n\\nSorted in ascending order we have [1, 1, 2, 2, 3, 4].`\\n\\n简单来说，题目就是给的一个数组 nums，让你求 nums 第 k 大的**任意两个数的差的绝对值**。当然，我们可以使用堆来做，只不过使用堆的时间复杂度会很高，导致无法通过所有的测试用例。这道题我们可以使用二分法来降维打击。\\n\\n对于这道题来说，解空间就是从 0 到数组 nums 中最大最小值的差，用区间表示就是 [0, max(nums) - min(nums)]。明确了解空间之后，我们就需要对解空间进行二分。对于这道题来说，可以选当前解空间的中间值 mid ，然后计算小于等于这个中间值的**任意两个数的差的绝对值**有几个，我们不妨令这个数字为 x。\\n\\n- 如果 x 大于 k，那么解空间中大于等于 mid 的数都不可能是答案，可以将其舍弃。\\n- 如果 x 小于 k，那么解空间中小于等于 mid 的数都不可能是答案，可以将其舍弃。\\n- 如果 x 等于 k，那么 mid 就是答案。\\n\\n基于此，我们可使用二分来解决。这种题型，我总结为**计数二分**。我会在后面的四大应用部分重点讲解。\\n\\n代码：\\n\\n`class Solution:\\n    def solve(self, A, k):\\n        A.sort()\\n        def count_not_greater(diff):\\n            i = ans = 0\\n            for j in range(1, len(A)):\\n                while A[j] - A[i] > diff:\\n                    i += 1\\n                ans += j - i\\n            return ans\\n        l, r = 0, A[-1] - A[0]\\n\\n        while l <= r:\\n            mid = (l + r) // 2\\n            if count_not_greater(mid) > k:\\n                r = mid - 1\\n            else:\\n                l = mid + 1\\n        return l`\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='757d5a48-5726-4054-abb6-f966579d657c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**一个中心**\\n\\n二分法的一个中心大家一定牢牢记住。其他（比如序列有序，左右双指针）都是二分法的手和脚，都是表象，并不是本质，而**折半才是二分法的灵魂**。\\n\\n前面已经给大家明确了解空间的概念。而这里的折半其实就是解空间的折半。\\n\\n比如刚开始解空间是 [1, n]（n 为一个大于 n 的整数）。通过**某种方式**，我们确定 [1, m] 区间都**不可能是答案**。那么解空间就变成了 (m,n]，持续此过程知道解空间变成平凡（直接可解）。\\n\\n> 注意区间 (m,n] 左侧是开放的，表示 m 不可能取到。\\n> \\n\\n显然折半的难点是**根据什么条件舍弃哪一步部分**。这里有两个关键字:\\n\\n1. 什么条件\\n2. 舍弃哪部分\\n\\n几乎所有的二分的难点都在这两个点上。如果明确了这两点，几乎所有的二分问题都可以迎刃而解。幸运的是，关于这两个问题的答案通常都是有限的，题目考察的往往就是那几种。这其实就是所谓的做题套路\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1c6a8eba-145f-4a23-b8d3-1145f3ce4121', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**问题定义**\\n\\n> 这里的问题定义是一个狭义的问题。而如果你理解了这个问题之后，可以将这个具体的问题进行推广以适应更复杂的问题。关于推广，我们之后再谈。\\n> \\n\\n给定一个由数字组成的有序数组 nums，并给你一个数字 target。问 nums 中是否存在 target。如果存在， 则返回其在 nums 中的索引。如果不存在，则返回 - 1。\\n\\n这是二分查找中最简单的一种形式。当然二分查找也有**很多的变形**，这也是二分查找容易出错，难以掌握的原因。\\n\\n常见变体有：\\n\\n- 如果存在多个满足条件的元素，返回最左边满足条件的索引。\\n- 如果存在多个满足条件的元素，返回最右边满足条件的索引。\\n- 数组不是整体有序的。 比如先升序再降序，或者先降序再升序。\\n- 将一维数组变成二维数组。\\n- 。。。\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a9117938-a27a-4a49-9e69-3ca674caba73', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**思维框架**\\n\\n如何将上面的算法转换为容易理解的可执行代码呢？\\n\\n大家不要小看这样的一个算法。就算是这样一个简简单单，朴实无华的二分查找， 不同的人写出来的差别也是很大的。 如果没有一个**思维框架指导你，不同的时间你可能会写出差异很大的代码。这样的话，犯错的几率会大大增加。这里给大家介绍一个我经常使用的思维框架和代码模板。**\\n\\n**首先定义解空间为 [left, right]，注意是左右都闭合，之后会用到这个点**\\n\\n> 你可以定义别的解空间形式，不过后面的代码也相应要调整，感兴趣的可以试试别的解空间。\\n> \\n- 由于定义的解空间为 [left, right]，因此当 left <= right 的时候，解空间都不为空，此时我们都需要继续搜索。 也就是说终止搜索条件应该为 left <= right。\\n\\n> 举个例子容易明白一点。 比如对于区间 [4,4]，其包含了一个元素 4，因此解空间不为空，需要继续搜索（试想 4 恰好是我们要找的 target，如果不继续搜索， 会错过正确答案）。而当解空间为 [left, right) 的时候，同样对于 [4,4]，这个时候解空间却是空的，因为这样的一个区间不存在任何数字·。\\n> \\n- 循环体内，我们不断计算 mid ，并将 nums[mid] 与 目标值比对。\\n    - 如果 nums[mid] 等于目标值， 则提前返回 mid（只需要找到一个满足条件的即可）\\n    - 如果 nums[mid] 小于目标值， 说明目标值在 mid 右侧，这个时候解空间可缩小为 [mid + 1, right] （mid 以及 mid 左侧的数字被我们排除在外）\\n    - 如果 nums[mid] 大于目标值， 说明目标值在 mid 左侧，这个时候解空间可缩小为 [left, mid - 1] （mid 以及 mid 右侧的数字被我们排除在外）\\n- 循环结束都没有找到，则说明找不到，返回 -1 表示未找到。\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='957d462b-da8b-4e66-b3de-53bda518a07a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**小结**\\n\\n对于二分题目首先要明确解空间，然后根据一定条件（通常是和中间值比较），舍弃其中一半的解。大家可以先从查找满足条件的值的二分入手，进而学习最左和最右二分。同时大家只需要掌握最左和最右二分即可，因为后者功能大于前者。\\n\\n对于最左和最右二分，简单用两句话总结一下：\\n\\n1. 最左二分不断收缩右边界，最终返回左边界\\n2. 最右二分不断收缩左边界，最终返回右边界\\n\\n\\n\\n\\n\\n\\n\\n!Untitled\\n\\n终止条件：左右指针之间的区间成了空区间；右指针到了作指针的左边：right +1 = left\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3bd04d69-3118-426e-bf6d-d987009fd753', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nheap priority queue\\n\\nHeaps, heapsort, and priority queues - Inside code - YouTube\\n\\n!Untitled\\n\\n!Untitled\\n\\n在binary heap里，知道12的index i，就可以知道他所有的child, parent index\\n\\n!Untitled\\n\\nsiftup 是把一个较小的node挪到合理的位置上，比如这里要把5 swap到10再到6\\n\\nSiftdown同理\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n```python\\nclass MinHeap:\\n    def __init__(self, arr=None):\\n        self.heap = []\\n        if type(arr) is list:\\n            self.heap = arr.copy()\\n            for i in range(len(self.heap))[::-1]:\\n                self._siftdown(i)\\n\\n    def _siftup(self, i):\\n        parent = (i-1)//2\\n        while i != 0 and self.heap[i] < self.heap[parent]:\\n            self.heap[i], self.heap[parent] = self.heap[parent], self.heap[i]\\n            i = parent\\n            parent = (i-1)//2\\n\\n    def _siftdown(self, i):\\n        left = 2*i + 1\\n        right = 2*i + 2\\n        while (left  self.heap[left]) or (right  self.heap[right]):\\n            smallest = left if (right >= len(self.heap) or self.heap[left] < self.heap[right]) else right\\n            self.heap[i], self.heap[smallest] = self.heap[smallest], self.heap[i]\\n            i = smallest\\n            left = 2*i + 1\\n            right = 2*i + 2\\n\\n    def insert(self, element):\\n        self.heap.append(element)\\n        self._siftup(len(self.heap)-1)\\n\\n    def get_min(self):\\n        return self.heap[0] if len(self.heap) > 0 else None\\n\\n    def extract_min(self):\\n        if len(self.heap) == 0:\\n            return None\\n        minval = self.heap[0]\\n        self.heap[0], self.heap[-1] = self.heap[-1], self.heap[0]\\n        self.heap.pop()\\n        self._siftdown(0)\\n        return minval\\n\\n    def update_by_index(self, i, new):\\n        old = self.heap[i]\\n        self.heap[i] = new\\n        if new < old:\\n            self._siftup(i)\\n        else:\\n            self._siftdown(i)\\n\\n    def update(self, old, new):\\n        if old in self.heap:\\n            self.update_by_index(self.heap.index(old), new)\\n\\nclass MaxHeap:\\n    def __init__(self, arr=None):\\n        self.heap = []\\n        if type(arr) is list:\\n            self.heap = arr.copy()\\n            for i in range(len(self.heap))[::-1]:\\n                self._siftdown(i)\\n\\n    def _siftup(self, i):\\n        parent = (i-1)//2\\n        while i != 0 and self.heap[i] > self.heap[parent]:\\n            self.heap[i], self.heap[parent] = self.heap[parent], self.heap[i]\\n            i = parent\\n            parent = (i-1)//2\\n\\n    def _siftdown(self, i):\\n        left = 2*i + 1\\n        right = 2*i + 2\\n        while (left < len(self.heap) and self.heap[i] < self.heap[left]) or (right < len(self.heap) and self.heap[i] < self.heap[right]):\\n            biggest = left if (right >= len(self.heap) or self.heap[left] > self.heap[right]) else right\\n            self.heap[i], self.heap[biggest] = self.heap[biggest], self.heap[i]\\n            i = biggest\\n            left = 2*i + 1\\n            right = 2*i + 2\\n\\n    def insert(self, element):\\n        self.heap.append(element)\\n        self._siftup(len(self.heap)-1)\\n\\n    def get_max(self):\\n        return self.heap[0] if len(self.heap) > 0 else None\\n\\n    def extract_max(self):\\n        if len(self.heap) == 0:\\n            return None\\n        maxval = self.heap[0]\\n        self.heap[0], self.heap[-1] = self.heap[-1], self.heap[0]\\n        self.heap.pop()\\n        self._siftdown(0)\\n        return maxval\\n\\n    def update_by_index(self, i, new):\\n        old = self.heap[i]\\n        self.heap[i] = new\\n        if new > old:\\n            self._siftup(i)\\n        else:\\n            self._siftdown(i)\\n\\n    def update(self, old, new):\\n        if old in self.heap:\\n            self.update_by_index(self.heap.index(old), new)\\n\\ndef heapsort(arr):\\n    heap = MinHeap(arr)\\n    return [heap.extract_min() for i in range(len(heap.heap))]\\n\\nclass PriorityQueue:\\n    def __init__(self):\\n        self.queue = MaxHeap()\\n\\n    def enqueue(self, element):\\n        self.queue.insert(element)\\n\\n    def peek(self):\\n        return self.queue.get_max()\\n\\n    def dequeue(self, element):\\n        return self.queue.extract_max()\\n\\n    def is_empty(self):\\n        return len(self.queue.heap) == 0\\n\\n    def change_priority_by_index(self, i, new):\\n        self.queue.update_by_index(i, new)\\n\\n    def change_priority(self, old, new):\\n        self.queue.update(old, new)\\n```', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='06c66bcc-0374-47fc-827f-aa85e902f3cc', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n哈希表\\n\\n当我们遇到了要快速判断一个元素是否出现集合里的时候，就要考虑哈希法了\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n在哈希表：两数之和中map正式登场。\\n\\n来说一说：使用数组和set来做哈希法的局限。\\n\\n- 数组的大小是受限制的，而且如果元素很少，而哈希值太大会造成内存空间的浪费。\\n- set是一个集合，里面放的元素只能是一个key，而两数之和这道题目，不仅要判断y是否存在而且还要记录y的下表位置，因为要返回x 和 y的下表。所以set 也不能用。\\n\\nmap是一种``的结构，本题可以用key保存数值，用value在保存数值所在的下表。所以使用map最为合适', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dccd2d41-7207-409f-9db2-0f1da2d35a4e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='|Name|Tags|note|rep1|rep2|rep3|Retention|\\n|---|---|---|---|---|---|---|\\n|1. Two Sum|\"array, hash\"|,No|No|No,|\\n|100. Same Tree|\"recur, tree\"|在每个node上，比较当前node，和他的left right child是不是一样|Yes|Yes|No|perfect|\\n|1005.\\xa0Maximize Sum Of Array After K Negations|\"array, greedy\"|巧用index找到min value，不用重复reorder|No|No|No,|\\n|101 Symmetric tree|\"VIP, bfs, dfs, recur, tree\"|\"关键是不能直接递归主函数; check my code 我的错误主要是，递归的处理晚了一步，导致无法take care of null，又不能加一堆if is not none； 观察可以发现不能从第一步就开始递归，要建立一个left, right为input的函数; 这题其实用bfs比较好理解\"|Yes|Yes|No|bad|\\n|1013.\\xa0Partition Array Into Three Parts With Equal Sum|\"array, greedy\"|,No|No|No,|\\n|102. Binary Tree Level Order Traversal|\"VIP, bfs, tree\"|node object and value object 注意要用不同的list来存不要弄混，所以这里需要多出一个level_result list; VIP for i in stack 是错的，deque object不能这样循环，看code; BFS 才是通解|Yes|Yes|Yes|mid|\\n|103. Binary Tree Zigzag Level Order Traversal|tree|\"记住appendleft, deque的使用方式: import collections, level_result = collections.deque()\"|No|No|No,|\\n|104 max depth of binary tree|\"VIP, recur, tree\"|\"递归类似一个人不知道自己在电影院的第几排，他问前面的人，前面的人也不知道在第几排，就继续往前问，直到问到第一排的人才知道答案，那第二排的人就知道自己在No2, 然后依次把答案传递回来\"|Yes|Yes|No|mid|\\n|1046.\\xa0Last Stone Weight|\"VIP, heap\"|,No|No|No,|\\n|105. Construct Binary Tree from Preorder and Inorder Traversal|tree|学lucifer的手动画一下图才能懂 https://lucifer.ren/blog/2020/02/08/%E6%9E%84%E9%80%A0%E4%BA%8C%E5%8F%89%E6%A0%91%E4%B8%93%E9%A2%98/|Yes|No|No|mid|\\n|106. Construct Binary Tree from Inorder and Postorder Traversal|tree|,No|No|No,|\\n|107. Binary Tree Level Order Traversal II|\"VIP, linked list, stack/queue, tree\"|注意update current level这种循环方式; 注意循环的层级; result[::-1]逆向list ; 递归和循环都要掌握|Yes|No|No|mid|\\n|108 Convert Sorted Array to Binary Search Tree|\"recur, tree\"|注意nums is none 和 not nums 的区别|Yes|Yes|No|mid|\\n|109.\\xa0Convert Sorted List to Binary Search Tree|\"linked list, recur, tree\"|,No|No|No,|\\n|109.\\xa0Convert Sorted List to Binary Search Tree|tree|,No|No|No,|\\n|11.\\xa0Container With Most Water|\"greedy, two_pointer\"|关键是理解，左右指针如何决定移哪个|No|No|No,|\\n|110. Balanced Binary Tree|\"VIP, recur, tree\"|check my own code to see how to reduce time complexity；学会这个-1的写法|Yes|Yes|No|bad|\\n|111. Minimum Depth of Binary Tree|\"recur, tree\"|,Yes|Yes|No|mid|\\n|112. Path Sum|\"VIP, bfs, dfs, recur, tree\"|递归的本质是，找到一个basic case，然后让后面的case全部可以用这个基础算式来算；bfs 比较简单的题目比如这种binary tree的，后面的node之间不会有交集，不需要marked set来记录是否已访问|Yes|Yes|No|bad|\\n|113.\\xa0Path Sum II|\"VIP, backtracking, dfs, recur, tree\"|,No|No|No,|\\n|113.\\xa0Path Sum II|\"dfs, tree\"|,Yes|No|No|perfect|\\n|114. Flatten Binary Tree to Linked List|\"VIP, tree\"|\"note that the recur function does not explicity return anything. It only changes the root in every round \"|No|No|No,|\\n|1146.\\xa0Snapshot Array|\"array, binary search, design, hash\"|,No|No|No,|\\n|116. Populating Next Right Pointers in Each Node|tree|\"难度不大，想清楚link怎么连的就行; 思考为何用preorder, 不用别的\"|Yes|No|No|good|\\n|118 triangle|\"array, dp, recur\"|,Yes|No|No|bad|\\n|121. Best Time to Buy and Sell Stock|\"dp, sliding_window\"|,Yes|No|No|bad|\\n|122. Best Time to Buy and Sell Stock II|\"VIP, dp\"|这道题是双变量的dp典型|Yes|No|No|mid|\\n|123. Best time to buy and sell stock 3|dp|,No|No|No,|\\n|124.\\xa0Binary Tree Maximum Path Sum|\"VIP, dfs, tree\"|\"vip的是这种，借助一个辅助函数的循环，在辅助函数里update res的思路；这个思路和110 check for balance binary tree 一样 \"|No|No|No,|\\n|125. Valid Palindrome|\"VIP, string, two_pointer\"|\"双指针是正常的办法；最好不要用python函数取巧； 注意除掉非字符或数字的元素的写法很关键要记住; a[::-1] 注意要用到这个方法反转一个string; 要用到x.isalnum() 来判断是否是字符; string = \\'\\'.join([x for x in s.lower() if x.isalnum()]) #先去除掉非字符或数字的元素， 要记住; 另一种方法是re.sub(r\\'\\\\W+\\', \\'\\', x); 注意默认return是true or false很重要\"|Yes|Yes|No|mid|\\n|128.\\xa0Longest Consecutive Sequence|\"VIP, array, hash\"|注意如何create a set of a list: myset = set(nums)|No|No|No,|\\n|1290. Convert Binary Number in a Linked List to Integer|\"VIP, recur\"|理解二进制的转换； 二进制转化的题一般都是递归|Yes|No|No|mid|\\n|13. Roman to Integer|\"hash, string\"|关键是花一点时间找到规律比如1994; 动笔写下来规律; range()的用法要熟练； #这个用n-2 来避免循环里出现空值的方法很关键|Yes|No|No|bad|\\n|130.\\xa0Surrounded Regions|\"VIP, graph, matrix\"|反向思维; 只有找邻居的步骤需要dfs|No|No|No,|\\n|131. Palindrome Partitioning|\"VIP, backtracking, dp\"|,No|No|No|bad|\\n|133.\\xa0Clone Graph|\"bfs, dfs, graph\"|,No|No|No,|\\n|\"1333.\\xa0Filter Restaurants by Vegan-Friendly, Price and Distance\"|\"array, sort\"|\"new_list.sort( key = lambda x: (-x[1], -x[0]))\"|No|No|No,|\\n|134.\\xa0Gas Station|greedy|,No|No|No,|\\n|135. Candy|greedy|greedy的题好多都要分情况讨论|No|No|No,|\\n|1351. Count Negative Numbers in a Sorted Matrix|\"VIP, binary search\"|binary search的灵活运用|Yes|No|No|bad|\\n|136 Single_Numbe|hash|try except keyerror的用法； set用法|Yes|No|No|perfect|\\n|138.\\xa0Copy List with Random Pointer|\"hash, linked list\"|\"有点奇怪的题，要熟悉; 问题的关键是，比如node2 的random指向了node 5, 但node5还不存在；解决办法是copy两轮\"|No|No|No,|\\n|139. Word Break|dp|典型的双重循环 dp|Yes|Yes|Yes|bad|\\n|14. Longest Common Prefix|\"VIP, string\"|strs[0][0:i] != strs[j][0:i] 比较一截string是否相等不用一个一个元素来比; 注意prefix一定是从第一个字母开始的|Yes|Yes|No|bad|\\n|141. Linked List Cycle|\"linked list, two_pointer\"|有快慢指针的思路在里面；写node的时候要保证note存在，所以要考虑用try except的写法; try的用法没懂|Yes|No|No|good|\\n|143.\\xa0Reorder List|\"VIP, linked list\"|\"一道很综合的题目，包含了反转等操作； VIP 因为这里fast, slow并没有指向同一个class object，所以他们不会改变head的位置；\"|No|No|No,|\\n|1436.\\xa0Destination City|hash|要记得用set; difference的用法|No|No|No,|\\n|1448.\\xa0Count Good Nodes in Binary Tree|\"bfs, dfs, tree\"|,No|No|No,|\\n|146.\\xa0LRU Cache|\"hash, linked list\"|,No|No|No,|\\n|147. Insertion Sort List|\"VIP, linked list, sort\"|,Yes|No|No,|\\n|15.\\xa03Sum|\"VIP, array, sort, two_pointer\"|\"sort 之后用two pointer，就可以让搜索两次n^2 变为搜索一次; tuple is essential to use set on list of list \"|No|No|No,|\\n|150. Evaluate Reverse Polish Notation|stack/queue|,No|No|No,|\\n|152. Maximum Product Subarray|dp|reversed 的用法要记住|Yes|No|No|mid|\\n|153 Find Minimum in Rotated Sorted Array|\"VIP, binary search\"|if left == right 作为搜索的最终点，说明只剩一个元素了； 而现在缺少直接判断的条件（mid==target）。幸运的是，我们还是能够通过二分法不断地缩小最终答案可能存在的区间，当区间只剩下一个元素时（l==h），那么它就是最终答案; 至今还没见过left = mid 的只见过right = mid 的，left 永远是 mid + 1|Yes|Yes|No|mid|\\n|155 min stack|stack/queue|注意全局变量所有地方都要加self; 用空间复杂度换时间复杂度|Yes|No|No,|\\n|160. Intersection of Two Linked Lists|\"hash, linked list, two_pointer\"|,Yes|No|No|mid|\\n|1603.\\xa0Design Parking System|design|,No|No|No,|\\n|162 find peak element|binary search|,Yes|No|No|mid|\\n|167.\\xa0Two Sum II - Input Array Is Sorted|two_pointer|如果是按大小排列，似乎用two pointer就很合理|No|No|No,|\\n|169. Majority Element|array|可用hash count|Yes|No|No|good|\\n|17. Letter Combinations of a Phone Number|\"VIP, backtracking, string\"|从空开始循环的方法很少见; 循环加递归的模板题目|No|No|No,|\\n|171. Excel Sheet Column Number|math|\"用类似二进制的操作，把A, AA, AAA都转化成数字; ord 的用法； 循环的写法；本质上要得出基于s每个字符加一位之后的递推公式就能写出来了，得出通项公式没用\"|Yes|No|No|bad|\\n|172. Factorial Trailing Zeroes|\"math, recur\"|,Yes|No|No|bad|\\n|19.Remove Nth Node From End of List|\"VIP, linked list, two_pointer\"|#print out所有步骤就会发现，fast = fast.next 这类操作不会改变linked list本身，只是head处在的位置不同了， 只有slow.next = slow.next.next会改变|Yes|No|No|mid|\\n|190. Reverse Bits|string|把数字变成string的常用方法|No|No|No,|\\n|198. House Robber|\"VIP, array, dp\"|注意base case需要考虑两个case|Yes|No|No|good|\\n|199. Binary Tree Right Side View|tree|\"when to add a new function besides the main function? when your new function needs to add more parameter than the original one \"|No|No|No,|\\n|2. Add Two Numbers|\"linked list, recur\"|\"#为何是return added不是current node? 因为added完全没有被修改过; reversed 函数的用法要掌握, 这函数返回的是一个iterator，不是一个简单的reversed list\"|Yes|No|No|good|\\n|20. Valid Parentheses|\"VIP, stack/queue, string\"|continue的用法要掌握; 用stack的题目，往往都是需要随着循环把前面的东西remove掉，露出顶部的|Yes|Yes|No|mid|\\n|200.\\xa0Number of Islands|\"VIP, array, bfs, dfs, matrix\"|[[]]|No|No|No,|\\n|202. Happy Number|\"VIP, hash, math\"|关键是想清楚啥时候会无限循环啥时候不会；set hash; 这类不知道循环次数的循环只能用while; 注意拆分数字为str的写法tmp = sum([int(c) ** 2 for c in str(n)])|Yes|No|No|bad|\\n|203. Remove Linked List Elements|\"VIP, linked list, recur\"|\"基础linked list 关于pre, curr 的设定\"|Yes|No|No|mid|\\n|204. Count Primes|\"hash, math\"|先全设为true，再通过条件筛选出false的值|Yes|No|No|bad|\\n|205.\\xa0Isomorphic Strings|\"hash, string\"|,No|No|No,|\\n|206. Reverse Linked List|\"VIP, linked list, recur\"|,Yes|No|No|mid|\\n|207.\\xa0Course Schedule|\"bfs, dfs, graph\"|topological sort|No|No|No,|\\n|209. Minimum Size Subarray Sum|sliding_window|\"if requires contagious numbers, always consider sliding window\"|No|No|No,|\\n|21. Merge Two Sorted Lists|\"VIP, linked list, recur, two_pointer\"|递归的方法is better; 掌握如何建立一个空链表，就像建立空list一样|Yes|No|No|mid|\\n|213. House robber 2|dp|,No|No|No,|\\n|215.\\xa0Kth Largest Element in an Array|,,No|No|No,|\\n|217_Contains_Duplicate|hash|\"要掌握set add的用法: set|\\n|.add|\\n|(elem)\"|Yes|No|No|perfect|\\n|218.\\xa0The Skyline Problem|heap|,No|No|No,|\\n|22. Generate Parentheses|\"array, backtracking\"|,No|No|No,|\\n|226. Invert Binary Tree|\"bfs, dfs, recur, tree\"|注意赋值时写成一排和两排的关键区别，这个区别在linked list里很重要|Yes|Yes|No|perfect|\\n|227.\\xa0Basic Calculator II|\"math, stack/queue, string\"|,No|No|No,|\\n|23.\\xa0Merge k Sorted Lists|\"heap, sort\"|merge sort 的经典|No|No|No,|\\n|230. Kth Smallest Element in a BST|\"VIP, dfs, tree\"|这种match一个target value的，都可以考虑update这个value; VIP inorder travesal creates a sorted list|Yes|No|No|bad|\\n|231. Power of Two|recur|连续乘积的都可以尝试递归|Yes|Yes|No|perfect|\\n|232.\\xa0Implement Queue using Stacks|stack/queue|,No|No|No,|\\n|234. Palindrome Linked List|\"linked list, two_pointer\"|linked list放到list里来解|Yes|No|No,|\\n|235. Lowest Common Ancestor of a Binary Search Tree|\"recur, tree\"|注意bst的性质； while 和 if区别； 用了递归的题一般不用while，因为递归就是在循环操作了；可以用递归和非递归的方式来写|Yes|Yes|No|perfect|\\n|236. Lowest Common Ancestor of a Binary Tree|\"VIP, tree\"|反直觉的dfs; 需要先想清楚规律|Yes|No|No|bad|\\n|237. Delete Node in a Linked List|linked list|,Yes|No|No|bad|\\n|238.\\xa0Product of Array Except Self|\"VIP, array, dp, prefix_sum\"|prefix 是一个词的前缀；suffix是一个词的后缀; VIP 本质是在循环前通过构建prefix把复杂度降低一级； reversed(range(n - 1))的用法|No|No|No,|\\n|240. Search a 2D Matrix II|binary search|为何这题不需要mid了；选取右上角或者左下角开始循环，才可以做到一增一减|Yes|No|No|mid|\\n|242. Valid Anagram|\"hash, string\"|dic.get 的用法要掌握，用来取代counter|Yes|Yes|No|perfect|\\n|251. Flatten 2D Vector|\"design, two_pointer\"|,No|No|No,|\\n|252: Meeting Rooms|greedy|intervals.sort(key=lambda x: x.start)|No|No|No,|\\n|253. Meeting Rooms II|,interval的题画图很重要|No|No|No,|\\n|257.\\xa0Binary Tree Paths|\"backtracking, dfs\"|,No|No|No,|\\n|258. Add Digits|recur|,Yes|No|No|bad|\\n|259. 3Sum Smaller|sliding_window|list.sort()要记住；不是list = list.sort(|No|No|No,|\\n|26 Remove Duplicates from Sorted Array|\"VIP, array, two_pointer\"|感觉duplicate相关的题都可以用指针|Yes|No|No|mid|\\n|268 Missing Number|array|,Yes|No|No|mid|\\n|270: Closest Binary Search Tree Value II | Leetcode lock|tree|关键是gap的定义在无穷大; BST的题目用interation比recurssion一般更直观|Yes|No|No|mid|\\n|273.\\xa0Integer to English Words|\"array, recur, string\"|,No|No|No,|\\n|276 paint fence|\"VIP, dp\"|dp 入门题目|Yes|Yes|No|bad|\\n|278.\\xa0First Bad Version|\"VIP, binary search\"|key to binary search is the boundary conditions; check ipad|No|No|No,|\\n|28. Implement strStr()|\"sliding_window, string, two_pointer\"|,Yes|No|No|good|\\n|283 Move Zeroes|\"array, stack/queue\"|注意在循环最后迭代index的写法；stack永远是不准创造新space的好选择; 注意最好不要用nums.remove(0)因为他会打乱loop|Yes|No|No|mid|\\n|287. Find the Duplicate Number|\"binary search, graph, linked list, two_pointer\"|,Yes|No|No|bad|\\n|3. Longest Substring Without Repeating Characters|\"sliding_window, string, 有疑问\"|双指针都是用while，不用if; 双指针关键是分别找到移动两个point的条件； set add的用法很关键|Yes|No|No|mid|\\n|3. Longest Substring Without Repeating Characters|\"VIP, sliding_window\"|\"add, remove, set 的用法要记住\"|No|No|No,|\\n|300. Longest Increasing Subsequence|\"VIP, binary search, dp, greedy\"|非常经典的题目；两层循环的dp|Yes|Yes|No|mid|\\n|310.\\xa0Minimum Height Trees|\"graph, tree\"|,No|No|No,|\\n|322. Coin Change|\"VIP, dp\"|尤其注意记忆化递归的易错点|No|Yes|No,|\\n|326. Power of Three|\"math, recur\"|这种需要连续乘积的，都可以试下用递归来做|Yes|No|No|perfect|\\n|33. Search in Rotated Sorted Array|\"VIP, binary search, middle\"|核心是判断啥时候target在mid的左边or右边；mid< target时有两种情况；#看做一个recursion，判断break在MR 还是LM之间，需要很多轮|Yes|No|No|mid|\\n|337. House Robber III|\"VIP, tree\"|,No|No|No,|\\n|34 Find First and Last Position of Element in Sorted Array|\"VIP, array, binary search\"|https://www.youtube.com/watch?v=bPdnC5X5xDw binary search的灵活运用|Yes|No|No|mid|\\n|341.\\xa0Flatten Nested List Iterator|\"design, dfs, stack/queue\"|,No|No|No,|\\n|344. Reverse String|\"recur, string, two_pointer\"|所有可以从前后两端考虑的问题，都可以用双指针; left < right 就是一个极佳的stopping condition，比recurssion的stop condition容易理解; 这道题的recur 写法有点binary search的思想在里面|Yes|Yes|No|mid|\\n|347.\\xa0Top K Frequent Elements|\"array, hash, heap\"|\"dd= sorted((dic[ip], ip) for ip in dic)  #VIP how to sort dic by their keys\"|No|No|No,|\\n|350 Intersection of Two Arrays II|\"hash, two_pointer\"|先把一部分data放到hash，再用另一部分去match|Yes|No|No|perfect|\\n|355.\\xa0Design Twitter|\"VIP, hash, heap\"|,No|No|No,|\\n|36.\\xa0Valid Sudoku|\"hash, matrix\"|注意set的写法：collections.defaultdict(set)|No|No|No,|\\n|362. Design Hit Counter|,,No|No|No,|\\n|367. Valid Perfect Square|binary search|\"注意考虑1, 0 等特殊情况； 注意取mid的时候的取整与否\"|Yes|No|No|perfect|\\n|376. Wiggle subsequence|dp|,No|No|No,|\\n|380.\\xa0Insert Delete GetRandom O(1)|\"array, design, hash\"|index = self.hmap[val] 关键是找到这个index的操作|No|No|No,|\\n|383.\\xa0Ransom Note|hash|,No|No|No|perfect|\\n|387. First Unique Character in a String|\"hash, string\"|掌握如何得到list的index: index = animals.index(\\'dog\\')|Yes|No|No|perfect|\\n|39. Combination Sum|\"VIP, backtracking\"|可以当模板; sort array before u do anything can be helpful； 循环加递归的套路在backtrack里极其常见，比如78 subsets; VIP 注意ans.append(path[:]) 不能直接append path|No|No|No,|\\n|40. Combination Sum II|\"VIP, backtracking\"|注意不能写candidates = candidates.sort()； #my work correct!!!  #VIP check my own solution 这个才是符合模板式解法的办法 https://www.youtube.com/watch?v=GBKI9VSKdGg|No|No|No,|\\n|404 Sum of Left Leaves|\"VIP, bfs, dfs, tree\"|注意这种自己加参数的写法|Yes|No|No|bad|\\n|409.\\xa0Longest Palindrome|,,No|No|No,|\\n|412. Fizz Buzz|string|记住整除的写法|Yes|No|No|perfect|\\n|416.\\xa0Partition Equal Subset Sum|dp|,No|No|No,|\\n|417.\\xa0Pacific Atlantic Water Flow|\"dfs, graph, matrix\"|,No|No|No,|\\n|42.\\xa0Trapping Rain Water|\"dp, stack/queue, two_pointer\"|,No|No|No,|\\n|424.\\xa0Longest Repeating Character Replacement|sliding_window|,No|No|No,|\\n|437. Path Sum III|\"VIP, tree\"|本质上是两层dfs，遍历整个tree两次|No|No|No,|\\n|438.\\xa0Find All Anagrams in a String|\"VIP, array, hash, sliding_window\"|vip用hashmap的话可以避免先求出所有的anagram; Counter的用法； 注意scount.pop(s[l]) 用来remove a key from a dict|No|No|No,|\\n|445. Add Two Numbers II|linked list|掌握linked list的反向遍历方法|No|No|No,|\\n|45.\\xa0Jump Game II|\"bfs, dp, greedy\"|看视频，很像bfs的思路，也是求最小值|No|No|No,|\\n|450.\\xa0Delete Node in a BST|\"dfs, tree\"|,No|No|No,|\\n|455 assign cookies|\"VIP, greedy\"|贪心问题的特点是，前面的决策会影响后面的，比如这题，被用过的饼干就不能再用来满足后面的kid了，所以要保证前面决策的时候就是可以实现全局最优的局部解; 每一步的最优解一定包含上一步的最优解; 注意break 的用法; 注意g.sort()，不是g = g.sort()|Yes|No|No|bad|\\n|46. permutations|\"VIP, backtracking\"|把code和树状图结合起来看就容易理解了; # 回溯法的含义是对每个可能的结果进行遍历，如果某个数字已经使用则跳过，如果没有使用则放入path中。这个“回溯”怎么理解？我认识是在递归的过程中使用了一个数组path来保存自己走过的路，# 如果沿着这条路走完了全部的解，则需要弹出path中的最后一个元素，相当于向后回退，于是叫做回溯法。|No|No|No,|\\n|463.\\xa0Island Perimeter|\"bfs, dfs, matrix\"|,No|No|No,|\\n|487 Max consecutive ones 2|\"dp, two_pointer\"|,No|No|No,|\\n|49.\\xa0Group Anagrams|\"VIP, array, hash\"|注意anagram的题很多都可以用上sort； 注意need to change list to tuple because list cant be keys in python； defaultdict(list) 不是defaultdict(tuple)|No|No|No,|\\n|5. Longest Palindromic Substring|\"VIP, dp, string, two_pointer\"|对称性的问题，都可以试试two pointer; 函数套函数是这题双指针写法的关键; 注意python 里string subset s[1:2]只能取到一个字母|Yes|No|No|bad|\\n|\"50. Pow(x, n)\"|recur|通过一些数字单双数之类的小trick来减少计算量；注意shift的操作>>|Yes|No|No|mid|\\n|51. N-Queens|backtracking|,No|No|No,|\\n|513.\\xa0Find Bottom Left Tree Value|\"bfs, dfs, tree\"|bfs 按每一环的遍历方式，本质上就是层序遍历|No|No|No,|\\n|513.\\xa0Find Bottom Left Tree Value|\"bfs, tree\"|,No|No|No,|\\n|525.\\xa0Contiguous Array|\"array, hash, prefix_sum\"|,No|No|No,|\\n|528.\\xa0Random Pick with Weight|\"VIP, binary search\"|本质上是一个mapping from index to value； if seed <= self.w[mid]:  # 最后输出的l是满足这个条件的最小index; 等号是关键|No|No|No,|\\n|53. Maximum Subarray|\"VIP, array, dp, greedy\"|\"key is definition of dp: dp[i] is the max sum ending at index i, so any dp[i] might be the global max; sorting an array 的 time complexity is nlogn \"|Yes|No|No|mid|\\n|54.\\xa0Spiral Matrix|\"matrix, simulation\"|,No|No|No,|\\n|542.\\xa001 Matrix|\"bfs, dp, matrix\"|[[]]|No|No|No,|\\n|543. Diameter of Binary Tree|\"VIP, tree\"|get depth; 注意用self.result来储存结果，不能直接用result; return的结果不是拿来作为最后输出的，而是为了recur到下一轮|Yes|No|No|mid|\\n|55. Jump Game|\"VIP, array, dp, greedy\"|\"greedy 经常可以做到linear time; dp 一般都是square time 因为dp要遍历所有path，只是可以用memory存一下； greedy 从末尾挪动goal position是关键, 看neetcode视频\"|Yes|No|No,|\\n|56.\\xa0Merge Intervals|\"VIP, array, sort\"|\"classic sorting question;lambda function intervals = sorted(intervals, key=lambda x:x[0])\"|No|No|No,|\\n|560.\\xa0Subarray Sum Equals K|\"VIP, dp, hash, prefix_sum\"|\"掌握cumulative sum 的用法，记住下面的模板，可以有效减少复杂度; 注意要用mymap = collections.defaultdict(int)，不能直接用普通的dic = {}定义, The difference is that a\\xa0defaultdict|\\n|will \"\"default\"\" a value if that key has not been set yet. If you didn\\'t use a\\xa0defaultdict|\\n|you\\'d have to check to see if that key exists, and if it doesn\\'t, set it to what you want.\"|No|No|No,|\\n|567.\\xa0Permutation in String|\"VIP, hash, sliding_window\"|掌握A = [ord(x) - ord(\\'a\\') for x in s1]的操作方式；同时在字典里提前加26个为0的key的操作也很经典；不一定要用dic来当hashtable用array也行|No|No|No,|\\n|57.\\xa0Insert Interval|VIP|\"先想res是什么样子的，再想怎么组合出res来；extend is the key; res.extend(intervals[i:])  # equal to for j in range(i, len(intervals)):  res.append(intervals[j])\"|Yes|No|No,|\\n|572.\\xa0Subtree of Another Tree|tree|,No|No|No,|\\n|59.\\xa0Spiral Matrix II|\"matrix, simulation\"|,No|No|No,|\\n|599.\\xa0Minimum Index Sum of Two Lists|\"array, hash, string\"|,No|No|No,|\\n|617. Merge Two Binary Trees|tree|,Yes|No|No|perfect|\\n|62. Unique Paths|dp|\"memory =[[0] * n for _ in range(m)]; 注意二维list的写法memo[row][col] 不是memo[row, col]\"|Yes|No|No,|\\n|621. Task Scheduler|\"greedy, heap\"|,No|No|No,|\\n|64.\\xa0Minimum Path Sum|\"dp, matrix\"|\"dp = [[0] * columns for _ in range|\\n|(rows)]\"|No|No|No,|\\n|66 plus one|\"array, math\"|看情况要选择倒转循环顺序; 提前return避免循环走完的写法|Yes|No|No|bad|\\n|674. Longest Continuous Increasing Subsequence|\"array, dp\"|\"How to create a list of empty lists: For arbitrary length lists, you can use [ [] for _ in range(N) ]|\\n|Do not use [ [] ] * N, as that will result in the list containing the same list object N times\"|Yes|No|No|mid|\\n|678.\\xa0Valid Parenthesis String|,,No|No|No,|\\n|680.\\xa0Valid Palindrome II|\"greedy, recur, two_pointer\"|,No|No|No,|\\n|684.\\xa0Redundant Connection|\"bfs, dfs, graph\"|,No|No|No,|\\n|69 sqrt|\"VIP, binary search\"|循环改变左右边界； 注意这里用while的循环，因为不知道for loop的次数； 循环停止的边界要自己举例来判断; 最后return left or right是关键|Yes|No|No|mid|\\n|692.\\xa0Top K Frequent Words|\"VIP, hash, sort\"|,No|No|No,|\\n|695.\\xa0Max Area of Island|\"bfs, dfs, graph\"|,No|No|No,|\\n|7. Reverse Integer|math|,No|No|No,|\\n|70. climbing stairs|\"VIP, dp\"|之所以要套两层函数，是因为memory function needs to be defined outside the recurssion; 一定要用memory不然会超时|Yes|No|No|mid|\\n|703.\\xa0Kth Largest Element in a Stream|\"design, heap\"|,No|No|No,|\\n|704 binary search|binary search|while left <= right: #这个等号是关键。 #取等号的时候，low = high = mid，就一个数了，判断他是否== target就可以，不等的话left 就比right 大了，就跳出循环返回-1|Yes|Yes|No|good|\\n|705.\\xa0Design HashSet|design|,No|No|No,|\\n|706. Design HashMap|hash|比较特殊的取余数法|No|No|No,|\\n|714. Best Time to Buy and Sell Stock with Transaction Fee|dp|这道题和122是双变量的dp典型|No|No|No,|\\n|721.\\xa0Accounts Merge|\"VIP, array, bfs, dfs, string\"|dfs也可以不用recur的写法|No|No|No,|\\n|733.\\xa0Flood Fill|\"bfs, dfs, matrix, recur\"|edge cases; # 注意这题不像 200 number of island那样，要loop 整个matrix，只需要loop 和起始点相邻的island就够了|Yes|No|No|mid|\\n|739.\\xa0Daily Temperatures|\"VIP, stack/queue\"|\"monotonic decreasing stack problem; 这是一类经典问题; stack经常用在，可以一边循环一边删掉一部分list ele的问题上, 比如只用考虑相邻元素的关系，不用管其他\"|No|No|No,|\\n|74.\\xa0Search a 2D Matrix|\"binary search, matrix\"|,No|No|No,|\\n|75.\\xa0Sort Colors|\"sort, two_pointer\"|,Yes|No|No|perfect|\\n|76.\\xa0Minimum Window Substring|\"hash, sliding_window\"|,No|No|No,|\\n|763.\\xa0Partition Labels|\"greedy, hash, two_pointer\"|last = {s[i]: i for i in range(L)} 的写法得出每个字母的last position|No|No|No,|\\n|78.\\xa0Subsets|\"VIP, backtracking, dfs, dp\"|\"dp 的难点在于不知道怎么定义状态; backtrack的写法注意用 backtrack(i + 1, path + [nums[i]])，不能写backtrack(i + 1, path.append(nums[i])\"|No|No|No,|\\n|79.\\xa0Word Search|\"backtracking, matrix\"|,No|No|No,|\\n|807. Max Increase to Keep City Skyline|greedy|max_j = max([row[j] for row in grid]) 这个求每列最大值的地方是关键；|No|No|No,|\\n|82. Remove Duplicates from Sorted List 2|\"VIP, linked list, two_pointer\"|如果简单遍历一遍linked list没法完成任务，就可以考虑双指针之类的办法; 双指针经典|Yes|No|No|bad|\\n|83. Remove Duplicates from Sorted List|linked list|,Yes|No|No|good|\\n|846. Hand of Straights|\"VIP, greedy, hash\"|collections.Counter(hand) 直接变成hashtable的操作十分关键; min(cards.keys()) 注意这个写法； dic.pop(key1)的写法|No|No|No,|\\n|853.\\xa0Car Fleet|\"VIP, stack/queue\"|zip的用法|No|No|No,|\\n|860.\\xa0Lemonade Change|\"array, greedy\"|没必要用哈希表来做counter，随便用一个int做counter也行，反正最后是判断这个counter是否大于零|No|No|No,|\\n|863.\\xa0All Nodes Distance K in Binary Tree|\"VIP, bfs, dfs, tree\"|\"defaultdict 的用法是关键 https://stackoverflow.com/questions/5900578/how-does-collections-defaultdict-work; has to create a graph first because we need to traverse parent nodes； vip is creating a graph from a tree, 彻底记住这个操作\"|No|No|No,|\\n|875. Koko Eating Bananas|binary search|这里的判断mid==target的条件要自己写函数定义; 注意余数怎么求|Yes|No|No|mid|\\n|876. Middle of the Linked List|\"linked list, two_pointer\"|所有找中点的题目都可以用双指针|Yes|No|No|good|\\n|88 Merge Sorted Array|\"VIP, array, sort, two_pointer\"|\"为了不打乱前面的顺序，从list末尾开始往前面填充；If you simply consider one element each at a time from the two arrays and make a decision and proceed accordingly, you will arrive at the optimal solution.； #VIP nums1[:]值整个list的每个值\"|Yes|No|No|mid|\\n|90.\\xa0Subsets II|\"array, backtracking\"|\"为了避免重复，关键是移动i的时候一次移动多个，跳过相等的数字比如[1|2,2]\"|No|No|No,|\\n|904. Fruit Into Baskets|sliding_window|,No|No|No,|\\n|929. Unique Email Addresses|\"VIP, hash, string\"|set用法；string的基本处理，split|Yes|No|No|mid|\\n|933.\\xa0Number of Recent Calls|\"design, stack/queue\"|,No|No|No,|\\n|94. Binary Tree Inorder Traversal|\"VIP, dfs, tree\"|掌握用另一个function存下result的方法； #需要新造一个function，不能直接用inorderTraversal的原因是，需要把res定义在函数外面，否则每次循环都会把res清空|Yes|No|No|perfect|\\n|96. Unique Binary Search Trees|\"dp, tree\"|,No|No|No,|\\n|973. K Closest Points to Origin|\"VIP, array, hash, heap\"|\"heap 只要On, sort takes Onlogn\"|No|No|No,|\\n|98. Validate Binary Search Tree|tree|update boundary; 要记住inf的写法|Yes|Yes|No|bad|\\n|981.\\xa0Time Based Key-Value Store|\"VIP, binary search, hash\"|VIP不要用普通的dic要用defaultdict; use turple to store pairs of values; 字典get的用法; 字典里一个key是可以对应好几个值的！！！|No|No|No,|\\n|N meetings in one room|greedy|\"核心是理解为何一个阶段的greedy可以实现总体最优解：我们希望meeting越短越好，这样才能排下更多meeting所以按end time 来排列更合理，如果按start time排列可能第一个meeting就很长. While choosing the meetings greedily with a minimum ending time first, we are left with more time to schedule more meetings in one room. Hence, as a result, we are able to allocate maximum meetings in one room.\"|No|No|No,|\\n|smallest difference|two_pointer|,No|No|No,|\\n|validate subsequence|\"array, two_pointer\"|理清楚思路，什么时候可以return True，就是sequence被耗完到最后的时候，所以要在sequence index下手; 指针问题|No|No|No,|\\n|剑指 Offer 22. 链表中倒数第k个节点|linked list|,No|No|No,|\\n|,hash|,No|No|No,|\\n|,linked list|,No|No|No||', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='398fe8e8-6079-4581-8e15-e5262fbf81a7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nbinary search 边界问题 boundary\\n\\n(45) Explore - LeetCode\\n\\nalgorithm - Binary search bounds - Stack Overflow\\n\\nHow to determine the boundaries in binary search? - Stack Overflow\\n\\nFinding the Boundary - Binary Search / Overview (algo.monster)\\n\\n(45) Binary Search 101 - LeetCode Discuss\\n\\n(45) [Python] Powerful Ultimate Binary Search Template. Solved many problems - LeetCode Discuss\\n\\nHow to find the starting/ending position of an element using Binary Search | by Calvin Chan | Medium\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\nBinary Search is quite easy to understand conceptually. Basically, it splits the search space into two halves and only keep the half that probably has the search target and throw away the other half that would not possibly have the answer. In this manner, we reduce the search space to half the size at every step, until we find the target. Binary Search helps us reduce the search time from linear O(n) to logarithmic O(log n).\\xa0**But when it comes to implementation, it\\'s rather difficult to write a bug-free code in just a few minutes**. Some of the most common problems include:\\n\\n- When to exit the loop? Should we use\\xa0`left < right`\\xa0or\\xa0`left <= right`\\xa0as the while loop condition?\\n- How to initialize the boundary variable\\xa0`left`\\xa0and\\xa0`right`?\\n- How to update the boundary? How to choose the appropriate combination from\\xa0`left = mid`,\\xa0`left = mid + 1`\\xa0and\\xa0`right = mid`,\\xa0`right = mid - 1`?\\n\\nA rather common misunderstanding of binary search is that people often think this technique could only be used in simple scenario like \"Given a sorted array, find a specific value in it\". As a matter of fact, it can be applied to much more complicated situations.\\n\\nAfter a lot of practice in LeetCode, I\\'ve made a powerful binary search template and solved many Hard problems by just slightly twisting this template. I\\'ll share the template with you guys in this post.\\xa0**I don\\'t want to just show off the code and leave. Most importantly, I want to share the logical thinking: how to apply this general template to all sorts of problems**. Hopefully, after reading this post, people wouldn\\'t be pissed off any more when LeetCoding, \"This problem could be solved with binary search! Why didn\\'t I think of that before!\"\\n\\n---\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='982f0189-165a-4ad8-bfb5-7aabf297e09f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n**>> Most Generalized Binary Search**\\n\\nSuppose we have a\\xa0**search space**. It could be an array, a range, etc. Usually it's sorted in ascending order. For most tasks, we can transform the requirement into the following generalized form:\\n\\n**Minimize k , s.t. condition(k) is True**\\n\\nThe following code is the most generalized binary search template:\\n\\n```\\ndef binary_search(array) -> int:\\n    def condition(value) -> bool:\\n        pass\\n\\n    left, right = min(search_space), max(search_space) # could be [0, n], [1, n] etc. Depends on problem\\n    while left < right:\\n        mid = left + (right - left) // 2\\n        if condition(mid):\\n            right = mid\\n        else:\\n            left = mid + 1\\n    return left\\n\\n```\\n\\nWhat's really nice of this template is that, for most of the binary search problems,\\xa0**we only need to modify three parts after copy-pasting this template, and never need to worry about corner cases and bugs in code any more**:\\n\\n**Minimize k , s.t. condition(k) is True**\\n\\n- Correctly initialize the boundary variables\\xa0`left`\\xa0and\\xa0`right`\\xa0to specify search space. Only one rule: set up the boundary to\\xa0**include all possible elements**;\\n- Decide return value. Is it\\xa0`return left`\\xa0or\\xa0`return left - 1`? Remember this:\\xa0**after exiting the while loop,\\xa0`left`\\xa0is the minimal k satisfying the\\xa0`condition`\\xa0function**;\\n- *这个模板里， right = mid should always be written first*\\n- Design the\\xa0`condition`\\xa0function. This is the most difficult and most beautiful part. Needs lots of practice.\\n\\nBelow I'll show you guys how to apply this powerful template to many LeetCode problems.\\n\\n---\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8107bd5f-0fe6-4c97-b6b6-843ee9be2fdf', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**>> Basic Application**\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='538482b6-925f-46c3-a104-8b8045adbaea', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n**278. First Bad Version [Easy]**\\n\\nYou are a product manager and currently leading a team to develop a new product. Since each version is developed based on the previous version, all the versions after a bad version are also bad. Suppose you have\\xa0`n`\\xa0versions\\xa0`[1, 2, ..., n]`\\xa0and you want to find out the first bad one, which causes all the following ones to be bad. You are given an API\\xa0`bool isBadVersion(version)`\\xa0which will return whether\\xa0`version`\\xa0is bad.\\n\\n**Example:**\\n\\n```\\nGiven n = 5, and version = 4 is the first bad version.\\n\\ncall isBadVersion(3) -> false\\ncall isBadVersion(5) -> true\\ncall isBadVersion(4) -> true\\n\\nThen 4 is the first bad version.\\n\\n```\\n\\nFirst, we initialize\\xa0`left = 1`\\xa0and\\xa0`right = n`\\xa0to include all possible values. Then we notice that we don't even need to design the\\xa0`condition`\\xa0function. It's already given by the\\xa0`isBadVersion`\\xa0API. Finding the first bad version is equivalent to finding the minimal k satisfying\\xa0`isBadVersion(k) is True`. Our template can fit in very nicely:\\n\\n```\\nclass Solution:\\n    def firstBadVersion(self, n) -> int:\\n        left, right = 1, n\\n        while left < right:\\n            mid = left + (right - left) // 2\\n            if isBadVersion(mid):\\n                right = mid\\n            else:\\n                left = mid + 1\\n        return left\\n\\n```\\n\\n---\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6749d288-24d4-4493-9ecc-2456c1c5c1c8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**69. Sqrt(x) [Easy]**\\n\\nImplement\\xa0`int sqrt(int x)`. Compute and return the square root of\\xa0*x*, where\\xa0*x*\\xa0is guaranteed to be a non-negative integer. Since the return type is an integer, the decimal digits are truncated and only the integer part of the result is returned.\\n\\n**Example:**\\n\\n```\\nInput: 4\\nOutput: 2\\n\\n```\\n\\n```\\nInput: 8\\nOutput: 2\\n\\n```\\n\\nEasy one. First we need to search for minimal k satisfying condition\\xa0`k^2 > x`, then\\xa0`k - 1`\\xa0is the answer to the question. We can easily come up with the solution. Notice that I set\\xa0`right = x + 1`\\xa0instead of\\xa0`right = x`\\xa0to deal with special input cases like\\xa0`x = 0`\\xa0and\\xa0`x = 1`.\\n\\n```\\ndef mySqrt(x: int) -> int:\\n    left, right = 0, x + 1\\n    while left < right:\\n        mid = left + (right - left) // 2\\n        if mid * mid > x:\\n            right = mid\\n        else:\\n            left = mid + 1\\n    return left - 1  # `left` is the minimum k value, `k - 1` is the answer\\n\\n```\\n\\n---\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fd5ae50e-e535-4d39-83ca-4b589ff9ea17', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n**35. Search Insert Position [Easy]**\\n\\nGiven a sorted array and a target value, return the index if the target is found. If not, return the index where it would be if it were inserted in order. You may assume no duplicates in the array.\\n\\n**Example:**\\n\\n```\\nInput: [1,3,5,6], 5\\nOutput: 2\\n\\n```\\n\\n```\\nInput: [1,3,5,6], 2\\nOutput: 1\\n\\n```\\n\\nVery classic application of binary search. We are looking for the minimal k value satisfying\\xa0`nums[k] >= target`, and we can just copy-paste our template. Notice that our solution is correct regardless of whether the input array\\xa0`nums`\\xa0has duplicates. Also notice that the input\\xa0`target`\\xa0might be larger than all elements in\\xa0`nums`\\xa0and therefore needs to placed at the end of the array. That's why we should initialize\\xa0`right = len(nums)`\\xa0instead of\\xa0`right = len(nums) - 1`.\\n\\n```python\\nclass Solution:\\n    def searchInsert(self, nums: List[int], target: int) -> int:\\n        left, right = 0, len(nums)\\n        while left < right:\\n            mid = left + (right - left) // 2\\n            if nums[mid] >= target:\\n                right = mid\\n            else:\\n                left = mid + 1\\n        return left\\n```\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='48f5cd4a-67c5-441c-ac56-8b0ab01935af', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Advanced Application**\\n\\nThe above problems are quite easy to solve, because they already give us the array to be searched. We\\'d know that we should use binary search to solve them at first glance. However,\\xa0**more often are the situations where the search space and search target are not so readily available**. Sometimes we won\\'t even realize that the problem should be solved with binary search -- we might just turn to dynamic programming or DFS and get stuck for a very long time.\\n\\nAs for the question \"When can we use binary search?\", my answer is that,\\xa0**If we can discover some kind of monotonicity, for example, if\\xa0`condition(k) is True`\\xa0then\\xa0`condition(k + 1) is True`, then we can consider binary search**.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c906f695-6520-4816-9679-bdb591a3c4f9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n**1011. Capacity To Ship Packages Within D Days [Medium]**\\n\\nA conveyor belt has packages that must be shipped from one port to another within\\xa0`D`\\xa0days. The\\xa0`i`-th package on the conveyor belt has a weight of\\xa0`weights[i]`. Each day, we load the ship with packages on the conveyor belt (in the order given by\\xa0`weights`). We may not load more weight than the maximum weight capacity of the ship.\\n\\nReturn the least weight capacity of the ship that will result in all the packages on the conveyor belt being shipped within\\xa0`D`\\xa0days.\\n\\n**Example :**\\n\\n```\\nInput: weights = [1,2,3,4,5,6,7,8,9,10], D = 5\\nOutput: 15\\nExplanation:\\nA ship capacity of 15 is the minimum to ship all the packages in 5 days like this:\\n1st day: 1, 2, 3, 4, 5\\n2nd day: 6, 7\\n3rd day: 8\\n4th day: 9\\n5th day: 10\\n\\nNote that the cargo must be shipped in the order given, so using a ship of capacity 14 and splitting the packages into parts like (2, 3, 4, 5), (1, 6, 7), (8), (9), (10) is not allowed.\\n\\n```\\n\\nBinary search probably would not come to our mind when we first meet this problem. We might automatically treat\\xa0`weights`\\xa0as search space and then realize we've entered a dead end after wasting lots of time. In fact, we are looking for the minimal one among all feasible capacities. We dig out the monotonicity of this problem: if we can successfully ship all packages within\\xa0`D`\\xa0days with capacity\\xa0`m`, then we can definitely ship them all with any capacity larger than\\xa0`m`. Now we can design a\\xa0`condition`\\xa0function, let's call it\\xa0`feasible`, given an input\\xa0`capacity`, it returns whether it's possible to ship all packages within\\xa0`D`\\xa0days. This can run in a greedy way: if there's still room for the current package, we put this package onto the conveyor belt, otherwise we wait for the next day to place this package. If the total days needed exceeds\\xa0`D`, we return\\xa0`False`, otherwise we return\\xa0`True`.\\n\\nNext, we need to initialize our boundary correctly. Obviously\\xa0`capacity`\\xa0should be at least\\xa0`max(weights)`, otherwise the conveyor belt couldn't ship the heaviest package. On the other hand,\\xa0`capacity`\\xa0need not be more than`sum(weights)`, because then we can ship all packages in just one day.\\n\\nNow we've got all we need to apply our binary search template:\\n\\n```\\ndef shipWithinDays(weights: List[int], D: int) -> int:\\n    def feasible(capacity) -> bool:\\n        days = 1\\n        total = 0\\n        for weight in weights:\\n            total += weight\\n            if total > capacity:  # too heavy, wait for the next day\\n                total = weight\\n                days += 1\\n                if days > D:  # cannot ship within D days\\n                    return False\\n        return True\\n\\n    left, right = max(weights), sum(weights)\\n    while left < right:\\n        mid = left + (right - left) // 2\\n        if feasible(mid):\\n            right = mid\\n        else:\\n            left = mid + 1\\n    return left\\n```\\n\\nThe following code is the solution for\\xa0leetcode 287 find the duplicate number\\n\\n```\\nclass Solution {\\npublic:\\n    int findDuplicate(vector& nums) {\\n        int low = 1, high = nums.size() - 1;\\n        while (low < high) {\\n            int mid = low + (high - low) * 0.5;\\n            int cnt = 0;\\n            for (auto a : nums) {\\n                if (a <= mid) ++cnt;\\n            }\\n            if (cnt <= mid) low = mid + 1;\\n            else high = mid;\\n        }\\n        return low;\\n    }\\n};\\n\\n```\\n\\nThere are several places I am confused about:\\n\\n1.the condition for the while loop\\xa0`low<high or low<=high`\\n\\n2.`a<=mid or a<mid`\\xa0(specific for this example)\\n\\n3.`cnt<= mid or cnt<mid`\\n\\n4.`low=mid+1 or low=mid`\\n\\n5.`high=mid or high=mid-1`\\n\\n6.which value do I return?\\n\\nWhen writing a binary search there are a couple of things to consider. The first is what interval range you are searching over and specifically, how you are defining it.\\n\\nFor example, it could be inclusive of both\\xa0`low`\\xa0and\\xa0`high`, meaning\\xa0`[low, high]`, but it could also be exclusive of\\xa0`high`,\\xa0`[low, high)`. Which of these you choose will change the rest of your algorithm.\\n\\nThe obvious implication is the initial values. Generally,\\xa0`high`\\xa0should be the length of the array if it is exclusive and it should be one less if it's inclusive, but it could be something entirely different depending on the problem you're solving.\\n\\nFor the while loop you want it to terminate when the search interval is empty, meaning there are no more candidates to check. If you are using the interval\\xa0`[low, high]`, then this will be empty when\\xa0`low`\\xa0is strictly greater than\\xa0`high`\\xa0(for example,\\xa0`[5,5]`\\xa0contains 5, but\\xa0`[6,5]`\\xa0contains nothing), so the while loop will check for the opposite,\\xa0`low <= high`. However, if you use the interval\\xa0`[low, high)`, then this interval is empty when\\xa0`low`\\xa0is equal to\\xa0`high`, so the while loop needs to check for\\xa0`low < high`.\\n\\nWithin the while loop, after checking\\xa0`mid`, you want to remove it from the interval so you don't check it again. If\\xa0`high`\\xa0is inclusive, then you have to use one less than\\xa0`mid`\\xa0as the new\\xa0`high`\\xa0in order to exclude it from the interval. But if\\xa0`high`\\xa0is exclusive, then setting\\xa0`high`\\xa0equal to\\xa0`mid`\\xa0is enough to exclude it.\\n\\nAs for when to update\\xa0`low`\\xa0vs\\xa0`high`, this depends on what you're searching for. Besides the basic binary search where you just want to know if something exists exactly in the collection, you will have to consider what to do when you are as close as you can get.\\n\\nIn C++ for example, the more useful versions of\\xa0`binary_search`\\xa0are called\\xa0`lower_bound`\\xa0and\\xa0`upper_bound`. If the value being searched for doesn't exist in the container, then these both return the same position, namely the first position which is larger than the search value. This is convenient since this is the position you should insert that value if you want to keep the container sorted. However, if the value is in the container, possibly multiple times, then\\xa0`lower_bound`\\xa0will return the first occurrence of the value, whereas\\xa0`upper_bound`\\xa0will still return the first position larger than the value (or in other words, a right bound to the location of the values).\\n\\nTo get these different behaviors you update either the\\xa0`low`\\xa0or\\xa0`high`\\xa0bound when\\xa0`mid`\\xa0is equal to the search value. If you want the lower bound, then you want to continue searching the lower half of your search range, so you bring\\xa0`high`\\xa0down. If you want the high bound, then you bring\\xa0`low`\\xa0up. In your example, it brings\\xa0`low`\\xa0up when\\xa0`cnt == mid`, so it will find an upper bound.\\n\\nAs for what to return, it depends on both your search interval and what you're looking for. In your example, the while loop is checking\\xa0`(low < high)`, so\\xa0`low`\\xa0and\\xa0`high`\\xa0will be equal when it breaks and it doesn't matter which you use, but even then you may want to return\\xa0`left - 1`\\xa0or\\xa0`left + 1`\\xa0depending on the problem. If the while loop is\\xa0`(low <= high)`\\xa0then when it breaks\\xa0`low == high + 1`, so it will depend on what you're looking for. When in doubt you can always think through an example.\\n\\nSo to put this all to use, here is a version of the solution you mentioned, but using an interval of [low, high] rather than [low, high):\\n\\n```\\nclass Solution {\\n    public:\\n        int findDuplicate(vector& nums) {\\n            int low = 1, high = nums.size() - 2;\\n            while (low <= high) {\\n                int mid = low + (high - low) * 0.5;\\n                int cnt = 0;\\n                for (auto a : nums) {\\n                    if (a <= mid) ++cnt;\\n                }\\n                if (cnt <= mid) low = mid + 1;\\n                else high = mid - 1;\\n            }\\n            return low;\\n        }\\n    };\\n\\n```\\n\\nPS: The reason I didn't mention the interval\\xa0`(low, high]`\\xa0or\\xa0`(low, high)`\\xa0is because it messes with the math around calculating the\\xa0`mid`\\xa0index. Because int math will round down, you can end up in a situation where\\xa0`mid`\\xa0is searched again. For example, if\\xa0`low`\\xa0is 7 and\\xa0`high`\\xa0is 9, then\\xa0`low + (high - low) * 0.5`\\xa0will be 8. After updating low to 8 (since it's exclusive you wouldn't add one),\\xa0`low + (high - low) * 0.5`\\xa0will still be 8 and your loop will never terminate. You can get around this by adding 1 to the part being divided by 2, but generally it's cleaner to go with an interval where\\xa0`low`\\xa0is inclusive.\\n\\nI had exactly the same issue until I figured out loop invariants along with predicates are the best and most consistent way of approaching all binary problems.\\n\\nPoint 1:\\xa0**Think of predicates**In general for all these 4 cases (and also the normal binary search for equality), imagine them as a predicate. So what this means is that some of the values are meeting the predicate and some some failing. So consider for example this array with a target of 5: [1, 2, 3, 4, 6, 7, 8]. Finding the first number greater than 5 is basically equivalent of finding the first one in this array: [0, 0, 0, 0, 1, 1, 1].\\n\\nPoint 2:\\xa0**Search boundaries inclusive**I like to have both ends always inclusive. But I can see some people like start to be inclusive and end exclusive (on len instead of len -1). I like to have all the elements inside of the array, so when referring to a[mid] I don't think whether that will give me an array out of bound. So my preference: Go inclusive!!!\\n\\nPoint 3:\\xa0**While loop condition <=**So we even want to process the subarray of size 1 in the while loop, and when the while loop finishes there should be no unprocessed element. I really like this logic. It's always solid as a rock. Initially all the elements are not inspected, basically they are unknown. Meaning that everything in the range of [st = 0, to end = len - 1] are not inspected. Then when the while loop finishes, the range of uninspected elements should be array of size 0!\\n\\nPoint 4:\\xa0**Loop invariants**Since we defined start = 0, end = len - 1, invariants will be like this: Anything left of start is smaller than target. Anything right of end is greater than or equal to the target.\\n\\nPoint 5:\\xa0**The answer**Once the loop finishes, basically based on the loop invariants anything to the left of start is smaller. So that means that start is the first element greater than or equal to the target. Equivalently, anything to the right of end is greater than or equal to the target. So that means the answer is also equal to end + 1.\\n\\nThe code:\\n\\n```\\npublic int find(int a[], int target){\\n  int start = 0;\\n  int end = a.length - 1;\\n  while (start <= end){\\n    int mid = (start + end) / 2; // or for no overflow start + (end - start) / 2\\n    if (a[mid] < target)\\n       start = mid + 1;\\n    else // a[mid] >= target\\n       end = mid - 1;\\n  }\\n  return start; // or end + 1;\\n}\\n\\n```\\n\\nvariations:**<**It's equivalent of finding the first 0. So basically only return changes.\\n\\n```\\nreturn end; // or return start - 1;\\n```\\n\\n**>**change the if condition to . No other change.\\n\\n**,\\xa0`return end; // or return start - 1;`\\n\\nSo in general with this model for all the 5 variations (, >=, normal binary search) only the condition in the if changes and the return statement. And figuring those small changes is super easy when you consider the invariants (point 4) and the answer (point 5).\\n\\nHope this clarifies for whoever reads this. If anything is unclear of feels like magic please ping me to explain. After understanding this method, everything for binary search should be as clear as day!\\n\\nExtra point: It would be a good practice to also try including the start but excluding the end. So the array would be initially [0, len). If you can write the invariants, new condition for the while loop, the answer and then a clear code, it means you learnt the concept.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8c2b6a34-6583-481c-b01c-c9c00f2e7f05', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\ncoding interview  经验\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n**https://blog.devgenius.io/six-steps-of-mental-framework-to-nail-a-coding-interview-3a27810f3e22?gi=bc6c8dd67aaa**\\n\\nleetcode/cheatsheet.pdf at master · azl397985856/leetcode (github.com)\\n\\nmodeling, clarification, assumption, reasoning, testing，是指的从一个问题抽象出模型，然后逐步优化，寻找边界，证明理由？这些都要给面试官说说？', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ff6d9ff3-e76e-4f2f-ae32-140a644c17b1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\ngreedy 贪心\\n\\n贪心法或许是最难的一种算法思想了。它是指每次根据问题的当前状态，选择一个局部最优策略，并且能够不断迭代，最后产生一个全局最优解。换句话说，每次都是从当前问题出发，而不考虑之前或之后的问题的状态，然后做出一个最有利于当前问题的决策，迭代更新问题，不断重复同样的操作直到问题得到解决，此时得到的解为全局最优解。一般而言，贪心法的题目只要求我们想到一个合理的局部最优策略，并且通过自己举例测试局部最优策略是否会出问题即可，而不需要去关注如何证明这个策略能够产生一个全局最优解。\\n\\n路志鹏 等. 算法通关之路 (Chinese Edition) (Kindle Locations 4135-4141). Kindle Edition.\\n\\n贪心和DP的比较：\\n\\n贪心算法（又称贪婪算法）是指，在对问题求解时，总是做出在当前看来最好的选择。也就是说，不从整体最优上加以考虑，他所作出的是在某种意义上的局部最优解。贪心算法和动态规划算法都是由局部最优导出全局最优，这里不得不比较下二者的区别。\\n\\n贪心算法：\\n1.贪心算法中，作出的每步贪心决策都无法改变，因为贪心策略是由上一步的最优解推导下一步的最优解，而上一步之前的最优解则不作保留；\\n2.由（1）中的介绍，可以知道贪心法正确的条件是：每一步的最优解一定包含上一步的最优解。\\n\\n动态规划算法：\\n1.全局最优解中一定包含某个局部最优解，但不一定包含前一个局部最优解，因此需要记录之前的所有最优解；\\n2.动态规划的关键是状态转移方程，即如何由以求出的局部最优解来推导全局最优解；\\n3.边界条件：即最简单的，可以直接得出的局部最优解', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b545aed9-15bc-40fb-87c0-acbfc8af83df', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n算法\\n\\neach topic: 5 easy; + 8 medium + 1 hard\\n\\n[[big O notation time and space complexity]]\\n\\n[[计算机科学基础]]\\n\\n[[coding interview  经验]]\\n\\n[[刷题 list 和经验]]\\n\\n[[高频题]]\\n\\n[[算法课 拉勾]]\\n\\n[[算法课 拉勾2]]\\n\\n[算法课 [Algorithmic [[算法课 Algorithmic Toolbox1]]\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n冲突，就使用链表进行存储。这样一来，不管数据量为多少，我们都能够灵活应对。\\n\\n如果数组的空间太小，使用哈希表的时候就容易发生冲突，线性查找的使用频率也\\n\\n会更高；反过来，如果数组的空间太大，就会出现很多空箱子，造成内存的浪费。因此，\\n\\n给数组设定合适的空间非常重要\\n\\n\\n\\n\\n\\n堆中最顶端的数据始终最小，所以无论数据量有多少，取出最小值的时间复杂度都\\n\\n为 *O*(1)。\\n\\n另外，因为取出数据后需要将最后的数据移到最顶端，然后一边比较它与子结点数据\\n\\n的大小，一边往下移动，所以取出数据需要的运行时间和树的高度成正比。假设数据量为\\n\\n*n*，根据堆的形状特点可知树的高度为 log2*n* ，那么重构树的时间复杂度便为 *O*(log*n*)。\\n\\n添加数据也一样。在堆的最后添加数据后，数据会一边比较它与父结点数据的大\\n\\n小，一边往上移动，直到满足堆的条件为止，所以添加数据需要的运行时间与树的高度\\n\\n成正比，也是 *O*(log*n*)\\n\\n如果需要频繁地从管理的数据中取出最小值，那么使用堆来操作会非常方便。比如\\n\\n4-5 节中提到的狄克斯特拉算法，每一步都需要从候补顶点中选择距离起点最近的那个\\n\\n顶点。此时，在顶点的选择上就可以用到堆\\n\\n\\n\\n\\n\\n\\n\\n我们可以把二叉查找树当作是二分查找算法思想的树形结构体现（二分查找的详细\\n\\n说明在 3-2 节）。因为它具有前面提到的那两个性质，所以在查找数据或寻找适合添加\\n\\n数据的位置时，只要将其和现有的数据比较大小，就可以根据比较结果得知该往哪边移\\n\\n动了。\\n\\n比较的次数取决于树的高度。所以如果结点数为 *n*，而且树的形状又较为均衡的话，\\n\\n比较大小和移动的次数最多就是 log2*n*。因此，时间复杂度为 *O*(log*n*)。但是，如果树的\\n\\n形状朝单侧纵向延伸，树就会变得很高，此时时间复杂度也就变成了 *O*(*n*)\\n\\n有很多以二叉查找树为基础扩展的数据结构，比如“平衡二叉查找树”。这种数据\\n\\n结构可以修正形状不均衡的树，让其始终保持均衡形态，以提高查找效率。\\n\\n另外，虽然文中介绍的二叉查找树中一个结点最多有两个子结点，但我们可以把子\\n\\n结点数扩展为 *m*（*m* 为预先设定好的常数）。像这种子结点数可以自由设定，并且形状均\\n\\n衡的树便是 B 树\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n快速排序是一种“分治法”。它将原本的问题分成两个子问题（比基准值小的数和\\n\\n比基准值大的数），然后再分别解决这两个问题。子问题，也就是子序列完成排序后，再\\n\\n像一开始说明的那样，把他们合并成一个序列，那么对原始序列的排序也就完成了。\\n\\n不过，解决子问题的时候会再次使用快速排序，甚至在这个快速排序里仍然要使用\\n\\n快速排序。只有在子问题里只剩一个数字的时候，排序才算完成。\\n\\n像这样，在算法内部继续使用该算法的现象被称为“递归”。关于递归的详细说明\\n\\n在 7-4 节。实际上前一节中讲到的归并排序也可看作是一种递归的分治法。\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[[哈希表 code]]\\n\\n[[two pointer 双指针]]\\n\\n[[链表 linked list]]\\n\\n[[链表 code]]\\n\\n[[binary search 二分查找]]\\n\\n[[排序算法]]\\n\\n[[递归1]]\\n\\n[[递归2  recursion (divide and conq, [[backtrack)]]]]\\n\\n[[Leetcode刷题]]\\n\\n[[图 graph]]\\n\\n[[树]]\\n\\n[[树 Code]]\\n\\n[[sliding window]]\\n\\n[[backtracking]]\\n\\n[[哈希表]]\\n\\n[[Dynamic programming 1]]\\n\\n[[Dynamic programming 2]]\\n\\n[[Dynamic programming 3]]\\n\\n[[《算法通关之路》 笔记]]\\n\\n[[about dummy node]]\\n\\n[[greedy 贪心]]\\n\\n[[mutable immutable class python]]\\n\\n[[facebook 题库]]\\n\\n[[BFS]]\\n\\n[[TUF 笔记 recur & backtrack & dfs]]\\n\\n[[TUF 笔记 trees]]\\n\\n[[LC DFS on trees 自己的笔记]]\\n\\n[[刷题清单2]]\\n\\n[[binary search 边界问题 boundary]]\\n\\n[[heap priority queue]]\\n\\n[[yelp 题库]]', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='74e1efbc-96a1-48ed-9a8e-37ca2021d035', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nfacebook 题库\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='64e21643-06b3-4389-abf3-307ed5d52d51', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n高频题\\n\\n1. 刷easy题。主要寻找做题的感觉。\\n2. 刷linkedlist, tree。 原因一是面试中这两类题考的频率较高， 原因二是刷这两类题有助于对递归的理解， 为后面刷DFS打下基础。\\n3. 刷DFS。 DFS非常重要，因为大部分 brute force 基本都可以用dfs来求解。 刚开始刷DFS， 可以重点关注pemutation 和 combination的题目， leetcode 这两类题也挺多的。 然后开始总结， 去理解 “状态(state)” 这个概念，对于每道DFS问题， 要清晰的定义出 “状态（state)”， 以及状态如何转移， 这方面的基础可以为DP打下基础。\\n4. 记忆化搜索。 也就是DFS + 记忆。记忆的存储通常用map或者set即可。\\n5. DP。 记忆化搜索刷熟了，可以尝试开始刷经典的DP，理解状态 和 状态转移，尝试自己去写出状态转移方程。 尝试DP的两种实现方式 top-down(也就是记忆化搜索) 和 bottom-up\\n6. 高频数据结构， stack, queue, heap。 这三个数据结构在面试中出现频率非常高。\\n7. 其他重点专题: sliding window, sort（特别是快速排序 和 归并排序)， two pointers, graph, bfs, trie, union find\\n个人的其他刷题方式，我通常会结合不同的刷题方式。\\n8. 只看题，不写代码。 我通常会从brute force 开始求解， 然后逐步优化。 如果10-20分钟以内想不出解法， 直接去看答案了。\\n9. 对于一道题， 精耕细作， 举一反三。 精耕细作是指 学习优秀代码， 一题多解 以及 如何从brute force推导到最优解，并且了解这个过程的思路是如何形成的。 举一反三是指看类似的题目， 总结共同特性。\\n10. 参加竞赛。 训练快速做题， debug ， 写代码的能力。\\n其他tips\\n11. 刷题初期追求量，一道题不可花太长时间， 控制在30分钟以内\\n12. 最好不依赖IDE， 训练直接在leetcode 页面写代码。\\n13. 定期回过头总结刷过的题。\\n14. 不要盲目刷题， 适可而止，刷够了，要转战到面试技巧上面， 面试技巧和刷题一样重要。（因为我就有一个面试挂在面试技巧上面， 非常后悔）\\n\\n\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ecbea58c-c8cd-4b7c-a32d-69017235ff80', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**LinkedList**\\n\\n- Linked List Cycle\\n- Linked List Cycle II\\n- Remove Duplicates from Sorted List\\n- Remove Duplicates from Sorted List II\\n- Add Two Numbers  medium\\n\\n- 21. 合并两个有序链表\\n- 82. 删除排序链表中的重复元素 II\\n- 83. 删除排序链表中的重复元素\\n- 86. 分隔链表\\n- 92. 反转链表 II\\n- 138. 复制带随机指针的链表\\n- 141. 环形链表\\n- 142. 环形链表 II\\n- 143. 重排链表\\n- 148. 排序链表\\n- 206. 反转链表\\n- 234. 回文链表\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='88970db7-d282-407e-b5f0-02c62fee4a25', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Stack**\\n\\n- Valid Parentheses\\n- Reverse Linked List\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='231eff45-bd16-4246-867d-8abd7a55574c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**HashMap**\\n\\n- Two Sum\\n- Group Anagrams medium\\n- Intersection of Two Arrays\\n- Unique Email Addresses\\n- First Unique Character in a String\\n- Subarray Sum Equals K\\n- Longest Substring Without Repeating Characters (3)\\n- Logger Rate Limiter (359)\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='52bb01ad-c8a0-42f5-a47e-4c5ecaf0e371', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Binary Search**\\n\\n- Search Insert Position\\n- Find Minimum in Rotated Sorted Array\\n- Search in Rotated Sorted Array\\n- Capacity To Ship Packages Within D Days\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='96a2489c-641e-4d83-9fd3-ae5be59678cb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Recursion**\\n\\n- Pow(x, n)\\n- K-th Symbol in Grammar\\n- Split BST\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='09edb542-1f6f-4092-9dc3-4d8241210b71', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Graph, BFS, DFS**\\n\\n- Number of Islands\\n- Max Area of Island\\n- Number of Connected Components in an Undirected Graph\\n- Word Ladder\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1c785701-76d7-46da-a44c-1feab1480a74', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Tree, BT, BST**\\n\\n- Maximum Depth of Binary Tree\\n- Minimum Depth of Binary Tree\\n- Merge Two Binary Trees\\n- Convert Sorted Array to Binary Search Tree\\n- Path Sum\\n- Binary Tree Level Order Traversal\\n- Binary Tree Zigzag Level Order Traversal\\n- Validate Binary Search Tree\\n- Construct Binary Tree from Preorder and Inorder Traversal\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='88b24097-2477-4f3b-b199-9655b9222e51', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Sort**\\n\\nCheck out\\xa0Sorting Algorithms Animations. Understand in which data set radix sort or insertion sort are better than general heap/merge sort. Go each of sorting algorithms and understand pros and cons.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1aaf546e-3506-4b4d-a3cf-d1301d03d28f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Dynamic Programming**\\n\\n- Paint Fence\\n- Longest Increasing Subsequence\\n- Maximum Subarray\\n- Unique Paths\\n- Unique Paths II\\n- House Robber\\n- House Robber II\\n- Best Time to Buy and Sell Stock\\n- Best Time to Buy and Sell Stock II\\n- Word Break\\n- Coin Change\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ec791e0e-e06a-463f-8ca4-ed571c0d29d1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Heap, PriorityQueue**\\n\\n- Kth Largest Element in a Stream\\n- Top K Frequent Elements\\n- Find K Pairs with Smallest Sums\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e36d6ef8-6ad9-4a07-8ee1-26f3167d3475', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Sliding Window**\\n\\n- Longest Substring Without Repeating Characters\\n- Minimum Size Subarray Sum\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cabd9165-2881-49f0-8700-6abb81f2e8b0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Greedy + Backtracking**\\n\\n- Permutations\\n- Subsets\\n- Combination Sum\\n- Generate Parentheses', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d744220a-b00b-409c-b017-060593ef82d7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n树\\n\\npre, in, post order 三种遍历方式，都属于DFS \\n\\n\\n\\n其他重要概念：\\n\\n- 树的高度：节点到叶子节点的最大值就是其高度。\\n- 树的深度：高度和深度是相反的，高度是从下往上数，深度是从上往下。因此根节点的深度和叶子节点的高度是 0。\\n- 树的层：根开始定义，根为第一层，根的孩子为第二层。\\n- 二叉树，三叉树，。。。N 叉树，由其子节点最多可以有几个决定，最多有 N 个就是 N 叉树\\n\\n而遍历不是目的，遍历是为了更好地做处理，这里的处理包括搜索，修改树等。树虽然只能从根开始访问，但是我们可以「选择」在访问完毕回来的时候做处理，还是在访问回来之前做处理，这两种不同的方式就是「后序遍历」和「先序遍历」\\n\\n而树的遍历又可以分为两个基本类型，分别是深度优先遍历和广度优先遍历。这两种遍历方式并不是树特有的，但却伴随树的所有题目。值得注意的是，这两种遍历方式只是一种逻辑而已，因此理论可以应用于任何数据结构，比如 365. 水壶问题[5] 中，就可以对水壶的状态使用广度优先遍历，而水壶的状态可以用「一个二元组」来表示\\n\\n很多小朋友表示二叉树前中后序的递归写法没问题，但是迭代就写不出来，问我有什么好的方法没有。\\n\\n这里就给大家介绍一种写迭代遍历树的实操技巧，统一三种树的遍历方式，包你不会错，这个方法叫做双色标记法。如果你会了这个技巧，那么你平时练习大可**「只用递归」**。然后面试的时候，真的要求用迭代或者是对性能有特别要求的那种题目，那你就用我的方法套就行了，下面我来详细讲一下这种方法。\\n\\n我们知道垃圾回收算法中，有一种算法叫三色标记法。即：\\n\\n- 用白色表示尚未访问\\n- 灰色表示尚未完全访问子节点\\n- 黑色表示子节点全部访问\\n\\n那么我们可以模仿其思想，使用双色标记法来统一三种遍历。\\n\\n其核心思想如下：\\n\\n- 使用颜色标记节点的状态，新节点为白色，已访问的节点为灰色。\\n- 如果遇到的节点为白色，则将其标记为灰色，然后将其右子节点、自身、左子节点依次入栈。\\n- 如果遇到的节点为灰色，则将节点的值输出。\\n\\n使用这种方法实现的中序遍历如下\\n\\n```\\nclass Solution:\\n    def inorderTraversal(self, root: TreeNode) -> List[int]:\\n        WHITE, GRAY = 0, 1\\n        res = []\\n        stack = [(WHITE, root)]\\n        while stack:\\n            color, node = stack.pop()\\n            if node is None: continue\\n            if color == WHITE:\\n                stack.append((WHITE, node.right)) #先append right，最后弹出right\\n                stack.append((GRAY, node))\\n                stack.append((WHITE, node.left))\\n            else:\\n                res.append(node.val)  #VIPVIP \\n        return res\\n```\\n\\n可以看出，实现上 WHITE 就表示的是递归中的第一次进入过程，Gray 则表示递归中的从叶子节点返回的过程。因此这种迭代的写法更接近递归写法的本质。\\n\\n如要**「实现前序、后序遍历，也只需要调整左右子节点的入栈顺序即可，其他部分是无需做任何变化」**\\n\\n\\n\\n上面提到了树的遍历有两种基本方式，分别是**「深度优先遍历（以下简称 DFS）和广度优先遍历（以下简称 BFS），这就是两个基本点」**。这两种遍历方式下面又会细分几种方式。比如\\xa0**「DFS 细分为前中后序遍历， BFS 细分为带层的和不带层的」**。\\n\\n**「DFS 适合做一些暴力枚举的题目，DFS 如果借助函数调用栈，则可以轻松地使用递归来实现。」**\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='28fd55f1-5a8b-4be4-8481-23f1dff3b483', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nDFS\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ed7e4d1e-5f58-4749-beb7-0d248adb95d2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**算法流程**\\n\\n1. 首先将根节点放入**「stack」**中。\\n2. 从**stack**中取出第一个节点，并检验它是否为目标。如果找到所有的节点，则结束搜寻并回传结果。否则将它某一个尚未检验过的直接子节点加入**「stack」**中。\\n3. 重复步骤 2。\\n4. 如果不存在未检测过的直接子节点。将上一级节点加入**「stack」**中。重复步骤 2。\\n5. 重复步骤 4。\\n6. 若**「stack」**为空，表示整张图都检查过了——亦即图中没有欲搜寻的目标。结束搜寻并回传“找不到目标”。\\n\\n**「这里的 stack 可以理解为自己实现的栈，也可以理解为调用栈。如果是调用栈的时候就是递归，如果是自己实现的栈的话就是迭代。」**\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='276e2630-d97e-445e-b98c-284f17e61455', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**算法模板**\\n\\n一个典型的通用的 DFS 模板可能是这样的：\\n\\n```\\n**#因此一个树的 DFS 更多是：**\\n\\nfunction dfs(root) {\\n if (满足特定条件）{\\n  // 返回结果 or 退出搜索空间\\n }\\n for (const child of root.children) {\\n        dfs(child)\\n }\\n}\\n\\n**#而几乎所有的题目几乎都是二叉树，因此下面这个模板更常见。**\\n\\nfunction dfs(root) {\\n if (满足特定条件）{\\n  // 返回结果 or 退出搜索空间\\n }\\n    dfs(root.left)\\n    dfs(root.right)\\n}\\n```\\n\\n**两种常见分类**\\n\\n前序遍历和后序遍历是最常见的两种 DFS 方式。而另外一种遍历方式 （中序遍历）一般用于平衡二叉树\\n\\n**前序遍历**\\n\\n如果你的代码大概是这么写的（注意主要逻辑的位置）：\\n\\n```\\nfunction\\xa0dfs(root)\\xa0{\\n\\xa0if\\xa0(满足特定条件）{\\n//\\xa0返回结果\\xa0or\\xa0退出搜索空间\\n\\xa0\\xa0\\xa0\\xa0}\\n//\\xa0主要逻辑\\n\\xa0\\xa0\\xa0\\xa0dfs(root.left)\\n\\xa0\\xa0\\xa0\\xa0dfs(root.right)\\n}\\n```\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c4c037e5-048f-44a9-bc83-3c95075728f1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**后续遍历**\\n\\n而如果你的代码大概是这么写的（注意主要逻辑的位置）：\\n\\n```\\nfunction\\xa0dfs(root)\\xa0{\\n\\xa0if\\xa0(满足特定条件）{\\n//\\xa0返回结果\\xa0or\\xa0退出搜索空间\\n\\xa0\\xa0\\xa0\\xa0}\\n\\xa0\\xa0\\xa0\\xa0dfs(root.left)\\n\\xa0\\xa0\\xa0\\xa0dfs(root.right)\\n//\\xa0主要逻辑\\n}\\n\\n```\\n\\n那么此时我们称为后序遍历\\n\\n值得注意的是， 我们有时也会会写出这样的代码：\\n\\n```\\nfunction\\xa0dfs(root)\\xa0{\\n\\xa0if\\xa0(满足特定条件）{\\n//\\xa0返回结果\\xa0or\\xa0退出搜索空间\\n\\xa0\\xa0\\xa0\\xa0}\\n//\\xa0做一些事\\n\\xa0\\xa0\\xa0\\xa0dfs(root.left)\\n\\xa0\\xa0\\xa0\\xa0dfs(root.right)\\n//\\xa0做另外的事\\n}\\n\\n```\\n\\n如上代码，我们在进入和退出左右子树的时候分别执行了一些代码。那么这个时候，是前序遍历还是后续遍历呢？实际上，这属于混合遍历了。不过我们这里只考虑**「主逻辑」**的位置，关键词是**「主逻辑」**。\\ndsadadad\\n\\n如果代码主逻辑在左右子树之前执行，那么就是前序遍历。如果代码主逻辑在左右子树之后执行，那么就是后序遍历\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9c093559-1ec7-4616-aa78-603b3659877a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nBFS\\n\\nBFS 也是图论中算法的一种，不同于 DFS， BFS 采用横向搜索的方式，在数据结构上通常采用队列结构。注意，DFS 我们借助的是栈来完成，而这里借助的是队列。\\n\\nBFS 比较适合找**「最短距离/路径」**和**「某一个距离的目标」**。比如`给定一个二叉树，在树的最后一行找到最左边的值。`，此题是力扣 513 的原题。这不就是求距离根节点**「最远距离」**的目标么？一个 BFS 模板就解决了\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='976d2802-b5d6-4eee-9bb6-708d93c0654b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**算法流程**\\n\\n1. 首先将根节点放入队列中。\\n2. 从队列中取出第一个节点，并检验它是否为目标。\\n    - 如果找到目标，则结束搜索并回传结果。\\n    - 否则将它所有尚未检验过的直接子节点加入队列中。\\n3. 若队列为空，表示整张图都检查过了——亦即图中没有欲搜索的目标。结束搜索并回传“找不到目标”。\\n4. 重复步骤 2。\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b6df82ce-2580-4bf6-9d37-ab4d012b7942', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**算法模板**\\n\\n```\\nconst\\xa0visited\\xa0=\\xa0{}\\nfunction\\xa0bfs()\\xa0{\\n\\xa0let\\xa0q\\xa0=\\xa0new\\xa0Queue()\\n\\xa0q.push(初始状态)\\n\\xa0while(q.length)\\xa0{\\n\\xa0\\xa0let\\xa0i\\xa0=\\xa0q.pop()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0if\\xa0(visited[i])\\xa0continue\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0if\\xa0(i\\xa0是我们要找的目标)\\xa0return\\xa0结果\\n\\xa0\\xa0for\\xa0(i的可抵达状态j)\\xa0{\\n\\xa0\\xa0\\xa0if\\xa0(j\\xa0合法)\\xa0{\\n\\xa0\\xa0\\xa0\\xa0q.push(j)\\n\\xa0\\xa0\\xa0}\\n\\xa0\\xa0}\\n\\xa0\\xa0\\xa0\\xa0}\\n\\xa0\\xa0\\xa0\\xa0return\\xa0没找到\\n}\\n```\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7b5ac202-fa62-4412-99cf-94357cd80fbb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**两种常见分类**\\n\\nBFS 我目前使用的模板就两种，这两个模板可以解决所有的树的 BFS 问题。\\n\\n前面我提到了“BFS 比较适合找**「最短距离/路径」**和**「某一个距离的目标」**”。如果我需要求的是最短距离/路径，我是不关心我走到第几步的，这个时候可是用不标记层的目标。而如果我需要求距离某个节点距离等于 k 的所有节点，这个时候第几步这个信息就值得被记录了。\\n\\n> ❝小于 k 或者 大于 k 也是同理。❞\\n> \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='15c68761-cdd7-4d35-95d8-59669c302b0c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**标记层**\\n\\n一个常见的 BFS 模板，代入题目只需要根据题目微调即可。\\n\\n```\\nclass\\xa0Solution:\\n\\xa0\\xa0\\xa0\\xa0def\\xa0bfs(k):\\n#\\xa0使用双端队列，而不是数组。因为数组从头部删除元素的时间复杂度为 N，双端队列的底层实现其实是链表。\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0queue\\xa0=\\xa0collections.deque([root])\\n#\\xa0记录层数\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0steps\\xa0=\\xa00\\n#\\xa0需要返回的节点\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0ans\\xa0=\\xa0[]\\n#\\xa0队列不空，生命不止！\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0while\\xa0queue:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0size\\xa0=\\xa0len(queue)\\n#\\xa0遍历当前层的所有节点\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0for\\xa0_\\xa0in\\xa0range(size):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0node\\xa0=\\xa0queue.popleft()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0if\\xa0(step\\xa0==\\xa0k)\\xa0ans.append(node)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0if\\xa0node.right:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0queue.append(node.right)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0if\\xa0node.left:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0queue.append(node.left)\\n#\\xa0遍历完当前层所有的节点后\\xa0steps\\xa0+\\xa01\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0steps\\xa0+=\\xa01\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return\\xa0ans\\n```\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7944aed3-1618-46ca-bfd3-fdfc6c6e061f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**不标记层**\\n\\n不带层的模板更简单，因此大家其实只需要掌握带层信息的目标就够了。\\n\\n一个常见的 BFS 模板，代入题目只需要根据题目微调即可。\\n\\n```\\nclass\\xa0Solution:\\n\\xa0\\xa0\\xa0\\xa0def\\xa0bfs(k):\\n#\\xa0使用双端队列，而不是数组。因为数组从头部删除元素的时间复杂度为 N，双端队列的底层实现其实是链表。\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0queue\\xa0=\\xa0collections.deque([root])\\n#\\xa0队列不空，生命不止！\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0while\\xa0queue:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0node\\xa0=\\xa0queue.popleft()\\n#\\xa0由于没有记录 steps，因此我们肯定是不需要根据层的信息去判断的。否则就用带层的模板了。\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0if\\xa0(node\\xa0是我们要找到的)\\xa0return\\xa0node\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0if\\xa0node.right:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0queue.append(node.right)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0if\\xa0node.left:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0queue.append(node.left)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return\\xa0-1\\n```\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='54500011-fde7-4b26-912b-4ced322a5d04', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**三种题型**\\n\\n树的题目就三种类型，分别是：**「搜索类，构建类和修改类，而这三类题型的比例也是逐渐降低的」**，即搜索类的题目最多，其次是构建类，最后是修改类。这一点和链表有很大的不同，链表更多的是修改类\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4867117e-2c32-4972-ba31-c6f10751c348', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**搜索类**\\n\\n搜索类的题目是树的题目的绝对大头。而搜索类只有两种解法，那就是 DFS 和 BFS，下面分别介绍。\\n\\n几乎所有的搜索类题目都可以方便地使用递归来实现，关于递归的技巧会在**「七个技巧中的单/双递归」**部分讲解。还有一小部分使用递归不好实现，我们可以使用 BFS，借助队列轻松实现，比如最经典的是求二叉树任意两点的距离，树的距离其实就是最短距离，因此可以用 BFS 模板解决。这也是为啥我说**「DFS 和 BFS」**是树的题目的两个基本点的原因。\\n\\n所有搜索类的题目只要把握三个核心点，即**「开始点」**，**「结束点」**\\xa0和\\xa0**「目标」**即可。\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='28d505be-b6cd-40f1-8b6c-9cf2667d89a9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**DFS 搜索**\\n\\nDFS 搜索类的基本套路就是从入口开始做 dfs，然后在 dfs 内部判断是否是结束点，这个结束点通常是**「叶子节点」**或**「空节点」**，关于结束这个话题我们放在**「七个技巧中的边界」**部分介绍，如果目标是一个基本值（比如数字）直接返回或者使用一个全局变量记录即可，如果是一个数组，则可以通过扩展参数的技巧来完成，关于扩展参数，会在**「七个技巧中的参数扩展」**部分介绍。这基本就是搜索问题的全部了，当你读完后面的七个技巧，回头再回来看这个会更清晰。\\n\\n套路模板：\\n\\n```\\n#\\xa0其中\\xa0path\\xa0是树的路径，\\xa0如果需要就带上，不需要就不带\\ndef\\xa0dfs(root,\\xa0path):\\n#\\xa0空节点\\n\\xa0\\xa0\\xa0\\xa0if\\xa0not\\xa0root:\\xa0return\\n#\\xa0叶子节点\\n\\xa0\\xa0\\xa0\\xa0if\\xa0not\\xa0root.left\\xa0and\\xa0not\\xa0root.right:\\xa0return\\n\\xa0\\xa0\\xa0\\xa0path.append(root)\\n#\\xa0逻辑可以写这里，此时是前序遍历\\n\\xa0\\xa0\\xa0\\xa0dfs(root.left)\\n\\xa0\\xa0\\xa0\\xa0dfs(root.right)\\n#\\xa0需要弹出，不然会错误计算。\\n#\\xa0比如对于如下树：\\n\\xa0\\xa0\\xa0\\xa0\"\"\"\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa05\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0/\\xa0\\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa04\\xa0\\xa0\\xa08\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0/\\xa0\\xa0\\xa0/\\xa0\\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa011\\xa0\\xa013\\xa0\\xa04\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0/\\xa0\\xa0\\\\\\xa0\\xa0\\xa0\\xa0/\\xa0\\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa07\\xa0\\xa0\\xa0\\xa02\\xa0\\xa05\\xa0\\xa0\\xa01\\n\\xa0\\xa0\\xa0\\xa0\"\"\"\\n#\\xa0如果不\\xa0pop，那么\\xa05\\xa0->\\xa04\\xa0->\\xa011\\xa0->\\xa02\\xa0这条路径会变成\\xa05\\xa0->\\xa04\\xa0->\\xa011\\xa0->\\xa07\\xa0->\\xa02，其\\xa07\\xa0被错误地添加到了\\xa0path\\n\\n\\xa0\\xa0\\xa0\\xa0path.pop()\\n#\\xa0逻辑也可以写这里，此时是后序遍历\\n\\n\\xa0\\xa0\\xa0\\xa0return\\xa0你想返回的数据\\n```\\n\\nhight是一个node到最下面子节点的距离，depth是一个node到他的root节点的距离\\n\\n这里节点的高度和深度可能容易记混，我们代入到现实即可。\\n\\n我们求物体深度时，从上往下测量，求高度时，从下往上测量，节点的高度和深度也是如此\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n在这个binary search tree里找4，通过大小关系查找很快\\n\\n\\n\\n\\n\\n要删除的树有左右两个kid; 可以找左子树里最大的那个来代替被删除节点，或者找右子树里最小的那个来代替被删除节点\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bda315cf-e535-4938-859d-2a0bd07c42c4', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n各种遍历\\n\\n前序遍历（Pre-Order）：根节点->左子树->右子树(NLR), node - left - right\\n中序遍历（In-Order）：左子树->根节点->右子树(LNR), left - node - right\\n后序遍历（Post-Order）：左子树->右子树->根节点(LRN), left - right - node \\n\\n\\n\\n\\n\\n\\n\\n从根结点出发，则第一次到达结点A，故输出A;\\n继续向左访问，第一次访问结点B，故输出B；\\n按照同样规则，输出D，输出H；\\n当到达叶子结点H，返回到D，此时已经是第二次到达D，故不在输出D，进而向D右子树访问，D右子树不为空，则访问至I，第一次到达I，则输出I；\\nI为叶子结点，则返回到D，D左右子树已经访问完毕，则返回到B，进而到B右子树，第一次到达E，故输出E；\\n向E左子树，故输出J；\\n按照同样的访问规则，继续输出C、F、G；\\n\\n则3.13所示二叉树的前序遍历输出为：\\n\\n**ABDHIEJCFG**\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='39a1e9ad-9cfe-473d-9433-06bbf522af70', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n3.8.3 中序遍历\\n\\n**中序遍历**就是从二叉树的根结点出发，当第二次到达结点时就输出结点数据，按照先向左在向右的方向访问。\\n\\n图3.13所示二叉树中序访问如下：\\n\\n> 从根结点出发，则第一次到达结点A，不输出A，继续向左访问，第一次访问结点B，不输出B；继续到达D，H；\\n到达H，H左子树为空，则返回到H，此时第二次访问H，故输出H；\\nH右子树为空，则返回至D，此时第二次到达D，故输出D；\\n由D返回至B，第二次到达B，故输出B；\\n按照同样规则继续访问，输出J、E、A、F、C、G；\\n> \\n\\n则3.13所示二叉树的中序遍历输出为：**HDIBJEAFCG**\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6c3d7846-bd2a-40ae-9b6d-8fb98886429e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n3.8.4 后序遍历\\n\\n**后序遍历**就是从二叉树的根结点出发，当第三次到达结点时就输出结点数据，按照先向左在向右的方向访问。\\n\\n图3.13所示二叉树后序访问如下：\\n\\n> 从根结点出发，则第一次到达结点A，不输出A，继续向左访问，第一次访问结点B，不输出B；继续到达D，H；\\n到达H，H左子树为空，则返回到H，此时第二次访问H，不输出H；\\nH右子树为空，则返回至H，此时第三次到达H，故输出H；\\n由H返回至D，第二次到达D，不输出D；\\n继续访问至I，I左右子树均为空，故第三次访问I时，输出I；\\n返回至D，此时第三次到达D，故输出D；\\n按照同样规则继续访问，输出J、E、B、F、G、C，A；\\n> \\n\\n则图3.13所示二叉树的后序遍历输出为：**HIDJEBFGCA**\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3119ee1a-ff13-45cc-8367-5c7b60469b92', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**3.8.5 层次遍历**\\n\\n层次遍历就是按照树的层次自上而下的遍历二叉树。针对图3.13所示二叉树的层次遍历结果为：**ABCDEFGHIJ**\\n\\n图有两种遍历方式，**深度优先遍历**（DFS）**和广度优先遍历**（BFS），我们一个一个来看。\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3f1d2545-21b8-447e-b0d4-229ccf7392b8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**深度优先遍历**\\n\\n深度优先遍历可以用 Stack 来实现，具体实现基本上可以分这么几步：\\n\\n1. 如果栈不空，出栈得到当前节点\\n2. 查看当前节点左右子树，如果不为空，入栈\\n\\n可以说非常简单了，下面是一个简单实现：\\n\\n`class Solution(object):\\n    def dfs(self, root):\\n        stack = [(root, 0)]  # 记录当前节点的深度，当然你也可以记录任何你想记录的东西\\n        while stack:   # 如果栈不为空\\n            node, depth = stack.pop()  # 出栈\\n\\n            if node is not None:\\n                stack.append((node.left, depth+1)) # 入栈\\n                stack.append((node.right, depth+1)) # 入栈\\n\\n        return`\\n\\n整个代码非常简洁。**需要注意的是**，在 Python里面，一个简单实现 Stack 的方式就是直接用 List，入栈就是 list.append()（从栈尾入栈），出栈是 list.pop()（从栈尾出栈）。\\n\\n这个时候，我们可能会回想到，对于树来说，**前中后序遍历**能不能统一进来？能，其实**前中后序遍历**都可以划归到深度优先遍历里面去，只是访问的顺序的问题\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6f79434b-db77-4c51-a71b-c089b4909a67', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**前序遍历**\\n\\n上面的那个代码，不用改就是**前序遍历**的代码：\\n\\n`class Solution(object):\\n    def dfs(self, root):\\n        stack = [(root, 0)]  # 记录当前节点的深度，当然你也可以记录任何你想记录的东西\\n        while stack:   # 如果栈不为空\\n            node, depth = stack.pop()  # 出栈\\n            # 前序遍历的操作在这里\\n            if node is not None:\\n                stack.append((node.right, depth+1)) # 入栈\\n                stack.append((node.left, depth+1)) # 入栈\\n                # 注意这里的入栈顺序会直接影响到最后的结果\\n                # 一般来说，我们是让左节点最后入栈，这样出栈的时候，能优先出栈\\n\\n        return`\\n\\n写成递归形式，大体长这样\\n\\n`def preorder(node):\\n  # 前序遍历的操作在这里\\n  preorder(node.left)\\n  preorder(node.right)`\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='413713a9-4229-460d-9040-8d5b092e52b2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**递归方式实现前序遍历**\\n\\n> 具体过程：先访问根节点再序遍历左子树最后序遍历右子树\\n> \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1e5051c9-c686-4d59-941a-0cb44984668c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**非递归方式实现前序遍历**\\n\\n> 具体过程：首先申请一个新的栈，记为stack；将头结点head压入stack中；每次从stack中弹出栈顶节点，记为cur，然后打印cur值，如果cur右孩子不为空，则将右孩子压入栈中；如果cur的左孩子不为空，将其压入stack中；重复步骤3，直到stack为空.\\n> \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='85a46fac-cb5c-434a-9b11-2e37006243b1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**中序遍历**\\n\\n中序遍历的实现跟上面相比，稍微有点复杂，但是只要想清楚了，其实只是比上面前序遍历复杂了一点点。\\n\\n首先，我们要知道为什么要使用栈来实现 DFS，因为在 DFS 里面，我们要一直往深度探索，这个没问题（入栈）。但是当我们遇到叶子节点要回溯的时候，一定是从**上一个最新深度**为起点接着探索（出栈）。而这些**前中后序**就是不同的出栈规则（操作总是紧跟出栈）。\\n\\n在前序遍历中，每当我们到一个新节点的时候，就立即将其出栈，然后把他的孩子节点入栈。那么对于中序遍历，可以想象到，每当我们到达一个新节点的时候，我们要做一个判断：\\n\\n1. 如果他的左节点不是空的，那么我们就不能出栈，而是要继续把他的左节点放进来\\n2. 如果他的左节点是空的，这个时候可以出栈了，但是记得出栈完把自己的右节点放进来。\\n\\n非递归代码如下，\\n\\n`class Solution(object):\\n    def dfs(self, root):\\n        stack = []  # 记录当前节点的深度，当然你也可以记录任何你想记录的东西\\n        while (len(stack)>0) or (root is not None):   # 如果栈不为空\\n            if root is not None:\\n            \\tstack.append(root)\\n            \\troot = root.left\\n            else:  # 如果左节点是空\\n            \\troot = stack.pop()\\n              # 中序遍历的操作在这里\\n              root = root.right\\n        return`\\n\\n递归实现就很简单了：\\n\\n`def preorder(node):\\n  preorder(node.left)\\n  # 中序遍历的操作在这里\\n  preorder(node.right)`\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2adcd827-dcf7-4218-87af-40965879dc6b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**递归方式实现中序遍历**\\n\\n> 具体过程：先中序遍历左子树再访问根节点最后中序遍历右子树\\n> \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8c9e2d52-bdee-4ad8-b675-7b3792dab987', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**非递归方式实现中序遍历**\\n\\n> 具体过程：申请一个新栈，记为stack，申请一个变量cur，初始时令cur为头节点；先把cur节点压入栈中，对以cur节点为头的整棵子树来说，依次把整棵树的左子树压入栈中，即不断令cur=cur.left，然后重复步骤2；不断重复步骤2，直到发现cur为空，此时从stack中弹出一个节点记为node，打印node的值，并让cur = node.right，然后继续重复步骤2；当stack为空并且cur为空时结束\\n> \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f95f0d18-719e-4754-a2b8-437a5e6ac798', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**后序遍历**\\n\\n后序遍历还要更复杂一些，因为后序遍历的定义就是，只有当一个节点的左右节点都访问过了，才能访问这个节点。\\n\\n思路是： 先将当前节点的所有左侧子结点压入栈，现在要保证在访问当前节点的右子结点之后才能访问当前节点。所以每次从栈中拿出节点时，都需要判断该节点的右子树是否存在或右子树是否被访问过，这里使用了一个 preNode 来记录刚被访问过的节点，这样就可以实现只有当前节点的右子结点访问完成，才能访问当前节点。\\n\\n`class Solution(object):\\n    def dfs(self, root):\\n        stack = []  # 记录当前节点及其深度，当然你也可以记录任何你想记录的东西\\n        node = root # 当前节点\\n        preNode = None # 上一个被访问的节点\\n        while (len(stack)>0) or (root is not None):   # 如果栈不为空\\n            while node is not None:\\n            \\tstack.append(root)\\n            \\troot = root.left\\n            if len(stack) > 0:\\n            \\ttmp = stack[-1].right  # 这个地方不能直接出栈，因为栈顶位置不一定能能方法，还需要判断其右节点被已经被访问过了\\n            \\tif (tmp is None) or (tmp==preNode):\\n              \\tnode = stack.pop()\\n              \\t# 中序遍历的操作在这里\\n              \\tpreNode = node\\n                node=None\\n              else:\\n              \\tnode = tmp  # 处理右节点\\n        return`\\n\\n递归实现就很简单了：\\n\\n`def preorder(node):\\n  preorder(node.left)\\n  preorder(node.right)\\n  # 后序遍历的操作在这里`\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fdd693af-4e7f-460a-bcfc-bc5df3828247', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**递归方式实现后序遍历**\\n\\n> 先后序遍历左子树再后序遍历右子树最后访问根节点\\n> \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8379fdf0-73d0-49a6-b61a-7d62a0b42671', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**非递归方式实现后序遍历一**\\n\\n> 具体过程：使用两个栈实现申请两个栈stack1，stack2，然后将头结点压入stack1中；从stack1中弹出的节点记为cur，然后先把cur的左孩子压入stack1中，再把cur的右孩子压入stack1中；在整个过程中，每一个从stack1中弹出的节点都放在第二个栈stack2中；不断重复步骤2和步骤3，直到stack1为空，过程停止；从stack2中依次弹出节点并打印，打印的顺序就是后序遍历的顺序；\\n> \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='30018d9a-5df2-4e12-8406-502a9f4fa429', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**广度优先遍历**\\n\\n广度优先遍历和深度优先相比，只需要把 Stack 换成 Queue。\\n\\n`from collection import deque\\nclass Solution(object):\\n    def dfs(self, root):\\n        queue = deque([(root, 0)])  # 记录当前节点的深度，当然你也可以记录任何你想记录的东西\\n        while queue:   # 如果队列不为空\\n            node, depth = queue.popleft()  # 出队列\\n\\n            if node is not None:\\n                queue.append((node.left, depth+1)) # 入队列\\n                queue.append((node.right, depth+1)) # 入队列\\n\\n        return`\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bd508944-fa49-4a73-8f13-129edb2db716', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**特点总结**\\n\\n**深度优先**：\\n\\n以中序遍历为例：某个节点被访问时，其左子树一定已经全部被访问，其右子树一定没有被访问。（“深度”对应“子树”的概念。）\\n\\n**广度优先**（层次遍历）：\\n\\n从根节点开始，访问顺序一定是按照深度递增的\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='435b0bf0-a4b5-465e-998e-804642163174', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**层序遍历**\\n\\n> 具体过程：首先申请一个新的队列，记为queue；将头结点head压入queue中；每次从queue中出队，记为node，然后打印node值，如果node左孩子不为空，则将左孩子入队；如果node的右孩子不为空，则将右孩子入队；重复步骤3，直到queue为空\\n>', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1e4e14f0-e51a-4b8d-963d-32e12f1f29f0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n《算法通关之路》 笔记\\n\\n!Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='78d5bac0-a727-448a-9c29-32140222a0bd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n算法课 拉勾\\n\\n\\n\\ncomplexity是啥意思？how does time and space scale with the size of input N\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nsparse data有时候可以用别的data structure来存来省内存\\n\\n\\n\\nspace resource is a sunk cost on your computer already. Time is flexible cost so its more important\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n链表只能按位置条件查找，所以时间复杂度是o(n)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n需要完成的操作包括，按照位置的查找、新增和按照数据数值的查找、删除\\n\\n\\n\\n数据处理的基本操作只有 3 个，分别是增、删、查。其中，增和删又可以细分为在数据结构中间的增和删，以及在数据结构最后的增和删。区别就在于原数据的位置是否发生改变。查找又可以细分为按照位置条件的查找和按照数据数值特征的查找。几乎所有的数据处理，都是这些基本操作的组合和叠加\\n\\n我们需要从时间复杂度和空间复杂度两个维度来考虑。常用的降低时间复杂度的方法有递归、二分法、排序算法、动态规划等，这些知识我们都会在后续课程中逐一学习，这里我先不讲。而降低空间复杂度的方法，就要围绕数据结构做文章了。\\n\\n降低空间复杂度的核心思路就是，能用低复杂度的数据结构能解决问题，就千万不要用高复杂度的数据结构\\n\\n根据这里的分析不难发现，链表在新增、删除数据都比较容易，可以在 O(1) 的时间复杂度内完成。但对于查找，不管是按照位置的查找还是按照数值条件的查找，都需要对全部数据进行遍历。这显然就是 O(n) 的时间复杂度。\\n\\n虽然链表在新增和删除数据上有优势，但仔细思考就会发现，这个优势并不实用。这主要是因为，在新增数据时，通常会伴随一个查找的动作。例如，在第五个结点后，新增一个新的数据结点，那么执行的操作就包含两个步骤：\\n\\n第一步，查找第五个结点；\\n\\n第二步，再新增一个数据结点。整体的复杂度就是 O(n) + O(1)\\n\\n根据我们前面所学的复杂度计算方法，这也等同于 O(n) 的时间复杂度。线性表真正的价值在于，它对数据的存储方式是按照顺序的存储。如果数据的元素个数不确定，且需要经常进行数据的新增和删除时，那么链表会比较合适。如果数据元素大小确定，删除插入的操作并不多，那么数组可能更适合些\\n\\n栈是什么\\n你需要牢记一点，栈是一种特殊的线性表。栈与线性表的不同，体现在增和删的操作。具体而言，栈的数据结点必须后进先出。后进的意思是，栈的数据新增操作只能在末端进行，不允许在栈的中间某个结点后新增数据。先出的意思是，栈的数据删除操作也只能在末端进行，不允许在栈的中间某个结点后删除数据\\n\\n浏览器都有页面前进和后退功能，这就是个很典型的后进先出的场景。假设你先后访问了五个页面，分别标记为 1、2、3、4、5。当前你在页面 5，如果执行两次后退，则退回到了页面 3，如果再执行一次前进，则到了页面 4。处理这里的页面链接存储问题，栈就应该是我们首选的数据结构\\n\\n栈的基本操作\\n如何通过栈这个后进先出的线性表，来实现增删查呢？初始时，栈内没有数据，即空栈。此时栈顶就是栈底。当存入数据时，最先放入的数据会进入栈底。接着加入的数据都会放入到栈顶的位置。如果要删除数据，也只能通过访问栈顶的数据并删除。对于栈的新增操作，通常也叫作 push 或压栈。对于栈的删除操作，通常也叫作 pop 或出栈\\n\\n通过分析你会发现，不管是顺序栈还是链栈，数据的新增、删除、查找与线性表的操作原理极为相似，时间复杂度完全一样，都依赖当前位置的指针来进行数据对象的操作。区别仅仅在于新增和删除的对象，只能是栈顶的数据结点\\n\\n**接下来，我们一起来看两个栈的经典案例，从中你可以更深切地体会到栈所发挥出的价值。**\\n\\n例 1，给定一个只包括 \\'(\\'，\\')\\'，\\'{\\'，\\'}\\'，\\'[\\'，\\']\\' 的字符串，判断字符串是否有效。有效字符串需满足：左括号必须与相同类型的右括号匹配，左括号必须以正确的顺序匹配。例如，{ [ ( ) ( ) ] } 是合法的，而 { ( [ ) ] } 是非法的。\\n\\n这个问题很显然是栈发挥价值的地方。原因是，在匹配括号是否合法时，左括号是从左到右依次出现，而右括号则需要按照“后进先出”的顺序依次与左括号匹配。因此，实现方案就是通过栈的进出来完成。\\n\\n具体为，从左到右顺序遍历字符串。当出现左括号时，压栈。当出现右括号时，出栈。并且判断当前右括号，和被出栈的左括号是否是互相匹配的一对。如果不是，则字符串非法。当遍历完成之后，如果栈为空。则合法。如下图所示：\\n\\n```python\\npublic static void main(String[] args) {\\nString s = \"{[()()]}\";\\nSystem.out.println(isLegal(s));\\n}\\nprivate static int isLeft(char c) {\\nif (c == \\'{\\' || c == \\'(\\' || c == \\'[\\') {\\nreturn 1;\\n} else {\\nreturn 2;\\n}\\n}\\nprivate static int isPair(char p, char curr) {\\nif ((p == \\'{\\' && curr == \\'}\\') || (p == \\'[\\' && curr == \\']\\') || (p == \\'(\\' && curr == \\')\\')) {\\nreturn 1;\\n} else {\\nreturn 0;\\n}\\n}\\nprivate static String isLegal(String s) {\\nStack stack = new Stack();\\nfor (int i = 0; i < s.length(); i++) {\\nchar curr = s.charAt(i);\\nif (isLeft(curr) == 1) {\\nstack.push(curr);\\n} else {\\nif (stack.empty()) {\\nreturn \"非法\";\\n}\\nchar p = (char) stack.pop();\\nif (isPair(p, curr) == 0) {\\nreturn \"非法\";\\n}\\n}\\n}\\nif (stack.empty()) {\\nreturn \"合法\";\\n} else {\\nreturn \"非法\";\\n}\\n}\\n```\\n\\n例 2，浏览器的页面访问都包含了后退和前进功能，利用栈如何实现？\\n\\n我们利用浏览器上网时，都会高频使用后退和前进的功能。比如，你按照顺序先后访问了 5 个页面，分别标记为 1、2、3、4、5。现在你不确定网页 5 是不是你要看的网页，需要回退到网页 3，则需要使用到两次后退的功能。假设回退后，你发现网页 4 有你需要的信息，那么就还需要再执行一次前进的操作。\\n\\n为了支持前进、后退的功能，利用栈来记录用户历史访问网页的顺序信息是一个不错的选择。此时需要维护两个栈，分别用来支持后退和前进。当用户访问了一个新的页面，则对后退栈进行压栈操作。当用户后退了一个页面，则后退栈进行出栈，同时前进栈执行压栈。当用户前进了一个页面，则前进栈出栈，同时后退栈压栈。我们以用户按照 1、2、3、4、5、4、3、4 的浏览顺序为例，两个栈的数据存储过程，如下图所示\\n\\n队列是什么\\n与栈相似，队列也是一种特殊的线性表，与线性表的不同之处也是体现在对数据的增和删的操作上。\\n\\n队列的特点是先进先出：\\n\\n先进，表示队列的数据新增操作只能在末端进行，不允许在队列的中间某个结点后新增数据;\\n\\n先出，队列的数据删除操作只能在始端进行，不允许在队列的中间某个结点后删除数据。也就是说队列的增和删的操作只能分别在这个队列的队尾和队头进行\\n\\n\\n\\n\\n\\n循环队列进行新增数据元素操作时，首先判断队列是否为满。如果不满，则可以将新元素赋值给队尾，然后让 rear 指针向后移动一个位置。如果已经排到队列最后的位置，则 rea r指针重新指向头部。\\n\\n循环队列进行删除操作时，即出队列操作，需要判断队列是否为空，然后将队头元素赋值给返回值，front 指针向后移一个位置。如果已经排到队列最后的位置，就把 front 指针重新指向到头部。这个过程就好像钟表的指针转到了表盘的尾部 12 点的位置后，又重新回到了表盘头部 1 点钟的位置。这样就能在不开辟大量存储空间的前提下，不采用 O(n) 的操作，也能通过移动数据来完成频繁的新增和删除数据\\n\\n\\n\\n数组的基本操作\\n数组在存储数据时是按顺序存储的，并且存储数据的内存也是连续的，这就造成了它具有增删困难、查找容易的特点。同时，栈和队列是加了限制的线性表，只能在特定的位置进行增删操作。相比之下，数组并没有这些限制，可以在任意位置增删数据，所以数组的增删操作会更为多样\\n\\n数组的新增操作\\n数组新增数据有两个情况：\\n\\n第一种情况，在数组的最后增加一个新的元素。此时新增一条数据后，对原数据产生没有任何影响。可以直接通过新增操作，赋值或者插入一条新的数据即可。时间复杂度是 O(1)。\\n\\n第二种情况，如果是在数组中间的某个位置新增数据，那么情况就完全不一样了。这是因为，新增了数据之后，会对插入元素位置之后的元素产生影响，具体为这些数据的位置需要依次向后挪动 1 个位置。\\n\\n例如，对于某一个长度为 4 的数组，我们在第 2 个元素之后插入一个元素，那么修改后的数组中就包含了 5 个元素，其中第 1、第 2 个元素不发生变化，第 3 个元素是新来的元素，第 4、第 5 个元素则是原来第 3、第 4 个元素。这一波操作，就需要对一般的数据进行重新搬家。而这个搬家操作，与数组的数据量线性相关，因此时间复杂度是 O(n)\\n\\n数组的删除操作\\n数组删除数据也有两种情况：\\n\\n第一种情况，在这个数组的最后，删除一个数据元素。由于此时删除一条数据后，对原数据没有产生任何影响。我们可以直接删除该数据即可，时间复杂度是 O(1)。\\n\\n第二种情况，在这个数组的中间某个位置，删除一条数据。同样的，这两种情况的区别在于，删除数据之后，其他数据的位置是否发生改变。由于此时的情况和新增操作高度类似，我们就不再举例子了。\\n\\n数组的查找操作\\n相比于复杂度较高的增删操作，数组的查找操作就方便一些了。由于索引的存在，数组基于位置的查找操作比较容易实现。我们可以索引值，直接在 O(1) 时间复杂度内查找到某个位置的元素。\\n\\n例如，查找数组中第三个位置的元素，通过 a[2] 就可以直接取出来。但对于链表系的数据结构，是没有这个优势的。\\n\\n不过，另外一种基于数值的查找方法，数组就没有什么优势了。例如，查找数值为 9 的元素是否出现过，以及如果出现过，索引值是多少。这样基于数值属性的查找操作，也是需要整体遍历一遍数组的。和链表一样，都需要 O(n) 的时间复杂度\\n\\n实际上数组是一种相当简单的数据结构，其增删查的时间复杂度相对于链表来说整体上是更优的。那么链表存在的价值又是什么呢？\\n\\n首先，链表的长度是可变的，数组的长度是固定的，在申请数组的长度时就已经在内存中开辟了若干个空间。如果没有引用 ArrayList 时，数组申请的空间永远是我们在估计了数据的大小后才执行，所以在后期维护中也相当麻烦。\\n\\n其次，链表不会根据有序位置存储，进行插入数据元素时，可以用指针来充分利用内存空间。数组是有序存储的，如果想充分利用内存的空间就只能选择顺序存储，而且需要在不取数据、不删除数据的情况下才能实现\\n\\n数组的案例\\n例题，假设数组存储了 5 个评委对 1 个运动员的打分，且每个评委的打分都不相等。现在需要你：\\n\\n用数组按照连续顺序保存，去掉一个最高分和一个最低分后的 3 个打分样本；\\n\\n计算这 3 个样本的平均分并打印。\\n\\n要求是，不允许再开辟 O(n) 空间复杂度的复杂数据结构。\\n\\n我们先分析一下题目：第一个问题，要输出删除最高分和最低分后的样本，而且要求是不允许再开辟复杂空间。因此，我们只能在原数组中找到最大值和最小值并删除。第二个问题，基于删除后得到的数组，计算平均值。所以解法如下：\\n\\n数组一次遍历，过程中记录下最小值和最大值的索引。对应下面代码的第 7 行到第 16 行。时间复杂度是 O(n)。\\n\\n执行两次基于索引值的删除操作。除非极端情况，否则时间复杂度仍然是 O(n)。对应于下面代码的第 18 行到第 30 行。\\n\\n计算删除数据后的数组元素的平均值。对应于下面代码的第 32 行到第 37 行。时间复杂度是 O(n)。\\n\\n因此，O(n) + O(n) + O(n) 的结果仍然是 O(n)。\\n\\n代码如下：\\n\\n```python\\n\\npublic void getScore() {\\nint a[] = { 2, 1, 4, 5, 3 };\\nmax_inx = -1;\\nmax_val = -1;\\nmin_inx= -1;\\nmin_val = 99;\\nfor (int i = 0; i < a.length; i++) {\\nif (a[i] > max_val) {\\nmax_val = a[i];\\nmax_inx = i;\\n}\\nif (a[i] < min_val) {\\nmin_val = a[i];\\nmin_inx = i;\\n}\\n}\\n```\\n\\n```\\ninx1 = max_inx;\\ninx2 = min_inx;\\nif (max_inx < min_inx){\\n    inx1 = min_inx;\\n    inx2 = max_inx;\\n}\\nfor (int i = inx1; i < a.length-1; i++) {\\n    a[i] = a[i+1];\\n}\\nfor (int i = inx2; i < a.length-1; i++) {\\n    a[i] = a[i+1];\\n}\\nsumscore = 0;\\nfor (int i = 0; i < a.length-2; i++) {\\n    sumscore += a[i];\\n}\\navg = sumscore/3.0;\\nSystem.out.println(avg);\\n}\\n```\\n\\n\\n\\n字符串的新增操作\\n字符串的新增操作和数组非常相似，都牵涉对插入字符串之后字符的挪移操作，所以时间复杂度是 O(n)。\\n\\n例如，在字符串 s1 = \"123456\" 的正中间插入 s2 = \"abc\"，则需要让 s1 中的 \"456\" 向后挪移 3 个字符的位置，再让 s2 的 \"abc\" 插入进来。很显然，挪移的操作时间复杂度是 O(n)。不过，对于特殊的插入操作时间复杂度也可以降低为 O(1)。这就是在 s1 的最后插入 s2，也叫作字符串的连接，最终得到 \"123456abc\"。\\n\\n字符串的删除操作\\n字符串的删除操作和数组同样非常相似，也可能会牵涉删除字符串后字符的挪移操作，所以时间复杂度是 O(n)。\\n\\n例如，在字符串 s1 = \"123456\" 的正中间删除两个字符 \"34\"，则需要删除 \"34\" 并让 s1 中的 \"56\" 向前挪移 2 个字符的位置。很显然，挪移的操作时间复杂度是 O(n)。不过，对于特殊的插入操作时间复杂度也可以降低为 O(1)。这就是在 s1 的最后删除若干个字符，不牵涉任何字符的挪移。\\n\\n字符串的查找操作\\n字符串的查找操作，是反映工程师对字符串理解深度的高频考点，这里需要你格外注意。\\n\\n例如，字符串 s = \"goodgoogle\"，判断字符串 t = \"google\" 在 s 中是否存在。需要注意的是，如果字符串 t 的每个字符都在 s 中出现过，这并不能证明字符串 t 在 s 中出现了。当 t = \"dog\" 时，那么字符 \"d\"、\"o\"、\"g\" 都在 s 中出现过，但他们并不连在一起\\n\\n子串查找（字符串匹配）\\n首先，我们来定义两个概念，主串和模式串。我们在字符串 A 中查找字符串 B，则 A 就是主串，B 就是模式串。我们把主串的长度记为 n，模式串长度记为 m。由于是在主串中查找模式串，因此，主串的长度肯定比模式串长，n>m。因此，字符串匹配算法的时间复杂度就是 n 和 m 的函数。\\n\\n假设要从主串 s = \"goodgoogle\" 中找到 t = \"google\" 子串。根据我们的思考逻辑，则有：\\n\\n首先，我们从主串 s 第 1 位开始，判断 s 的第 1 个字符是否与 t 的第 1 个字符相等。\\n\\n如果不相等，则继续判断主串的第 2 个字符是否与 t 的第1 个字符相等。直到在 s 中找到与 t 第一个字符相等的字符时，然后开始判断它之后的字符是否仍然与 t 的后续字符相等。\\n\\n如果持续相等直到 t 的最后一个字符，则匹配成功。\\n\\n如果发现一个不等的字符，则重新回到前面的步骤中，查找 s 中是否有字符与 t 的第一个字符相等。\\n\\n如下图所示，s 的第1 个字符和 t 的第 1 个字符相等，则开始匹配后续。直到发现前三个字母都匹配成功，但 s 的第 4 个字母匹配失败，则回到主串继续寻找和 t 的第一个字符相等的字符\\n\\n在二叉树中，有下面两个特殊的类型，如下图所示：\\n\\n满二叉树，定义为除了叶子结点外，所有结点都有 2 个子结点。\\n\\n完全二叉树，定义为除了最后一层以外，其他层的结点个数都达到最大，并且最后一层的叶子结点都靠左排列\\n\\n\\n\\n\\n\\n\\n\\n遍历一棵树，有非常经典的三种方法，分别是前序遍历、中序遍历、后序遍历。这里的序指的是父结点的遍历顺序，前序就是先遍历父结点，中序就是中间遍历父结点，后序就是最后遍历父结点。不管哪种遍历，都是通过递归调用完成的。如下图所示：\\n\\n前序遍历，对树中的任意结点来说，先打印这个结点，然后前序遍历它的左子树，最后前序遍历它的右子树。\\n\\n中序遍历，对树中的任意结点来说，先中序遍历它的左子树，然后打印这个结点，最后中序遍历它的右子树。\\n\\n后序遍历，对树中的任意结点来说，先后序遍历它的左子树，然后后序遍历它的右子树，最后打印它本身。\\n\\n\\n\\n不难发现，二叉树遍历过程中，每个结点都被访问了一次，其时间复杂度是 O(n)。接着，在找到位置后，执行增加和删除数据的操作时，我们只需要通过指针建立连接关系就可以了。对于没有任何特殊性质的二叉树而言，抛开遍历的时间复杂度以外，真正执行增加和删除操作的时间复杂度是 O(1)。树数据的查找操作和链表一样，都需要遍历每一个数据去判断，所以时间复杂度是 O(n)。\\n\\n我们上面讲到二叉树的增删查操作很普通，时间复杂度与链表并没有太多差别。但当二叉树具备一些特性的时候，则可以利用这些特性实现时间复杂度的降低。接下来，我们详细介绍二叉查找树的特性。\\n\\n二叉查找树的特性\\n二叉查找树（也称作二叉搜索树）具备以下几个的特性：\\n\\n在二叉查找树中的任意一个结点，其左子树中的每个结点的值，都要小于这个结点的值。\\n\\n在二叉查找树中的任意一个结点，其右子树中每个结点的值，都要大于这个结点的值。\\n\\n在二叉查找树中，会尽可能规避两个结点数值相等的情况。\\n\\n对二叉查找树进行中序遍历，就可以输出一个从小到大的有序数据队列。如下图所示，中序遍历的结果就是 10、13、15、16、20、21、22、26。\\n\\n二叉查找树的查找操作\\n在利用二叉查找树执行查找操作时，我们可以进行以下判断：\\n\\n首先判断根结点是否等于要查找的数据，如果是就返回。\\n\\n如果根结点大于要查找的数据，就在左子树中递归执行查找动作，直到叶子结点。\\n\\n如果根结点小于要查找的数据，就在右子树中递归执行查找动作，直到叶子结点。\\n\\n这样的“二分查找”所消耗的时间复杂度就可以降低为 O(logn)。关于二分查找，我们会在后续的分治法一讲中详细讲述。\\n\\n二叉查找树的插入操作\\n在二叉查找树执行插入操作也很简单。从根结点开始，如果要插入的数据比根结点的数据大，且根结点的右子结点不为空，则在根结点的右子树中继续尝试执行插入操作。直到找到为空的子结点执行插入动作。\\n\\n如下图所示，如果要插入数据 X 的值为 14，则需要判断 X 与根结点的大小关系：\\n\\n由于 14 小于 16，则聚焦在其左子树，继续判断 X 与 13 的关系。\\n\\n由于 14 大于 13，则聚焦在其右子树，继续判断 X 与15 的关系。\\n\\n由于 14 小于 15，则聚焦在其左子树。\\n\\n因为此时左子树为空，则直接通过指针建立 15 结点的左指针指向结点 X 的关系，就完成了插入动作。\\n\\n二叉查找树插入数据的时间复杂度是 O(logn)。但这并不意味着它比普通二叉树要复杂。原因在于这里的时间复杂度更多是消耗在了遍历数据去找到查找位置上，真正执行插入动作的时间复杂度仍然是 O(1)。\\n\\n二叉查找树的删除操作会比较复杂，这是因为删除完某个结点后的树，仍然要满足二叉查找树的性质。我们分为下面三种情况讨论。\\n\\n情况一，如果要删除的结点是某个叶子结点，则直接删除，将其父结点指针指向 null 即可\\n\\n情况二，如果要删除的结点只有一个子结点，只需要将其父结点指向的子结点的指针换成其子结点的指针即可\\n\\n情况三，如果要删除的结点有两个子结点，则有两种可行的操作方式。\\n\\n第一种，找到这个结点的左子树中最大的结点，替换要删除的结点。\\n\\n第二种，找到这个结点的右子树中最小的结点，替换要删除的结点。\\n\\n总结\\n本课时的内容围绕着不同种类树的原理、二叉树对于数据的增删查操作展开。要想利用二叉树实现增删查操作，你需要熟练掌握二叉树的三种遍历方式。遍历的时间复杂度是 O(n)。有了遍历方式之后，你可以完成在指定位置的数据增删操作。增删操作的时间复杂度都是 O(1)。\\n\\n对于查找操作，如果是普通二叉树，则查找的时间复杂度和遍历一样，都是 O(n)。如果是二叉查找树，则可以在 O(logn) 的时间复杂度内完成查找动作。树结构在存在“一对多”的数据关系中，可被高频使用，这也是它区别于链表系列数据结构的关键点\\n\\n对于数据处理它们彼此之间各有千秋，例如：\\n\\n线性表中的栈和队列对增删有严格要求，它们会更关注数据的顺序。\\n\\n数组和字符串需要保持数据类型的统一，并且在基于索引的查找上会更有优势。\\n\\n树的优势则体现在数据的层次结构上。\\n\\n但它们普遍都存在这样的缺陷，那就是数据数值条件的查找，都需要对全部数据或者部分数据进行遍历。那么，有没有一种方法可以省去数据比较的过程，从而进一步提升数值条件查找的效率呢？答案当然是：有。这一课时我们就来介绍这样一种高效率的查找神器，哈希表。\\n\\n什么是哈希表\\n哈希表名字源于 Hash，也可以叫作散列表。哈希表是一种特殊的数据结构，它与数组、链表以及树等我们之前学过的数据结构相比，有很明显的区别。\\n\\n哈希表的核心思想\\n在我们之前学过的数据结构里，数据的存储位置和数据的具体数值之间不存在任何关系。因此，在面对查找问题时，这些数据结构必须采取逐一比较的方法去实现。\\n\\n而哈希表的设计采用了函数映射的思想，将记录的存储位置与记录的关键字关联起来。这样的设计方式，能够快速定位到想要查找的记录，而且不需要与表中存在的记录的关键字比较后再来进行查找。\\n\\n我们回顾一下数组的查找操作。数组是通过数据的索引（index）来取出数值的，例如要找出 a 数组中，索引值为 1 的元素。在前面的课时中，我们讲到索引值是数据存储的位置，因此，直接通过 a[1] 就可以取出这个数据。通过这样的方式，数组实现了“地址 = f (index)”的映射关系。\\n\\n如果用哈希表的逻辑来理解的话，这里的 f () 就是一个哈希函数。它完成了索引值到实际地址的映射，这就让数组可以快速完成基于索引值的查找。然而，数组的局限性在于，它只能基于数据的索引去查找，而不能基于数据的数值去查找。\\n\\n如果有一种方法，可以实现“地址 = f (关键字)”的映射关系，那么就可以快速完成基于数据的数值的查找了。这就是哈希表的核心思想。 下面我们通过一个例子来体会一下。\\n\\n假如，我们要对一个手机通讯录进行存储，并要根据姓名找出一个人的手机号码，如下所示：\\n\\n张一：155555555\\n\\n张二：166666666\\n\\n张三：177777777\\n\\n张四：188888888\\n\\n一个可行的方法是，定义包含姓名、手机号码的结构体，再通过链表把 4 个联系人的信息存起来。当要判断“张四”是否在链表中，或者想要查找到张四的手机号码时，就需要从链表的头结点开始遍历。依次将每个结点中的姓名字段，同“张四”进行比较。直到查找成功或者全部遍历一次为止。显然，这种做法的时间复杂度为 O(n)。\\n\\n如果要降低时间复杂度，就需要借助哈希表的思路，构建姓名到地址的映射函数“地址 = f (姓名)”。这样，我们就可以通过这个函数直接计算出”张四“的存储位置，在 O(1) 时间复杂度内就可以完成数据的查找。\\n\\n通过这个例子，不难看出 Hash 函数设计的好坏会直接影响到对哈希表的操作效率。假如对上面的例子采用的 Hash 函数为，姓名的每个字的拼音开头大写字母的 ASCII 码之和。即：\\n\\naddress (张一) = ASCII (Z) + ASCII (Y) = 90 + 89 = 179；\\n\\naddress (张二) = ASCII (Z) + ASCII (E) = 90 + 69 = 159；\\n\\naddress (张三) = ASCII (Z) + ASCII (S) = 90 + 83 = 173；\\n\\naddress (张四) = ASCII (Z) + ASCII (S) = 90 + 83 = 173；\\n\\n我们发现这个哈希函数存在一个非常致命的问题，那就是 f ( 张三) 和 f (张四) 都是 173。这种现象称作哈希冲突，是需要在设计哈希函数时进行规避的。\\n\\n从本质上来看，哈希冲突只能尽可能减少，不能完全避免。这是因为，输入数据的关键字是个开放集合。只要输入的数据量够多、分布够广，就完全有可能发生冲突的情况。因此，哈希表需要设计合理的哈希函数，并且对冲突有一套处理机制\\n\\n如何设计哈希函数\\n我们先看一些常用的设计哈希函数的方法：\\n\\n第一，直接定制法\\n\\n哈希函数为关键字到地址的线性函数。如，H (key) = a*key + b。\\xa0这里，a 和 b 是设置好的常数。\\n\\n第二，数字分析法\\n\\n假设关键字集合中的每个关键字 key 都是由 s 位数字组成（k1,k2,…,Ks），并从中提取分布均匀的若干位组成哈希地址。上面张一、张二、张三、张四的手机号信息存储，就是使用的这种方法。\\n\\n第三，平方取中法\\n\\n如果关键字的每一位都有某些数字重复出现，并且频率很高，我们就可以先求关键字的平方值，通过平方扩大差异，然后取中间几位作为最终存储地址。\\n\\n第四，折叠法\\n\\n如果关键字的位数很多，可以将关键字分割为几个等长的部分，取它们的叠加和的值（舍去进位）作为哈希地址。\\n\\n第五，除留余数法\\n\\n预先设置一个数 p，然后对关键字进行取余运算。即地址为 key mod p\\n\\n如何解决哈希冲突\\n上面这些常用方法都有可能会出现哈希冲突。那么一旦发生冲突，我们该如何解决呢？\\n\\n常用的方法，有以下两种：\\n\\n第一，开放定址法\\n\\n即当一个关键字和另一个关键字发生冲突时，使用某种探测技术在哈希表中形成一个探测序列，然后沿着这个探测序列依次查找下去。当碰到一个空的单元时，则插入其中。\\n\\n常用的探测方法是线性探测法。 比如有一组关键字 {12，13，25，23}，采用的哈希函数为 key mod 11。当插入 12，13，25 时可以直接插入，地址分别为 1、2、3。而当插入 23 时，哈希地址为 23 mod 11 = 1。然而，地址 1 已经被占用，因此沿着地址 1 依次往下探测，直到探测到地址 4，发现为空，则将 23 插入其中。如下图所示\\n\\n第二，链地址法\\n\\n将哈希地址相同的记录存储在一张线性链表中。\\n\\n例如，有一组关键字 {12,13,25,23,38,84,6,91,34}，采用的哈希函数为 key mod 11。如下图所示\\n\\n哈希表相对于其他数据结构有很多的优势。它可以提供非常快速的插入-删除-查找操作，无论多少数据，插入和删除值需要接近常量的时间。在查找方面，哈希表的速度比树还要快，基本可以瞬间查找到想要的元素。\\n\\n哈希表也有一些不足。哈希表中的数据是没有顺序概念的，所以不能以一种固定的方式（比如从小到大）来遍历其中的元素。在数据处理顺序敏感的问题时，选择哈希表并不是个好的处理方法。同时，哈希表中的 key 是不允许重复的，在重复性非常高的数据中，哈希表也不是个好的选择\\n\\n哈希表的基本操作\\n在很多高级语言中，哈希函数、哈希冲突都已经在底层完成了黑盒化处理，是不需要开发者自己设计的。也就是说，哈希表完成了关键字到地址的映射，可以在常数级时间复杂度内通过关键字查找到数据。\\n\\n至于实现细节，比如用了哪个哈希函数，用了什么冲突处理，甚至某个数据记录的哈希地址是多少，都是不需要开发者关注的。接下来，我们从实际的开发角度，来看一下哈希表对数据的增删查操作。\\n\\n哈希表中的增加和删除数据操作，不涉及增删后对数据的挪移问题（数组需要考虑），因此处理就可以了。\\n\\n哈希表查找的细节过程是：对于给定的 key，通过哈希函数计算哈希地址 H (key)。\\n\\n如果哈希地址对应的值为空，则查找不成功。\\n\\n反之，则查找成功。\\n\\n虽然哈希表查找的细节过程还比较麻烦，但因为一些高级语言的黑盒化处理，开发者并不需要实际去开发底层代码，只要调用相关的函数就可以了\\n\\n哈希表的案例\\n下面我们来讲解两个案例，帮助你进一步理解哈希表的操作过程。\\n\\n例 1，将关键字序列 {7, 8, 30, 11, 18, 9, 14} 存储到哈希表中。哈希函数为： H (key) = (key * 3) % 7，处理冲突采用线性探测法。\\n\\n接下来，我们分析一下建立哈希表和查找关键字的细节过程。\\n\\n首先，我们尝试建立哈希表，求出这个哈希地址：\\n\\nH (7) = (7 * 3) % 7 = 0\\n\\nH (8) = (8 * 3) % 7 = 3\\n\\nH (30) = 6\\n\\nH (11) = 5\\n\\nH (18) = 5\\n\\nH (9) = 6\\n\\nH (14) = 0\\n\\n按关键字序列顺序依次向哈希表中填入，发生冲突后按照“线性探测”探测到第一个空位置填入\\n\\n\\n\\n接着，有了这个表之后，我们再来看一下查找的流程：\\n\\n查找 7。输入 7，计算得到 H (7) = 0，根据哈希表，在 0 的位置，得到结果为 7，跟待匹配的关键字一样，则完成查找。\\n\\n查找 18。输入 18，计算得到 H (18) = 5，根据哈希表，在 5 的位置，得到结果为 11，跟待匹配的关键字不一样（11 不等于 18）。因此，往后挪移一位，在 6 的位置，得到结果为 30，跟待匹配的关键字不一样（11 不等于 30）。因此，继续往后挪移一位，在 7 的位置，得到结果为 18，跟待匹配的关键字一样，完成查找\\n\\n例 2，假设有一个在线系统，可以实时接收用户提交的字符串型关键字，并实时返回给用户累积至今这个关键字被提交的次数。\\n\\n例如，用户输入\"abc\"，系统返回 1。用户再输入\"jk\"，系统返回 1。用户再输入\"xyz\"，系统返回 1。用户再输入\"abc\"，系统返回 2。用户再输入\"abc\"，系统返回 3。\\n\\n一种解决方法是，用一个数组保存用户提交过的所有关键字。当接收到一个新的关键字后，插入到数组中，并且统计这个关键字出现的次数。\\n\\n根据数组的知识可以计算出，插入到最后的动作，时间复杂度是 O(1)。但统计出现次数必须要全部数据遍历一遍，时间复杂度是 O(n)。随着数据越来越多，这个在线系统的处理时间将会越来越长。显然，这不是一个好的方法。\\n\\n如果采用哈希表，则可以利用哈希表新增、查找的常数级时间复杂度，在 O(1) 时间复杂度内完成响应。预先定义好哈希表后（可以采用 Map  d = new HashMap  (); ）对于关键字（用变量 key_str 保存），判断 d 中是否存在 key_str 的记录。\\n\\n如果存在，则把它对应的value（用来记录出现的频次）加 1；\\n\\n如果不存在，则把它添加到 d 中，对应的 value 赋值为 1。最后，打印处 key_str 对应的 value，即累积出现的频次。\\n\\n代码如下：\\n\\n复制代码\\nif (d.containsKey(key_str) {\\nd.put(key_str, d.get(key_str) + 1);\\n}\\nelse{\\nd.put(key_str, 1);\\n}\\nSystem.out.println(d.get(key_str));\\n\\n哈希表在我们平时的数据处理操作中有着很多独特的优点，不论哈希表中有多少数据，查找、插入、删除只需要接近常量的时间，即 O(1）的时间级。\\n\\n实际上，这只需要几条机器指令。哈希表运算得非常快，在计算机程序中，如果需要在一秒钟内查找上千条记录通常使用哈希表（例如拼写检查器)，哈希表的速度明显比树快，树的操作通常需要 O(n) 的时间级。哈希表不仅速度快，编程实现也相对容易。如果不需要有序遍历数据，并且可以提前预测数据量的大小。那么哈希表在速度和易用性方面是无与伦比的\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4589e286-8d60-4094-9d39-044f824051de', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nDynamic programming 3\\n\\n!Untitled\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ee281d3b-11fc-46cd-8e8d-41609eff4653', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**滚动数组优化**\\n\\n爬楼梯我们并没有必要使用一维数组，而是借助两个变量来实现的，空间复杂度是 O(1)。代码：\\n\\n`function climbStairs(n) {\\n  if (n === 1) return 1;\\n  if (n === 2) return 2;\\n\\n  let a = 1;\\n  let b = 2;\\n  let temp;\\n\\n  for (let i = 3; i <= n; i++) {\\n    temp = a + b;\\n    a = b;\\n    b = temp;\\n  }\\n\\n  return temp;\\n}`\\n\\n之所以能这么做，是因为爬楼梯问题的状态转移方程中**当前状态只和前两个状态有关**，因此只需要存储这两个即可。 动态规划问题有很多这种讨巧的方式，这个技巧叫做滚动数组\\n\\n回答上面的问题：记忆化递归和动态规划除了一个用递归一个用迭代，其他没差别。那两者有啥区别呢？我觉得最大的区别就是记忆化递归无法使用滚动数组优化。\\n\\n不信你用上面的爬楼梯试一下，下面代码如何使用滚动数组优化？\\n\\n`const memo = {};\\nfunction dp(n) {\\n  if (n === 1) return 1;\\n  if (n === 2) return 2;\\n  if (n in memo) return memo[n];\\n  const ans = dp(n - 1) + dp(n - 2);\\n  memo[n] = ans;\\n  return ans;\\n}`\\n\\n本质上来说， 记忆化递归采用的方式是 DFS，因此会一条路走到黑，然后返回来继续其他可行的路一口气再次走到黑。而迭代使用的是类似 BFS 的方式，这样一层层访问， � 太远的层可能用不到了，就可以直接抹去，这就是滚动数组的本质。因此我的建议就是没空间优化需求直接就记忆化，否则用迭代 dp\\n\\n再次强调一下：\\n\\n- 如果说递归是从问题的结果倒推，直到问题的规模缩小到寻常。 那么动态规划就是从寻常入手， 逐步扩大规模到最优子结构。\\n- 记忆化递归和动态规划没有本质不同。都是枚举状态，并根据状态直接的联系逐步推导求解。\\n- 动态规划性能通常更好。 一方面是递归的栈开销，一方面是滚动数组的技巧\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e256f1cc-80c9-4c72-ab51-09c1c7d652b2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**动态规划的基本概念**\\n\\n我们先来学习动态规划最重要的两个概念：最优子结构和无后效性。\\n\\n其中：\\n\\n- 无后效性决定了是否可使用动态规划来解决。\\n- 最优子结构决定了具体如何解决。\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9a0cd085-7acb-422f-a2d4-bd0ac39faf3b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**动态规划三要素**\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f451380b-f6da-4841-9d49-69167fe7b000', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**状态定义**\\n\\n动态规划的中心点是什么？如果让我说的话，那就是**定义状态**。\\n\\n动态规划解题的第一步就是定义状态。定义好了状态，就可以画出递归树，聚焦最优子结构写转移方程就好了，因此我才说状态定义是动态规划的核心，动态规划问题的状态确实不容易看出。\\n\\n但是一旦你能把状态定义好了，那就可以顺藤摸瓜画出递归树，画出递归树之后就聚焦最优子结构就行了。但是能够画出递归树的前提是：对问题进行划分，专业点来说就是定义状态。那怎么才能定义出状态呢？\\n\\n好在状态的定义都有特点的套路。 比如一个字符串的状态，通常是 dp[i] 表示字符串 s 以 i 结尾的 ....。 比如两个字符串的状态，通常是 dp[i][j] 表示字符串 s1 以 i 结尾，s2 以 j 结尾的 ....。也就是说状态的定义通常有不同的套路，大家可以在做题的过程中进行学习和总结\\n\\n总的来说，动态规划的空间和时间复杂度**打底就是状态的个数**，而状态的个数通常是参数的笛卡尔积，这是由动态规划的无后向性决定的。\\n\\n**临界条件是比较最容易的**\\n\\n当你定义好了状态，剩下就三件事了：\\n\\n1. 临界条件\\n2. 状态转移方程\\n3. 枚举状态\\n\\n在上面讲解的爬楼梯问题中，如果我们用 f(n) 表示爬 n 级台阶有多少种方法的话，那么：\\n\\n`f(1) 与 f(2) 就是【边界】\\nf(n) = f(n-1) + f(n-2) 就是【状态转移公式】`\\n\\n我用动态规划的形式表示一下：\\n\\n`dp[0] 与 dp[1] 就是【边界】\\ndp[n] = dp[n - 1] + dp[n - 2] 就是【状态转移方程】`\\n\\n可以看出记忆化递归和动态规划是多么的相似。\\n\\n实际上临界条件相对简单，大家只有多刷几道题，里面就有感觉。困难的是找到状态转移方程和枚举状态。这两个核心点的都建立在**已经抽象好了状态**的基础上。比如爬楼梯的问题，如果我们用 f(n) 表示爬 n 级台阶有多少种方法的话，那么 f(1), f(2), ... 就是各个**独立的状态**。\\n\\n搞定了状态的定义，那么我们来看下状态转移方程。\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8144f111-4162-43db-ae87-bce172671b77', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**状态转移方程**\\n\\n动态规划中当前阶段的状态往往是上一阶段状态和上一阶段决策的结果。这里有两个关键字，分别是 ：\\n\\n- 上一阶段状态\\n- 上一阶段决策\\n\\n也就是说，如果给定了第 k 阶段的状态 s[k] 以及决策 choice(s[k])，则第 k+1 阶段的状态 s[k+1] 也就完全确定，用公式表示就是：s[k] + choice(s[k]) -> s[k+1]， 这就是状态转移方程。需要注意的是 choice 可能有多个，因此每个阶段的状态 s[k+1]也会有多个。\\n\\n继续以上面的爬楼梯问题来说，爬楼梯问题由于上第 n 级台阶一定是从 n - 1 或者 n - 2 来的，因此 上第 n 级台阶的数目就是\\xa0`上 n - 1 级台阶的数目加上 n - 2 级台阶的数目`。\\n\\n上面的这个理解是核心， 它就是我们的状态转移方程，用代码表示就是\\xa0`f(n) = f(n - 1) + f(n - 2)`。\\n\\n实际操作的过程，有可能题目和爬楼梯一样直观，我们不难想到。也可能隐藏很深或者维度过高。 如果你实在想不到，可以尝试画图打开思路，这也是我刚学习动态规划时候的方法。当你做题量上去了，你的题感就会来，那个时候就可以不用画图了。\\n\\n比如我们定义了状态方程，据此我们定义初始状态和目标状态。然后聚焦最优子结构，**思考每一个状态究竟如何进行扩展使得离目标状态越来越近**。\\n\\n本质上来说， 记忆化递归采用的方式是 DFS，因此会一条路走到黑，然后返回来继续其他可行的路一口气再次走到黑。而迭代使用的是类似 BFS 的方式，这样一层层访问， � 太远的层可能用不到了，就可以直接抹去，这就是滚动数组的本质。\\n\\n记忆化调用栈的开销比较大（复杂度不变，你可以认为空间复杂度常数项更大），不过几乎不至于 TLE 或者 MLE。**因此我的建议就是没空间优化需求直接就记忆化，否则用迭代 dp**。\\n\\n再次强调一下：\\n\\n- 如果说递归是从问题的结果倒推，直到问题的规模缩小到寻常。 那么动态规划就是从寻常入手， 逐步扩大规模到最优子结构。\\n- 记忆化递归和动态规划没有本质不同。都是枚举状态，并根据状态直接的联系逐步推导求解。\\n- 动态规划性能通常更好。 一方面是递归的栈开销，一方面是滚动数组的技巧\\n\\n动态规划往往用来处理最优解问题，而回溯法往往用来计算所有的可能组合\\n\\n路志鹏 等. 算法通关之路 (Chinese Edition) (Kindle Locations 2726-2727). Kindle Edition.\\n\\n回溯法的优化点往往在于剪枝，可以避免走进根本不可能为结果的分支\\n\\n路志鹏 等. 算法通关之路 (Chinese Edition) (Kindle Locations 2735-2736). Kindle Edition.\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c0280c66-83ae-484b-8ff9-376061c7dacb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nsliding window\\n\\n\\n\\n如果按每三个三个的加和的话，就是n * k的复杂度，注意这样的话，2+3， 3+4这类的计算都不止一次，所以有优化的空间\\n\\nsliding window的思路：两个指针先放在首位，sum = 2, maxsum = 2, k=1<3，所以把右指针往右移动到3，这时sum=2+3 = 5, maxsum = 5, k=2<3, 继续移动右指针，sum=2+3+4 = 9, maxsum = 9；之后同时移动左右两个指针，sum需要加右边的新数，剪掉左边移走的数\\n\\n这里注意enumerate 里面，end, val都是有特殊含义的记号可以直接用\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d19154da-6b2f-4124-be0d-5b48b50e90a9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n链表 linked list\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='32d18e62-6bcc-4440-82cf-16090f9300fc', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n链表的概念\\n\\n我们知道数组是很常用的数据储存方式，而链表就是继数组之后，第二种最通用的数据储存方式了。数组需要存放在连续的空间，计算机很容易实现。而链表的好处是不用确定空间长度，不够的时候，直接申请新的节点，帮助插入。所以链表可以更灵活地进行内存分配。\\n\\n链表（linked list）是一种序列形的数据结构，其中包含了很多通过链接 (link) 被串起来的节点。每个节点有一个数据域，储存着节点的数值，还有一个指针域，指向下一个节点。\\n\\n!https://i1.wp.com/www.tutorialspoint.com/data_structures_algorithms/images/linked_list.jpg?w=1170&ssl=1\\n\\n每个节点包含两个属性，一个是数值，记录了节点的数据，另一个是指针，记录了下一个节点的位置。正因为节点的指针能够记录其他节点的位置，所以我们不需要将节点按顺序排列\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n只需执行简单的数学运算就知道：04。需要随机地读取元素时，数组的\\n效率很高，因为可迅速找到数组的任何元素。在链表中，元素并非靠在\\n一起的，你无法迅速计算出第五个元素的内存地址，而必须先访问第一\\n个元素以获取第二个元素的地址，再访问第二个元素以获取第三个元素\\n的地址，以此类推，直到访问第五个元素\\n\\n需要随机地读取元素时，数组的\\n效率很高，因为可迅速找到数组的任何元素。**在链表中，元素并非靠在\\n一起的，你无法迅速计算出第五个元素的内存地址，而必须先访问第一\\n个元素以获取第二个元素的地址**，再访问第二个元素以获取第三个元素\\n的地址，以此类推，直到访问第五个元素\\n\\n\\n\\n\\n\\nlinked list相关的题目，大概有一半可以用two pointer解决\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n这里关键是交换指向，不止是交换值\\n\\n\\n\\n\\n\\n链表的题目 90% 的 bug 都出现在：\\n\\n1. 头尾节点的处理\\n2. 指针循环引用导致死循环\\n\\n因此大家对这两个问题要保持 100% 的警惕。\\n\\n\\n\\n**做链表反转的时候要小心死循环**\\n\\n\\n\\n\\n\\n```python\\nclass Solution:\\n    # 翻转一个子链表，并且返回新的头与尾\\n    def reverse(self, head: ListNode, tail: ListNode, terminal:ListNode):\\n        cur = head\\n        pre = None\\n        while cur != terminal:\\n            # 留下联系方式\\n            next = cur.next\\n            # 修改指针\\n            cur.next = pre\\n\\n            # 继续往下走\\n            pre = cur\\n            cur = next\\n         # 反转后的新的头尾节点返回出去\\n        return tail, head\\n```\\n\\n**链表最容易出错的地方就是我们应该注意的地方。链表最容易出的错 90 % 集中在以下三种情况：**\\n\\n- **出现了环，造成死循环。**\\n- **分不清边界，导致边界条件出错。**\\n- **搞不懂递归怎么做**\\n\\n相信做过链表的小伙伴都听过这么个名字。为什么它这么好用？它的作用无非就两个：\\n\\n- 将头节点变成中间节点，简化判断。\\n- 通过在合适的时候断开链接，返回链表的中间节点。\\n\\n我上面提到了链表的三个注意，有一个是边界。头节点是最常见的边界，那如果**「我们用一个虚拟头指向头节点，虚拟头就是新的头节点了，而虚拟头不是题目给的节点，不参与运算，因此不需要特殊判断」**，虚拟头就是这个作用\\n\\n如果题目需要返回链表中间的某个节点呢？实际上也可借助虚拟节点。由于我上面提到的指针的操作，实际上，你可以新建一个虚拟头，然后让虚拟头在恰当的时候（刚好指向需要返回的节点）断开连接，这样我们就可以返回虚拟头的 next 就 ok 了。25. K 个一组翻转链表 就用到了这个技巧\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n这里ans和head的链接没有切断\\n\\n\\n\\n\\n\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c86eee85-e290-4113-a0b8-ffafa80f9a31', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n哈希表 code\\n\\n```python\\n#make 哈希表 code\\nfor index, num in enumerate([3, 2, 4]):\\n    try:\\n        hash_nums[num].append(index)\\n    except KeyError:\\n        hash_nums[num] = [index]\\n\\nhash_nums\\n```\\n\\n哈希表的本质是结合了数组和链表\\n\\n\\n\\n输入值X进到哈希函数后算出Y，取余，就得到了X的position\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n填装因子越低，发生 冲突的可能性越小，散列表的性能越高。一个不错的经验规则是：一旦 填装因子大于0.7，就调整散列表的长度', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='352a8702-e9e0-481a-bc99-ddeda3ecfba9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n算法课 Algorithmic Toolbox1\\n\\n\\n\\n对于计算时间的估计关键是看input 进去之后，output怎样scale\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n就是用最高的那个指数取代整个公式\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n贪心算法greedy algor\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3576201e-4cc5-4c41-8dd8-3ebcb7daccab', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nbig O notation time and space complexity\\n\\n!Untitled\\n\\nstack is FILO, queue is FIFO\\n\\n!Untitled\\n\\nlogn 有三种情况，binary search on array, binary search on BST, heap push and pop\\n\\nnlogn 一般只出现在sort, most built in sorting function is nlogn\\n\\nheap sort also is nlogn \\n\\n2^n 这种complexity，一般只出现在recurring的情况，比如高度为n的二分树，每一级有两个选择\\n\\n!Untitled\\n\\nn! 一般只出现在permutation problem; travelling salesman problem; \\n\\nL2. Problems on Recursion - YouTube\\n\\nspace complexity is essentially stack space size\\n\\nspace complexity是要考虑这个stack space的。在这个简单的print name N times recurssion里，stack里头是会放N个函数的，so space complex is On 因为一环套一环，直到最后一个函数return，前面的函数才能return清空前面的所有函数\\n\\n!Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ea003e1c-ff08-4aee-b1cb-54d521059bac', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n计算机科学基础\\n\\n学python的时候很多问题都不理解，就像浅拷贝和深拷贝，但学完c语言一下就明白了原理。假如有一个数组B，现在要进行操作a=B，如果是浅拷贝的话，就是把B的地址丢给a；而深拷贝的话，则是重新划分一块内存给a，把B中的元素丢到内存里去\\n\\n真实世界里哪些问题可以投射为指令，哪些可以投射为数据\\n\\n物理学是把物理世界投射到数学中去解决问题，解决办法是给数字加上量纲，比如1摄氏度和1米，物理学有七类基本量纲\\n\\n数字和数字之间只有量的区别没有类型区别。需要给数字定义类型；某一个数字的类型是由可以对它操作的所有运算的总和所决定的；（人是社会关系的总和）\\n\\n决定这个数字100的，是他的运算关系，他可能是数字或者str；当你看到他可以进行加法运算，才知道他是数字\\n\\n之所以要把数据分为float, int这些不同类型，是因为他们在计算机里存储和实现是不一样的\\n\\n!Untitled\\n\\n创建一个新的数据类型本质上是规定，内存里这个数据，只能进行某些特定的操作，比如返回id, 返回名字\\n\\n!Untitled\\n\\n函数本质是对指定打个包；\\n\\n把问题拆解为数据+指令，本质就是建模\\n\\n数据的本质是对不同的抽象；1和100，你只知道他们是不同的，除此之外不知道其他，连他们大小也不知道；\\n\\n所以哪些东西可以投射成数据？任何不同的地方，不同的东西\\n\\n投射应该怎么操作？如果大人投射到 100， 少年200， 婴儿150，这个逻辑就是乱的，但下面的逻辑是对的。图灵机一定能找到一种完备的投射方式找到真实的逻辑\\n\\n!Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b748473f-965d-4768-81ae-350d21d7c3a2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nDynamic programming 2\\n\\nhttps://www.bilibili.com/video/BV1gf4y1i78H\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n每一轮里有四种状态，取这四种状态的最大值\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='950b06b3-8631-4c45-8d79-a1147658d7a6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n排序算法\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n插入排序\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4cb0fdba-6ef3-469f-b222-a37f0f139939', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\ntwo pointer 双指针\\n\\n\\n\\ni 是下一个可填充的位置\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n这里定义一个left pointer, 一个right pointer，这些指针其实是position index\\n\\n\\n\\n\\n\\n第一个while是找到right 指针的位置，放在第一个正数上，然后left指针在right针左边\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='05d59910-141e-41c9-ab13-3ad42a908fff', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nLC DFS on trees 自己的笔记\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d7572347-43ae-4a59-88b9-71bc88a4fc9e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n两种最常见题型\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='60395f85-cf33-400a-a5fe-a24b2b884077', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**算法模板**\\n\\n一个典型的通用的 DFS 模板可能是这样的：\\n\\n```\\n**#因此一个树的 DFS 更多是：**\\n\\nfunction dfs(root) {\\n if (满足特定条件）{\\n  // 返回结果 or 退出搜索空间\\n }\\n for (const child of root.children) {\\n        dfs(child)\\n }\\n}\\n\\n**#而几乎所有的题目几乎都是二叉树，因此下面这个模板更常见。**\\n\\nfunction dfs(root) {\\n if (满足特定条件）{\\n  // 返回结果 or 退出搜索空间\\n }\\n    dfs(root.left)\\n    dfs(root.right)\\n}\\n```\\n\\n**两种常见分类**\\n\\n前序遍历和后序遍历是最常见的两种 DFS 方式。而另外一种遍历方式 （中序遍历）一般用于平衡二叉树\\n\\n**前序遍历**\\n\\n如果你的代码大概是这么写的（注意主要逻辑的位置）：\\n\\n```\\nfunction\\xa0dfs(root)\\xa0{\\n\\xa0if\\xa0(满足特定条件）{\\n//\\xa0返回结果\\xa0or\\xa0退出搜索空间\\n\\xa0\\xa0\\xa0\\xa0}\\n//\\xa0主要逻辑\\n\\xa0\\xa0\\xa0\\xa0dfs(root.left)\\n\\xa0\\xa0\\xa0\\xa0dfs(root.right)\\n}\\n```\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d5e59b00-b51e-4878-bec4-09aa05d16d4c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**后续遍历**\\n\\n而如果你的代码大概是这么写的（注意主要逻辑的位置）：\\n\\n```\\nfunction\\xa0dfs(root)\\xa0{\\n\\xa0if\\xa0(满足特定条件）{\\n//\\xa0返回结果\\xa0or\\xa0退出搜索空间\\n\\xa0\\xa0\\xa0\\xa0}\\n\\xa0\\xa0\\xa0\\xa0dfs(root.left)\\n\\xa0\\xa0\\xa0\\xa0dfs(root.right)\\n//\\xa0主要逻辑\\n}\\n\\n```\\n\\n那么此时我们称为后序遍历\\n\\n值得注意的是， 我们有时也会会写出这样的代码：\\n\\n```\\nfunction\\xa0dfs(root)\\xa0{\\n\\xa0if\\xa0(满足特定条件）{\\n//\\xa0返回结果\\xa0or\\xa0退出搜索空间\\n\\xa0\\xa0\\xa0\\xa0}\\n//\\xa0做一些事\\n\\xa0\\xa0\\xa0\\xa0dfs(root.left)\\n\\xa0\\xa0\\xa0\\xa0dfs(root.right)\\n//\\xa0做另外的事\\n}\\n\\n```\\n\\n如上代码，我们在进入和退出左右子树的时候分别执行了一些代码。那么这个时候，是前序遍历还是后续遍历呢？实际上，这属于混合遍历了。不过我们这里只考虑**「主逻辑」**的位置，关键词是**「主逻辑」**。\\n\\n如果代码主逻辑在左右子树之前执行，那么就是前序遍历。如果代码主逻辑在左右子树之后执行，那么就是后序遍历\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='408bfb0f-3005-4cdf-a5a6-323b0fc0f458', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nIteration 遍历模板\\n\\n我们知道垃圾回收算法中，有一种算法叫三色标记法。即：\\n\\n- 用白色表示尚未访问\\n- 灰色表示尚未完全访问子节点\\n- 黑色表示子节点全部访问\\n\\n那么我们可以模仿其思想，使用双色标记法来统一三种遍历。\\n\\n其核心思想如下：\\n\\n- 使用颜色标记节点的状态，新节点为白色，已访问的节点为灰色。\\n- 如果遇到的节点为白色，则将其标记为灰色，然后将其右子节点、自身、左子节点依次入栈。\\n- 如果遇到的节点为灰色，则将节点的值输出。\\n\\n使用这种方法实现的中序遍历如下\\n\\n```\\nclass Solution:\\n    def inorderTraversal(self, root: TreeNode) -> List[int]:\\n        WHITE, GRAY = 0, 1\\n        res = []\\n        stack = [(WHITE, root)]\\n        while stack:\\n            color, node = stack.pop()\\n            if node is None: continue\\n            if color == WHITE:\\n                stack.append((WHITE, node.right)) #先append right，最后弹出right\\n                stack.append((GRAY, node))\\n                stack.append((WHITE, node.left))\\n            else:\\n                res.append(node.val)  #VIPVIP \\n        return res\\n```\\n\\n可以看出，实现上 WHITE 就表示的是递归中的第一次进入过程，Gray 则表示递归中的从叶子节点返回的过程。因此这种迭代的写法更接近递归写法的本质。\\n如要**「实现前序、后序遍历，也只需要调整左右子节点的入栈顺序即可，其他部分是无需做任何变化」**\\n\\n\\n\\n上面提到了树的遍历有两种基本方式，分别是**「深度优先遍历（以下简称 DFS）和广度优先遍历（以下简称 BFS），这就是两个基本点」**。这两种遍历方式下面又会细分几种方式。比如\\xa0**「DFS 细分为前中后序遍历， BFS 细分为带层的和不带层的」**。\\n**「DFS 适合做一些暴力枚举的题目，DFS 如果借助函数调用栈，则可以轻松地使用递归来实现。」**\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8733cb6c-1931-469b-abac-d5fb3fb1d5e7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n理解stack dfs的写法\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='66e0c3c0-c21e-4535-831d-92929705c122', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n112 path sum\\n\\n!Untitled\\n\\n```python\\n#DFS的非递归实现，用栈实现；这里是用postorder的顺序，因为root最先压入stack，最后出\\nclass Solution(object):\\n    def hasPathSum(self, root, sum):\\n        \"\"\"\\n        :type root: TreeNode\\n        :type sum: int\\n        :rtype: bool\\n        \"\"\"\\n        stack = [(root, sum)]\\n        while len(stack) > 0:\\n            node, tmp_sum = stack.pop()\\n            if node:\\n                if not node.left and not node.right and node.val == tmp_sum:\\n                    return True\\n                stack.append((node.right, tmp_sum-node.val))\\n                stack.append((node.left, tmp_sum-node.val))\\n        return False\\n```\\n\\n!Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2dde2a72-ad58-46e2-85fd-29e625a5a28c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n图 graph\\n\\n\\n\\n顶点和连接线的集合\\n\\n\\n\\n\\n\\n（1,0） 和(0,1) 都是1，说明这两个点是互连的\\n\\n\\n\\n举例：从1出发，有到0，到4，到2和3的。链表的好处是不用记录没有连线的数据\\n\\n\\n\\n\\n\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f723b15b-05bf-4642-a1b9-9a3bf8e0a51a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nTUF 笔记 trees\\n\\ninterative 的写法也要掌握，是通过stack的方式实现。要注意左右的顺序\\n\\nL9. Iterative Preorder Traversal in Binary Tree | C++ | Java | Stack - YouTube', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f08b36f0-2cd2-40d4-b595-b64bb8933981', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nhttps://www.youtube.com/watch?v=69ZCDFy-OUo&list=PLgUwDviBIf0rGlzIn_7rsaR2FQ5e6ZOL9&index=3\\n\\nL3. Parameterised and Functional Recursion - YouTube\\n\\n\\nhttps://leetcode.com/discuss/study-guide/662866/DP-for-Beginners-Problems-or-Patterns-or-Sample-Solutions\\n\\n(37) DP for Beginners [Problems | Patterns | Sample Solutions] - LeetCode Discuss\\n\\nhttps://codeforces.com/blog/entry/43256\\n\\nEverything About Dynamic Programming - Codeforces\\n\\n\\n\\n\\n看完solution之后自己默写一遍\\n\\n  \\n\\n1 I went straight to the video solution and watched it in full. If my concentration lapsed even for 5 seconds, I rewinded the video. Sometimes it would take me 30 minutes to get through a 10 minute solution. Again, your focus in these moments is insanely important for building up a fundamental understanding of the solution.\\n\\n2 I checked the comments. Sometimes the comments had cleaner solutions that made more sense. Sometimes his time/space complexity explanation is wrong. I tried to make sure I had a good approach I liked with the right time/space complexity before moving on.\\n\\n3 I coded up the solution. A lot of the time it was identical to NeetCode's solution, but sometimes it was slightly different for reason #2, but also because being consistent across your approaches is important. Like for a backtracking problem he might approach his base case slightly different from one video to another. Try to look at your other backtracking solutions and maintain consistency with your approaches. This is important in the learning process and will also be crucial in recalling later if you're stuck but you remember your general technique for a certain pattern.\\n\\n4 I ran through the problem from start to finish on my whiteboard. A piece of paper or drawing application would work just as well, I just wanted to mimic an interview environment. As for how I ran through the problem: #1: Read through the problem description. #2: Explain to myself the general approach I will use (and explain an obvious brute-force solution if I remember to do so). Also think about edge cases. #3: Explain the time/space complexity implications from the brute-force solution to the better solution I will be coding up. #4: Run through the example(s) LeetCode provides in the description. If it involves recursion like backtracking I'd typically draw one branch of the recursion tree, but if it wasn't that big of a tree I'd draw the whole thing. #5: Code up the solution. This is where I would often be hung up in how to approach certain parts of the algorithm. I would think hard for maybe 30 seconds and then check my coded solution on my computer to get unstuck. These times of uncertainty in writing the solution are very important in identifying the areas you don't understand well. Muscle memory with a keyboard might assist you more than you think when coding up a solution, so writing it by hand at this stage is important.\\n\\n5 Weeks or a month later if I couldn't visualize an approach from start to finish in my head (just general strategy, not specific code) I would revisit the problem by doing step 4 again. Solution approaches stick in your head longer with the more problems you do due to many factors that are beyond my knowledge. Just know that all of this gets easier with time and practice.\\n\\n\\n\\n\\n\\nHello everyone. Today I want to write a post about a program for learning data structures and algorithms. I heard this question about studying a program a lot and wanted to create a styling program for beginners and intermediate levels. Also, I saw many mistakes which my friends, and coders and I made and by analyzing all of such mistakes I wanted to write and construct the correct program. In the beginning, I created this program for myself, I tested it and it helped me to learn data structures and algorithms just within 6 months. Within these 6 months I learned and mastered the following topics:\\n\\n1. Linked List\\n2. Stack\\n3. Queue\\n4. Binary Search Tree\\n5. Hash Table and Map\\n6. Sorting algorithms\\n7. Searching algorithms\\n8. Greedy Algorithms\\n9. Dynamic Programming basics\\n10. Graph BFS and DFS\\n\\nI am now continuing to learn by the program which I created and it helps me to learn topics faster, efficiently and on a long-term basis. I can solve the problems which I solved and similar problems in a fast way. So let's dive into the program which I created for myself and you.\\n\\nFirst of all, let me analyze all the common mistakes which coders continue to make.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='52e1853d-f5a8-456b-a20c-6c7ee9fc9627', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**1. UNDERESTIMATE TIME COMPLEXITY FINDING**\\n\\nI always see the coders who underestimate and do not study time complexities. What do you think will be the result of not knowing the time complexities? Yes, the probability that you will have Time Limit Exceeded is very high. I strongly advise firstly to learn time complexities. Before even writing your code you should be able to analyze its time complexity.You may ask how? For example, if you are going to use for the inside of other for then the time complexity of your program will be O(n^2), if you are going to use map inside for then your time complexity will probably be O(nlogn), if you are going to use bubble sort then your time complexity will be O(n^2).\\n\\n**Tips before writing a code:**\\n\\n1. Look for the input range. Is it small or too big? Calculate in such a way: 10^9 operations will take around 1 second so try to make the number of operations which your code will run under 10^9.\\n2. Try to think of all possible data structures you can apply and analyze time complexity. If you have an option between using for inside other for and hash table inside one for then the time complexity of the first solution will be O(n^2) and of the second option will be O(n). It is obvious that it is better to use a hash table to make less time complexity.\\n3. Have strong knowledge of time complexities.\\n\\n**Resources for learning time complexities:**\\n\\n1. MIT 6.001 course, Understanding Program Efficiency Part 1 and Part 2 on YouTube\\n2. MIT 6.006 course on YouTube\\n3. Abdul Bari Algorithms on YouTube\\n\\nAll of these resources helped me to master time complexity finding. They show deep analysis in finding different time complexities.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c50d7d31-ecd2-4501-9ea0-31ad53f67c46', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n**2. DO NOT SPEND TIME FOR LEARNING THEORY**\\n\\nYou always should have a knowledge and understanding of the data structure or algorithm which you are going to use. You should know the working principle of different data structures and algorithms. For example in which cases it is better to use insertion sort rather than quicksort, how stack and queue work, or when it is useful to apply hash table for solving problems. For example, a stack can be used for counting parentheses and reversing a string, hash table for fast find the value by key, for fast access in other words, etc. I promise you to deeply learn theory but do not spend too much time on it.\\n\\n**Best resources for learning theory:**\\n\\n1. MIT Algorithms on YouTube\\n2. Jenny's Lectures on YouTube\\n3. Abdul Bari Algorithms on YouTube\\n4. Mycodeschool on YouTube\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d746eba6-9765-4514-b657-b52aca10694e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n**3. NOT REVISING AND DOING A LOT OF PROCRASTINATION**\\n\\nI always heard that people cannot or spend too much time for solving problems which they solved already. Some are not sure that they can solve the problem which they already solved. That is a huge problem and I think that the possible reasons are:\\n\\n1. Not revising\\n2. Not enough learning a theory\\n3. Procrastinating by studying one day and having a rest for several days.\\n\\nI think that programming is like a sport. For example, to succeed in the sport you will always need to train, repeat your training program, again and again, add some new training methods and exercises, and never give up. The main thing is discipline. You always should revise the topics by solving several problems for them to not forget them and master them. You should apply the knowledge and techniques which you learned by studying the theory immediately to problem-solving. Learn theory and immediately go to practice. Think that theory is your personal trainer and you are the athlete who is going to practice that again and again while you do not master it.\\n\\n**Tips for learning for the long term:**\\n\\n1. Revising all topics, even two times a week\\n2. Solve some problems and revise your solved problems every week\\n3. Write contests\\n4. Learn theory and have a strong foundation\\n5. Do something each day\\n\\nYou may ask why I mentioned writing contests? The answer is that competition will give you additional motivation for learning. You will see your level and will want to have a better rating and will you will begin training by studying and practising. Also try to solve at least one problem each day for 21 days, after 21 days you will automatically have a habit to go and solve problems. Do not procrastinate. NEVER DO THAT! If you will procrastinate for 3 days you will have a habit to continue doing that and as a result, the work that you already did will have no meaning, you will just lose your time.Discipline and hard work are the keys to success.\\n\\nNow let's consider the topics which you need to learn and how. Let's discuss the program.\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='03c0f0ff-830d-45a8-984a-1eb1a20c89b8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Program:**\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3d53da2f-9b14-48b4-b283-1ce685833e14', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**1. Time complexities**\\n\\n1. Learn mainly Big-O notation and analyzing the time complexity of the program which you wrote\\n2. Practice by considering tests and some quizzes from the internet\\n3. Advised time to spend: 1 week\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2aed01e9-a90f-45e2-8e00-0f0ce36c5f6e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**2. Recursion**\\n\\n1. Learn how the recursion works\\n2. Try to write some basic problems by recursion and solve LeetCode problems for recursion\\n3. Advised time to spend: 1 week\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e9370fd3-d15a-4ed3-a031-742b90b87d71', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**3. Linked List**\\n\\n1. Learn the concept of a linked list\\n2. Insertion and deletion of nodes at different positions\\n3. Time complexity and advantages of using it\\n4. When and in what problems to apply\\n5. Linked List traversals. Both iterative and recursive ways\\n6. Solve LeetCode problems\\n7. Advised time to spend: 1-1.5 weeks\\n8. List of advised problems:\\xa0https://leetcode.com/list/50sfo32d\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='97a4ca64-1b4f-4772-9c3e-f3c77c852e43', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**4. Stack and Queue**\\n\\n1. Learn the basics of stack and queue\\n2. How they work and how to implement them\\n3. Their basic operations: push(), pop(), top() and etc.\\n4. Time complexities and advantages of using them\\n5. Consider different standard problems such as Valid Parentheses, Reversing Linked List and String\\n6. Advised time to spend: 1-1.5 weeks\\n7. List of advised problems:\\xa0https://leetcode.com/list/504xdrcr\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fafa43c3-6e9f-43d5-bf5e-037c8758de15', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**5. Binary Search Tree**\\n\\n1. Properties, terms, and the meaning of Binary Search Trees\\n2. How to search, add and delete nodes in BST and their time complexities\\n3. Main traversals such as BFS: Level-order-traversal and DFS: Preorder, Inorder, Postorder. Think about both recursive ways and iterative ways where will need to apply the knowledge of stack and queue\\n4. Advised time to spend: 2-2.5 weeks\\n5. List of advised problems:\\xa0https://leetcode.com/list/504mfxd2\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a26766c6-9e8c-4a74-bf2e-0ca87bd54398', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**6. Hash Table and Map**\\n\\n1. The meaning and work principle of Hash Table and Map\\n2. The difference between Hash Table and Map\\n3. Their time complexities and how to apply during problem-solving\\n4. Learn the main functions and their library\\n5. Advised time to spend: 2-2.5 weeks\\n6. List of advised problems:\\xa0https://leetcode.com/list/504wrexe\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7e600623-5801-4fc2-b209-4608e72646b9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**7. Sorting and Searching Algorithms**\\n\\n1. Bubble Sort, Insertion Sort, Selection Sort, Merge Sort, Quick Sort, Count Sort, Radix Sort.\\n2. Learn the working principle and time complexity of each of the sorting algorithms and their advantages over each other\\n3. Linear Search, Binary Search, Ternary Search\\n4. Learn the working principle and time complexity of each of the searching algorithms and their advantages over each other\\n5. Advised time to spend: 2-3 weeks\\n6. List of advised problems for sorting:\\xa0https://leetcode.com/list/5047kw65\\n7. List of advised problems for searching:\\xa0https://leetcode.com/list/504ixc37\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2f765ae3-dc09-4304-bcca-a884a3596c9f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**8. Greedy Algorithms**\\n\\n1. How to use sorting and hash table\\n2. Understand the logic of some basic problems\\n3. The main thing to master greedy algorithms is to practice a lot\\n4. Advised time to spend: 2.5-3 weeks\\n5. List of advised problems:\\xa0https://leetcode.com/list/5ik01ftj\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0749807e-a0df-4d8b-a58f-1f84283bef5c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**9. Dynamic Programming**\\n\\n1. Solutions of basic problems\\n2. Check my this post:\\xa0https://leetcode.com/discuss/general-discussion/1081421/powerful-dynamic-programming-explanation\\n3. Advised time to spend: 4-5 weeks\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b059be41-5ab2-4a60-8b8c-9afaec591b03', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**10. Graphs**\\n\\n1. Learn different graph traversals such as BFS and DFS\\n2. Learn Minimum Spanning Trees and their different algorithms\\n3. Learn the Shortest Paths and its algorithms\\n4. Learn Topological sort and priority queue\\n5. Learn the basic work principle of each algorithm and the way of applying them\\n6. Advised time to spend: 4-5 weeks\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f1a0d3f5-d358-4a57-a890-82ba0219fdf3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**11. String Processing**\\n\\n1. Learn different algorithms on strings\\n2. Learn trie and solve problems\\n\\n1. Recursion / Backtracking\\n a. 39. Combination Sum\\n\\nhttps://leetcode.com/problems/combina...\\n\\nb. 40. Combination Sum II\\n\\nhttps://leetcode.com/problems/combina...\\n\\nc. 78. Subsets\\n\\nhttps://leetcode.com/problems/subsets/\\n\\nd. 90. Subsets II\\n\\nhttps://leetcode.com/problems/subsets...\\n\\ne. 46. Permutations\\n\\nhttps://leetcode.com/problems/permuta...\\n\\nf. 47. Permutations II\\n\\nhttps://leetcode.com/problems/permuta...\\n\\n2. Graph Traversal - DFS, BFS, Topological Sorting\\n a. 133. Clone Graph\\n\\nhttps://leetcode.com/problems/clone-g...\\n\\nBFS / DFS\\n b. 127. Word Ladder\\n\\nhttps://leetcode.com/problems/word-la...\\n\\nBFS\\n c. 490. The Maze\\n\\nhttps://leetcode.com/problems/the-maze/\\n\\nd. 210. Course Schedule II\\n\\nhttps://leetcode.com/problems/course-...\\n\\n(Topological Sorting)\\n e. 269. Alien Dictionary\\n\\nhttps://leetcode.com/problems/alien-d...\\n\\n(Topological Sorting)\\n\\n3. Binary Tree / Binary Search Tree (BST)\\n a. 94. Binary Tree Inorder Traversal\\n\\nhttps://leetcode.com/problems/binary-...\\n\\nb. 236. Lowest Common Ancestor of a Binary Tree\\n\\nhttps://leetcode.com/problems/lowest-...\\n\\nc. 297. Serialize and Deserialize Binary Tree\\n\\nhttps://leetcode.com/problems/seriali...\\n\\nd. 98. Validate Binary Search Tree\\n\\nhttps://leetcode.com/problems/validat...\\n\\ne. 102. Binary Tree Level Order Traversal\\n\\nhttps://leetcode.com/problems/binary-...\\n\\n(BFS)\\n\\n4. Binary Search\\n a. 34. Find First and Last Position of Element in Sorted Array\\n\\nhttps://leetcode.com/problems/find-fi...\\n\\nb. 162. Find Peak Element\\n\\nhttps://leetcode.com/problems/find-pe...\\n\\nc. 69. Sqrt(x)\\n\\nhttps://leetcode.com/problems/sqrtx/\\n\\n5. Data Structure\\n a. 242. Valid Anagram\\n\\nhttps://leetcode.com/problems/valid-a...\\n\\n(Hash Table)\\n b. 133. Clone Graph\\n\\nhttps://leetcode.com/problems/clone-g...\\n\\n(Hash Table)\\n c. 127. Word Ladder\\n\\nhttps://leetcode.com/problems/word-la...\\n\\n(Hash Table)\\n d. 155. Min Stack\\n\\nhttps://leetcode.com/problems/min-stack/\\n\\n(Stack)\\n e. 225. Implement Stack using Queues\\n\\nhttps://leetcode.com/problems/impleme...\\n\\n(Stack / Queue)\\n f. 215. Kth Largest Element in an Array\\n\\nhttps://leetcode.com/problems/kth-lar...\\n\\n(PriorityQueue)\\n g. 23. Merge k Sorted Lists\\n\\nhttps://leetcode.com/problems/merge-k...\\n\\n(PriorityQueue)\\n\\n6. Linked List Manipulation\\n a. 237. Delete Node in a Linked List\\n\\nhttps://leetcode.com/problems/delete-...\\n\\nb. 92. Reverse Linked List II\\n\\nhttps://leetcode.com/problems/reverse...\\n\\nc. 876. Middle of the Linked List\\n\\nhttps://leetcode.com/problems/middle-...\\n\\nd. 143. Reorder List\\n\\nhttps://leetcode.com/problems/reorder...\\n\\n7. Pointer Manipulation\\n a. 239. Sliding Window Maximum\\n\\nhttps://leetcode.com/problems/sliding...\\n\\nb. 3. Longest Substring Without Repeating Characters\\n\\nhttps://leetcode.com/problems/longest...\\n\\nc. 76. Minimum Window Substring\\n\\nhttps://leetcode.com/problems/minimum...\\n\\n8. Sorting\\n a. Time -- O(N log N)\\n b. Merge Sort -- Space O(N)\\n c. Quick Sort\\n d. 148. Sort List\\n\\nhttps://leetcode.com/problems/sort-list/\\n\\n9. Convert Real Life Problem to Code \\n a. 146. LRU Cache\\n\\nhttps://leetcode.com/problems/lru-cache/\\n\\nb. 1066. Compus Bike\\n\\nhttps://leetcode.com/problems/campus-...\\n\\nc. 490. The Maze\\n\\nhttps://leetcode.com/problems/the-maze/\\n\\n10. Time Space Complexity\\n a. 一般面试的时候 你说完算法 就要说 这个算法的 time / space complexity是什么\\n b. 每次你做完一道题 给自己养成一个习惯 就是想一下他的时间空间复杂度是多少\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2ede3a9f-6905-4aca-86b1-68f52c3613b0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**1. Pattern: Sliding window，滑动窗口类型**\\n\\n滑动窗口类型的题目经常是用来执行数组或是链表上某个区间（窗口）上的操作。比如找最长的全为1的子数组长度。滑动窗口一般从第一个元素开始，一直往右边一个一个元素挪动。当然了，根据题目要求，我们可能有固定窗口大小的情况，也有窗口的大小变化的情况。\\n\\n下面**是一些我们用来判断我们可能需要上滑动窗口策略的方法**：\\n\\n- 这个问题的输入是一些线性结构：比如链表呀，数组啊，字符串啊之类的\\n- 让你去求最长/最短子字符串或是某些特定的长度要求\\n\\n**经典题目：**\\n\\nMaximum Sum Subarray of Size K (easy)\\n\\nSmallest Subarray with a given sum (easy)\\n\\nLongest Substring with K Distinct Characters (medium)\\n\\nFruits into Baskets (medium)\\n\\nNo-repeat Substring (hard)\\n\\nLongest Substring with Same Letters after Replacement (hard)\\n\\nLongest Subarray with Ones after Replacement (hard)\\n\\n---\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cec7eb91-2bd0-44b0-9001-46b74427cb2a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**2. Pattern: two points,\\xa0双指针类型**\\n\\n双指针是这样的模式：两个指针朝着左右方向移动（双指针分为同向双指针和异向双指针），直到他们有一个或是两个都满足某种条件。双指针通常用在排好序的数组或是链表中寻找对子。比如，你需要去比较数组中每个元素和其他元素的关系时，你就需要用到双指针了。\\n\\n我们需要双指针的原因是：如果你只用一个指针的话，你得来回跑才能在数组中找到你需要的答案。这一个指针来来回回的过程就很耗时和浪费空间了 — 这是考虑算法的复杂度分析的时候的重要概念。虽然brute force一个指针的解法可能会奏效，但时间复杂度一般会是O(n²)。在很多情况下，双指针能帮助我们找到空间或是时间复杂度更低的解。\\n\\n识别使用双指针的招数：\\n\\n- 一般来说，数组或是链表是排好序的，你得在里头找一些组合满足某种限制条件\\n- 这种组合可能是一对数，三个数，或是一个子数组\\n\\n**经典题目：**\\n\\nPair with Target Sum (easy)\\n\\nRemove Duplicates (easy)\\n\\nSquaring a Sorted Array (easy)\\n\\nTriplet Sum to Zero (medium)\\n\\nTriplet Sum Close to Target (medium)\\n\\nTriplets with Smaller Sum (medium)\\n\\nSubarrays with Product Less than a Target (medium)\\n\\nDutch National Flag Problem (medium)\\n\\n---\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7d0f7be4-9101-46ea-baf5-0ed371288739', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**3. Pattern: Fast & Slow pointers,\\xa0快慢指针类型**\\n\\n**这种模式，有一个非常出门的名字，叫龟兔赛跑。**咱们肯定都知道龟兔赛跑啦。但还是再解释一下快慢指针：这种算法的两个指针的在数组上（或是链表上，序列上）的移动速度不一样。还别说，**这种方法在解决有环的链表和数组时特别有用**。\\n\\n通过控制指针不同的移动速度（比如在环形链表上），这种算法证明了他们肯定会相遇的。快的一个指针肯定会追上慢的一个（可以想象成跑道上面跑得快的人套圈跑得慢的人）。\\n\\n咋知道需要用快慢指针模式勒？\\n\\n- 问题需要处理环上的问题，比如环形链表和环形数组\\n- 当你需要知道链表的长度或是某个特别位置的信息的时候\\n\\n那啥时候用快慢指针而不是上面的双指针呢？\\n\\n- 有些情形下，咱们不应该用双指针，比如我们在单链表上不能往回移动的时候。一个典型的需要用到快慢指针的模式的是当你需要去判断一个链表是否是回文的时候。\\n\\n**经典题目：**\\n\\nLinkedList Cycle (easy)\\n\\nStart of LinkedList Cycle (medium)\\n\\nHappy Number (medium)\\n\\nMiddle of the LinkedList (easy)\\n\\n---\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0aa1a28c-7aa8-4380-9a5a-a501ae8487cd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**4. Pattern: Merge Intervals，区间合并类型**\\n\\n区间合并模式是一个用来处理有区间重叠的很高效的技术。在设计到区间的很多问题中，通常咱们需要要么判断是否有重叠，要么合并区间，如果他们重叠的话。这个模式是这么起作用的：\\n\\n给两个区间，一个是a，另外一个是b。别小看就两个区间，他们之间的关系能跑出来6种情况。详细的就看图啦。\\n\\n理解和识别这六种情况，灰常重要。因为这能帮你解决一大堆问题。这些问题从插入区间到优化区间合并都有。\\n\\n怎么识别啥时候用合并区间模式呀？\\n\\n- 当你需要产生一堆相互之间没有交集的区间的时候\\n- 当你听到重叠区间的时候\\n\\n**经典题目：**\\n\\nMerge Intervals (medium)\\n\\nInsert Interval (medium)\\n\\nIntervals Intersection (medium)\\n\\nConflicting Appointments (medium)\\n\\n---\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cfa8f5af-43c6-4e21-a82d-d70c7cf35bce', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**5. Pattern: Cyclic Sort，循环排序**\\n\\n这种模式讲述的是一直很好玩的方法：可以用来处理数组中的数值限定在一定的区间的问题。这种模式一个个遍历数组中的元素，如果当前这个数它不在其应该在的位置的话，咱们就把它和它应该在的那个位置上的数交换一下。你可以尝试将该数放到其正确的位置上，但这复杂度就会是O(n^2)。这样的话，可能就不是最优解了。因此循环排序的优势就体现出来了。\\n\\n咋鉴别这种模式？\\n\\n- 这些问题一般设计到排序好的数组，而且数值一般满足于一定的区间\\n- 如果问题让你需要在排好序/翻转过的数组中，寻找丢失的/重复的/最小的元素\\n\\n**经典题目：**\\n\\nCyclic Sort (easy)\\n\\nFind the Missing Number (easy)\\n\\nFind all Missing Numbers (easy)\\n\\nFind the Duplicate Number (easy)\\n\\nFind all Duplicate Numbers (easy)\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1f8c45ed-d076-443e-b1a1-5f55ec0d854e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**6. Pattern: In-place Reversal of a LinkedList，链表翻转**\\n\\n---\\n\\n在众多问题中，题目可能需要你去翻转链表中某一段的节点。通常，要求都是你得原地翻转，就是重复使用这些已经建好的节点，而不使用额外的空间。这个时候，原地翻转模式就要发挥威力了。\\n\\n这种模式每次就翻转一个节点。一般需要用到多个变量，一个变量指向头结点（下图中的current），另外一个（previous）则指向咱们刚刚处理完的那个节点。在这种固定步长的方式下，你需要先将当前节点（current）指向前一个节点（previous），再移动到下一个。同时，你需要将previous总是更新到你刚刚新鲜处理完的节点，以保证正确性。\\n\\n咱们怎么去甄别这种模式呢？\\n\\n- 如果你被问到需要去翻转链表，要求不能使用额外空间的时候\\n\\n**经典题目：**\\n\\nReverse a LinkedList (easy)\\n\\nReverse a Sub-list (medium)\\n\\nReverse every K-element Sub-list (medium)\\n\\n---\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d54aeaeb-53cb-451d-bce7-4bade1ddc31d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**7. Pattern: Tree Breadth First Search，树上的BFS**\\n\\n这种模式基于宽搜（Breadth First Search (BFS)），适用于需要遍历一颗树。借助于队列数据结构，从而能保证树的节点按照他们的层数打印出来。打印完当前层所有元素，才能执行到下一层。所有这种需要遍历树且需要一层一层遍历的问题，都能用这种模式高效解决。\\n\\n这种树上的BFS模式是通过把根节点加到队列中，然后不断遍历直到队列为空。每一次循环中，我们都会把队头结点拿出来（remove），然后对其进行必要的操作。在删除每个节点的同时，其孩子节点，都会被加到队列中。\\n\\n识别树上的BFS模式：\\n\\n- 如果你被问到去遍历树，需要按层操作的方式（也称作层序遍历）\\n\\n**经典题目：**\\n\\nBinary Tree Level Order Traversal (easy)\\n\\nReverse Level Order Traversal (easy)\\n\\nZigzag Traversal (medium)\\n\\nLevel Averages in a Binary Tree (easy)\\n\\nMinimum Depth of a Binary Tree (easy)\\n\\nLevel Order Successor (easy)\\n\\nConnect Level Order Siblings (medium)\\n\\n---\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8938f10c-6c6d-4e07-91e6-b004c2b5ef35', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**8. Pattern: Tree Depth First Search，树上的DFS**\\n\\n树形DFS基于深搜（Depth First Search (DFS)）技术来实现树的遍历。\\n\\n咱们可以用递归（或是显示栈，如果你想用迭代方式的话）来记录遍历过程中访问过的父节点。\\n\\n该模式的运行方式是从根节点开始，如果该节点不是叶子节点，我们需要干三件事：\\n\\n1. 需要区别我们是先处理根节点（pre-order，前序），处理孩子节点之间处理根节点（in-order，中序），还是处理完所有孩子再处理根节点（post-order，后序）。\\n2. 递归处理当前节点的左右孩子。\\n\\n识别树形DFS：\\n\\n- 你需要按前中后序的DFS方式遍历树\\n- 如果该问题的解一般离叶子节点比较近。\\n\\n**经典题目：**\\n\\nBinary Tree Path Sum (easy)\\n\\nAll Paths for a Sum (medium)\\n\\nSum of Path Numbers (medium)\\n\\nPath With Given Sequence (medium)\\n\\nCount Paths for a Sum (medium)\\n\\n---\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f4eaf176-9b34-45a9-8770-3312b42a1c25', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**9. Pattern: Two Heaps，双堆类型**\\n\\n很多问题中，我们被告知，我们拿到一大把可以分成两队的数字。为了解决这个问题，我们感兴趣的是，怎么把数字分成两半？使得：小的数字都放在一起，大的放在另外一半。双堆模式就能高效解决此类问题。\\n\\n正如名字所示，该模式用到了两个堆，是不是很难猜？一个最小堆用来找最小元素；一个最大堆，拿到最大元素。这种模式将一半的元素放在最大堆中，这样你可以从这一堆中秒找到最大元素。同理，把剩下一半丢到最小堆中，O(1)时间找到他们中的最小元素。通过这样的方式，这一大堆元素的**中位数**就可以从两个堆的堆顶拿到数字，从而计算出来。\\n\\n判断双堆模式的秘诀：\\n\\n- 这种模式在优先队列，计划安排问题（Scheduling）中有奇效\\n- 如果问题让你找一组数中的最大/最小/中位数\\n- 有时候，这种模式在涉及到二叉树数据结构时也特别有用\\n\\n**经典题目：**\\n\\nFind the Median of a Number Stream (medium)\\n\\nSliding Window Median (hard)\\n\\nMaximize Capital (hard)\\n\\n---\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5958a394-e8aa-4d52-865a-5cf8272b4970', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**10. Pattern: Subsets，子集类型，一般都是使用多重DFS**\\n\\n超级多的编程面试问题都会涉及到排列和组合问题。子集问题模式讲的是用BFS来处理这些问题。\\n\\n这个模式是这样的：\\n\\n给一组数字 [1, 5, 3]\\n\\n1. 我们从空集开始：[[]]\\n2. 把第一个数（1），加到之前已经存在的集合中：[[], [1]];\\n3. 把第二个数（5），加到之前的集合中得到：[[], [1], [5], [1,5]];\\n4. 再加第三个数（3），则有：[[], [1], [5], [1,5], [3], [1,3], [5,3], [1,5,3]].\\n\\n该模式的详细步骤如下：\\n\\n如果判断这种子集模式：\\n\\n- 问题需要咱们去找数字的组合或是排列\\n\\n**经典题目：**\\n\\nSubsets (easy)\\n\\nSubsets With Duplicates (easy)\\n\\nPermutations (medium)\\n\\nString Permutations by changing case (medium)\\n\\nBalanced Parentheses (hard)\\n\\nUnique Generalized Abbreviations (hard)\\n\\n---\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5f9f082d-d261-4e0d-99e7-dfd8885672b1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**11. Pattern: Modified Binary Search，改造过的二分**\\n\\n当你需要解决的问题的输入是排好序的数组，链表，或是排好序的矩阵，要求咱们寻找某些特定元素。这个时候的不二选择就是二分搜索。这种模式是一种超级牛的用二分来解决问题的方式。\\n\\n对于一组满足上升排列的数集来说，这种模式的步骤是这样的：\\n\\n1. 首先，算出左右端点的中点。最简单的方式是这样的：middle = (start + end) / 2。但这种计算方式有不小的概率会出现整数越界。因此一般都推荐另外这种写法：middle\\xa0= start + (end — start) / 2\\n2. 如果要找的目标改好和中点所在的数值相等，我们返回中点的下标就行\\n3. 如果目标不等的话：我们就有两种移动方式了\\n4. 如果目标比中点在的值小（key < arr[middle]）：将下一步搜索空间放到左边（end = middle - 1）\\n5. 如果比中点的值大，则继续在右边搜索，丢弃左边：left = middle + 1\\n\\n图示该过程的话，如下图所示：\\n\\n**经典题目：**\\n\\nOrder-agnostic Binary Search (easy)\\n\\nCeiling of a Number (medium)\\n\\nNext Letter (medium)\\n\\nNumber Range (medium)\\n\\nSearch in a Sorted Infinite Array (medium)\\n\\nMinimum Difference Element (medium)\\n\\nBitonic Array Maximum (easy)\\n\\n---\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e9a59380-fd6a-4c27-aaed-1a4f934424f0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**12. Pattern: Top ‘K’ Elements，前K个系列**\\n\\n任何让我们求解最大/最小/最频繁的K个元素的题，都遵循这种模式。\\n\\n用来记录这种前K类型的最佳数据结构就是堆了（译者注：在Java中，改了个名，叫优先队列（PriorityQueue））。这种模式借助堆来解决很多这种前K个数值的问题。\\n\\n这个模式是这样的：\\n\\n1. 根据题目要求，将K个元素插入到最小堆或是最大堆。\\n2. 遍历剩下的还没访问的元素，如果当前出来到的这个元素比堆顶元素大，那咱们把堆顶元素先删除，再加当前元素进去。\\n\\n注意这种模式下，咱们不需要去排序数组，因为堆具有这种良好的局部有序性，这对咱们需要解决问题就够了。\\n\\n识别最大K个元素模式：\\n\\n- 如果你需要求最大/最小/最频繁的前K个元素\\n- 如果你需要通过排序去找一个特定的数\\n\\n**经典题目：**\\n\\nTop ‘K’ Numbers (easy)\\n\\nKth Smallest Number (easy)\\n\\n‘K’ Closest Points to the Origin (easy)\\n\\nConnect Ropes (easy)\\n\\nTop ‘K’ Frequent Numbers (medium)\\n\\nFrequency Sort (medium)\\n\\nKth Largest Number in a Stream (medium)\\n\\n‘K’ Closest Numbers (medium)\\n\\nMaximum Distinct Elements (medium)\\n\\nSum of Elements (medium)\\n\\nRearrange String (hard)\\n\\n---\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dbccbeea-0a86-4b50-adb8-887bf3daeac8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**13. Pattern: K-way merge，多路归并**\\n\\nK路归并能帮咱们解决那些涉及到多组排好序的数组的问题。\\n\\n每当你的输入是K个排好序的数组，你就可以用堆来高效顺序遍历其中所有数组的所有元素。你可以将每个数组中最小的一个元素加入到最小堆中，从而得到全局最小值。当我们拿到这个全局最小值之后，再从该元素所在的数组里取出其后面紧挨着的元素，加入堆。如此往复直到处理完所有的元素。\\n\\n该模式是这样的运行的：\\n\\n1. 把每个数组中的第一个元素都加入最小堆中\\n2. 取出堆顶元素（全局最小），将该元素放入排好序的结果集合里面\\n3. 将刚取出的元素所在的数组里面的下一个元素加入堆\\n4. 重复步骤2，3，直到处理完所有数字\\n\\n识别K路归并：\\n\\n- 该问题的输入是排好序的数组，链表或是矩阵\\n- 如果问题让咱们合并多个排好序的集合，或是需要找这些集合中最小的元素\\n\\n**经典题目：**\\n\\nMerge K Sorted Lists (medium)\\n\\nKth Smallest Number in M Sorted Lists (Medium)\\n\\nKth Smallest Number in a Sorted Matrix (Hard)\\n\\nSmallest Number Range (Hard)\\n\\n---\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='321aa3a6-b587-40de-b58e-bc470ecbe6d1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**14. Pattern: 0/1 Knapsack (Dynamic Programming)，0/1背包类型**\\n\\n经典题目：\\n\\n0/1 Knapsack (medium)\\n\\nEqual Subset Sum Partition (medium)\\n\\nSubset Sum (medium)\\n\\nMinimum Subset Sum Difference (hard)\\n\\n---\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='56b23ee7-04dc-421c-bcd8-9e310beebdb9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**15. Pattern: Topological Sort (Graph)，拓扑排序类型**\\n\\n拓扑排序模式用来寻找一种线性的顺序，这些元素之间具有依懒性。比如，如果事件B依赖于事件A，那A在拓扑排序顺序中排在B的前面。\\n\\n这种模式定义了一种简单方式来理解拓扑排序这种技术。\\n\\n这种模式是这样奏效的：\\n\\n1. 初始化a) 借助于HashMap将图保存成邻接表形式。b) 找到所有的起点，用HashMap来帮助记录每个节点的入度\\n2. 创建图，找到每个节点的入度a) 利用输入，把图建好，然后遍历一下图，将入度信息记录在HashMap中\\n3. 找所有的起点a) 所有入度为0的节点，都是有效的起点，而且我们讲他们都加入到一个队列中\\n4. 排序a) 对每个起点，执行以下步骤—i) 把它加到结果的顺序中— ii)将其在图中的孩子节点取到— iii)将其孩子的入度减少1— iv)如果孩子的入度变为0，则改孩子节点成为起点，将其加入队列中b) 重复（a）过程，直到起点队列为空。\\n\\n拓扑排序模式识别：\\n\\n- 待解决的问题需要处理无环图\\n- 你需要以一种有序的秩序更新输入元素\\n- 需要处理的输入遵循某种特定的顺序\\n\\n**经典题目：**\\n\\nTopological Sort (medium)\\n\\nTasks Scheduling (medium)\\n\\nTasks Scheduling Order (medium)\\n\\nAll Tasks Scheduling Orders (hard)\\n\\nAlien Dictionary (hard)\\n\\n大家好好练练这些题目，面试中遇到中高等难度的题目，应该就能解得不错了。\\n\\n!Untitled\\n\\n作者：编程指北\\n\\n链接：https://www.zhihu.com/question/28580777/answer/1961384750\\n\\n来源：知乎\\n\\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\\n\\n**排序类（Sort）：**\\n\\n- 基础知识：快速排序（Quick Sort）， 归并排序（Merge Sort）的原理与代码实现。需要能讲明白代码中每一行的目的。快速排序时间复杂度平均状态下O（NlogN），空间复杂度O（1），归并排序最坏情况下时间复杂度O（NlogN），空间复杂度O（N）\\n- 入门题目：\\n    - Leetcode 148. Sort List\\n    - Leetcode 56. Merge Intervals\\n- 进阶题目：\\n    - Leetcode 179. Largest Number\\n    - Leetcode 75. Sort Colors\\n    - Leetcode 215. Kth Largest Element\\n    - Leetcode 4. Median of Two Sorted Arrays\\n\\n注意：后两题是与快速排序非常相似的快速选择（Quick Select）算法，面试中很常考\\n\\n**链表类（Linked List）：**\\n\\n- 基础知识：链表如何实现，如何遍历链表。链表可以保证头部尾部插入删除操作都是O（1），查找任意元素位置O（N）\\n- 基础题目：\\n    - Leetcode 206. Reverse Linked List\\n    - Leetcode 876. Middle of the Linked List\\n\\n注意：快慢指针和链表反转几乎是所有链表类问题的基础，尤其是反转链表，代码很短，建议直接背熟。\\n\\n- 进阶题目:\\n    - Leetcode 160. Intersection of Two Linked Lists\\n    - Leetcode 141. Linked List Cycle (Linked List Cycle II)\\n    - Leetcode 92. Reverse Linked List II\\n    - Leetcode 328. Odd Even Linked List\\n\\n**堆（Heap or Priority Queue）、栈（Stack）、队列（Queue）、哈希表类（Hashmap、Hashset）：**\\n\\n- 基础知识：各个数据结构的基本原理，增删查改复杂度。\\n- Queue题目：\\n    - Leetcode 225. Implement Stack using Queues\\n    - Leetcode 346. Moving Average from Data Stream\\n    - Leetcode 281. Zigzag Iterator\\n    - Leetcode 1429. First Unique Number\\n    - Leetcode 54. Spiral Matrix\\n    - Leetcode 362. Design Hit Counter\\n- Stack题目：\\n    - Leetcode 232. Implement Queue using Stacks\\n    - Leetcode 150. Evaluate Reverse Polish Notation\\n    - Leetcode 224. Basic Calculator II (I, II, III, IV)\\n    - Leetcode 20. Valid Parentheses\\n    - Leetcode 1472. Design Browser History\\n    - Leetcode 1209. Remove All Adjacent Duplicates in String II\\n    - Leetcode 1249. Minimum Remove to Make Valid Parentheses\\n    - Leetcode 735. Asteroid Collision\\n- Hashmap/ Hashset题目：\\n    - Leetcode 1. Two Sum\\n    - Leetcode 146. LRU Cache (Python中可以使用OrderedDict来代替)\\n    - Leetcode 128. Longest Consecutive Sequence\\n    - Leetcode 73. Set Matrix Zeroes\\n    - Leetcode 380. Insert Delete GetRandom O(1)\\n    - Leetcode 49. Group Anagrams\\n    - Leetcode 350. Intersection of Two Arrays II\\n    - Leetcode 299. Bulls and Cows\\n    - Leetcode 348 Design Tic-Tac-Toe\\n- Heap／Priority Queue题目：\\n    - Leetcode 973. K Closest Points\\n    - Leetcode 347. Top k Largest Elements\\n    - Leetcode 23. Merge K Sorted Lists\\n    - Leetcode 264. Ugly Number II\\n    - Leetcode 1086. High Five\\n    - Leetcode 68. Merge Sorted Arrays\\n    - Leetcode 692. Top K Frequent Words\\n    - Leetcode 378. Kth Smallest Element in a Sorted Matrix\\n    - Leetcode 295. Find Median from Data Stream\\n    - Leetcode 767. Reorganize String\\n    - Leetcode 1438. Longest Continuous Subarray With Absolute Diff Less Than or Equal to Limit\\n    - Leetcode 895. Maximum Frequency Stack\\n\\n**二分法（Binary Search）：**\\n\\n- 基础知识：二分法是用来解法基本模板，时间复杂度logN；常见的二分法题目可以分为两大类，显式与隐式，即是否能从字面上一眼看出二分法的特点：要查找的数据是否可以分为两部分，前半部分为X，后半部分为O\\n- 显式二分法：\\n    - Leetcode 34. Find First and Last Position of Element in Sorted Array\\n    - Leetcode 33. Search in Rotated Sorted Array\\n    - Leetcode 1095. Find in Mountain Array\\n    - Leetcode 162. Find Peak Element\\n    - Leetcode 278. First Bad Version\\n    - Leetcode 74. Search a 2D Matrix\\n    - Leetcode 240. Search a 2D Matrix II\\n- 隐式二分法：\\n    - Leetcode 69. Sqrt(x)\\n    - Leetcode 540. Single Element in a Sorted Array\\n    - Leetcode 644. Maximum Average Subarray II\\n    - Leetcode 528. Random Pick with Weight\\n    - Leetcode 1300. Sum of Mutated Array Closest to Target\\n    - Leetcode 1060. Missing Element in Sorted Array\\n\\n**双指针（2 Pointer）：**\\n\\n- 基础知识：常见双指针算法分为三类，同向（即两个指针都相同一个方向移动），背向（两个指针从相同或者相邻的位置出发，背向移动直到其中一根指针到达边界为止），相向（两个指针从两边出发一起向中间移动直到两个指针相遇）\\n- 背向双指针：(基本上全是回文串的题)\\n    - Leetcode 409. Longest Palindrome\\n    - Leetcode 125. Valid Palindrome\\n    - Leetcode 5. Longest Palindromic Substring\\n- 相向双指针：(以two sum为基础的一系列题)\\n    - Leetcode 1. Two Sum (这里使用的是先排序的双指针算法，不同于hashmap做法)\\n    - Leetcode 167. Two Sum II - Input array is sorted\\n    - Leetcode 15. 3Sum\\n    - Leetcode 16. 3Sum Closest\\n    - Leetcode 18. 4Sum\\n    - Leetcode 454. 4Sum II\\n    - Leetcode 277. Find the Celebrity\\n- 同向双指针：（个人觉得最难的一类题）\\n    - Leetcode 283. Move Zeroes\\n    - Leetcode 26. Remove Duplicate Numbers in Array\\n    - Leetcode 395. Longest Substring with At Least K Repeating Characters\\n    - Leetcode 340. Longest Substring with At Most K Distinct Characters\\n    - Leetcode 76. Minimum Window Substring\\n    - Leetcode 3. Longest Substring Without Repeating Characters\\n\\n**宽度优先搜索（BFS）：面试中与DFS都为几乎必考的题目**\\n\\n- 基础知识：\\n    - 常见的BFS用来解决什么问题？(1) 简单图（有向无向皆可）的最短路径长度（2）拓扑排序 （3） 遍历一个图（或者树）\\n- BFS基本模板（需要记录层数或者不需要记录层数）\\n- 多数情况下时间复杂度空间复杂度都是O（N+M），N为节点个数，M为边的个数\\n- 基于树的BFS：不需要专门一个set来记录访问过的节点\\n    - Leetcode 102 Binary Tree Level Order Traversal\\n    - Leetcode 103 Binary Tree Zigzag Level Order Traversal\\n    - Leetcode 297 Serialize and Deserialize Binary Tree （很好的BFS和双指针结合的题）\\n- Leetcode 314 Binary Tree Vertical Order Traversal\\n- 基于图的BFS：（一般需要一个set来记录访问过的节点）\\n    - Leetcode 200. Number of Islands\\n    - Leetcode 133. Clone Graph\\n    - Leetcode 127. Word Ladder\\n    - Leetcode 490. The Maze\\n    - Leetcode 323. Connected Component in Undirected Graph\\n    - Leetcode 130. Surrounded Regions\\n    - Leetcode 752. Open the Lock\\n    - Leetcode 815. Bus Routes\\n    - Leetcode 1091. Shortest Path in Binary Matrix\\n    - Leetcode 542. 01 Matrix\\n    - Leetcode 1293. Shortest Path in a Grid with Obstacles Elimination\\n- 拓扑排序：（https://zh.wikipedia.org/wiki/%E6%8B%93%E6%92%B2%E6%8E%92%E5%BA%8F）\\n    - Leetcode 207 Course Schedule （I, II）\\n    - Leetcode 444 Sequence Reconstruction\\n    - Leetcode 269 Alien Dictionary\\n\\n**深度优先搜索（DFS）：面试中与BFS都为几乎必考的题目**\\n\\n- 基础知识：\\n    - 常见的DFS用来解决什么问题？(1) 图中（有向无向皆可）的符合某种特征（比如最长）的路径以及长度（2）排列组合（3） 遍历一个图（或者树）（4）找出图或者树中符合题目要求的全部方案\\n    - DFS基本模板（需要记录路径，不需要返回值 and 不需要记录路径，但需要记录某些特征的返回值）\\n    - 除了遍历之外多数情况下时间复杂度是指数级别，一般是O(方案数×找到每个方案的时间复杂度)\\n    - 递归题目都可以用非递归迭代的方法写，但一般实现起来非常麻烦\\n- 基于树的DFS：需要记住递归写前序中序后序遍历二叉树的模板\\n    - Leetcode 543 Diameter of Binary Tree\\n    - Leetcode 226 Invert Binary Tree\\n    - Leetcode 124 Binary Tree Maximum Path Sum\\n    - Leetcode 236 Lowest Common Ancestor of a Binary Tree\\n    - Leetcode 101 Symmetric Tree\\n    - Leetcode 105 Construct Binary Tree from Preorder and Inorder Traversal\\n    - Leetcode 104 Maximum Depth of Binary Tree\\n    - Leetcode 951 Flip Equivalent Binary Trees\\n    - Leetcode 987 Vertical Order Traversal of a Binary Tree\\n    - Leetcode 1485 Clone Binary Tree With Random Pointer\\n    - Leetcode 572 Subtree of Another Tree\\n    - Leetcode 863 All Nodes Distance K in Binary Tree\\n- 二叉搜索树（BST）：BST特征：中序遍历为单调递增的二叉树，换句话说，根节点的值比左子树任意节点值都大，比右子树任意节点值都小，增删查改均为O（h）复杂度，h为树的高度；注意不是所有的BST题目都需要递归，有的题目只需要while循环即可\\n    - Leetcode 230 Kth Smallest element in a BST\\n    - Leetcode 98 Validate Binary Search Tree\\n    - Leetcode 270 Cloest Binary Search Tree Value\\n    - Leetcode 235 Lowest Common Ancestor of a Binary Search Tree\\n    - Leetcode 669 Trim a Binary Search Tree\\n    - Leetcode 700 Search Range in Binary Search Tree\\n    - Leetcode 108 Convert Sorted Array to Binary Search Tree\\n    - Leetcode 333 Largest BST Subtree\\n    - Leetcode 510 Inorder Successor in BST II\\n- 基于图的DFS: 和BFS一样一般需要一个set来记录访问过的节点，避免重复访问造成死循环\\n    - Leetcode 341 Flatten Nested List Iterator\\n    - Leetcode 394 Decode String\\n    - Leetcode 51 N-Queens\\n    - Leetcode 291 Word Pattern II (I为简单的Hashmap题)\\n    - Leetcode 126 Word Ladder II （I为BFS题目）\\n    - Leetcode 1110 Delete Nodes And Return Forest\\n    - Leetcode 93 Restore IP Addresses\\n    - Leetcode 22 Generate Parentheses\\n    - Leetcode 37 Sodoku Solver\\n    - Leetcode 301 Remove Invalid Parentheses\\n    - Leetcode 212 Word Search II （I, II）\\n    - Leetcode 1087 Brace Expansion\\n    - Leetcode 399 Evaluate Division\\n    - Leetcode 1274 Number of Ships in a Rectangle\\n    - Leetcode 1376 Time Needed to Inform All Employees\\n    - Leetcode 694 Number of Distinct Islands\\n    - Leetcode 586 Score of Parentheses\\n- 基于排列组合的DFS: 其实与图类DFS方法一致，但是排列组合的特征更明显\\n    - Leetcode 17 Letter Combinations of a Phone Number\\n    - Leetcode 39 Combination Sum （I, II, III, IV）\\n    - Leetcode 90 Subsets II （重点在于如何去重）\\n    - Leetcode 47 Permutation II\\n    - Leetcode 77 Combinations\\n    - Leetcode 526 Beautiful Arrangement\\n- 记忆化搜索（DFS + Memoization Search）：算是动态规划的一种，递归每次返回时同时记录下已访问过的节点特征，避免重复访问同一个节点，可以有效的把指数级别的DFS时间复杂度降为多项式级别\\n    - Leetcode 139 Word Break II\\n    - Leetcode 131 Palindrome Partitioning\\n    - Leetcode 72 Edit Distance\\n    - Leetcode 377 Combination Sum IV\\n    - Leetcode 1335 Minimum Difficulty of a Job Schedule\\n\\n**前缀和（Prefix Sum）**\\n\\n- 基础知识：前缀和本质上是在一个list当中，用O（N）的时间提前算好从第0个数字到第i个数字之和，在后续使用中可以在O（1）时间内计算出第i到第j个数字之和，一般很少单独作为一道题出现，而是很多题目中的用到的一个小技巧\\n- 常见题目：\\n    - Leetcode 53 Maximum Subarray\\n    - Leetcode 1423 Maximum Points You Can Obtain from Cards\\n    - Leetcode 1031 Maximum Sum of Two Non-Overlapping Subarrays\\n    - Leetcode 523 Continuous Subarray Sum\\n\\n---\\n\\n以上内容皆为面试中高频的知识点，以下知识点和题目在面试中属于中等频率（大概面10道题会遇到一次），时间不足的情况下，请以准备上面的知识点为主。\\n\\n**并查集（Union Find）：把两个或者多个集合合并为一个集合**\\n\\n- 基础知识：如果数据不是实时变化，本类问题可以用BFS或者DFS的方式遍历，如果数据实时变化（data stream）则并查集每次的时间复杂度可以视为O（1）；需要牢记合并与查找两个操作的模板\\n- 常见题目：\\n    - Leetcode 721 Accounts Merge\\n    - Leetcode 547 Number of Provinces\\n    - Leetcode 737 Sentence Similarity II\\n    - Leetcode 434 Number of Islands II\\n\\n**字典树（Trie）**\\n\\n- 基础知识：（https://zh.wikipedia.org/wiki/Trie）；多数情况下可以通过用一个set来记录所有单词的prefix来替代，时间复杂度不变，但空间复杂度略高\\n- 常见题目：\\n    - Leetcode 208 Implement Trie (Prefix Tree)\\n    - Leetcode 211 Design Add and Search Words Data Structure\\n    - Leetcode 1268 Search Suggestions System\\n    - Leetcode 79 Word Search\\n\\n**单调栈与单调队列（Monotone Stack／Queue）**\\n\\n- 基础知识：单调栈一般用于解决数组中找出每个数字的第一个大于／小于该数字的位置或者数字；单调队列只见过一道题需要使用；不论单调栈还是单调队列，单调的意思是保留在栈或者队列中的数字是单调递增或者单调递减的\\n- 常见题目：\\n    - Leetcode 85 Maximum Rectangle\\n    - Leetcode 84 Largest Rectangle in Histogram\\n    - Leetcode 739 Daily Temperatures\\n    - Leetcode 901 Online Stock Span\\n    - Leetcode 503 Next Greater Element II\\n    - Leetcode 239 Sliding Window Maximum （唯一的单调队列题）\\n\\n**扫描线算法（Sweep Line）**\\n\\n- 基础知识：一个很巧妙的解决时间安排冲突的算法，本身比较容易些也很容易理解\\n- 常见题目：\\n    - Leetcode 253 Meeting Room II（Meeting Room I也可以使用）\\n    - Leetcode 218 The Skyline Problem\\n    - Leetcode 759 Employee Free Time\\n\\n**TreeMap**\\n\\n- 基础知识：基于红黑树（平衡二叉搜索树）的一种树状 hashmap，增删查改、找求大最小均为logN复杂度，Python当中可以使用SortedDict替代\\n- 常见题目：\\n- Leetcode 729 My Calendar I\\n- Leetcode 981 Time Based Key-Value Store\\n- Leetcode 846 Hand of Straights\\n- Leetcode 826 Most Profit Assigning Work\\n\\n**动态规划（Dynamic Programming）**\\n\\n- 基础知识：这里指的是用for循环方式的动态规划，非Memoization Search方式。DP可以在多项式时间复杂度内解决DFS需要指数级别的问题。常见的题目包括找最大最小，找可行性，找总方案数等，一般结果是一个Integer或者Boolean。动态规划有很多分支，暂时还没想好怎么去写这部分，后面想好了再具体写吧。\\n- 常见题目：\\n    - Leetcode 674 Longest Continuous Increasing Subsequence\\n    - Leetcode 62 Unique Paths II\\n    - Leetcode 70 Climbing Stairs\\n    - Leetcode 64 Minimum Path Sum\\n    - Leetcode 368 Largest Divisible Subset\\n    - Leetcode 300 Longest Increasing Subsequence\\n    - Leetcode 354 Russian Doll Envelopes\\n    - Leetcode 256 Paint House\\n    - Leetcode 121 Best Time to Buy and Sell Stock\\n    - Leetcode 55 Jump Game\\n    - Leetcode 45 Jump Game II\\n    - Leetcode 403 Frog Jump\\n    - Leetcode 132 Palindrome Partitioning II\\n    - Leetcode 312 Burst Balloons\\n    - Leetcode 1143 Longest Common Subsequence\\n    - Leetcode 115 Distinct Subsequences\\n    - Leetcode 72 Edit Distance\\n    - Leetcode 91 Decode Ways\\n    - Leetcode 639 Decode Ways II\\n    - Leetcode 712 Minimum ASCII Delete Sum for Two Strings\\n    - Leetcode 221 Maximal Square\\n    - Leetcode 198 House Robber\\n    - Leetcode 213 House Robber II\\n    - Leetcode 87 Scramble String\\n    - Leetcode 1062 Longest Repeating Substring\\n    - Leetcode 1140 Stone Game II\\n    - Leetcode 322 Coin Change\\n    - Leetcode 518 Coin Change II\\n    - Leetcode 97 Interleaving String\\n    \\n    作者：编程指北\\n    \\n    链接：https://www.zhihu.com/question/28580777/answer/1961384750\\n    \\n    来源：知乎\\n    \\n    著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\\n    \\n    ### 1. 线性表：\\n    \\n    **1.1 Remove Duplicates from Sorted Array**\\n    \\n    **1.2 Remove Duplicates from Sorted Array II**\\n    \\n    **1.3 Search in Rotated Sorted Array II**\\n    \\n    **1.4 Median of Two Sorted Arrays**\\n    \\n    **1.5 Longest Consecutive Sequence**\\n    \\n    **1.6 Two Sum**\\n    \\n    **1.7 Valid Sudoku**\\n    \\n    **1.8 Trapping Rain Water**\\n    \\n    **1.9 Swap Nodes in Pairs**\\n    \\n    **1.10 Reverse Nodes in k-Group**\\n    \\n    ### 2. 字符串\\n    \\n    ### 2.1 **Valid Palindrome**\\n    \\n    **2.2 Implement strStr()**\\n    \\n    **3.3 String to Integer (atoi)**\\n    \\n    **3.4 Add Binary**\\n    \\n    **3.5 Longest Palindromic Substring**\\n    \\n    **3.6 Regular Expression Matching**\\n    \\n    **3.7 Wildcard Matching**\\n    \\n    **3.8 Longest Common Prefix**\\n    \\n    **3.9 Valid Number**\\n    \\n    **3.10 Integer to Roman**\\n    \\n    ### 3. 树、二叉树\\n    \\n    3.1 **Binary Tree Preorder Traversal**\\n    \\n    **3.2 Binary Tree Inorder Traversal**\\n    \\n    **3.3 Binary Tree Postorder Traversal**\\n    \\n    **3.4 Binary Tree Level Order Traversal II**\\n    \\n    **3.5 Binary Tree Zigzag Level Order Traversal**\\n    \\n    **3.6 Construct Binary Tree from Preorder and Inorder Traversal**\\n    \\n    **3.7 Unique Binary Search Trees**\\n    \\n    3.8 **Validate Binary Search Tree**\\n    \\n    **3.9 Convert Sorted Array to Binary Search Tree**\\n    \\n    ### 4. 排序\\n    \\n    4.1 **Merge Sorted Array**\\n    \\n    **4.2 Merge Two Sorted Lists** **4.3 Merge k Sorted Lists**\\n    \\n    **4.4 Insertion Sort List**\\n    \\n    **4.5 Sort List**\\n    \\n    **4.6 First Missing Positive**\\n    \\n    ### 5. 暴力枚举\\n    \\n    5.1 **Subsets**\\n    \\n    **5.1 Subsets II**\\n    \\n    **5.3 Permutations**\\n    \\n    **5.4 Letter Combinations of a Phone Number**\\n    \\n    ### 6. 深度优先搜索\\n    \\n    6.1 **Palindrome Partitioning**\\n    \\n    **6.2 Unique Paths**\\n    \\n    **6.3 Unique Paths II**\\n    \\n    ### 7. 回溯\\n    \\n    ### 8. 深搜与递归的区别\\n    \\n    ### 9. 分治法\\n    \\n    ### 10. 贪心法\\n    \\n    10.1 **Jump Game**\\n    \\n    **10.2 Best Time to Buy and Sell Stock**\\n    \\n    **10.3 Best Time to Buy and Sell Stock II**\\n    \\n    **10.4 Longest Substring Without Repeating Characters**\\n    \\n    **10.5 Container With Most Water**\\n    \\n    ### 11. 动态规划\\n    \\n    11.1 **Triangle**\\n    \\n    **11.2 Maximum Subarray**\\n    \\n    **11.3 Palindrome Partitioning II**\\n    \\n    **11.4  Maximal Rectangle**\\n    \\n    **11.5** **Best Time to Buy and Sell Stock III**\\n    \\n    **11.6 Interleaving String**\\n    \\n    ## **①基本数据类型**\\n    \\n    https://www.lintcode.com/problem/1\\n    \\n    https://www.lintcode.com/problem/37\\n    \\n    https://www.lintcode.com/problem/764\\n    \\n    https://www.lintcode.com/problem/1300\\n    \\n    ## **②判断语句**\\n    \\n    https://www.lintcode.com/problem/23\\n    \\n    https://www.lintcode.com/problem/766\\n    \\n    https://www.lintcode.com/problem/145\\n    \\n    https://www.lintcode.com/problem/1141\\n    \\n    https://www.lintcode.com/problem/478\\n    \\n    https://www.lintcode.com/problem/283\\n    \\n    ## **③数组与循环**\\n    \\n    https://www.lintcode.com/problem/25\\n    \\n    https://www.lintcode.com/problem/214\\n    \\n    https://www.lintcode.com/problem/485\\n    \\n    https://www.lintcode.com/problem/539\\n    \\n    https://www.lintcode.com/problem/297\\n    \\n    https://www.lintcode.com/problem/484\\n    \\n    https://www.lintcode.com/problem/9\\n    \\n    https://www.lintcode.com/problem/220\\n    \\n    https://www.lintcode.com/problem/407\\n    \\n    https://www.lintcode.com/problem/807\\n    \\n    https://www.lintcode.com/problem/463\\n    \\n    https://www.lintcode.com/problem/298\\n    \\n    https://www.lintcode.com/problem/479\\n    \\n    https://www.lintcode.com/problem/46\\n    \\n    https://www.lintcode.com/problem/768\\n    \\n    https://www.lintcode.com/problem/1334\\n    \\n    https://www.lintcode.com/problem/767\\n    \\n    https://www.lintcode.com/problem/235\\n    \\n    https://www.lintcode.com/problem/53\\n    \\n    https://www.lintcode.com/problem/50\\n    \\n    ## **④字符串与循环**\\n    \\n    https://www.lintcode.com/problem/8\\n    \\n    https://www.lintcode.com/problem/491\\n    \\n    https://www.lintcode.com/problem/146\\n    \\n    https://www.lintcode.com/problem/422\\n    \\n    https://www.lintcode.com/problem/353\\n    \\n    https://www.lintcode.com/problem/936\\n    \\n    https://www.lintcode.com/problem/241\\n    \\n    https://www.lintcode.com/problem/13\\n    \\n    https://www.lintcode.com/problem/1535\\n    \\n    https://www.lintcode.com/problem/1343\\n    \\n    https://www.lintcode.com/problem/133\\n    \\n    ## **⑤栈与队列**\\n    \\n    https://www.lintcode.com/problem/263\\n    \\n    https://www.lintcode.com/problem/423\\n    \\n    https://www.lintcode.com/problem/495\\n    \\n    https://www.lintcode.com/problem/492\\n    \\n    https://www.lintcode.com/problem/771\\n    \\n    ## **⑥简单递归**\\n    \\n    https://www.lintcode.com/problem/366\\n    \\n    https://www.lintcode.com/problem/66\\n    \\n    https://www.lintcode.com/problem/67\\n    \\n    https://www.lintcode.com/problem/68\\n    \\n    ## **链表：**\\n    \\n    https://www.lintcode.com/problem/35\\n    \\n    https://www.lintcode.com/problem/36\\n    \\n    https://www.lintcode.com/problem/450\\n    \\n    https://www.lintcode.com/problem/228\\n    \\n    https://www.lintcode.com/problem/102\\n    \\n    https://www.lintcode.com/problem/103\\n    \\n    98 · Sort List - LintCode\\n    \\n    ## **二分法：**\\n    \\n    https://www.lintcode.com/problem/14\\n    \\n    https://www.lintcode.com/problem/28\\n    \\n    https://www.lintcode.com/problem/75\\n    \\n    https://www.lintcode.com/problem/457\\n    \\n    https://www.lintcode.com/problem/458\\n    \\n    ## **二分答案：**\\n    \\n    https://www.lintcode.com/problem/183\\n    \\n    https://www.lintcode.com/problem/437\\n    \\n    https://www.lintcode.com/problem/319\\n    \\n    https://www.lintcode.com/problem/963\\n    \\n    ## **相向双指针：**\\n    \\n    https://www.lintcode.com/problem/56\\n    \\n    https://www.lintcode.com/problem/57\\n    \\n    https://www.lintcode.com/problem/58\\n    \\n    https://www.lintcode.com/problem/363\\n    \\n    https://www.lintcode.com/problem/539\\n    \\n    https://www.lintcode.com/problem/6\\n    \\n    https://www.lintcode.com/problem/32\\n    \\n    https://www.lintcode.com/problem/521\\n    \\n    https://www.lintcode.com/problem/1870\\n    \\n    https://www.lintcode.com/problem/328\\n    \\n    https://www.lintcode.com/problem/547\\n    \\n    https://www.lintcode.com/problem/406\\n    \\n    ## **宽度优先搜索：**\\n    \\n    https://www.lintcode.com/problem/433\\n    \\n    https://www.lintcode.com/problem/615\\n    \\n    https://www.lintcode.com/problem/630\\n    \\n    https://www.lintcode.com/problem/120\\n    \\n    https://www.lintcode.com/problem/178/\\n    \\n    https://www.lintcode.com/problem/278\\n    \\n    https://www.lintcode.com/problem/787\\n    \\n    ## **二叉树遍历：**\\n    \\n    https://www.lintcode.com/problem/66\\n    \\n    https://www.lintcode.com/problem/67\\n    \\n    https://www.lintcode.com/problem/68\\n    \\n    https://www.lintcode.com/problem/69\\n    \\n    https://www.lintcode.com/problem/73\\n    \\n    https://www.lintcode.com/problem/72\\n    \\n    ## **二叉树＆分治法：**\\n    \\n    https://www.lintcode.com/problem/468\\n    \\n    https://www.lintcode.com/problem/854\\n    \\n    https://www.lintcode.com/problem/596\\n    \\n    https://www.lintcode.com/problem/628\\n    \\n    https://www.lintcode.com/problem/597\\n    \\n    ## **二叉搜索树：**\\n    \\n    https://www.lintcode.com/problem/902\\n    \\n    https://www.lintcode.com/problem/915\\n    \\n    https://www.lintcode.com/problem/85\\n    \\n    https://www.lintcode.com/problem/95\\n    \\n    https://www.lintcode.com/problem/689\\n    \\n    ## **深度优先搜索：**\\n    \\n    https://www.lintcode.com/problem/1909\\n    \\n    https://www.lintcode.com/problem/634\\n    \\n    https://www.lintcode.com/problem/802\\n    \\n    https://www.lintcode.com/problem/652\\n    \\n    https://www.lintcode.com/problem/169\\n    \\n    https://www.lintcode.com/problem/425\\n    \\n    https://www.lintcode.com/problem/33\\n    \\n    ## **坐标型动态规划：**\\n    \\n    https://www.lintcode.com/problem/114\\n    \\n    https://www.lintcode.com/problem/115\\n    \\n    https://www.lintcode.com/problem/1861\\n    \\n    https://www.lintcode.com/problem/1827\\n    \\n    https://www.lintcode.com/problem/76\\n    \\n    https://www.lintcode.com/problem/109\\n    \\n    https://www.lintcode.com/problem/1702\\n    \\n    ## **背包型动态规划：**\\n    \\n    https://www.lintcode.com/problem/669\\n    \\n    https://www.lintcode.com/problem/564\\n    \\n    https://www.lintcode.com/problem/92\\n    \\n    https://www.lintcode.com/problem/1915\\n    \\n    https://www.lintcode.com/problem/1800\\n    \\n    https://www.lintcode.com/problem/125\\n    \\n    https://www.lintcode.com/problem/440\\n    \\n    https://www.lintcode.com/problem/562\\n    \\n    https://www.lintcode.com/problem/563\\n    \\n    https://www.lintcode.com/problem/724', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='325e9258-9079-4b93-9393-54bf24d8328e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n45.\\xa0Jump Game II\\n\\nTags: bfs, dp, greedy\\nnote: 看视频，很像bfs的思路，也是求最小值\\nrep1: No\\nrep2: No\\nrep3: No\\n\\nJump Game II - Greedy - Leetcode 45 - Python - YouTube', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0cd6187b-2c15-44f4-aafe-947993ed8acd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n116. Populating Next Right Pointers in Each Node\\n\\nTags: tree\\nnote: 难度不大，想清楚link怎么连的就行; 思考为何用preorder, 不用别的\\nrep1: Yes\\nRetention: good\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='676f1386-281f-4a72-80b9-fb6e615e16f7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n134.\\xa0Gas Station\\n\\nTags: greedy\\nrep1: No\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d70eeaf8-9165-49b8-a172-fd29840f0001', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n409.\\xa0Longest Palindrome\\n\\nrep1: No\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='045e2382-1404-4f1c-b5f7-ba74338ede71', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n172. Factorial Trailing Zeroes\\n\\nTags: math, recur\\nrep1: Yes\\nRetention: bad\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a6223200-8ccb-40ba-82a6-d46a6a411950', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n147. Insertion Sort List\\n\\nTags: VIP, linked list, sort\\nrep1: Yes\\nrep2: No\\nrep3: No\\n\\nhttps://blog.51cto.com/wdswds/1737499\\n\\nhttps://www.youtube.com/watch?v=-0w2uswTST8\\n\\nhttps://www.youtube.com/watch?v=N1VVLLan6S0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6d83a63d-4396-4cd1-8878-6d768aad8c26', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n122. Best Time to Buy and Sell Stock II\\n\\nTags: VIP, dp\\nnote: 这道题是双变量的dp典型\\nrep1: Yes\\nRetention: mid\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d32df935-14ea-45be-96d3-c8342b3a0adb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n82. Remove Duplicates from Sorted List 2\\n\\nTags: VIP, linked list, two_pointer\\nnote: 如果简单遍历一遍linked list没法完成任务，就可以考虑双指针之类的办法; 双指针经典\\nrep1: Yes\\nRetention: bad\\nrep2: No\\nrep3: No\\n\\n\\n\\nhttps://www.youtube.com/watch?v=ff6LbGhd1AU\\n\\nhttps://www.youtube.com/watch?v=UESeImUsUdw', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a9ecedb4-3600-4378-a707-36504d286bf5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n62. Unique Paths\\n\\nTags: dp\\nnote: memory =[[0] * n for _ in range(m)]; 注意二维list的写法memo[row][col] 不是memo[row, col]\\nrep1: Yes\\nrep2: No\\nrep3: No\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='014c77aa-27db-45e5-98b7-465cb815311d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n763.\\xa0Partition Labels\\n\\nTags: greedy, hash, two_pointer\\nnote: last = {s[i]: i for i in range(L)} 的写法得出每个字母的last position\\nrep1: No\\nrep2: No\\nrep3: No\\n\\nPartition Labels - Leetcode 763 - Python - YouTube\\n\\n!Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='898f2bca-361d-4cf1-b5af-d1142a5465a7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n21. Merge Two Sorted Lists\\n\\nTags: VIP, linked list, recur, two_pointer\\nnote: 递归的方法is better; 掌握如何建立一个空链表，就像建立空list一样\\nrep1: Yes\\nRetention: mid\\nrep2: No\\nrep3: No\\n\\nrecurssion style\\n\\nhttps://www.youtube.com/watch?v=IJDJ0kBx2LM\\n\\n#Confused about dummy nodes in linked list questions : leetcode (reddit.com)\\n\\nA typical python solution looks like this:\\n\\n```\\nclass Solution:\\n    def mergeTwoLists(self, l1, l2):\\n        dummy = ListNode(None)\\n        res = dummy\\n       # why can't I just define res as res = ListNode(None)\\n\\n        while l1 and l2:\\n            if l1.val<= l2.val:\\n                res.next = l1\\n                l1 = l1.next\\n            else:\\n                res.next = l2\\n                l2 = l2.next\\n            res = res.next\\n\\n        if l1:\\n            res.next = l1\\n        if l2:\\n            res.next = l2\\n        return dummy.next\\n\\n```\\n\\n**My question is:**\\xa0why can't I just define\\xa0*res*\\xa0as res = ListNode(None) at the beginning and return\\xa0*res*.next as the output? What's the function of the dummy node here?\\n\\nAlthough the above code works, I also can't understand why. We initiated\\xa0*res*\\xa0as the dummy node, but we did not change the variable\\xa0*dummy*\\xa0at all in the subsequent code. So the variable\\xa0*dummy*\\xa0should remain as ListNode(None) the entire time, and we should return res.next instead of\\xa0*dummy*.next. Then why do we return\\xa0*dummy*.next at the end?\\n\\nTo illustrate better, I think the code above is doing something similar to the below example, which does not make much sense to me\\n\\n```\\na = 3\\nb = a\\nb = b+2  #or any code that changes something in b, but does not change a\\nreturn a # and the output turns out to be a =5, which is a wrong answer\\n\\n##############################\\nThe above linked list did similar things:\\n\\ndummy = ListNode(None)\\nres = dummy\\n\\nsome other code #The while loop that did something to res, but did nothing to dummy\\n\\nreturn dummy\\n#how can we possibly get a dummy that is not None, since the while loop did nothing to it\\n```\\n\\nHere the variable dummy has a listnode obj. Which is same as res before the while loop starts. During the first loop iteration res.next gets updated which means dummy.next also gets updated because both res and dummy points to the same listnode. When the loop ends res has been moved to other nodes. So we return the first node ie. Dummy.next.\\n\\nHere dummy and res are reference to the class objects. Check the first answer in\\xa0this\\n. For some reason we can't have reference for an nteger in python, whereas classes in python are by default referenced.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7dd0b9ea-278d-46d8-8a32-dec9f2c8cde8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n205.\\xa0Isomorphic Strings\\n\\nTags: hash, string\\nrep1: No\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a69c5975-da99-43f5-aa0c-539d6966e4f0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n100. Same Tree\\n\\nTags: recur, tree\\nnote: 在每个node上，比较当前node，和他的left right child是不是一样\\nrep1: Yes\\nRetention: perfect\\nrep2: Yes\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1e1a960a-0c8a-484c-b2f8-dc7edfafe56f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n684.\\xa0Redundant Connection\\n\\nTags: bfs, dfs, graph\\nrep1: No\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='10a08a47-380a-4d7a-98e7-4415ce360b93', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n344. Reverse String\\n\\nTags: recur, string, two_pointer\\nnote: 所有可以从前后两端考虑的问题，都可以用双指针; left < right 就是一个极佳的stopping condition，比recurssion的stop condition容易理解; 这道题的recur 写法有点binary search的思想在里面\\nrep1: Yes\\nRetention: mid\\nrep2: Yes\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a53abb9b-9e13-4c64-b580-98b22cc7db7d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n714. Best Time to Buy and Sell Stock with Transaction Fee\\n\\nTags: dp\\nnote: 这道题和122是双变量的dp典型\\nrep1: No\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='28ecb034-71fa-430d-b577-94eed6870a9d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n190. Reverse Bits\\n\\nTags: string\\nnote: 把数字变成string的常用方法\\nrep1: No\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d2802f3e-239e-4dca-8185-ca9e41ae83a9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n46. permutations\\n\\nTags: VIP, backtracking\\nnote: 把code和树状图结合起来看就容易理解了; # 回溯法的含义是对每个可能的结果进行遍历，如果某个数字已经使用则跳过，如果没有使用则放入path中。这个“回溯”怎么理解？我认识是在递归的过程中使用了一个数组path来保存自己走过的路，# 如果沿着这条路走完了全部的解，则需要弹出path中的最后一个元素，相当于向后回退，于是叫做回溯法。\\nrep1: No\\nrep2: No\\nrep3: No\\n\\n[[my own note ]]\\n\\nmy own note \\n\\nhttps://www.youtube.com/watch?v=KukNnoN-SoY\\n\\n!Untitled\\n\\n!Untitled\\n\\nPermutations | Leet code 46 | Theory explained + Python code - YouTube\\n\\n!Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9ea53413-cc77-4eb8-8510-2bc059e97292', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n20. Valid Parentheses\\n\\nTags: VIP, stack/queue, string\\nnote: continue的用法要掌握; 用stack的题目，往往都是需要随着循环把前面的东西remove掉，露出顶部的\\nrep1: Yes\\nRetention: mid\\nrep2: Yes\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='54ee71d7-7e03-43b3-b976-8efe3a4ac796', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n135. Candy\\n\\nTags: greedy\\nnote: greedy的题好多都要分情况讨论\\nrep1: No\\nrep2: No\\nrep3: No\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1d7558b4-8ff7-4baf-acb8-8400803b5e02', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n860.\\xa0Lemonade Change\\n\\nTags: array, greedy\\nnote: 没必要用哈希表来做counter，随便用一个int做counter也行，反正最后是判断这个counter是否大于零\\nrep1: No\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='42b22352-12cc-49a4-b4cc-b97c4f098ded', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n88 Merge Sorted Array\\n\\nTags: VIP, array, sort, two_pointer\\nnote: 为了不打乱前面的顺序，从list末尾开始往前面填充；If you simply consider one element each at a time from the two arrays and make a decision and proceed accordingly, you will arrive at the optimal solution.； #VIP nums1[:]值整个list的每个值\\nrep1: Yes\\nRetention: mid\\nrep2: No\\nrep3: No\\n\\nhttps://zhuanlan.zhihu.com/p/124356219\\n\\n**2. 算法步骤**\\n\\n1. 申请空间，使其大小为两个已经排序序列之和，该空间用来存放合并后的序列；\\n2. 设定两个指针，最初位置分别为两个已经排序序列的起始位置；\\n3. 比较两个指针所指向的元素，选择相对小的元素放入到合并空间，并移动指针到下一位置；\\n4. 重复步骤 3 直到某一指针达到序列尾；\\n5. 将另一序列剩下的所有元素直接复制到合并序列尾。\\n\\n归并排序; 分治法\\n\\n我们需要三个指针：\\n写指针 current， 用于记录当前填补到那个位置了\\nm 用于记录 nums1 数组处理到哪个元素了\\nn 用于记录 nums2 数组处理到哪个元素了\\n如图所示：\\n灰色代表 num2 数组已经处理过的元素\\n红色代表当前正在进行比较的元素\\n绿色代表已经就位的元素\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='79b8d54e-d380-4e88-a0ea-9575caa4f9d6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**实例分析**\\n\\n假设现在有输入数组 A{6,5,3,1,8,7,2,4}.\\n\\n先第一步，递**归**分解成一个个子数组。\\n\\n将输入数组分解成一个个单一元素组成的子数组：6,5,3,1,8,7,2,4.\\n\\n再第二步，将分解出来的子数组合**并**成一个排好序的数列。\\n\\n两两对比各个子数组，6和5比，6比5大，所以{5,6}；3和1比，3比1大，所以{1,3}；8和7比，8比7大，所以{7,8}；2和4比，2比4小，所以{2,4}.\\n\\n接着再对比新的子数组，按照子数组中的元素大小进行排列合并成新的子数组。{5,6}和{1,3}进行对比，里面的元素按照大小排列好合并成新的子数组{1,3,5,6}；接着{7,8}和{2,4}进行对比，里面的元素按照大小排列好合并成新的子数组{2,4,7,8}.\\n\\n然后对比新的子数组，按照子数组中的元素大小进行排列合并成新的子数组。{1,3,5,6}和{2,4,7,8}中的元素进行大小对比，再合并成新的数组{1,2,3,4,5,6,7,8}.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7adae6e5-d376-4b8c-9431-67d1d4d28bd5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n337. House Robber III\\n\\nTags: VIP, tree\\nrep1: No\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6d850600-9e7e-4cdb-950e-af506f50aaf8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n139. Word Break\\n\\nTags: dp\\nnote: 典型的双重循环 dp\\nrep1: Yes\\nRetention: bad\\nrep2: Yes\\nrep3: Yes\\n\\nhttps://zxi.mytechroad.com/blog/leetcode/leetcode-139-word-break/\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1c00fc41-b834-415e-a349-2090d00a8db7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n15.\\xa03Sum\\n\\nTags: VIP, array, sort, two_pointer\\nnote: sort 之后用two pointer，就可以让搜索两次n^2 变为搜索一次; tuple is essential to use set on list of list \\nrep1: No\\nrep2: No\\nrep3: No\\n\\nPython Programming Practice: Leetcode #15 -- 3Sum - YouTube\\n\\npython - How to overcome TypeError: unhashable type: 'list' - Stack Overflow\\n\\n!Untitled\\n\\n```python\\ndef threeNumberSum(array, targetSum):\\n    # Write your code here.\\n    res = []\\n    array = sorted(array)\\n    for i in range(len(array)):\\n        l = i+1\\n        r = len(array)-1\\n        while l<r:\\n            if array[i]+array[l]+array[r]==targetSum:\\n                res.append([array[i], array[l], array[r]])\\n                l+=1\\n            elif array[i]+ array[l]+ array[r]>targetSum:\\n                r-=1\\n            else:\\n                l+=1\\n        print(res)\\n    return res\\n```\\n\\nHint 2\\nnumber, the left number, and the right number sum up to the target sum. How can you proceed from there, remembering the fact that you sorted the array?\\nTry sorting the array and traversing it once. At each number, place a left pointer on the number immediately\\nto the right of your current number and a right pointer on the final number in the array. Check if the current\\nHint 3\\nSince the array is now sorted (see Hint #2), you know that moving the left pointer mentioned in Hint #2 one place to the right will lead to a greater left number and thus a greater sum. Similarly, you know that moving the right pointer one place to the left will lead to a smaller right number and thus a smaller sum. This means that, depending on the size of each triplet's (current number, left number, right number) sum relative to the target sum, you should either move the left pointer, the right pointer, or both to obtain a potentially valid triplet.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7f40b423-a64f-4300-ae3b-b646b9bd9424', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n276 paint fence\\n\\nTags: VIP, dp\\nnote: dp 入门题目\\nrep1: Yes\\nRetention: bad\\nrep2: Yes\\nrep3: No\\n\\n!Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='52e1b917-3ccd-472a-81b8-6788457aaac6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n438.\\xa0Find All Anagrams in a String\\n\\nTags: VIP, array, hash, sliding_window\\nnote: vip用hashmap的话可以避免先求出所有的anagram; Counter的用法； 注意scount.pop(s[l]) 用来remove a key from a dict\\nrep1: No\\nrep2: No\\nrep3: No\\n\\nfind all anagrams in a string python | find all anagrams in a string leetcode python | leetcode 438 - YouTube\\n\\n!Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2e3715e5-877d-40e6-9db3-8f9eef12cdcc', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n973. K Closest Points to Origin\\n\\nTags: VIP, array, hash, heap\\nnote: heap 只要On, sort takes Onlogn\\nrep1: No\\nrep2: No\\nrep3: No\\n\\nhttps://www.youtube.com/watch?v=G9VcMTSZ1Lo&list=PL6i_0cc-sEeLfUzEutrYpwMVlXq1Ejni\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a7c71ea7-c0f8-44cd-a2de-acf2c0bee13b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n167.\\xa0Two Sum II - Input Array Is Sorted\\n\\nTags: two_pointer\\nnote: 如果是按大小排列，似乎用two pointer就很合理\\nrep1: No\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c1aac92f-2cb1-4a76-a6e0-b889c2c34de6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n416.\\xa0Partition Equal Subset Sum\\n\\nTags: dp\\nrep1: No\\nrep2: No\\nrep3: No\\n\\n!Untitled\\n\\nPartition Equal Subset Sum | Leet code 416 | Theory explained + Python code - YouTube\\n\\n!Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='43a4be86-102b-40a5-be5a-71f7e5273a56', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n863.\\xa0All Nodes Distance K in Binary Tree\\n\\nTags: VIP, bfs, dfs, tree\\nnote: defaultdict 的用法是关键 https://stackoverflow.com/questions/5900578/how-does-collections-defaultdict-work; has to create a graph first because we need to traverse parent nodes； vip is creating a graph from a tree, 彻底记住这个操作\\nrep1: No\\nrep2: No\\nrep3: No\\n\\n!Untitled\\n\\nAll Nodes Distance K In A Binary Tree - Performing Bidirectional Search On A Tree Using A Hashtable - YouTube\\n\\nComplexities\\n\\nn = total amount of nodes in the binary tree\\nm = total edges\\n\\nTime: O( n + m )\\n\\nThis is standard to Breadth First Search. We upper bound the time by the number of nodes we can visit and edges we can traverse (at maximum).\\n\\nSpace: O( n )\\n\\nWe have a hashtable upper bounded by n mappings, a mapping to each node's parent.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b74d3e5e-dae8-45a8-bca3-1906fdd6172a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n110. Balanced Binary Tree\\n\\nTags: VIP, recur, tree\\nnote: check my own code to see how to reduce time complexity；学会这个-1的写法\\nrep1: Yes\\nRetention: bad\\nrep2: Yes\\nrep3: No\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='524841fc-9ced-4c37-89d2-f2a2f796cd6a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nTags: hash, string\\nnote: dic.get 的用法要掌握，用来取代counter\\nrep1: Yes\\nRetention: perfect\\nrep2: Yes\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ac1364f8-f21a-4565-85b7-4f8a385e925a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n383.\\xa0Ransom Note\\n\\nTags: hash\\nrep1: No\\nRetention: perfect\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bab4563e-2dce-4b7e-9892-48644701e3e5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n83. Remove Duplicates from Sorted List\\n\\nTags: linked list\\nrep1: Yes\\nRetention: good\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='085b365f-3cdc-4d16-972b-14230f696cf5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n234. Palindrome Linked List\\n\\nTags: linked list, two_pointer\\nnote: linked list放到list里来解\\nrep1: Yes\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bd94ace4-093d-4062-b8d8-0fe9947a3d60', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n171. Excel Sheet Column Number\\n\\nTags: math\\nnote: 用类似二进制的操作，把A, AA, AAA都转化成数字; ord 的用法； 循环的写法；本质上要得出基于s每个字符加一位之后的递推公式就能写出来了，得出通项公式没用\\nrep1: Yes\\nRetention: bad\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='11a4011a-d1c4-4f22-be8b-87a9049665a7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n237. Delete Node in a Linked List\\n\\nTags: linked list\\nrep1: Yes\\nRetention: bad\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cc3935a0-e5a5-40b8-a4ac-711abe87a02f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n217_Contains_Duplicate\\n\\nTags: hash\\nnote: 要掌握set add的用法: set\\n.add\\n(elem)\\nrep1: Yes\\nRetention: perfect\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='33e7d2da-75ea-4a34-bdfc-69fb5f68f03a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n51. N-Queens\\n\\nTags: backtracking\\nrep1: No\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='42096e18-1e85-4b23-b8a3-8f832d3e481f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n240. Search a 2D Matrix II\\n\\nTags: binary search\\nnote: 为何这题不需要mid了；选取右上角或者左下角开始循环，才可以做到一增一减\\nrep1: Yes\\nRetention: mid\\nrep2: No\\nrep3: No\\n\\n为何这题不要mid了？如果用mid的话，只会从每一行的中点，搜索到末尾；前半部分搜索不到；因为这题有两个维度，所以每个数都必须搜到，不能每次cut half，可能会在另一个维度遗漏\\n\\n二分查找的本质是，想要找到等于target的数，直接看范围里最中间的数字，如果比target大或小，就可以排除掉一半的范围', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='264742fa-2811-4ff0-9df3-54f775a6ed87', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n1146.\\xa0Snapshot Array\\n\\nTags: array, binary search, design, hash\\nrep1: No\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='213d8018-013a-4836-95d8-8f771871e278', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n111. Minimum Depth of Binary Tree\\n\\nTags: recur, tree\\nrep1: Yes\\nRetention: mid\\nrep2: Yes\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f4e5e063-ce78-496d-a2b1-15c01e22a808', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n445. Add Two Numbers II\\n\\nTags: linked list\\nnote: 掌握linked list的反向遍历方法\\nrep1: No\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='66f1942e-2c9c-4e14-8567-add21b083e3a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n455 assign cookies\\n\\nTags: VIP, greedy\\nnote: 贪心问题的特点是，前面的决策会影响后面的，比如这题，被用过的饼干就不能再用来满足后面的kid了，所以要保证前面决策的时候就是可以实现全局最优的局部解; 每一步的最优解一定包含上一步的最优解; 注意break 的用法; 注意g.sort()，不是g = g.sort()\\nrep1: Yes\\nRetention: bad\\nrep2: No\\nrep3: No\\n\\n!Untitled\\n\\n贪心算法：\\n1.贪心算法中，作出的每步贪心决策都无法改变，因为贪心策略是由上一步的最优解推导下一步的最优解，而上一步之前的最优解则不作保留；\\n2.由（1）中的介绍，可以知道贪心法正确的条件是：每一步的最优解一定包含上一步的最优解。\\n\\n动态规划算法：\\n1.全局最优解中一定包含某个局部最优解，但不一定包含前一个局部最优解，因此需要记录之前的所有最优解；\\n2.动态规划的关键是状态转移方程，即如何由以求出的局部最优解来推导全局最优解；\\n3.边界条件：即最简单的，可以直接得出的局部最优解\\n\\n!Untitled\\n\\n!Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bd618f8f-cda5-4017-a233-b78f8b4cea44', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n1005.\\xa0Maximize Sum Of Array After K Negations\\n\\nTags: array, greedy\\nnote: 巧用index找到min value，不用重复reorder\\nrep1: No\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d875e609-c9f4-427d-befb-cbdbf9bd12cc', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n362. Design Hit Counter\\n\\nrep1: No\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e9ea6291-925b-4ac4-8ca9-1bc042ab7bf6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n107. Binary Tree Level Order Traversal II\\n\\nTags: VIP, linked list, stack/queue, tree\\nnote: 注意update current level这种循环方式; 注意循环的层级; result[::-1]逆向list ; 递归和循环都要掌握\\nrep1: Yes\\nRetention: mid\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e87b222c-5111-4329-8af4-d4ae463f8700', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n1046.\\xa0Last Stone Weight\\n\\nTags: VIP, heap\\nrep1: No\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2b0dece4-6a66-45c0-8498-0828ac1200c0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n733.\\xa0Flood Fill\\n\\nTags: bfs, dfs, matrix, recur\\nnote: edge cases; # 注意这题不像 200 number of island那样，要loop 整个matrix，只需要loop 和起始点相邻的island就够了\\nrep1: Yes\\nRetention: mid\\nrep2: No\\nrep3: No\\n\\nBreadth First Search (BFS): Visualized and Explained - YouTube\\n\\n!Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='78ee1f02-6f2d-42c9-aeb5-329be502900a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n113.\\xa0Path Sum II\\n\\nTags: VIP, backtracking, dfs, recur, tree\\nrep1: No\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ef99832e-9af5-458f-b67c-1d0e7885b4b6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n36.\\xa0Valid Sudoku\\n\\nTags: hash, matrix\\nnote: 注意set的写法：collections.defaultdict(set)\\nrep1: No\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f850469c-7b55-4430-a8f8-0ed13473f8e6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n347.\\xa0Top K Frequent Elements\\n\\nTags: array, hash, heap\\nnote: dd= sorted((dic[ip], ip) for ip in dic)  #VIP how to sort dic by their keys\\nrep1: No\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b2df65b6-6be3-4d71-9950-ec5d66754848', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n124.\\xa0Binary Tree Maximum Path Sum\\n\\nTags: VIP, dfs, tree\\nnote: vip的是这种，借助一个辅助函数的循环，在辅助函数里update res的思路；这个思路和110 check for balance binary tree 一样 \\nrep1: No\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8c55e020-f8da-4b53-a5ba-77b90669df3d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n206. Reverse Linked List\\n\\nTags: VIP, linked list, recur\\nrep1: Yes\\nRetention: mid\\nrep2: No\\nrep3: No\\n\\n注意这不是顺序执行，是平行执行！！！所以head.next = dummy.next 这里的赋值，用的是还没变过的dummy.next = None\\n\\n\\n\\n背下来；reverse linked list的基本操作\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fc476bf9-0890-4aa1-b3f2-be9c1ab206f2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n关于recurring 的写法\\n\\n```python\\nclass Solution:\\n    def reverseList(self, head: ListNode) -> ListNode:\\n        if not head or not head.next:\\n            return head\\n        tmp = head.next\\n        pre = self.reverseList(head.next)\\n        tmp.next = head\\n        head.next = None\\n        return pre\\n```\\n\\n[[206_reverse__LL.pdf]]', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f5b8a813-775b-4b36-8f7b-a96e1c415e80', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n57.\\xa0Insert Interval\\n\\nTags: VIP\\nnote: 先想res是什么样子的，再想怎么组合出res来；extend is the key; res.extend(intervals[i:])  # equal to for j in range(i, len(intervals)):  res.append(intervals[j])\\nrep1: Yes\\nrep2: No\\nrep3: No\\n\\n`append`\\xa0appends object at the end.\\n\\n```\\n>>> x = [1, 2, 3]\\n>>> x.append([4, 5])\\n>>> print(x)\\n[1, 2, 3, [4, 5]]\\n\\n```\\n\\n`extend`\\xa0extends list by appending elements from the iterable.\\n\\n```\\n>>> x = [1, 2, 3]\\n>>> x.extend([4, 5])\\n>>> print(x)\\n[1, 2, 3, 4, 5]\\n```', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c77abea0-5380-4566-ac3d-3aed78e9fba7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n207.\\xa0Course Schedule\\n\\nTags: bfs, dfs, graph\\nnote: topological sort\\nrep1: No\\nrep2: No\\nrep3: No\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\nLeetCode 207.\\xa0Course Schedule | DFS | Visualization | Python - YouTube\\n\\nTopological Sort Algorithm | Graph Theory - YouTube\\n\\n!Untitled\\n\\n花花酱 LeetCode 207. Course Schedule - 刷题找工作 EP93 - YouTube', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='34dd86b1-bbd7-4ad8-b950-42ce01bcf129', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n981.\\xa0Time Based Key-Value Store\\n\\nTags: VIP, binary search, hash\\nnote: VIP不要用普通的dic要用defaultdict; use turple to store pairs of values; 字典get的用法; 字典里一个key是可以对应好几个值的！！！\\nrep1: No\\nrep2: No\\nrep3: No\\n\\n标准的****寻找最右插入位置题型****\\n\\n[[]] at master · azl397985856/leetcode [[]]', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='409b2ada-ade4-4d28-93cb-139ca05f54ad', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n933.\\xa0Number of Recent Calls\\n\\nTags: design, stack/queue\\nrep1: No\\nrep2: No\\nrep3: No', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents1_clean = []\n",
    "for doc in documents1:\n",
    "    if doc.text == '':\n",
    "        continue\n",
    "    else:\n",
    "        documents1_clean.append(doc)\n",
    "documents1_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='0ff7fe14-1944-4d74-b6af-e1eeebaf4be3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**我的business sense 有多好**：\\n\\nmy mentor says she has a project relevant to marketing. that I find the three promising suggestions that she had worked for more than a month\\n\\nHow to be different from forbidden city; where to find the breakthrough point\\n\\nFrom category view: mature category, use design to be unique; niche category, which criteria to have to select category; from consumer view;\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents1_clean[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n**我的business sense 有多好**：\\n\\nmy mentor says she has a project relevant to marketing. that I find the three promising suggestions that she had worked for more than a month\\n\\nHow to be different from forbidden city; where to find the breakthrough point\\n\\nFrom category view: mature category, use design to be unique; niche category, which criteria to have to select category; from consumer view;\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents1_clean[500].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nmentorship you’ve given and received\\n\\n**given**: research assistants and lower grade phd students;\\n\\nsituation: they work for me as volunteer; no pay\\n\\ntask: make them feel rewarding and learn sth; its my responsibility\\n\\naction: 1. diverse task: try not put repetitive work to the same person; give my best guidance 2. they want to learn data analysis; organized workshop; organized slack channel, and answer Qs on the channel every few days; 3. I also helped them with grad school application etc.\\n\\nresult: they learned how to use R because they work for me; they refer their friends to work as my RA; very good relationship; ; we were still in contact; one Indian student become a data science guy after graduation; she said she was so afraid of math and coding before the workshop;\\n\\nmy professor is still using my slides and video to train his student in R\\n\\n**received: CTO and other colleagues;** \\n\\nsituation: tech stack and infrastructure; git pull request\\n\\ntask: learn about how to work as an engineer \\n\\n**Naming of things** \\n\\n`module_name`,\\xa0`package_name`,\\xa0`ClassName`,\\xa0`method_name`,\\xa0`ExceptionName`,\\xa0`function_name`,\\xa0`GLOBAL_CONSTANT_NAME`,\\xa0`global_var_name`,\\xa0`instance_var_name`,\\xa0`function_parameter_name`,\\xa0`local_var_name`,\\xa0`query_proper_noun_for_thing`,\\xa0`send_acronym_via_https`.\\n\\naction: 1. we discussed what he think I should learn; what I think I want to learn; we pick the overlap like docker, git, how to do code review etc. Also there are things that we didnt expected but I still found out need to learn; like naming of things; \\n\\n1. I read books, videos, list my questions on a 1on1; I ask samples from him; which project on our git has the docker module; I ask at our engineer team channel if I don’t understand something specific about a project;  \\n2. learn from mistake: large file git made an error and cleaned previous git history; he fixed it and expalined to me why it happened; I created a dummy git repo myself, everything I need to do I do it at my fake repo first; \\n\\nresult: I can work as an engineer with others; pretty confident with git; reviewed other people’s python code \\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents1_clean[501].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n****What\\'s your favorite product and why?****\\n\\n\"What do you mean by favorite product? Are you thinking specifically hardware, software, or a feature within those, or something non-electronic?\\xa0**Dealer\\'s Choice.**\\n\\n\"Are you asking why I love this product, or to explain why this product is a market leader independent of how i feel about it?\\xa0**Talk about why YOU love this product**.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents1_clean[502].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38991 1167\n"
     ]
    }
   ],
   "source": [
    "longest_doc = 0\n",
    "longest_index = 0\n",
    "long_docs = []\n",
    "for index, item in enumerate(documents1_clean):\n",
    "    if len(item.text) > longest_doc:\n",
    "        longest_doc = len(item.text)\n",
    "        longest_index = index\n",
    "\n",
    "print(longest_doc, longest_index)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_docs = []\n",
    "for index, item in enumerate(documents1_clean):\n",
    "    if len(item.text) > 10000:\n",
    "        long_docs.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='0d11cb32-05c0-4f1e-bee5-ce3e31c83c37', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Experiment Design**\\n\\n- What do you mean by A/B testing?\\n    - https://dimensionless.in/data-science-interview-questions-with-answers/\\n    - As well as being perhaps the most accurate tool for estimating effect size (and therefore ROI), it is also able to provide us with causality, a very elusive thing in data science! With causality we can finally lay to rest the “correlation vs causation” argument, and prove that our new product actually works.\\n    - https://towardsdatascience.com/data-science-you-need-to-know-a-b-testing-f2f12aff619a\\n    - Type I error\\u200a—\\u200aor falsely concluding that your intervention was successful (which here might be falsely concluding that layout B is better than Layout A). Also known as a false positive result.\\n    - Type II error\\u200a—\\u200afalsely concluding that your intervention was not successful. Also known as a false negative result.\\n    - http://www.cs.cornell.edu/courses/cs578/2006fa/design.html\\n    - https://www.analyticsvidhya.com/blog/2017/05/41-questions-on-statisitics-data-scientists-analysts/\\n- Which of the following measures of central tendency will always change if a single value in the data changes?\\n    - Mean, median, mode, or all?\\n    - A - The mean of the dataset would always change if we change any value of the data set. Since we are summing up all the values together to get it, every value of the data set contributes to its value. Median and mode may or may not change with altering a single value in the dataset.\\n- What does it mean for a result to be statistically significant?\\n    - https://www.quora.com/What-kind-of-A-B-testing-questions-should-I-expect-in-a-data-scientist-interview-and-how-should-I-prepare-for-such-questions\\n    - Statistically significant is the likelihood that a relationship between two or more variables is caused by something other than chance.\\n    - Statistical hypothesis testing is used to determine whether the result of a data set is statistically significant.\\n    - This test provides a p-value, representing the probability that random chance could explain the result; in general, a p-value of 5% or lower is considered to be statistically significant.\\n    - Statistical significance is used to accept or reject the null hypothesis, which hypothesizes that there is no relationship between measured variables. A data set is statistically significant when the set is large enough to accurately represent the phenomenon or population sample being studied. A data set is typically deemed to be statistically significant if the probability of the phenomenon being random is less than 1/20, resulting in a p-value of 5%. When the test result exceeds the p-value, the null hypothesis is accepted. When the test result is less than the p-value, the null hypothesis is rejected.\\n- What is a confidence interval?\\n    - In statistics, a confidence interval (CI) is a type of interval estimate, computed from the statistics of the observed data **that might contain the true value of an unknown population parameter.**\\n    - The interval has an associated confidence level that, loosely speaking, quantifies the level of confidence that the parameter lies in the interval. More strictly speaking, the confidence level represents the frequency (i.e. the proportion) of possible confidence intervals that contain the true value of the unknown population parameter.\\n    - In other words, if confidence intervals are constructed using a given confidence level from an infinite number of independent sample statistics, the proportion of those intervals that contain the true value of the parameter will be equal to the confidence level.\\n    - https://en.wikipedia.org/wiki/Confidence_interval\\n    - \\n    - [[diff in diff and instrumental variables IV]]\\n- What are instrumental variables and why are they important for experiment design?\\n    - https://www.quora.com/What-are-some-data-science-interview-questions-Do-they-include-canonical-algorithm-questions-such-as-search-graphs-data-structures-etc\\n    - In statistics, econometrics, epidemiology and related disciplines, the method of instrumental variables (IV) is used to estimate causal relationships when controlled experiments are not feasible or when a treatment is not successfully delivered to every unit in a randomized experiment.[1] Intuitively, IVs are used when an explanatory variable of interest is correlated with the error term, in which case ordinary least squares and ANOVA give biased results. A valid instrument induces changes in the explanatory variable but has no independent effect on the dependent variable, allowing a researcher to uncover the causal effect of the explanatory variable on the dependent variable.\\n    - Instrumental Variables regression (IV) basically splits your explanatory variable into two parts: one part that could be correlated with ε and one part that probably isn’t. By isolating the part with no correlation, it’s possible to estimate β in the regression equation:Yi = β0 + β1Xi + εi.\\n    - https://www.statisticshowto.datasciencecentral.com/experimental-design/confounding-variable/\\n- When building a recommender system, how do you know if your model is working? Is there a rigorous way to test your model?\\n    - Is there a more rigorous —> number associating with match?\\n        - Manually rate random matches then see how ranks compare\\n            - Compare to random guessing\\n            - Benchmark is almost always random guessing\\n        - Take a list of 10 sentences then 1 test sentence —> then rank those sentences —> “is the best match pair of sentences in the top 3 of what the model picked out” —> the top x and top n in recommender systems\\n            - Getting creative with measuring the success of your model?\\n            - As a user what matters to me!\\n\\n- What proportion is more than 2.0 standard deviations from the mean?\\n    - 95.45%\\n- What proportion is between 1.25 and 2.1 standard deviations above the mean?\\n    - \\n        \\n        https://lh3.googleusercontent.com/PaFTE14_OF-SQGzoRpiJxtA2k8-dza1aisdEhj9arcJlCerAK19KQe2DLDYd7olEdTuU9kqrPBAbb6VRv9_9ZRCu5U4qTMgOIhsXhR9iYmIh5vJ3GGWd-JEYkumX1h_V9nmx_K6T\\n        \\n- What term refers to the standard deviation of the sampling distribution?\\n    - The standard error (of the mean) is the standard deviation of the sampling distribution from the sample mean.\\n- What is the shape of the sampling distribution of r? In what way does the shape depend on the size of the population correlation?\\n    - The shape of the sampling distribution of r is usually (negatively) skewed, unless r is 0. The reason for this is that r cannot take on values greater than 1.0 or less than -1.0, so its distribution cannot extend as far in one direction as it can in the other direction. The greater the value of p, the more pronounced the skew of r’s distribution. To convert Pearson’s r to a value that’s normally distributed, we can use a transformation called Fisher’s z transformation.\\n    - The shape of the sampling distribution of r is negatively skewed. Higher the size of the population correlation, more pronounced the skew.\\n    - http://onlinestatbook.com/2/sampling_distributions/samp_dist_r.html\\n- When solving problems where you need the sampling distribution of r, what is the reason for converting from r to z\\'? (relevant section)\\n    - The reason we convert r to z’ is to transform the variables into ones that have a normal distribution. Without a normal distribution, it would be very difficult to compute probabilities and the standard errors of the sampling distribution of r. Thus, by using the Fisher method, we can transform r into a variable that is normally distributed and that has a known standard error of and is thus much easier to work with.\\n- What is the p-value?\\n    - When we execute a hypothesis test in statistics, a p-value helps us in determine the significance of our results. These Hypothesis tests are nothing but to test the validity of a claim that is made about a population. A null hypothesis is a situation when the hypothesis and the specified population is with no significant difference due to sampling or experimental error.\\n    - https://www.educba.com/statistics-interview-questions/\\n- When should you use a t-test vs a z-test ?\\n    - https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/hypothesis-testing/t-score-vs-z-score/\\n    - Use a t-test when you have a smaller sample, N<30 , and a z-test when you have a larger sample\\n    - Both are used for hypothesis testing, but for a z-test, you don’t need to know the SD of your population.\\n    - \\n        \\n        https://lh3.googleusercontent.com/Ce7-NfnjU3Vl8PuHHqP0jJFNMySGyZp7OlUKzya7B-573ZKUPOBvtrY3_xLHURLGXnwmYB2dRp2svXLtjRCFbf33_0w9gvJJgcBdevlND2Pm48Cb7A7JmDGHdnawjvqV2j9w8BCF\\n        \\n- How is power defined in hypothesis testing?\\n    - power is the probability of correctly rejecting the null hypothesis. We’re typically only interested in the power of a test when the null is in fact false. This definition also makes it more clear that power is a conditional probability: the null hypothesis makes a statement about parameter values, but the power of the test is conditional upon what the values of those parameters really are.\\n    - https://apcentral.collegeboard.org/courses/ap-statistics/classroom-resources/power-in-tests-of-significance\\n    - \\n        \\n        https://lh5.googleusercontent.com/52nYhqos80JydcvTvawu5s2B-RIbgAocPU9v7pFYWdsM9RT-MJwCCWnNbyvyZNeaiBoxRjewjiCth27SZnNvYJMSj4VmCKKoF69bD3QNz9rwKWz7aF7K-2DMy__W0VblV5IiHv09\\n        \\n    - Power is the probability of rejecting the null hypothesis when in fact it is false.\\n    - Power is the probability of making a correct decision (to reject the null hypothesis) when the null hypothesis is false.\\n    - Power is the probability that a test of significance will pick up on an effect that is present.\\n    - Power is the probability that a test of significance will detect a deviation from the null hypothesis, should such a deviation exist.\\n    - Power is the probability of avoiding a Type II error.\\n- What is the difference between data science, ML and AI?\\n    - In computer science, **artificial intelligence** (**AI**), sometimes called **machine intelligence**, is intelligence demonstrated by machines, in contrast to the **natural intelligence** displayed by humans and other animals. Computer science defines AI research as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.[1] More specifically, Kaplan and Haenlein define AI as “a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation”.[2]Colloquially, the term \"artificial intelligence\" is used to describe machines that mimic \"cognitive\" functions that humans associate with other human minds, such as \"learning\" and \"problem solving\".[3]\\n    - Understanding the nature of intelligence in general\\n    - https://en.wikipedia.org/wiki/Artificial_intelligence\\n    \\n    !Untitled\\n    \\n    为了绕开观测数据因果推断的问题，我们引入了准实验。从目前因果推断整体的分析框架中可以看到准实验所处的位置，左图包含实验数据和观测数据的因果推断。其中，在观测数据的因果推断中，我们会优先看数据是否满足DID（Differences In Difference，双重差分）、工具变量和断点回归的前提要求。如果满足，会优先使用这三种方法；如果不满足，才会使用PSM（Propensity Score Matching，倾向评分匹配）和混淆PSM方法。这种优先级的原因是相比于PSM，前三种方法绕开了混杂因子，这是唯一的也是最重要的区别。因此它们依赖的假设在业务层面更容易得到满足，同时也很容易被检验，这样的结论也更容易被信服。我们把上面的三种方法称为准实验方法\\n    \\n    DID在腾讯看点中是一个常用的方法，我们用DID发现了在极端天气下，天气资讯对用户留存的影响。去年8月6号，是台风黑格比经过的时间，我们希望在这样极端的天气下，推送天气的咨询是否能提升用户留存。\\n    \\n    对于这个问题，我们首先想到如下实验：\\n    \\n    - 实验组：8月6号曝光天气的用户\\n    - 对照组：8月6号未曝光天气的用户\\n    \\n    结论：曝光天气的用户次留相比于未曝光天气的用户次留高了20%。\\n    \\n    事实上，这个结论肯定是错误的。因为曝光天气和未曝光天气这两组用户本身就不平衡，因为我们通常是给活跃用户曝光。因此，这样得到的结论是带有混淆偏差的。\\n    \\n    因此，我们又想到如下实验：\\n    \\n    - 实验组：前期未曝光天气，8月6号曝光天气的用户作为实验组\\n    - 对照组：前期未曝光天气，8月6号未曝光天气的用户作为对照组\\n    \\n    结论：曝光天气的用户相比于未曝光天气的用户在受到干预之后，次留扩大了1.4%\\n    \\n    基于上述结论，我们判断天气内容的曝光对次留是有因果效应的。为什么说这就是因果效应呢？双重差分中，第一层差分指的是实验组和对照组在实验前后的差异，我们在右上图看到了实验前的平行性是满足的，可以认为混淆变量对实验组和对照组的第一重差分是相等的，那么影响第二重差分（实验组和对照组差分的差分）的因素就只有干预本身了。因此，我们可以通过二次差分得到一个因果效应，也就是这里的1.4%。\\n    \\n    !Untitled\\n    \\n    针对这类问题，我们提出一套通用的观测数据因果推断分析方式来给出答案。我们主要关注三个问题，第一个问题启动重置对下一次的使用有没有影响？\\xa0第二个问题 一段时间的启动重置下来对用户的未来的打开次数，活跃，收入是否有影响？\\xa0前两个问题解决后，我们关心是否存在部分人群能够既不影响体验，又不影响收入增加和其他功能的导流。这三个问题又称为短期影响、长期影响和用户异质性分析。\\n    \\n    考虑前面给出的分析框架，我们发现都有相应的解法。\\n    \\n    **①\\xa0短期影响**\\n    \\n    由于用户是否被启动重置，只取决于用户的访问时间在40分钟右侧还是左侧，那么对于这类问题很适合用断点回归的方式解决。\\n    \\n    **②\\xa0长期影响**\\n    \\n    长期影响依赖于很多混淆变量，它适合用PSM、混淆控制的方式处理。前面提到，如果我们考虑PSM和匹配方法有一个难题——它的结论很容易被挑战，因为不存在遗漏的混杂因子是无法被证明的。如何解决这个问题是个难点。\\n    \\n    **③\\xa0用户异质性**\\n    \\n    异质性分析的前提是实验数据，或者说准实验数据，如何去获并分析短期和长期干预的准实验数据呢？同时在我们的场景中，我们关注多个指标和解释性，异质性没有一个直接可以满足的方法。那么现有的下钻分析和uplift能满足这样的目标吗？\\n    \\n    针对这三个问题，我们分别进行阐述\\n    \\n    !Untitled\\n    \\n    !Untitled\\n    \\n    针对长期问题，可以画出如上因果图，考虑一段时间启动重置累积后对用户的影响。长期问题的难点是无法绕开遗漏的混杂因子。比如，我们通过混淆控制的方法去解决这个长期问题，我们先尝试控制用户的活跃度，使其在一段时间内的访问次数都是21，发现击中比例越高的用户的访问天数越多。如果访问次数已经囊括了所有的混淆变量的话，这个结论就是正确的。事实上我们发现，当访问次数都是21的时候，击中比例越高的用户，相当于他们的间隔都比较长，也就是他们是低频高日活型的用户，而击中比例越低的用户，他们正好是高频低日活型的用户。也就是说，我们控制了访问次数，却没有控制住用户的访问模式。这样得出的结论也是错误的。\\n    \\n    当然，我们可以用PSM把这些所有可能的混淆变量一步步都考虑进去。但同样会存在两个问题，一是局部性问题，PSM匹配的样本只是样本中的一小部分，无法代表整体样本，二是遗漏的混杂因子的问题依然无法解决。下面给出我们的解决办法。\\n    \\n    !Untitled\\n    \\n    **准实验：**\\n    \\n    在短期的断点回归中，我们可以看到因为访问间隔会随机地落在40分钟的左右两侧，因此在40分钟邻域构成一个准实验。从业务的视角看，这个准实验是用户无法感知这次访问距离上一次是过了39分钟还是41分钟，他是无法感知到这个差异的，这导致来访的用户的各种变量也是随机分配到这个区间的。那么这个邻域是否能一定程度地扩大呢，能否扩大到30到50分钟或者20到60分钟呢？\\n    \\n    **邻域选择：**\\n    \\n    邻域的选择是置信度和随机性的折中。当范围越大的时候，我们覆盖的样本就越多，但随机性会变差。当范围越小的时候，随机性很好，但覆盖的样本很少，从而置信度会受到质疑。最终，选择了20到60分钟这个区间。我们还通过特征平衡性来证明这两个区间的样本在各项重要特征上都是比较接近的。\\n    \\n    **构造变量：**\\n    \\n    因为我们已经证明了用户的访问行为落在20到40分钟和40到60分钟是一个几乎随机的事件，那么我们可以基于这个事件去构造一个长期的随机变量，就是用户在一段时间内落在40到60分钟的次数除以落在20到40分钟的次数，用这个比例作为长期的准实验变量\\n    \\n    !Untitled\\n    \\n    我们用上表按照长期的击中比例来分组，我们发现两组用户在两周内各项数据都没有明显差异。也就是说，我们的长期Rate比例是与各种混淆因子独立的，也就是T独立于X。那么我们可以证明，Rate和活跃天数Y的因果性是等于相关性的。在右图做了大量证明，我们说明了准实验变量的相关性是等于因果性的，我们就可以直接去观测T和Y的关系，也就是我们构造出来的Rate和活跃天数的关系。\\n    \\n    !Untitled\\n    \\n    !Untitled\\n    \\n    如果说整体上的结论，短期整体和长期整体的结论是显而易见而且直觉的，那么第三个问题细分人群的结论就不是那么显而易见了，异质性分析的前提是实验或者准实验。前面，我们已经构造了准实验变量，创造了无偏样本。下面，我们希望通过异质性分析找到不同人群在不同干预措施下的不同效果，然后去去改善策略。\\n    \\n    比如，我们发现主动打开为主的活跃用户在被启动重置打断后的活跃度和收入都出现了下降，那么对于这类用户我们就应该下架策略。又比如，我们发现启动重置打断不仅会增加频繁打开信息流用户的活跃度和信息流的时长，还不影响他们的搜索时长，那么对于这类用户我们就可以执行启动重置策略。\\n    \\n    这里的难点是我们的目标指标有多个，包括搜索时长、信息流时长、收入。同时，用户的标签维度很高，包括主动打开、频繁打开信息流等。同时，我们要把这样的结论通过算法解释并满足通用性。需要同时满足这四个要求是个难点\\n    \\n    !Untitled\\n    \\n    通过调研发现，这四个要求是很难同时满足的。从前面的分析框架中，我们可以看到，异质性分析主要包括下钻分析和Uplift分析。在下钻分析和Uplift分析的调研中，我们发现了解释性、通用性和细粒度之间矛盾。下钻分析有比较好的解释性但通用性比较差，因为它不太适合处理连续变量，而且它一旦遭遇维度比较高的问题会有搜索效率的问题。Uplift在通用性和研究粒度上没有问题，但是它的解释性较差，比较适合高维和复杂业务。可以看到，在我们的问题中Uplift更加满足要求。\\n    \\n    我们继续调研Uplift发现，Transform outcome的方法是更满足我们的要求的，它相比Meta-learner有更高的准确性，同时相比于Direct uplift model有更低的实验成本，但问题是，它只适合于单指标的建模。那么多指标的uplift的建模，我们目前了解到的只有Mr-uplift方法。它的实验方法是用多个outcome组成一个新的outcome，然后对新的变量建模。这个转换是不可逆的，也就是说我们的变量对原始的outcome的uplift是无法被复原的。因此我们发现，只有Transform outcome最满足我们的要求，下面我们对其进行改造\\n    \\n    !Untitled\\n    \\n    我们的算法目标有3个：\\n    \\n    - 多指标的实验uplift拟合\\n    - 模型可解释\\n    - 算法通用、可处理高维度\\n    \\n    下面，我们用伪代码来呈现我们是怎么达到以上目标的，主要是四个步骤：\\n    \\n    Step1：在数据处理后，先通过Transform outcome去转换我们原始的Y和G。新的变量会被称为Y*和G*。然后对新的变量分别用CatBoost拟合模型。\\n    \\n    Step2：输出模型的重要特征，并选择出现次数最多的，用前15个或前10个解释细分人群。\\n    \\n    Step3：通过两个模型预测的uplift的正负值划分四个象限，比较不同象限的人群在Step2中得到的重要特征的均值差异，得到一个定性的结论。\\n    \\n    Step4：通过Step3的定性结论做一个单维度的搜索，得到定量的结论。然后输出每个维度子人群的uplift的绝对量值以及置信度。\\n    \\n    !Untitled\\n    \\n    我们通过Gini Score来评估这4种模型方案的准确性，黑线代表的是随机实验的效果，蓝线代表的是当前模型的效果，与黑线构成的面积越大效果越好。红线是理论上能够达到的最大值，但是它不能说明是最优效果，只能说是一个量高。我们发现Transform outcome加CatBoost的模型效果最好，Gini面积达到了0.1387，比单模型方法的效果好两倍\\n    \\n    !Untitled\\n    \\n    拿短期异质性来举例，我们希望知道不同上下文的访问行为在被启动重置打断后，在搜索使用时长和总使用时长上有没有什么不同的表现。首先，我们根据算法画4个象限图，我们根据总时长和搜索使用时长分别建立一个uplift模型，横轴为搜索使用时长的uplift，纵轴为总使用时长的uplift，每个点表示一次不同上下文的访问行为。那么第一象限代表被启动重置后，其总时长和搜索使用时长都会有提升，第三象限代表被启动重置后，其总时长和搜索时长都会有明显的下降。\\n    \\n    接着，我们得到这两个模型的重要特征，然后对比四象限的人群在这些重要特征的均值上的差异。对比第一象限和第二象限，我们发现第一象限的人群搜索时长相比于第二象限的人群搜索时长的占比更低，这说明启动重置策略对搜索时长占比较高的用户可能会下降搜索意愿。对比第一象限和第三象限，我们发现第一象限的打开方式有多种，而第三象限的打开方式主要是主动打开，这说明对主动打开的用户，启动重置策略会引起反感，不仅会降低搜索意愿并且对信息流导流不感兴趣。对于这类用户，我们需要采用下架策略。这样的定性结论到底是正确还是错误，我们还需要定量验证。\\n    \\n    对前两次打开方式做一个细分，每一种上下文我们都区分实验组和对照组。通过对比这四种细分上下文实验组和对照组的总时长和搜索时长的差异，得到真实的离线数据的总时长uplift和搜索时长uplift。最终来确定量化的uplift和置信度。\\n    \\n    这是短期异质性的四象限分析算法效果，长期异质性也是一致的\\n    \\n    !Untitled\\n    \\n    最终我们得到，整体上短期和长期的启动重置策略都有副作用。但区分用户看，可以发现可以对搜索活跃度较低的用户保持现有策略，对搜索活跃度相对较高的用户下架现有策略。更加精细化地，我们可以区分不同session上下文的行为。到这里，我们已经说完了异质性的结论。在分析过程中，我们发现启动重置对搜索用户的影响更大。因此，我们特别对产品平台上做了建议，就是在搜索用户搜索完退出再返回时，切换时增加一个动画，提醒用户之前的上下文已经被收纳到这个窗口里了，让用户主动选择是继续之前的上下文还是来到新的信息流页面。\\n    \\n    到这里，我们就已经解决了之前提出的3个问题，我们用断点分析解决短期影响，用uplift解决长期影响，用改良的准实验构造解决用户异质性。', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c0b4c553-2b3a-4664-849e-6083039f446a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**How could you collect and analyze data to use social media to predict the weather?**\\n\\n**How would you design the people you may know feature on LinkedIn or Facebook?**\\n\\n- Find strong unconnected people in weighted connection graph\\n    - Define similarity as how strong the two people are connected\\n    - Given a certain feature, we can calculate the similarity based on\\n        - friend connections (neighbors)\\n        - Check-in’s people being at the same location all the time.\\n        - same college, workplace\\n        - Have randomly dropped graphs test the performance of the algorithm\\n    - News Feed Optimization\\n        - Affinity score: how close the content creator and the users are\\n        - Weight: weight for the edge type (comment, like, tag, etc.). Emphasis on features the company wants to promote\\n        - Time decay: the older the less important\\n1. **How would you predict who someone may want to send a Snapchat or Gmail to?**\\n- for each user, assign a score of how likely someone would send an email to\\n- the rest is feature engineering:\\n    - number of past emails, how many responses, the last time they exchanged an email, whether the last email ends with a question mark, features about the other users, etc.\\n- Ask someone for more details.\\n- People who someone sent emails the most in the past, conditioning on time decay.\\n\\n**How would you suggest to a franchise where to open a new store?**\\n\\n- build a master dataset with local demographic information available for each location.\\n    - local income levels, proximity to traffic, weather, population density, proximity to other businesses\\n    - a reference dataset on local, regional, and national macroeconomic conditions (e.g. unemployment, inflation, prime interest rate, etc.)\\n    - any data on the local franchise owner-operators, to the degree the manager\\n- identify a set of KPIs acceptable to the management that had requested the analysis concerning the most desirable factors surrounding a franchise\\n    - quarterly operating profit, ROI, EVA, pay-down rate, etc.\\n- run econometric models to understand the relative significance of each variable\\n- run machine learning algorithms to predict the performance of each location candidate\\n\\n**In a search engine, given partial data on what the user has typed, how would you predict the user’s eventual search query?**\\n\\n- Based on the past frequencies of words shown up given a sequence of words, we can construct conditional probabilities of the set of next sequences of words that can show up (n-gram). The sequences with highest conditional probabilities can show up as top candidates.\\n- To further improve this algorithm,\\n    - we can put more weight on past sequences which showed up more recently and near your location to account for trends\\n    - show your recent searches given partial data\\n\\n**Given a database of all previous alumni donations to your university, how would you predict which recent alumni are most likely to donate?**\\n\\n- Based on frequency and amount of donations, graduation year, major, etc, construct a supervised regression (or binary classification) algorithm.\\n\\n**You’re Uber and you want to design a heatmap to recommend to drivers where to wait for a passenger. How would you approach this?**\\n\\n- Based on the past pickup location of passengers around the same time of the day, day of the week (month, year), construct\\n- Ask someone for more details.\\n- Based on the number of past pickups\\n    - account for periodicity (seasonal, monthly, weekly, daily, hourly)\\n    - special events (concerts, festivals, etc.) from tweets\\n\\n**How would you build a model to predict a March Madness bracket?**\\n\\n- One vector each for team A and B. Take the difference of the two vectors and use that as an input to predict the probability that team A would win by training the model. Train the models using past tournament data and make a prediction for the new tournament by running the trained model for each round of the tournament\\n- Some extensions:\\n    - Experiment with different ways of consolidating the 2 team vectors into one (e.g concantenating, averaging, etc)\\n    - Consider using a RNN type model that looks at time series data.\\n\\n**Given training data on tweets and their retweets, how would you predict the number of retweets of a given tweet after 7 days after only observing 2 days worth of data?**\\n\\n- Build a time series model with the training data with a seven day cycle and then use that for a new data with only 2 days data.\\n\\n1. **What are some differences you would expect in a model that minimizes squared error, versus a model that minimizes absolute error? In which cases would each error metric be appropriate?**\\n- MSE is more strict to having outliers. MAE is more robust in that sense, but is harder to fit the model for because it cannot be numerically optimized. So when there are less variability in the model and the model is computationally easy to fit, we should use MAE, and if that’s not the case, we should use MSE.\\n- MSE: easier to compute the gradient, MAE: linear programming needed to compute the gradient\\n- MAE more robust to outliers. If the consequences of large errors are great, use MSE\\n- MSE corresponds to maximizing likelihood of Gaussian random variables\\n\\n**What are various ways to predict a binary response variable? Can you compare two of them and tell me when one would be more appropriate? What’s the difference between these? (SVM, Logistic Regression, Naive Bayes, Decision Tree, etc.)**\\n\\n- Things to look at: N, P, linearly seperable?, features independent?, likely to overfit?, speed, performance, memory usage\\n- Logistic Regression\\n    - features roughly linear, problem roughly linearly separable\\n    - robust to noise, use l1,l2 regularization for model selection, avoid overfitting\\n    - the output come as probabilities\\n    - efficient and the computation can be distributed\\n    - can be used as a baseline for other algorithms\\n    - (-) can hardly handle categorical features\\n- SVM\\n    - with a nonlinear kernel, can deal with problems that are not linearly separable\\n    - (-) slow to train, for most industry scale applications, not really efficient\\n- Naive Bayes\\n    - computationally efficient when P is large by alleviating the curse of dimensionality\\n    - works surprisingly well for some cases even if the condition doesn’t hold\\n    - with word frequencies as features, the independence assumption can be seen reasonable. So the algorithm can be used in text categorization\\n    - (-) conditional independence of every other feature should be met\\n- Tree Ensembles\\n    - good for large N and large P, can deal with categorical features very well\\n    - non parametric, so no need to worry about outliers\\n    - GBT’s work better but the parameters are harder to tune\\n    - RF works out of the box, but usually performs worse than GBT\\n- Deep Learning\\n    - works well for some classification tasks (e.g. image)\\n    - used to squeeze something out of the problem\\n\\nWhat are the steps for wrangling and cleaning data before applying machine learning algorithms?\\n\\n在应用机器学习算法之前纠正和清理数据的步骤是什么？\\n\\n数据预处理：缺失值，脏数据，异常点检查 和 处理\\n\\n数据归一化：最大-最小归一化 ，Z-分数, 对数log，分段归一化，排序归一\\n\\n特征选择： Filter（基于相关统计量）， Wraper（特征子集搜索）， Embedding（lasso,ridge） ， 降维\\n\\nHow do you measure distance between data points?\\n\\n如何测量数据点之间的距离？\\n\\n标称数据：Jaccard\\n\\n序数数据：可以变换成 数值数据 或者 标称数据\\n\\n数值数据：p-范数，余弦相似性\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\nWhat features would you use to build a recommendation algorithm for users?\\n\\n你会使用什么功能来为用户构建推荐算法？\\n\\n用户分群\\n普通用户\\n普通粉丝\\n忠实用户\\n核心用户\\nCollaborative filtering\\nContent-based filtering\\nHybrid recommender systems\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\n1. Pick any product or app that you really like and describe how you would improve it.\\n    \\n    选择任何一个你真正喜欢的产品或应用程序，并描述如何改善它。\\n    \\n\\nHow would you find an anomaly in a distribution ?\\n\\n如何在分布中发现异常？\\n\\n参数法：高斯模型\\n\\n非参数法：直方图，箱形图 ，散点图\\n\\n聚类：稀疏的簇是异常的可能性比较大\\n\\n分类：One-Class SVM ， KNN\\n\\nHow would you go about investigating if a certain trend in a distribution is due to an anomaly?\\n\\n如何检查分布中的某个趋势是否是由于异常产生的？\\n\\n问题描述的不清楚\\n\\n“某个趋势是有异常产生的”\\n\\n对比下 Spearman 和 pearman 计算相关系数\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\nWhat metrics would you consider using to track if Uber’s paid advertising strategy to acquire new customers actually works? How would you then approach figuring out an ideal customer acquisition cost?\\n\\n你会考虑用什么指标来跟踪 Uber 付费广告策略在吸引新用户上是否有效？然后，你想用什么办法估算出理想的客户购置成本？\\n\\n(1)\\n\\n付费前后的海盗指标对比\\n\\n详细点说下底层的数据模型怎么实现（E-R模型，维度模型）\\n\\n(2)\\n\\n举个实际例子说明我这个行业是这样计算获客成本的：\\n\\n指标：\\n\\n平均每个新增活跃用户净利润/年\\n\\n平均新增有效“有效户”净利润/年\\n\\n算法：\\n\\n用户与利润增长模型\\n\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\nWhy do you use feature selection?\\n\\n为什么要使用特征选择（feature selection）？\\n\\n业务角度：可解析性，输入对输出的影响程度\\n\\n算法角度：维度灾难，降维，降低学习任务的难度\\n\\n特征选择方法：\\n\\nWhat is the effect on the coefficients of logistic regression if two predictors are highly correlated? What are the confidence intervals of the coefficients?\\n\\n如果两个预测变量高度相关，它们对逻辑回归系数的影响是什么？系数的置信区间是什么？\\n\\n系数影响：增大回归系数的方差\\n业务上： 可解释性变差\\n系数的置信区间：变小\\n\\n什么是方差膨胀因子 (VIF)？\\n\\nWhat is the effect of having correlated predictors in a multiple regression model?\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\nWhat’s the difference between Gaussian Mixture Model and K-Means?\\n\\n高斯混合模型（Gaussian Mixture Model）和 K-Means 之间有什么区别？\\n\\nK-Means：非参数方法，非概率模型，相似度角度衡量\\n\\nGMM：参数方法，概率模型，概率角度衡量\\n\\nK-means is a special case of Mixture ofGaussian, and Mixture of Gaussian is a special case ofExpectation-Maximization.\\nThe biggest difference between K-meanand GMM in practice is:\\nK-Mean only detect spherical cluster.\\nGMM can adjust its self to ellipticshape cluster.\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\nHow do you pick k for K-Means?\\n\\n在 K-Means 中如何拾取 k？\\n\\n根据业务理解\\n代价函数RMSE 与K的函数图 ，曲线拐点。\\n其他方法：Unsupervised-model-11.pdf\\nHow do you know when Gaussian Mixture Model is applicable?\\n\\n你如何知道高斯混合模型是不是适用的？\\n\\nQuora:GMM vs K-means\\n\\n概率模型：指对样本的概率密度分布进行估计，\\n\\n分类问题：输出不是确定的分类标记，而是得到每个类的概率。\\n\\n对样本中的数据分别在几个高斯模型上投影，就会分别得到在各个类上的概率。然后我们可以选取概率最大的类所为判决结果\\n\\n理论上通过增加Model的个数，可以用GMM近似任何概率分布\\n\\nGMM 由 K 个 Gaussian 分布组成，每个 Gaussian 称为一个“Component”，这些 Component 线性加成在一起就组成了 GMM 的概率密度函数：\\n\\nAssuming a clustering model’s labels are known, how do you evaluate the performance of the model?\\n\\n假设聚类模型的标签是已知的，你如\\n\\n何评估模型的性能？\\n\\n分类问题，混淆矩阵，准确率，召回率等\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\nWhat’s the difference between L1 and L2 regularization?\\n\\nL1 和 L2 正则化之间有什么区别？\\n\\n贝叶斯角度：\\n\\nL1 代价函数相当于拉普拉斯先验，根据最大后验估计进行回归系数求解\\n\\nL2是高斯分布先验进行，最大后验的求解\\n\\n对回归系数影响\\nL2 penalizes one big weight more than many small weights.\\nL1 doesn’t.\\nSo with L2, you tend to end up with many small weights, while with L1, you tend to end up with larger weights, but more zeros.\\n\\n[[Differences between L1 and L2 as Loss Function and Regularization]]\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\nName and describe three different kernel functions and in what situation you would use each.\\n\\n点出及描述三种不同的内核函数，在哪些情况下使用哪种？\\n\\nDescribe a method used in machine learning.\\n\\n随意解释机器学习里的一种方法。\\n\\nHow do you deal with sparse data?\\n\\n如何应付稀疏数据？\\n\\nRidge can handle both sparse and nonsparse data.\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\nHow do you prevent overfitting?\\n\\n如何防止过拟合（overfitting）？\\n\\nPenalty methods\\nHoldout and Cross-validation methods\\nEnsembles\\n\\nHow do you deal with outliers in your data?\\n\\n如何处理数据中的离群值？\\n\\n对比下drop 和 不drop 对模型结果产生影响\\nOutliers: To Drop or Not to Drop\\nrobust statistic\\nHow to Deal with Outliers in Your Data\\nHow do you analyze the performance of the predictions generated by regression models versus classification models?\\n\\nclassification models: 混淆矩阵\\n\\nregression models : RMSE ,MAP\\n\\nHow do you assess logistic regression versus simple linear regression models?\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\n!Untitled\\n\\nWhat’s the difference between supervised learning and unsupervised learning?\\n\\nWhat is cross-validation and why would you use it?\\n\\n什么是交叉验证（cross-validation），为什么要使用它？\\n\\nHoldout Method 缺点：\\n样本不足\\n样本不够随机，训练处的模型不够robust\\nCross-validation:\\n\\nRandom Subsampling\\nK-Fold Cross-Validation\\nLeave-one-out Cross-Validation\\n作用：\\n\\n模型和参数选择\\n性能评估\\nWhat’s the name of the matrix used to evaluate predictive models?\\n\\n用于评估预测模型的矩阵的称为什么？\\n\\n混淆矩阵\\n\\nWhat relationships exist between a logistic regression’s coefficient and the Odds Ratio?\\n\\n逻辑回归系数和胜算比（Odds Ratio）之间存在怎样的关联？\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\nWhat’s the relationship between Principal Component Analysis (PCA) and Linear & Quadratic Discriminant Analysis (LDA & QDA)\\n\\n主成分分析（PCA）与线性判别分析（LDA）、二次判别分析（QDA）之间存在怎样的关联？\\n\\n都是降维的方法\\n\\nPCA\\n\\n非监督\\n投影方向，使得数据尽可能的分散开，数据的方差最大\\nLDA，QDA （生成模型）\\n\\n监督\\n投影方向使得数据尽可能分类开来\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\n!Untitled\\n\\nWhat’s the difference between logistic and linear regression? How do you avoid local minima?\\n（行业分析师）逻辑与线性回归有什么区别？如何避免局部极小值？\\n\\n区别对比见IBM第4题\\n\\n解决local minima问题：用 cross entropy loss作为cost function ,它是covex function\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\n!Untitled\\n\\nHow would you build a model to predict credit card fraud?\\n\\n如何构建一个模型来预测信用卡诈骗？\\n\\n分类问题\\nLogistic Regression\\n特征选择\\nHow do you handle missing or bad data?\\n\\n如何处理丢失或不良数据？\\n\\n忽略该记录\\n全局变量替换确实值\\n均值 ，中位数，最可能的值代替缺失值，\\nRobust 算法\\nHow would you derive new features from features that already exist?\\n————————————————\\n版权声明：本文为CSDN博主「jackly231」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/liweijie231/article/details/81623656\\n\\nSuppose you were given two years of transaction history. What features would you use to predict credit risk?\\n\\nWhy does SVM need to maximize the margin between support vectors?\\n\\nExplain what heteroskedasticity is and how to solve it\\n\\nWhy not logistic regression, why GBM?\\n\\n- Simulate a bivariate normal\\n- Derive variance of a distribution\\n\\nRF meaning random forest? If so, they are correct that multi-collinearity is not a problem for random forest models as a predictor.\\n\\nHowever, random forests are typically not used for media mix models because they only really \"care\" about predicting the outcome, not building a theoretical model of the data generating process - this is why multi-collinearity isn\\'t really an issue.\\n\\nLet\\'s say you have 2 variables in the model: total media spend and a holiday index and you are predicting sales. These features are probably colinear - your brand probably spends more around holidays, and both are going to be correlated with sales. If you look at e.g. SHAP plots, partial dependency plots, variable importance (i.e. the tools people usually use to \"interpret\" random forest models) then both holiday and sales will show up as \"highly influential\" because they both are - but these tools won\\'t tell you the effect of marketing spend while controlling for the holiday index simply because this is not how a random forest model gets constructed from the data.\\n\\nYou should be using some sort of GLM. Linear regression is a standard for MMM in industry for the reasons I\\'ve described above, especially the Bayesian versions of it, including penalized variants such as Ridge regression. I have also seen Facebook\\'s Prophet model being used since it is a linear regression model masquerading as a time series model (and as such may be able to model/control for seasonal-type effects more elegantly than a standard GLM).\\n\\nTL;DR RF is not really appropriate for a MMM. You should figure out how they are estimating the effect sizes for media spend on sales with their RF model and think critically about why that method doesn\\'t really give the information that you want to provide to your marketing team.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5d11674a-5850-4506-a268-0d4be33f5d66', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n[[linkedin coffee chat & relationship build message]] \\n\\nto recruiter \\n\\n*I’m a social media strategist with six years of experience and currently seeking new opportunities. I’d love to chat about whether my background might be a fit for any of your openings, and I’d also be happy to connect you with other professionals in my field.*\\n\\n*Looking forward to hearing from you,*\\n\\n*Kendra Holloway*\\n\\n*Hi Peter. I am a Commercial Finance Manager with years of experience in the e-commerce industry. I’ve been interested in working for Expedia for a long time, so when I saw that you were looking for a Sr. Commercial Finance Manager, I applied right away.\\xa0I haven’t yet heard back on my application. However, I thought I’d reach out to introduce myself directly because I believe my experience is very relevant to what you’re looking for in your job ad.*\\n\\nHi Ram,\\n\\nI am an experienced data scientist and PhD from the University of Toronto. I saw that Narvar is recruiting a Senior Data Scientist in Canada and I am very interested in this position.\\xa0I have experience working in startup environment and I am very familiar with the retail industry with a background in marketing. I have submitted my application but haven’t yet heard back. However, I thought I’d reach out to introduce myself directly because I believe my experience is very relevant to what you’re looking for in your job post.\\n\\nThank you \\n\\nCairo \\n\\n*“Hi [insert recruiter name],My name is [insert name] and I recently applied for [insert job title.] I want to reaffirm my interest in being considered for the role, and confidence in my ability to bring value to your team. I look forward to the next step – is there any additional information I can provide on my end to help move the process forward?*\\n\\n*Thank you for your time,[insert name/signature]*\\n\\n*“Hello [insert recruiter name ], hope you had a great weekend. My name is [insert your name] and I recently applied for the [insert job title]. Since it has been more than a week and I haven’t had any response, I was just wondering if there’s anything else left to be shared or sent across. I am willing to disclose additional information, which you may find helpful in processing my application further.*\\n\\n*Thank you and have a good day.*\\n\\n*Sincerely,[insert name]”*\\n\\n*Hi [insert recruiter name],*\\n\\n*My name is [insert name] and I recently applied for [insert job title.] I want to reaffirm my interest in being considered for the role, and confidence in my ability to bring value to your team. I look forward to the next step – is there any additional information I can provide on my end to help move the process forward?*\\n\\nHi Jacob, \\n\\nHope you had a great holiday. \\n\\nHi Jacob,\\n\\nI am an experienced data scientist and PhD from the University of Toronto. I saw that Narvar is recruiting a Senior Data Scientist in Canada and I am very interested in this position. I have experience working in startup environment and I am very familiar with the retail industry. If it makes sense to talk, I would love to chat further about how my skills and interests might fit into your team.\\n\\nThank you\\nCairo\\n\\nHi $NAME,\\n\\nI'm interested in working at $COMPANY as a (data scientist | data analyst). Would you be able to give me a few minutes of your time to chat about your company? I know your time is valuable, so I would appreciate any amount of time you could give me.\\n\\nBest,\\n\\nNicholas\\n\\nHi Nakisa,\\nIt was a pleasure meeting you on the Toronto Machine Learning Summit. I am really interested in the data scientist position at Overbond. Is it possible to connect to learn more about your company?\\n\\nThank you\\nCairo\\n\\nto ppl \\n\\nHi Farnoush,\\n\\nI am an experienced data scientist and PhD from the University of Toronto. I saw that Flipp is recruiting a Machine Learning Scientist and I am very interested in this position. Is it possible to connect to learn more about your work?\\n\\nThank you\\n\\nCairo\\n\\nI have  worked on both traditional ML problems like recommender systems and STOA models like BERT. \\n\\nHi Ping,\\n\\nI am an experienced data scientist and PhD from the University of Toronto. I saw that Inworld AI is recruiting a Senior Data Scientist and I am very interested in this position. I have experience working in startup environment and I am passionate about blockchain and  Metaverse.\\xa0If it makes sense to talk, I would love to chat further about how my skills and interests might fit into your company.\\n\\nThank you\\n\\nCairo\\n\\nHello Atkins,\\n\\nI am an experienced fintech data scientist and PhD from the University of Toronto. I saw that Mercury is recruiting an\\xa0Data Scientist\\xa0and I am very interested in this position.\\n\\nI’m particularly excited about Mercury because my work experiences focus heavily on analyzing financial data and text. Most recently, I trained a XGBoost + BERT model to predict the success rate of startup capital raises on data extracted from SEC documents, and deployed the model as an internal tool for the sales team.\\n\\nIf it makes sense to talk, I would love to chat further about how my skills and interests might fit into your company.\\n\\nThank you\\n\\nCairo\\n\\nto unrelated ppl in the same company  \\n\\nHi Ping, \\n\\nI am a PhD from  with an undergraduate degree in materials chemistry. I saw that Spectra Plasmonics is recruiting a Data Scientist\\xa0and I am very interested in this position. Is it possible to connect to learn more about your work?\\n\\nThank you\\n\\nCairo\\n\\nHi Daniel,\\n\\nI am an experienced machine learning engineer and PhD from the University of Toronto. I saw that Lemay AI is recruiting a consultant and I am very interested in this position. I have experience building traditional ML product like recommender system and worked with SOTA models like transformers.  Most recently, I trained a XGBoost + BERT model to predict the success rate of startup capital raises on data extracted from SEC documents, and deployed the model as an internal tool for the sales team.\\n\\nIf it makes sense to talk, I would love to chat further about how my skills and interests might fit into your company.\\n\\nThank you\\n\\nCairo\\n\\nHi Mahsa, \\n\\nI am an experienced data scientist and PhD from the University of Toronto. I saw that Extreme Networks is recruiting a Data Scientist in Canada and I am very interested in this position. Is it possible to connect to learn more about your work?\\n\\nThank you\\n\\nCairo\\n\\nHi Owen,\\n\\nI am an experienced data scientist and marketing PhD from the University of Toronto. I am really interested in the data scientist position at Thumbtack. Is it possible to connect to learn more about your work?\\n\\nThank you\\nCairo\\n\\nHi Samuel,\\n\\nI am an experienced nlp engineer and PhD from the University of Toronto. As a long-time Grammarly user, I am really interested in the machine learning positions at your company. Is it possible to connect to learn more about your work?\\n\\nThank you\\nCairo\\n\\nHi Jeeyoung,\\n\\nI am an experienced data scientist and PhD from the University of Toronto. I am really interested in the data scientist position at Sanofi. Is it possible to connect to learn more about your work?\\n\\nThank you\\nCairo\\n\\nHi Hamoon,\\n\\nI am an experienced NLP data scientist and I am really interested in the scientist position at FutureFit AI. I share the vision of helping people better understand themselves with technology. Is it possible to connect to learn more about your company?\\n\\nThank you\\nCairo\\n\\nHi name,\\nI saw a DE posrting at Charle Sschwab that I'm really interested in. Would love to learn more about what it's like working there. Would you be open to\\na quick call in the next week or so?\\n\\nto regular people\\n\\n*Dear Ron,*\\n\\n*I’m also in the Society of Professional Journalists, and I’ve really enjoyed reading your posts. The piece you shared a week or two ago about the future of data journalism was pretty thought-provoking. I’d love to keep in touch and learn more about your work.*\\n\\nI’ll be in your area in a few weeks for vacation; if you have any free time, I’d love to meet up for coffee.\\n\\nIf you ever have 20 or so minutes, I’d love to hear more about how you started working in the field and what skills you believe are most relevant to the profession.\\n\\nHi Heindrik,\\n\\nI am an experienced data scientist and PhD from UofT. I am really interested in the data scientist position at GoBolt. Is it possible to connect to learn more about your team?\\n\\nThank you\\nCairo\\n\\nHi Yan,\\n\\nI am an experienced machine learning engineer and marketing PhD. I am really interested in the MLE position at Criteo. Is it possible to connect to learn more about your work?\\n\\nThank you very much\\nCairo\\n\\n\\nHi Monica,\\n\\nI am an experienced data scientist and PhD from the University of Toronto. I am really interested in the DS/MLE positions at Affirm. Is it possible to connect to learn more about your work?\\n\\nThank you very much\\nCairo\\n\\n\\n\\nHi Da,\\n\\nI am an experienced data scientist and PhD from the University of Toronto. I am really interested in the senior DS positions at Samsara in Canada. Is it possible to connect to learn more about your company?\\n\\nThank you very much\\nCairo\\n\\n\\n\\nHi James,\\n\\nI am an experienced data scientist and PhD from the University of Toronto. I am really interested in the DS positions at Jungle Scout in Canada. Is it possible to connect to learn more about your work?\\n\\nThank you very much\\nCairo\\n\\n\\nHi Saeed,\\nI am an experienced data scientist and PhD from the University of Toronto. I am really interested in the data science positions at S&P Global in Canada. Is it possible to connect to learn more about your work?\\n\\nThank you very much\\nCairo\\n\\n\\n\\nHi Molley,\\nI am an experienced data scientist and PhD from the University of Toronto. I am really interested in the data science positions at FlyWheel in Canada. Is it possible to connect to learn more about your work?\\n\\nThank you very much\\nCairo\\n\\n\\n\\nHi Dan,\\nI am an experienced data scientist and PhD from the University of Toronto. I did my dissertation on consumer behavior on social media and I am really interested in the DS positions at Sprout Social. Is it possible to connect to learn more about your work?\\n\\nThank you very much\\nCairo\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2263ef6a-5738-4c2c-b90f-60c2f7d5ba75', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nfollow up\\n\\n> Hi {NAME},\\n> \\n\\n> I wanted to follow up with you in case my previous email got buried. Did you have any thoughts or feedback on whether my skills and interests might fit into your team?\\n> \\n\\n> If you\\'re too busy to respond, no problem. I want you to know in any case that what {YOUR COMPANY / GROUP} are working on has been a real source of inspiration for me. If I can be of any help whatsoever going forward, please don\\'t hesitate to reach out!\\n> \\n\\n> Bob Mentee\\n> \\n\\n> SUBJECT:\\n> \\n\\n> Data Science role at XYZ Robotics\\n> \\n\\n> MESSAGE:\\n> \\n\\n> Hi Ron,\\n> \\n\\n> I\\'ve been following XYZ Robotics for a while and I\\'ve found your Medium articles very interesting. They provide a clear understanding of how XYZ Robotics is tackling the problems that the autonomous vehicle industry is currently facing. Particularly your explanation for not including LIDAR in your systems is a good reminder of why you select the tool based on the problem you\\'re solving, rather than the other way around.\\n> \\n\\n> I\\'m fascinated by your idea of \"one step at a time\" problem solving, and I would love to talk about what I can offer to your mission.\\n> \\n\\n> For some context on myself, I\\'m a machine learning researcher with experience in signal and image processing. I\\'ve been working on the problem of brainwave classification using convolutional neural networks where I achieved a 124% performance improvement over traditional methods. This research has allowed me to keep myself updated on the CNN and computer vision literature. I\\'m also part of a data science mentorship program in which I\\'ve been working on achieving state-of-the-art performance for music genre classification with a Machine Learning Scientist at Amazon.\\n> \\n\\n> I\\'ve attached my resume, which provides a more in-depth view of my qualification and experience.\\n> \\n\\n> I\\'m excited to take the skills and knowledge I have and apply them to help make safe autonomous trucks a reality. I\\'d love to schedule a phone call to chat a bit about what I can offer you and to see if I will be a good fit on your team.\\n> \\n\\n> Best,\\n> \\n\\n> Bob Mentee\\n> \\n\\n> bobmentee.com\\n> \\n\\n> LinkedIn\\n> \\n\\n**Let\\'s break down this email:**\\n\\n- The mentee highlights the experience he has that\\'s 1)\\xa0**most relevant**\\xa0(computer vision, because the company is building autonomous trucks) and 2)\\xa0**most impressive**\\xa0(beating state of the art in brainwave and music classification).\\n- The email opens with a personal compliment to the recipient. We all love getting personal compliments.\\n- The email also quotes one of the recipient\\'s own insights, from Medium. This shows the mentee took the time to read something the recipient wrote.\\n- The company\\'s name was spelled correctly in the original email!! (I\\'ve anonymized it in this one.) Nothing destroys your chances faster than spelling a company\\'s name wrong. It seems insane to make this mistake, but it happens\\xa0**frighteningly**\\xa0often. 🤦\\u200d♂️\\n- Most sentences are short and crisp. The mentee used\\xa0Sapling\\xa0to proofread his email before he sent it.\\n- The mentee\\'s personal website is in his email signature. That means he doesn\\'t need to attach his resume to the email he sends, although he did that too. If the recipient is curious, he can just go to the website and find links to everything he needs.\\n- The mentee used\\xa0**Streak**\\xa0to confirm that the recipient opened the email.\\n\\nI saw [on LinkedIn / on the company’s job page / wherever] that [COMPANY] is looking to hire [ROLE]. I am a [data scientist / data analyst / data engineer / whatever you are] [plus one brief identifying detail]. I am excited about the work you’re doing at [COMPANY] and I would love to talk to you about how my skill set might be a fit for the [SPECIFIC ROLE FROM THEIR JOB WEBSITE] role.\\n\\nI just completed my [DEGREE] in [SUBJECT] at [SCHOOL], where I [name the most impressive thing you did in school or an award you won].\\n\\nI have experience [building/doing/using] [list of 3-ish things you’ve done or skills you have that are relevant to this job]. Most recently, I [built a specific project] that [name 2-3 impressive details about what the result of the project did or how the project worked] (which you can [check out on my Github (LINK IT) / read more about in this Medium post (LINK IT)]).\\n\\nI’m particularly excited about [COMPANY] because [compelling reason! are you passionate about the mission, curious about the data, interested in a specific challenge you think they might have, etc], and based on my skillset and background I thought I would be a good fit for [ROLE].\\n\\nIf it makes sense to talk, I would love to chat further about how my skills and interests might fit into your team.\\n\\nYour signature should include four things:\\n\\n1. Your full name\\n2. A link to your LinkedIn profile (make the text say “LinkedIn” and put the link underneath that text—don’t just put the raw link)\\n3. A link to your portfolio—either Github or a personal website\\n4. Contact info—email address & phone number\\n\\n> Hi Chris,\\n> \\n\\n> I saw a LinkedIn post that Muse is looking for a BioSignal Research Engineer. I’m a researcher with experience in machine learning, mindful meditation, and EEG. I\\'m very excited about the work you\\'re doing with neurofeedback and I would love to talk to you about how my skill set might be a fit for this role.\\n> \\n\\n> For some context, I’m about to complete my MSc in Medical Biophysics at the University of Toronto, where I developed methods for automated brain tumor detection and prediction of patient response to radiotherapy. I have experience in building machine learning models in Python (you can check out my GitHub: github.com/xxxxxx/OCT-Image-Classification), working with time-series EEG data from my undergraduate work in physics, and leading mindful meditation sessions from volunteer work during my BSc. I’m particularly interested in Muse because it intersects with so many areas I find incredibly interesting: finding insights from data, Buddhism/mindful meditation, and neuroscience. If it makes sense to talk, I would love to chat further about how my skills and interests might fit into your team. Would you be free anytime in the next week or two for a quick call on Hangouts or Skype?\\n> \\n\\n> I\\'ve got time between X-X pm next X, X and X. Please let me know if any of those time slots would work. (If none of them do, please feel free to book me on [Calendly] - might save us a bit of back and forth in scheduling.)\\n> \\n\\n> Best Regards,\\n> \\n\\n> Andrei Mouraviev\\n> \\n\\n> LinkedIn: linkedin.com/in/xxxxx/\\n> \\n\\n> GitHub: github.com/xxxxxx\\n> \\n\\n> Website: xxxxx.github.io/\\n> \\n\\n> Contact: xxxxx@example.com – xxx xxx xxxx\\n> \\n\\n> Hi Ryan,\\n> \\n\\n> I follow the fintech space closely and love what you\\'re building towards with Borrowell. I\\'m acutely aware of the various issues that have surface among the legacy players in space recently (e.g., Equifax, etc.) and I\\'m convinced it\\'s possible to significantly improve the credit monitoring and scoring experience for consumers.\\n> \\n\\n> I am an experienced data scientist with sound knowledge of data analytics and agile framework. I have provided effective solutions in line with business needs by successfully building models using advanced machine learning algorithms. I also worked as a developer (C#.NET) which compliments my work as a data scientist to understand the product implementation of results which comes out from complex data analysis.\\n> \\n\\n> I am currently working as a data scientist for a retail startup in Canada, building a regressor model using Python and scikit-learn for sales prediction and optimizing the inventory allocation to different stores by calculating the sales probability of every bought item. I\\'m also building dashboards using Tableau for the operations and marketing teams to reduce the time that\\'s needed for data preparation. Prior to this role, I was working in the manufacturing industry building ML models to predict product flaws based on data collected at sample points throughout our assembly lines. I also developed a model (R, C#) for a pharmaceutical company to predict the probability of patients adhering to the prescribed medications, during the industry project of data science professional course. My GitHub will give an overview of the work I have done -\\xa0https://github.com/xxxxxxx\\n> \\n\\n> I\\'ve attached my resume, which provides a more in-depth view of my qualification and experience.\\n> \\n\\n> I\\'d love to schedule a phone call to chat a bit about what I can offer you and to see if I would be a good fit for your team.\\n> \\n\\n> Best,\\n> \\n\\n> Shivam Kanoria\\n> \\n\\n> Data Scientist\\n> \\n\\n> https://www.linkedin.com/in/xxxxxxx/\\n> \\n\\n> Hi Alison,\\n> \\n\\n> I came across RepairSmith on LinkedIn and saw that you’re looking to hire a data scientist. I’m a data scientist with experience building data pipelines from start to finish. I’m very impressed with the work you’re doing at RepairSmith, and would love to talk to you about how my skill set would fit for the data scientist role.\\n> \\n\\n> I have 2 years of industry experience in data science, after completing my Master’s degree in Statistics. I have experience building fraud detection systems, image classification + localization, and conducting independent research. Most recently, I’ve built a deep neural network using transfer learning to detect out of stock items on supermarket shelves, without bounding boxes to train on. You can check it out here:\\xa0https://example.com/blog/shelf-detection/. I’m particularly excited about RepairSmith because of its journey into machine learning as a startup, and based on my skillset and background I thought I’d be a good fit for the data scientist role.\\n> \\n\\n> If it makes sense to talk, I would love to chat further about how my skills and interests might fit into your team.\\n> \\n\\n> Jamel M. Thomas,\\n> \\n\\n> LinkedIn:\\xa0https://www.linkedin.com/in/xxxxx/\\n> \\n\\n> Personal Blog:\\xa0https://example.com/\\n> \\n\\n> Contact Info: contact@example.com - (xxx) xxx - xxxx\\n> \\n\\n> Hi Kurt,\\n> \\n\\n> I follow the fintech space closely and love what you\\'re building towards with Debtsy. I\\'ve spent time working with the complaints data provided by the Consumer Financial Protection Bureau and am convinced it\\'s possible to significantly improve the consumer loan experience for both lenders and borrowers.\\n> \\n\\n> A bit about myself: After studying quantitative economics at UC Berkeley, along with coursework in computer science, I spent time working at an investment firm evaluating early-stage technology companies. Most recently, I was a researcher at a start-up asset manager where I twice presented research to the U.S. Securities and Exchange Commission (SEC) at its headquarters in Washington, D.C. I have extensive experience conducting data analysis (Python, Pandas, SQL) and building predictive models (Python, scikit-learn) and enjoy tacking business problems involving tabular and time series data. Most recently, I build a series of models to predict changes in the Zillow Home Value Index by ZIP Code which you can read more about in this write-up [actual email has link].\\n> \\n\\n> Given my background and skillset, I thought I would be a good fit for the various data problems Debtsy faces, inlcuding estimating a borrower\\'s capacity and willingness to resolve outstanding balances. If it makes sense to talk, I would love to chat further about how my skills and interests might fit into your team.\\n> \\n\\n> Best,\\n> \\n\\n> Phil\\n> \\n\\n> --\\n> \\n\\n> Phil Glazer\\n> \\n\\n> example.com | LinkedIn\\n> \\n\\n> phil@example.com\\n> \\n\\n> xxx-xxx-xxxx\\n> \\n\\n> Hi Cynthia,\\n> \\n\\n> I follow the fintech space closely and love what you\\'re building towards with PayJoy. I\\'m a strong believer in the power extending credit can have for people in developing regions and am fascinated by the challenge of building novel credit models to assess the creditworthiness of underserved groups.\\n> \\n\\n> A bit about myself: After studying quantitative economics at UC Berkeley, along with coursework in computer science, I spent time working at an investment firm evaluating early-stage technology companies. Most recently, I was a researcher at Bitwise, a start-up asset manager, where I twice presented research to the U.S. Securities and Exchange Commission (SEC) at its headquarters in Washington, D.C. I have extensive experience conducting data analysis (Python, Pandas, SQL) and building predictive models (Python, scikit-learn) and enjoy tacking business problems involving tabular and time series data. For example, I recently built a series of models to predict changes in the Zillow Home Value Index by ZIP Code, which you can read more about [[in this write-up] hyperlinked project write-up on Medium here].\\n> \\n\\n> Given my background and skillset, I thought I would be a good fit for the various data problems PayJoy faces, including developing credit and fraud prevention models. If it makes sense to talk, I would love to chat further about how my skills and interests might fit into your team.\\n> \\n\\n> Best,\\n> \\n\\n> Phil\\n> \\n\\n> --\\n> \\n\\n> Phil Glazer\\n> \\n\\n> example.com | LinkedIn\\n> \\n\\n> Dear Kate,\\n> \\n\\n> Thanks for adding me on LinkedIn. I saw in Key Values\\'s newsletter that Asana is looking to hire a Full Stack engineer. I am a data driven Full Stack engineer currently working on a fun end-to-end engineering product that involves OpenCV, People\\'s Bookshelves, and the Dewey Decimal System. I am excited about the work you’re doing at Asana and I would love to talk to you about how my skill set might be a fit for the Full Stack Engineering role.\\n> \\n\\n> I am transitioning into engineering from education, and my most recent role was as a high school and middle school math teacher at Hill Learning Center which is a school for kids with dyslexia, ADHD, and processing disabilities. Part of my job was to teach Executive Functioning skills -- in layman\\'s terms that meant I helped highly distractible kids learn to manage their workflows and build skills that helped them achieve success -- much like Asana\\'s mission. I used technology and other means to help my students learn and part of my teaching philosophy included structuring the environment so that the brain could get into deep thought and that state of psychological \\'flow\\'. On the technical side, I\\'m currently integrating APIs, computer vision, and recommender systems into my bookshelf project that I hope to launch soon. If you would like to see this in action, it\\'s on my GitHub\\xa0https://github.com/xxxxx. For me, I would love to know how Asana leverages computer vision in its work flow processes.\\n> \\n\\n> This is all to say, I think Asana may be a good fit for me as I\\'d like to stay in a space that encourages empowerment and success as part of their product and I think I have some unique insights and skills to offer.\\n> \\n\\n> If it makes sense to talk, I would love to chat further about how my skills and interests might fit into your team.\\n> \\n\\n> Thank you for reading and enjoy your week.\\n> \\n\\n> Best,\\n> \\n\\n> Sarah\\n>', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9da16219-e4ab-4bba-b92d-3dd8a54169da', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Bias**\\n\\nOne challenge in inferring guest preference from past bookings is that the booking decisions are not solely a function of guest preference. They are also influenced by the position in which the listings are shown in the search results. Attention of users drops monotonically as we go down the list of results, so we can infer that higher ranked listings have a better chance of getting booked solely due to their position. This creates a feedback loop, where listings ranked highly by previous models continue to maintain higher positions in the future, even when they could be misaligned with guest preferences.\\n\\nTo address this bias, we add position as a feature in the DNN. To avoid over reliance on the position feature, we introduce it along with a dropout rate. In this case, we set the position feature to 0 probabilistically 15% of the time during training. This additional information lets the DNN learn the influence of both the position and the quality of the listing on the booking decision of a user. While ranking listings for future users, we then set the input position to 0, effectively leveling the playing field for all listings. Correcting for positional bias led to an increase of +0.7% in bookings in an online A/B test.\\n\\nThe candidate generator is responsible for narrowing millions of possible video recommendations down to hundreds of recommendation candidates. It aims to provide a relevant candidate subset with a high degree of precision (as opposed to recall). This allows us to later use a model to score these candidates with a more detailed representation of users and videos without having to consider millions of videos. This two-stage approach allows us to provide inferences with reasonable latency also allows us to use more than a single candidate generator. The features we use for the candidate generator should include at least the video watch history. Here, we consider a video as watched if the user watched an entire video to completion. To represent the historical watches, we\\'ll use a continuous bag of words (CBOW) embedding. For the input to the CBOW each video will be represented as a one-hot encoding of the million most popular videos and the embedding layer creates a dense representation of the video. The CBOW will consider the previous 50 video watches as input. Since each of these one-hot encoded videos will produce a dense vector after the embedding layer, we\\'ll average the 50 embeddings together to get a single, fixed-length representation of a user\\'s watch history. If a watched video is not present in the list of the one million most popular videos, that video will be assigned to a zero vector instead of being one-hot encoded. In addition to historical watches, we can include historical search queries the user made. We can use a similar approach to historical watches with another CBOW embedding. Each of the previous 50 search queries will be tokenized into unigrams and bigrams. Bigrams will be selected by how frequently unigrams are adjacent to each other. These tokens will be one-hot encoded among the most frequent one million tokens. If a token a user searched is not in the set of the one million most frequent tokens, a vector of zeros will be used to represent that query. Similarly, the 50 search query embeddings will be averaged together to obtain one fixed-length embedding of the user search history. \\n\\nThe label for the model will be the subsequent video watched by the user. Here, that means the label will be the one-hot encoded 51st video the user watched. Since the output of the model is also a one-hot encoded video, it means that we have a multiclass prediction across one million classes. This is called an extreme multiclass classification problem. Calculating these million class probabilities for hundreds of billions of training examples is intractable so we\\'ll have to use candidate sampling. This just means we\\'ll sample a few thousand of the incorrect class outputs as well as the correct class output and only consider the gradient calculation among the sampled classes (instead of all one million classes). Classes will be sampled in proportion to the frequency that the label appears in the training set. As well, we\\'ll need to use importance sampling to account for only updating samples of the class parameters and not all of them. This just means we\\'ll scale the impact each class has on a parameter update in proportion to how much our sampling technique differs from updating the class parameters without sampling. This allows us to speed up training significantly without heavily influencing or some cases, improving the model performance. Architecturally, a maximum of 50 one-hot encoded vectors for each the video and search histories will be fed into separate CBOW embedding layers to produce a 256 dimension embedding. This will feed into fully-connected layer of 2048 units with the ReLU activation function. Each successive layer will halve the number of units until a 256 unit layer remains. This layer represents an embedding for the user. A 256 unit embedding is appended to the network which feeds into a million unit softmax layer which predicts the next video a user will watch. The 256 unit embedding directly preceding the softmax layer represents an unnormalized distribution of the one million video in a 256 dimensional space. This means that the softmax layer just decodes the video embedding into a normalized probability distribution across all one million videos. Once trained, an unseen example is provided to the model. The model will produce a 256 dimension user embedding representing a single user. The dot product of the user embedding and any of the million video embeddings represents the similarity between the user embedding and the video embedding. Theoretically, we can find the dot product for all one million videos and the user embedding. The videos producing the highest resulting dot products can be used as recommendation candidates. We\\'ll talk more about how to do this practically in the Model Hosting section to avoid one million dot product calculations during inference. The offline evaluation metric we\\'ll use for the candidate generator is the mean average precision for the top k candidates (MAP@k) produced by the test set. We can improve the MAP by rejecting candidates based on explicit feedback such as dislikes or implicit feedback like partial watches.\\n\\nThe ranking model is used to score each of the recommendation candidates returned by the candidate generator. Since the candidate generator only provides a few hundred videos to the ranking model, we can use a richer feature set than the candidate generator and still maintain a reasonable amount of latency and cost per recommendation. Finally, the ranking model is useful in the case that we want to use more than one candidate generator. Some of the ranking features will include the same features used by the candidate generator such as a user\\'s previously watched videos. We can still average the CBOW embeddings for the videos watched (and searches) by the user to summarize the user\\'s preference. We can also include specific features of how long it\\'s been since a user watched a video from a particular channel, how long it\\'s been since a user watched a video about a certain topic, how many videos have been impressed on the user, and finally we can even provide the candidate generators score of the item (in this case the distance from the user/video embedding dot product to the candidate video). For these continuous features, scaling significantly impacts model performance. Using normalization ensures each feature lies between 0 and 1. As well, we can provide transformations of the continuous features to the model including the squared feature and the square root of the feature. It\\'s important to note that features obtained from videos (last video watched, last video shown to the user) all use the same embedding to learn a generalized representation of videos. The same is true for search terms. Since we\\'re using far more features in the ranking model than the candidate generator, we should use a fewer number of embedding units such as 32 instead of 256 to speed up the latency of inferences. In addition, we\\'ll also want to incorporate the impressed videos that a user was shown for that particular training example. This impressed video is also sent through the CBOW layer but instead of averaging the resulting embedding along with the video watch history, it will be concatenated alongside the other features we provide to the ranking model. The label will inform the network whether the user clicked on that impression. Labels will simply be binary based on whether or not a user clicked on the impressed video. This impressed video is included in the features as an input to the model. Architecturally, the ranking model will be similar to the candidate generator such that it will also take on a \"tower\" deep neural network. The final layer of the network will use a sigmoid function to output the probability of the user clicking on the candidate video. Each of the candidates will then be sorted based on these click probabilities. A dozen or so videos at the top of this list will be shown to the user as recommendations. The ranker should be evaluated in terms of its recall.\\n\\nAs our model currently stands, it won\\'t account for several biases. We need to make sure that our model mitigates the following:\\n\\n- Model the freshness of videos\\n- Discount popular videos\\n- Limit the positive feedback loop created by the model\\n- Prevent exploitation of the site structure\\n- Prevent highly active users from overinfluencing the loss\\n- Discouraging click-bait\\n\\nThe first bias we should consider is the fact that users favor videos that are new (or fresh). We should add a feature to both the candidate generator as well as the ranking model which represents the days since the video was posted. This allows the model to build an awareness of how fresh a video is. We can even make the feature value negative in the case that an uploader scheduled a post through the UI. This feature will have to be normalized just as the other features are. As well, our model will likely favor more popular videos since most of the features are derived from video watches. If we don\\'t account for some videos being more popular than others, then our model could over exploit popular videos instead of exploring more relevant videos that are less popular. We can mitigate this bias by downsampling videos in the training examples in proportion to their popularity. In line with the trade-off of exploration vs. exploitation, we should be sure to include video watches that weren\\'t a direct result of our recommendation model. We can accomplish this by providing training examples to the network that came from views of videos embedded on other websites. \\n\\nAs well, the site structure can be learned and exploited by the model if search queries are sequentially tied to video views, which we don\\'t want. For instance, if a user searches for something then watches the top video of the search results, then the model would learn that a particular search history is associated strongly with a particular video watch. This will result in the model recommending videos which user has already searched for - which is not a meaningful recommendation. Fortunately, our model already mitigates this by having no sequential relationships between search histories and videos views due to separately averaging each of their embeddings. \\n\\nWe also need to prevent highly active users from over influencing the loss during training. This naturally happens because the training examples will include far more instances from users who use the platform often. If we ensure that each user has the same number of associated training examples in the training set, then each user will be represented the same number of times which will prevent the overinflunce of highly active users. \\n\\nLastly, the ranking model is trained on click through rate (which is whether a user clicked on a recommended video impression). This can lead to biasing the model toward click-bait. Click-bait is defined as a video which is good at attracting clicks (by means of an attractive thumbnail or title) but which doesn\\'t retain the user once they begin watching the video. This often is the result of the video not delivering on the promise impliplied by the title or thumbnail. To reduce the ranking model\\'s bias for favoring click-bait, we can instead weight the loss in terms of how long that particular user watched the video supplied in the input of the model (and therefore impressed on the user). Videos which were impressed on the user but not clicked will receive a unit weight of one to indicate zero watch time for that video. Framing the loss in this way means that the longer a user watched a video, the more influence that example will have on the model. This will correct for the click-bait bias.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='46ea47df-efce-441a-a561-9f7f87cfd68a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n算法课 拉勾2\\n\\n递归有两层含义：\\n\\n递归问题必须可以分解为若干个规模较小、与原问题形式相同的子问题。并且这些子问题可以用完全相同的解题思路来解决；\\n\\n递归问题的演化过程是一个对原问题从大到小进行拆解的过程，并且会有一个明确的终点（临界点）。一旦原问题到达了这个临界点，就不用再往更小的问题上拆解了。最后，从这个临界点开始，把小问题的答案按照原路返回，原问题便得以解决。\\n\\n简而言之，递归的基本思想就是把规模大的问题转化为规模小的相同的子问题来解决。 在函数实现时，因为大问题和小问题是一样的问题，因此大问题的解决方法和小问题的解决方法也是同一个方法。这就产生了函数调用它自身的情况，这也正是递归的定义所在。\\n\\n格外重要的是，这个解决问题的函数必须有明确的结束条件，否则就会导致无限递归的情况。总结起来，递归的实现包含了两个部分，一个是递归主体，另一个是终止条件\\n\\n写出递归代码的关键在于，写出递推公式和找出终止条件。\\n\\n也就是说我们需要：首先找到将大问题分解成小问题的规律，并基于此写出递推公式；然后找出终止条件，就是当找到最简单的问题时，如何写出答案；最终将递推公式和终止条件翻译成实际代码\\n\\n分治法是什么？\\n计算机求解问题所需的计算时间，与其涉及的数据规模强相关。简而言之，问题所涉及的数据规模越小，它所需的计算时间也越少；反之亦然。\\n\\n我们来看一个例子：在一个包含 n 个元素的无序数组中，要求按照从小到大的顺序打印其 n 个元素。\\n\\n假设我们采用 n 个元素之间的两两比较的计算方法，去得到从小到大的序列。分析如下：\\n\\n当数据量 n = 1 时，不需任何计算，直接打印即可；\\n\\n当数据量 n = 2 时 ，那需要做 1 次比较即可达成目标；\\n\\n当数据量 n = 3 时，要对这 3 个元素进行两两比较，共计 3 次比较；\\n\\n而当数据量 n = 10 时，问题就不那么容易处理了，我们需要 45 次比较（计算方式是 0.5*n(n-1) ）。\\n\\n因此，要想通过上述方法直接解决一个规模较大的问题，其实是相当困难的。\\n\\n基于此，分治法的核心思想就是分而治之。具体来说，它先将一个难以直接解决的大问题，分割成一些可以直接解决的小问题。如果分割后的问题仍然无法直接解决，那么就继续递归地分割，直到每个小问题都可解。\\n\\n通常而言，这些子问题具备互相独立、形式相同的特点。这样，我们就可以采用同一种解法，递归地去解决这些子问题。最后，再将每个子问题的解合并，就得到了原问题的解\\n\\n例如下面这个问题，在 1000 个有序数字构成的数组 a 中，判断某个数字 c 是否出现过。\\n\\n第一种方法，全局遍历。 复杂度 O(n)。采用 for 循环，对 1000 个数字全部判断一遍。\\n\\n第二种方法，采用二分查找。 复杂度 O(logn)。递归地判断 c 与 a 的中位数的大小关系，并不断缩小范围。\\n\\n这两种方法，对时间的消耗几乎一样。那分治法的价值又是什么呢？\\n\\n其实，在小数据规模上，分治法没有什么特殊价值。无非就是让代码显得更牛一些。只有在大数据集上，分治法的价值才能显现出来\\n\\n分治法的使用方法\\n前面我们讲到分治法的核心思想是“分而治之”，当你需要采用分治法时，一般原问题都需要具备以下几个特征：\\n\\n难度在降低，即原问题的解决难度，随着数据的规模的缩小而降低。这个特征绝大多数问题都是满足的。\\n\\n问题可分，原问题可以分解为若干个规模较小的同类型问题。这是应用分治法的前提。\\n\\n解可合并，利用所有子问题的解，可合并出原问题的解。这个特征很关键，能否利用分治法完全取决于这个特征。\\n\\n相互独立，各个子问题之间相互独立，某个子问题的求解不会影响到另一个子问题。如果子问题之间不独立，则分治法需要重复地解决公共的子问题，造成效率低下的结果。\\n\\n根据前面我们对分治法的分析，你一定能迅速联想到递归。分治法需要递归地分解问题，再去解决问题。因此，分治法在每轮递归上，都包含了分解问题、解决问题和合并结果这 3 个步骤。\\n\\n为了让大家对分治法有更清晰地了解，我们以二分查找为例，看一下分治法如何使用。关于分治法在排序中的使用，我们会在第 11 课时中讲到。查找问题指的是，在一个有序的数列中，判断某个待查找的数字是否出现过。二分查找，则是利用分治法去解决查找问题。通常二分查找需要一个前提，那就是输入的数列是有序的。\\n\\n二分查找的思路比较简单，步骤如下：\\n\\n选择一个标志 i 将集合 L 分为二个子集合，一般可以使用中位数；\\n\\n判断标志 L(i) 是否能与要查找的值 des 相等，相等则直接返回结果；\\n\\n如果不相等，需要判断 L(i) 与 des 的大小；\\n\\n基于判断的结果决定下步是向左查找还是向右查找。如果向某个方向查找的空间为 0，则返回结果未查到；\\n\\n回到步骤 1。\\n\\n我们对二分查找的复杂度进行分析。二分查找的最差情况是，不断查找到最后 1 个数字才完成判断。那么此时需要的最大的复杂度就是 O(logn)\\n\\n下面我们一起来看一个例子。在数组 { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 } 中，查找 8 是否出现过。\\n\\n首先判断 8 和中位数 5 的大小关系。因为 8 更大，所以在更小的范围 6, 7, 8, 9, 10 中继续查找。此时更小的范围的中位数是 8。由于 8 等于中位数 8，所以查找到并打印查找到的 8 对应在数组中的 index 值\\n\\n从代码实现的角度来看，我们可以采用两个索引 low 和 high，确定查找范围。最初 low 为 0，high 为数组长度减 1。在一个循环体内，判断 low 到 high 的中位数与目标变量 targetNumb 的大小关系。根据结果确定向左走（high = middle - 1）或者向右走（low = middle + 1），来调整 low 和 high 的值。直到 low 反而比 high 更大时，说明查找不到并跳出循环。我们给出代码如下：\\n\\n复制代码\\npublic static void main(String[] args) {\\n// 需要查找的数字\\nint targetNumb = 8;\\n// 目标有序数组\\nint[] arr = { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 };\\nint middle = 0;\\nint low = 0;\\nint high = arr.length - 1;\\nint isfind = 0;\\n\\n```\\nwhile (low <= high) {\\n\\tmiddle = (high + low) / 2;\\n\\tif (arr[middle] == targetNumb) {\\n\\t\\tSystem.out.println(targetNumb + \" 在数组中,下标值为: \" + middle);\\n        isfind = 1;\\n\\t\\tbreak;\\n\\t} else if (arr[middle] > targetNumb) {\\n\\t\\t// 说明该数在low~middle之间\\n\\t\\thigh = middle - 1;\\n\\t} else {\\n\\t\\t// 说明该数在middle~high之间\\n\\t\\tlow = middle + 1;\\n\\t}\\n}\\nif (isfind == 0) {\\n\\t\\tSystem.out.println(\"数组不含 \" + targetNumb);\\n}\\n\\n```\\n\\n}\\n\\n什么是排序问题\\n排序，就是让一组无序数据变成有序的过程。 一般默认这里的有序都是从小到大的排列顺序。下面我们先来讲讲，如何判断不同的排序算法的优劣。\\n\\n衡量一个排序算法的优劣，我们主要会从以下 3 个角度进行分析：\\n\\n1．时间复杂度，具体包括，最好时间复杂度、最坏时间复杂度以及平均时间复杂度。\\n\\n2．空间复杂度，如果空间复杂度为 1，也叫作原地排序。\\n\\n3．稳定性，排序的稳定性是指相等的数据对象，在排序之后，顺序是否能保证不变。\\n\\n常见的排序算法及其思想\\n接下来，我们就开始详细地介绍一些经典的排序算法。\\n\\n冒泡排序\\n1、冒泡排序的原理\\n\\n从第一个数据开始，依次比较相邻元素的大小。如果前者大于后者，则进行交换操作，把大的元素往后交换。通过多轮迭代，直到没有交换操作为止。 冒泡排序就像是在一个水池中处理数据一样，每次会把最大的那个数据传递到最后\\n\\n2、冒泡排序的性能\\n\\n冒泡排序最好时间复杂度是 O(n)，也就是当输入数组刚好是顺序的时候，只需要挨个比较一遍就行了，不需要做交换操作，所以时间复杂度为 O(n)。\\n\\n冒泡排序最坏时间复杂度会比较惨，是 O(n*n)。也就是说当数组刚好是完全逆序的时候，每轮排序都需要挨个比较 n 次，并且重复 n 次，所以时间复杂度为 O(n*n)。\\n\\n很显然，当输入数组杂乱无章时，它的平均时间复杂度也是 O(n*n)。\\n\\n冒泡排序不需要额外的空间，所以空间复杂度是 O(1)。冒泡排序过程中，当元素相同时不做交换，所以冒泡排序是稳定的排序算法\\n\\npublic static void main(String[] args) {\\nint[] arr = { 1, 0, 3, 4, 5, -6, 7, 8, 9, 10 };\\nSystem.out.println(\"原始数据: \" + Arrays.toString(arr));\\nfor (int i = 1; i < arr.length; i++) {\\nfor (int j = 0; j < arr.length - i; j++) {\\nif (arr[j] > arr[j + 1]) {\\nint temp = arr[j];\\narr[j] = arr[j + 1];\\narr[j + 1] = temp;\\n}\\n}\\n}\\nSystem.out.println(\"冒泡排序: \" + Arrays.toString(arr));\\n}\\n\\n插入排序\\n1、插入排序的原理\\n\\n选取未排序的元素，插入到已排序区间的合适位置，直到未排序区间为空。插入排序顾名思义，就是从左到右维护一个已经排好序的序列。直到所有的待排数据全都完成插入的动作\\n\\n2、插入排序的性能\\n\\n插入排序最好时间复杂度是 O(n)，即当数组刚好是完全顺序时，每次只用比较一次就能找到正确的位置。这个过程重复 n 次，就可以清空未排序区间。\\n\\n插入排序最坏时间复杂度则需要 O(n*n)。即当数组刚好是完全逆序时，每次都要比较 n 次才能找到正确位置。这个过程重复 n 次，就可以清空未排序区间，所以最坏时间复杂度为 O(n*n)。\\n\\n插入排序的平均时间复杂度是 O(n*n)。这是因为往数组中插入一个元素的平均时间复杂度为 O(n)，而插入排序可以理解为重复 n 次的数组插入操作，所以平均时间复杂度为 O(n*n)。\\n\\n插入排序不需要开辟额外的空间，所以空间复杂度是 O(1)\\n\\npublic static void main(String[] args) {\\nint[] arr = { 2, 3, 5, 1, 23, 6, 78, 34 };\\nSystem.out.println(\"原始数据: \" + Arrays.toString(arr));\\nfor (int i = 1; i < arr.length; i++) {\\nint temp = arr[i];\\nint j = i - 1;\\nfor (; j >= 0; j--) {\\nif (arr[j] > temp) {\\narr[j + 1] = arr[j];\\n} else {\\nbreak;\\n}\\n}\\narr[j + 1] = temp;\\n}\\nSystem.out.println(\"插入排序: \" + Arrays.toString(arr));\\t\\n}\\n\\n相同点\\n\\n插入排序和冒泡排序的平均时间复杂度都是 O(n*n)，且都是稳定的排序算法，都属于原地排序。\\n\\n差异点\\n\\n冒泡排序每轮的交换操作是动态的，所以需要三个赋值操作才能完成；\\n\\n而插入排序每轮的交换动作会固定待插入的数据，因此只需要一步赋值操作。\\n\\n以上两种排序算法都比较简单，通过这两种算法可以帮助我们对排序的思想建立基本的了解，接下来再介绍一些时间复杂度更低的排序算法，它们的时间复杂度都可以达到 O(nlogn)。\\n\\n归并排序\\n1、归并排序的原理\\n\\n归并排序的原理其实就是我们上一课时讲的分治法。它首先将数组不断地二分，直到最后每个部分只包含 1 个数据。然后再对每个部分分别进行排序，最后将排序好的相邻的两部分合并在一起，这样整个数组就有序了。\\n\\npublic static void main(String[] args) {\\nint[] arr = { 49, 38, 65, 97, 76, 13, 27, 50 };\\nint[] tmp = new int[arr.length];\\nSystem.out.println(\"原始数据: \" + Arrays.toString(arr));\\ncustomMergeSort(arr, tmp, 0, arr.length - 1);\\nSystem.out.println(\"归并排序: \" + Arrays.toString(arr));\\n}\\n\\npublic static void customMergeSort(int[] a, int[] tmp, int start, int end) {\\nif (start < end) {\\nint mid = (start + end) / 2;\\n// 对左侧子序列进行递归排序\\ncustomMergeSort(a, tmp, start, mid);\\n// 对右侧子序列进行递归排序\\ncustomMergeSort(a, tmp,mid + 1, end);\\n// 合并\\ncustomDoubleMerge(a, tmp, start, mid, end);\\n}\\n}\\n\\npublic static void customDoubleMerge(int[] a, int[] tmp, int left, int mid, int right) {\\nint p1 = left, p2 = mid + 1, k = left;\\nwhile (p1 <= mid && p2 <= right) {\\nif (a[p1] <= a[p2])\\ntmp[k++] = a[p1++];\\nelse\\ntmp[k++] = a[p2++];\\n}\\nwhile (p1 <= mid)\\ntmp[k++] = a[p1++];\\nwhile (p2 <= right)\\ntmp[k++] = a[p2++];\\n// 复制回原素组\\nfor (int i = left; i <= right; i++)\\na[i] = tmp[i];\\n\\n2、归并排序的性能\\n\\n对于归并排序，它采用了二分的迭代方式，复杂度是 logn。\\n\\n每次的迭代，需要对两个有序数组进行合并，这样的动作在 O(n) 的时间复杂度下就可以完成。因此，**归并排序的复杂度就是二者的乘积 O(nlogn)。**同时，它的执行频次与输入序列无关，因此，归并排序最好、最坏、平均时间复杂度都是 O(nlogn)。\\n\\n空间复杂度方面，由于每次合并的操作都需要开辟基于数组的临时内存空间，所以空间复杂度为 O(n)。归并排序合并的时候，相同元素的前后顺序不变，所以归并是稳定的排序算法。\\n\\n快速排序\\n1、快速排序法的原理\\n\\n快速排序法的原理也是分治法。它的每轮迭代，会选取数组中任意一个数据作为分区点，将小于它的元素放在它的左侧，大于它的放在它的右侧。再利用分治思想，继续分别对左右两侧进行同样的操作，直至每个区间缩小为 1，则完成排序\\n\\npublic static void main(String[] args) {\\nint[] arr = { 6, 1, 2, 7, 9, 11, 4, 5, 10, 8 };\\nSystem.out.println(\"原始数据: \" + Arrays.toString(arr));\\ncustomQuickSort(arr, 0, arr.length - 1);\\nSystem.out.println(\"快速排序: \" + Arrays.toString(arr));\\n}\\n\\npublic void customQuickSort(int[] arr, int low, int high) {\\nint i, j, temp, t;\\nif (low >= high) {\\nreturn;\\n}\\n\\n```\\ni = low;\\nj = high;\\ntemp = arr[low];\\nwhile (i < j) {\\n\\t// 先看右边，依次往左递减\\n\\twhile (temp <= arr[j] && i < j) {\\n\\t\\tj--;\\n\\t}\\n\\t// 再看左边，依次往右递增\\n\\twhile (temp >= arr[i] && i < j) {\\n\\t\\ti++;\\n\\t}\\n\\tt = arr[j];\\n\\tarr[j] = arr[i];\\n\\tarr[i] = t;\\n}\\narr[low] = arr[i];\\narr[i] = temp;\\n// 递归调用左半数组\\ncustomQuickSort(arr, low, j - 1);\\n// 递归调用右半数组\\ncustomQuickSort(arr, j + 1, high);\\n\\n```\\n\\n}\\n\\n2、快速排序法的性能\\n\\n在快排的最好时间的复杂度下，如果每次选取分区点时，都能选中中位数，把数组等分成两个，那么此时的时间复杂度和归并一样，都是 O(n*logn)。\\n\\n而在最坏的时间复杂度下，也就是如果每次分区都选中了最小值或最大值，得到不均等的两组。那么就需要 n 次的分区操作，每次分区平均扫描 n / 2 个元素，此时时间复杂度就退化为 O(n*n) 了。\\n\\n快速排序法在大部分情况下，统计上是很难选到极端情况的。因此它平均的时间复杂度是 O(n*logn)。\\n\\n快速排序法的空间方面，使用了交换法，因此空间复杂度为 O(1)。\\n\\n很显然，快速排序的分区过程涉及交换操作，所以快排是不稳定的排序算法。\\n\\n排序算法的性能分析\\n我们先思考一下排序算法性能的下限，也就是最差的情况。在前面的课程中，我们写过求数组最大值的代码，它的时间复杂度是 O(n)。对于 n 个元素的数组，只要重复执行 n 次最大值的查找就能完成排序。因此排序最暴力的方法，时间复杂度是 O(n*n)。这恰如冒泡排序和插入排序。\\n\\n当我们利用算法思维去解决问题时，就会想到尝试分治法。此时，利用归并排序就能让时间复杂度降低到 O(nlogn)。然而，归并排序需要额外开辟临时空间。一方面是为了保证稳定性，另一方面则是在归并时，由于在数组中插入元素导致了数据挪移的问题。\\n\\n为了规避因此而带来的时间损耗，此时我们采用快速排序。通过交换操作，可以解决插入元素导致的数据挪移问题，而且降低了不必要的空间开销。但是由于其动态二分的交换数据，导致了由此得出的排序结果并不稳定。\\n\\n经过以上分析，我们对方法论进行提练，宏观上的步骤总结为以下 4 步：\\n\\n复杂度分析。估算问题中复杂度的上限和下限。\\n\\n定位问题。根据问题类型，确定采用何种算法思维。\\n\\n数据操作分析。根据增、删、查和数据顺序关系去选择合适的数据结构，利用空间换取时间。\\n\\n编码实现。\\n\\n这套方法适用于绝大多数的问题，在实战中需要你灵活运用\\n\\n案例\\n梳理完方法论之后，我们回过头来再看一下以前的例子，看看采用方法论是如何分析题目并找到答案的。\\n\\n例 1，在一个数组 a = [1, 3, 4, 3, 4, 1, 3] 中，找到出现次数最多的那个数字。如果并列存在多个，随机输出一个。\\n\\n我们先来分析一下复杂度。假设我们采用最暴力的方法。利用双层循环的方式计算：\\n\\n第一层循环，我们对数组中的每个元素进行遍历；\\n\\n第二层循环，对于每个元素计算出现的次数，并且通过当前元素次数 time_tmp 和全局最大次数变量 time_max 的大小关系，持续保存出现次数最多的那个元素及其出现次数。\\n\\n由于是双层循环，这段代码在时间方面的消耗就是 n*n 的复杂度，也就是 O(n²)。这段代码我们在第 1 课时中的例子里讲过，这里就不再赘述了。\\n\\n接着，我们思考一下这段代码最低的复杂度可能是多少？\\n\\n不难发现，这个问题的复杂度最低低不过 O(n)。这是因为某个数字的数值是完全有可能影响最终结果。例如，a = [1, 3, 4, 3, 4, 1]，随机输出 1、3、4 都可以。如果 a 中增加一个元素变成，a = [1, 3, 4, 3, 4, 1, 3, 1]，则结果为 1。\\n\\n由此可见，这个问题必须至少要对全部数据遍历一次，所以复杂度再低低不过 O(n)。\\n\\n显然，这个问题属于在一个数组中，根据某个条件进行查找的问题。既然复杂度低不过 O(n)，我们也不用考虑采用二分查找了。此处是用不到任何算法思维。那么如何让 O(n²) 的复杂度降低为 O(n) 呢？\\n\\n只有通过巧妙利用数据结构了。分析这个问题就可以发现，此时不需要关注数据顺序。因此，栈、队列等数据结构用到的可能性会很低。如果采用新的数据结构，增删操作肯定是少不了的。而原问题就是查找类型的问题，所以查找的动作一定是非常高频的。在我们学过的数据结构中，查找有优势，同时不需要考虑数据顺序的只有哈希表，因此可以很自然地想到用哈希表解决问题。\\n\\n哈希表的结构是“key-value”的键值对，如何设计键和值呢？哈希表查找的 key，所以 key 一定存放的是被查找的内容，也就是原数组中的元素。数组元素有重复，但哈希表中 key 不能重复，因此只能用 value 来保存频次。\\n\\n分析到这里，所有解决方案需要用到的关键因素就出来了，我们总结为以下 2 点：\\n\\n预期的时间复杂度是 O(n)，这就意味着编码采用一层的 for 循环，对原数组进行遍历。\\n\\n数据结构需要额外设计哈希表，其中 key 是数组的元素，value 是频次。这样可以支持 O(1) 时间复杂度的查找动作。\\n\\n因此，这个问题的代码就是\\n\\npublic void s2_4() {\\n\\n```\\nint a[] = { 1, 3, 4, 3, 4, 1, 3, 1 };\\n\\nMap d = new HashMap();\\n\\nfor (int i = 0; i < a.length; i++) {\\n\\n\\tif (d.containsKey(a[i])) {\\n\\n\\t\\td.put(a[i], d.get(a[i]) + 1);\\n\\n\\t} else {\\n\\n\\t\\td.put(a[i], 1);\\n\\n\\t}\\n\\n}\\n\\nint val_max = -1;\\n\\nint time_max = 0;\\n\\nfor (Integer key : d.keySet()) {\\n\\n\\tif (d.get(key) > time_max) {\\n\\n\\t\\ttime_max = d.get(key);\\n\\n\\t\\tval_max = key;\\n\\n\\t}\\n\\n}\\n\\nSystem.out.println(val_max);\\n\\n```\\n\\n}\\n\\n**例 2**，这个问题是力扣的经典问题，two sums。给定一个整数数组 arr 和一个目标值 target，请你在该数组中找出加和等于目标值的两个整数，并返回它们在原数组中的下标。\\n\\n你可以假设，原数组中没有重复元素，而且有且只有一组答案。但是，数组中的元素只能使用一次。例如，arr = [1, 2, 3, 4, 5, 6]，target = 4。因为，arr[0] + arr[2] = 1 + 3 = 4 = target，则输出 0，2。\\n\\n**首先，我们来分析一下复杂度**。假设我们采用最暴力的方法，利用双层循环的方式计算，步骤如下：\\n\\n- 第一层循环，我们对数组中的每个元素进行遍历；\\n- 第二层循环，对于第一层的元素与 target 的差值进行查找。\\n\\n例如，第一层循环遍历到了 1，第二层循环就需要查找 target - arr[0] = 4 - 1 = 3 是否在数组中。由于是双层循环，这段代码在时间方面的消耗就是 n*n 的复杂度，也就是 O(n²)。\\n\\n**接下来，我们看看下限**。很显然，某个数字是否存在于原数组对结果是有影响的。因此，复杂度再低低不过 O(n)。\\n\\n这里的问题是在数组中基于某个条件去查找数据的问题。然而可惜的是原数组并非有序，因此采用二分查找的可能性也会很低。那么如何把 O(n²) 的复杂度降低到 O(n) 呢？路径只剩下了数据结构。\\n\\n在暴力的方法中，第二层循环的目的是查找 target - arr[i] 是否出现在数组中。很自然地就会联想到可能要使用哈希表。同时，这个例子中对于数据处理的顺序并不关心，栈或者队列使用的可能性也会很低。因此，不妨试试如何用哈希表去降低复杂度。\\n\\n既然是要查找 target - arr[i] 是否出现过，因此哈希表的 key 自然就是 target - arr[i]。而 value 如何设计呢？这就要看一下结果了，最终要输出的是查找到的 arr[i] 和 target - arr[i] 在数组中的索引，因此 value 存放的必然是 index 的索引值。\\n\\n**基于上面的分析，我们就能找到解决方案，分析如下**：\\n\\n1. 预期的时间复杂度是 O(n)，这就意味着编码采用一层的 for 循环，对原数组进行遍历。\\n2. 数据结构需要额外设计哈希表，其中 key 是 target - arr[i]，value 是 index。这样可以支持 O(1) 时间复杂度的查找动作。\\n\\n因此，代码如下\\n\\nprivate static int[] twoSum(int[] arr, int target) {\\n\\n```\\nMap map = new HashMap();\\n\\nfor (int i = 0; i < arr.length; i++) {\\n\\n\\tmap.put(arr[i], i);\\n\\n}\\n\\nfor (int i = 0; i < arr.length; i++) {\\n\\n\\tint complement = target - arr[i];\\n\\n\\tif (map.containsKey(complement) && map.get(complement) != i) {\\n\\n\\t\\treturn new int[] { map.get(complement), i };\\n\\n\\t}\\n\\n}\\n\\n    return null;\\n\\n```\\n\\n}\\n\\n在这段代码中我们采用了两个 for 循环，时间复杂度就是 O(n) + O(n) = O(n)。额外使用了 map，空间复杂度也是 O(n)。第一个 for 循环，把数组转为字典，存放的是“数值 -index”的键值对。第二个 for 循环，在字典中依次判断，target - arr[i] 是否出现过。如果它出现过，且不是它自己，则打印 target - arr[i] 和 arr[i] 的索引\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='96275f6a-2154-464b-bfd0-39e191e0f8ac', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n链表 code\\n\\n为何一般的题目都要在前面加一个dummy node? 因为往往在循环之后失去了最开始的head地址，所以需要一个dummy node来存下他，记住return的位置一般在dummy.next\\n\\n```python\\nclass ListNode(object):\\n    def __init__(self, x):\\n        self.val = x\\n        self.next = None\\n\\na = ListNode(2)\\na1 = ListNode(3)\\na.next = a1\\nb = ListNode(1)\\nb1 = ListNode(1)\\nb.next = b1\\n```\\n\\n```python\\n**class** **SingleLinkList**(object):\\n    \"\"\"单链表\"\"\"\\n\\n    **def** **__init__**(self):\\n        self**.**_head **=** **Nonedef** **is_empty**(self):\\n        \"\"\"判断链表是否为空\"\"\"\\n        **return** self**.**_head **is** **Nonedef** **length**(self):\\n        \"\"\"链表长度\"\"\"\\n        *# 初始指针指向head*cur **=** self**.**_head\\n        count **=** 0\\n        *# 指针指向None 表示到达尾部***while** cur **is** **not** **None**:\\n            count **+=** 1\\n            *# 指针下移*cur **=** cur**.**next\\n        **return** count\\n\\n    **def** **items**(self):\\n        \"\"\"遍历链表\"\"\"\\n        *# 获取head指针*cur **=** self**.**_head\\n        *# 循环遍历***while** cur **is** **not** **None**:\\n            *# 返回生成器***yield** cur**.**item\\n            *# 指针下移*cur **=** cur**.**next\\n\\n    **def** **add**(self, item):\\n        \"\"\"向链表头部添加元素\"\"\"\\n        node **=** Node(item)\\n        *# 新结点指针指向原头部结点*node**.**next **=** self**.**_head\\n        *# 头部结点指针修改为新结点*self**.**_head **=** node\\n\\n    **def** **append**(self, item):\\n        \"\"\"尾部添加元素\"\"\"\\n        node **=** Node(item)\\n        *# 先判断是否为空链表***if** self**.**is_empty():\\n            *# 空链表，_head 指向新结点*self**.**_head **=** node\\n        **else**:\\n            *# 不是空链表，则找到尾部，将尾部next结点指向新结点*cur **=** self**.**_head\\n            **while** cur**.**next **is** **not** **None**:\\n                cur **=** cur**.**next\\n            cur**.**next **=** node\\n\\n    **def** **insert**(self, index, item):\\n        \"\"\"指定位置插入元素\"\"\"\\n        *# 指定位置在第一个元素之前，在头部插入***if** index **<=** 0:\\n            self**.**add(item)\\n        *# 指定位置超过尾部，在尾部插入***elif** index **>** (self**.**length() **-** 1):\\n            self**.**append(item)\\n        **else**:\\n            *# 创建元素结点*node **=** Node(item)\\n            cur **=** self**.**_head\\n            *# 循环到需要插入的位置***for** i **in** range(index **-** 1):\\n                cur **=** cur**.**next  #这个cur是待插入位置的前驱节点\\n            node**.**next **=** cur**.**next\\n            cur**.**next **=** node\\n\\n    **def** **remove**(self, item):\\n        \"\"\"删除节点\"\"\"\\n        cur **=** self**.**_head\\n        pre **=** **Nonewhile** cur **is** **not** **None**:\\n            *# 找到指定元素***if** cur**.**item **==** item:\\n                *# 如果第一个就是删除的节点***if** **not** pre:\\n                    *# 将头指针指向头节点的后一个节点*self**.**_head **=** cur**.**next\\n                **else**:\\n                    *# 将删除位置前一个节点的next指向删除位置的后一个节点*pre**.**next **=** cur**.**next\\n                **return** **Trueelse**:\\n                *# 继续按链表后移节点*pre **=** cur\\n                cur **=** cur**.**next\\n#待删除位置的前驱节点.next = 待删除位置的前驱节点.next.next\\n    **def** **find**(self, item):\\n        \"\"\"查找元素是否存在\"\"\"\\n        **return** item **in** self**.**items()\\n```\\n\\n```python\\n\\n#定义循环链表\\nclass SingleCycleLinkList(object):\\n\\n    def __init__(self):\\n        self._head = None\\n\\n    def is_empty(self):\\n        \"\"\"判断链表是否为空\"\"\"\\n        return self._head is None\\n\\n    def length(self):\\n        \"\"\"链表长度\"\"\"\\n        # 链表为空\\n        if self.is_empty():\\n            return 0\\n        # 链表不为空\\n        count = 1\\n        cur = self._head\\n        while cur.next != self._head:\\n            count += 1\\n            # 指针下移\\n            cur = cur.next\\n        return count\\n\\n    def items(self):\\n        \"\"\" 遍历链表 \"\"\"\\n        # 链表为空\\n        if self.is_empty():\\n            return\\n        # 链表不为空\\n        cur = self._head\\n        while cur.next != self._head:\\n            yield cur.item\\n            cur = cur.next\\n        yield cur.item\\n\\n    def add(self, item):\\n        \"\"\" 头部添加结点\"\"\"\\n        node = Node(item)\\n        if self.is_empty():  # 为空\\n            self._head = node\\n            node.next = self._head\\n        else:\\n            # 添加结点指向head\\n            node.next = self._head\\n            cur = self._head\\n            # 移动结点，将末尾的结点指向node\\n            while cur.next != self._head:\\n                cur = cur.next\\n            cur.next = node\\n        # 修改 head 指向新结点\\n        self._head = node\\n\\n    def append(self, item):\\n        \"\"\"尾部添加结点\"\"\"\\n        node = Node(item)\\n        if self.is_empty():  # 为空\\n            self._head = node\\n            node.next = self._head\\n        else:\\n            # 寻找尾部\\n            cur = self._head\\n            while cur.next != self._head:\\n                cur = cur.next\\n            # 尾部指针指向新结点\\n            cur.next = node\\n            # 新结点指针指向head\\n            node.next = self._head\\n\\n    def insert(self, index, item):\\n        \"\"\" 指定位置添加结点\"\"\"\\n        if index <= 0:  # 指定位置小于等于0，头部添加\\n            self.add(item)\\n        # 指定位置大于链表长度，尾部添加\\n        elif index > self.length() - 1:\\n            self.append(item)\\n        else:\\n            node = Node(item)\\n            cur = self._head\\n            # 移动到添加结点位置\\n            for i in range(index - 1):\\n                cur = cur.next\\n            # 新结点指针指向旧结点\\n            node.next = cur.next\\n            # 旧结点指针 指向 新结点\\n            cur.next = node\\n\\n    def remove(self, item):\\n        \"\"\" 删除一个结点 \"\"\"\\n        if self.is_empty():\\n            return\\n        cur = self._head\\n        pre = Node\\n        # 第一个元素为需要删除的元素\\n        if cur.item == item:\\n            # 链表不止一个元素\\n            if cur.next != self._head:\\n                while cur.next != self._head:\\n                    cur = cur.next\\n                # 尾结点指向 头部结点的下一结点\\n                cur.next = self._head.next\\n                # 调整头部结点\\n                self._head = self._head.next\\n            else:\\n                # 只有一个元素\\n                self._head = None\\n        else:\\n            # 不是第一个元素\\n            pre = self._head\\n            while cur.next != self._head:\\n                if cur.item == item:\\n                    # 删除\\n                    pre.next = cur.next\\n                    return True\\n                else:\\n\\n                    pre = cur  # 记录前一个指针\\n                    cur = cur.next  # 调整指针位置\\n        # 当删除元素在末尾\\n        if cur.item == item:\\n            pre.next = self._head\\n            return True\\n\\n    def find(self, item):\\n        \"\"\" 查找元素是否存在\"\"\"\\n        return item in self.items()\\n\\nif __name__ == \\'__main__\\':\\n    link_list = SingleCycleLinkList()\\n    print(link_list.is_empty())\\n    # 头部添加元素\\n    for i in range(5):\\n        link_list.add(i)\\n    print(list(link_list.items()))\\n    # 尾部添加元素\\n    for i in range(6):\\n        link_list.append(i)\\n    print(list(link_list.items()))\\n    # 添加元素\\n    link_list.insert(3, 45)\\n    print(list(link_list.items()))\\n    # 删除元素\\n    link_list.remove(5)\\n    print(list(link_list.items()))\\n    # 元素是否存在\\n    print(4 in link_list.items())\\n```\\n\\n```python\\n#定义双向链表结点\\nclass Node(object):\\n    \"\"\"双向链表的结点\"\"\"\\n\\n    def __init__(self, item):\\n        # item存放数据元素\\n        self.item = item\\n        # next 指向下一个节点的标识\\n        self.next = None\\n        # prev 指向上一结点\\n        self.prev = None\\n\\n#定义双向链表\\nclass BilateralLinkList(object):\\n    \"\"\"双向链表\"\"\"\\n\\n    def __init__(self):\\n        self._head = None\\n\\n    def is_empty(self):\\n        \"\"\"判断链表是否为空\"\"\"\\n        return self._head is None\\n\\n    def length(self):\\n        \"\"\"链表长度\"\"\"\\n        # 初始指针指向head\\n        cur = self._head\\n        count = 0\\n        # 指针指向None 表示到达尾部\\n        while cur is not None:\\n            count += 1\\n            # 指针下移\\n            cur = cur.next\\n        return count\\n\\n    def items(self):\\n        \"\"\"遍历链表\"\"\"\\n        # 获取head指针\\n        cur = self._head\\n        # 循环遍历\\n        while cur is not None:\\n            # 返回生成器\\n            yield cur.item\\n            # 指针下移\\n            cur = cur.next\\n\\n    def add(self, item):\\n        \"\"\"向链表头部添加元素\"\"\"\\n        node = Node(item)\\n        if self.is_empty():\\n            # 头部结点指针修改为新结点\\n            self._head = node\\n        else:\\n            # 新结点指针指向原头部结点\\n            node.next = self._head\\n            # 原头部 prev 指向 新结点\\n            self._head.prev = node\\n            # head 指向新结点\\n            self._head = node\\n\\n    def append(self, item):\\n        \"\"\"尾部添加元素\"\"\"\\n        node = Node(item)\\n        if self.is_empty():  # 链表无元素\\n            # 头部结点指针修改为新结点\\n            self._head = node\\n        else:  # 链表有元素\\n            # 移动到尾部\\n            cur = self._head\\n            while cur.next is not None:\\n                cur = cur.next\\n            # 新结点上一级指针指向旧尾部\\n            node.prev = cur\\n            # 旧尾部指向新结点\\n            cur.next = node\\n\\n    def insert(self, index, item):\\n        \"\"\" 指定位置插入元素\"\"\"\\n        if index <= 0:\\n            self.add(item)\\n        elif index > self.length() - 1:\\n            self.append(item)\\n        else:\\n            node = Node(item)\\n            cur = self._head\\n            for i in range(index):\\n                cur = cur.next\\n            # 新结点的向下指针指向当前结点\\n            node.next = cur\\n            # 新结点的向上指针指向当前结点的上一结点\\n            node.prev = cur.prev\\n            # 当前上一结点的向下指针指向node\\n            cur.prev.next = node\\n            # 当前结点的向上指针指向新结点\\n            cur.prev = node\\n\\n    def remove(self, item):\\n        \"\"\" 删除结点 \"\"\"\\n        if self.is_empty():\\n            return\\n        cur = self._head\\n        # 删除元素在第一个结点\\n        if cur.item == item:\\n            # 只有一个元素\\n            if cur.next is None:\\n                self._head = None\\n                return True\\n            else:\\n                # head 指向下一结点\\n                self._head = cur.next\\n                # 下一结点的向上指针指向None\\n                cur.next.prev = None\\n                return True\\n        # 移动指针查找元素\\n        while cur.next is not None:\\n            if cur.item == item:\\n                # 上一结点向下指针指向下一结点\\n                cur.prev.next = cur.next\\n                # 下一结点向上指针指向上一结点\\n                cur.next.prev = cur.prev\\n                return True\\n            cur = cur.next\\n        # 删除元素在最后一个\\n        if cur.item == item:\\n            # 上一结点向下指针指向None\\n            cur.prev.next = None\\n            return True\\n\\n    def find(self, item):\\n        \"\"\"查找元素是否存在\"\"\"\\n        return item in self.items()\\n```\\n\\n为了能够获取和设置Node里面的信息，我们还需要定义几个方法，代码如下：\\n\\n`def __init__(self, data):\\n        self.data = data\\n        self.next = None\\n    \\n    # 获取node里面的数据\\n    def getData(self):\\n        return self.data\\n    \\n    # 获取下一个节点的引用\\n    def getNext(self):\\n        return self.next\\n    \\n    # 设置node里面的数据\\n    def setData(self, newdata):\\n        self.data = newdata\\n    \\n    # 设置下一个节点的引用\\n    def setNext(self, newnext):\\n        self.next = newnext`\\n\\n这些方法用于存取Node里面的数据，方便在链表结构里面去使用。\\n\\nNode对象定义好之后，接下来我们就可以开始定义链表对象了。我们这里讲的是单向链表，所以英文成为Single Link List。定义链表时，最主要的是定义好链表的头(head)，因为之前我们说过，我们只要找到了链表的头，就能够沿着这个头找到其他所有的node。所以，链表的初始定义很简单，我们只要定义一个head属性即可，代码如下：\\n\\n`class MySingleLinkList():\\n    \\n\\n    def __init__(self):\\n\\n        # 定义链表的头结点\\n\\n        self.head = None`\\n\\n**这里大家要注意一点，链表对象本身是不包含任何节点Node对象的，相反，它只包含对链接结构中第一个节点的单个引用(self.head)，这个head实际上永远会指向链表中的第一个节点。如果head为None，实际上意味着这是一个空的链表。链表LinkList对象和Node对象从定义上是独立的，互相并不包含对方。**这个基本思想很重要，只要大家记住这个基本原则，那么我们就可以开始接着实现链表中的其他方法。\\n\\n!https://pic2.zhimg.com/80/v2-825cceb9d3c1a11a17969465bcea06d1_720w.jpg\\n\\n图2\\n\\n一般来说，一个链表中应该包含的基本操作主要有以下几个：\\n\\n- 判断链表是否为空 isEmpty()\\n\\n`def isEmpty(self):\\n    \\'\\'\\'\\n    判断head指向的节点是否为None，如果head指向None，说明该链表为空\\n    \\'\\'\\'\\n    return self.head == None`\\n\\n- 获取链表的长度 size()\\n\\n获取链表的长度的关键是要遍历这个链表，并对节点数进行计数。遍历链表是链表操作中会被频繁使用到的基本操作，像链表节点的查询、删除等操作都会涉及到链表的遍历。我们之前说过，遍历链表时，我们必须先找到链表的head节点，从链表的头部开始不断地查找每个节点的next节点，最终直到某一个节点的next指向None，就说明遍历完成了。\\n\\n`def size(self):\\n    current = self.head  # 将链表的头节点赋值给current，代表当前节点\\n    count = 0\\n    while current != None:\\n        count += 1\\n        current = current.getNext()  # 计数后，不断把下一个节点的引用赋值给当前节点，这样我们就能不断向后面的节点移动\\n    return count`\\n\\n- 向链表中增加一个节点 add()\\n\\n向链表中增加一个节点的时候，我们要考虑两个问题。第一个问题是，新加入的节点该加到哪里？大家可以想一想，链表是一个无序结构，其实把新的节点加到哪里对链表本身来说是无所谓的。但加入到链表的不同位置，对于我们的代码操作难度是有区别的。因为我们之前定义的链表结构始终只保持对第一个节点的引用，所以从这个角度来看，最简单的方法就是把新的节点加入到链表的头部，使新节点成为链表的第一个节点，然后以前的节点依次后移。第二个问题是，加入新节点的时候，要进行哪些操作？实际上要加入一个新的节点，需要两个步骤。首先需要把之前的head节点赋值给新节点的下一个节点，也就是新节点的next，然后再把新节点赋值给head节点，让它成为新的head（如图3所示）。\\n\\n!https://pic4.zhimg.com/80/v2-c1cf651aff22ea10cc54badddcca6aeb_720w.jpg\\n\\n图3\\n\\n`def add(self, val):\\n    temp = Node(val)\\n    temp.next = self.head   # 将原来的开始节点设置为新开始节点的下一节点\\n    self.head = temp        # 将新加入节点设置为现在的第一个节点`\\n\\n必须要注意temp.next = self.head和self.head=temp这两个语句的先后顺序，如果把self.head=temp写在前面，则会使得head原来指向的下一个节点的信息全部丢失，这并不是我们想要的结果(如图4所示)。\\n\\n!https://pic3.zhimg.com/80/v2-b4d28539d2e121c81510b2e675bcaede_720w.jpg\\n\\n图4\\n\\n- 查找指定节点是否在链表中 search()\\n\\n要实现查找算法，必然也是要遍历链表的，我们可以设置一个布尔变量作为是否查找到目标元素的标志，然后通过遍历链表中的每个元素，判断该元素的值是否等于要查找的值，如果是，则将布尔值设置为True，最后返回该布尔值即可。代码如下：\\n\\n`def search(self, item):\\n    current = self.head\\n    found = False\\n    while current != None and not found:\\n        if current.getData() == item:\\n            found = True\\n        else:\\n            current = current.getNext()\\n        \\n    return found`\\n\\n- 移除指定节点 remove()\\n\\n移除指定节点也是在链表中一个常见的操作，在移除指定节点时，除了要先遍历链表找到指定元素外，还需要对这个即将被移除的节点做一些处理，以确保剩下的节点能够正常工作。在我们找到要被移除的节点时，按照之前写过的遍历方法我们知道，current应该是指向要被移除的节点。可问题是怎么才能移除掉该节点呢？为了能够移除节点，我们需要修改上一个节点中的链接，以便使其直接指向当前将被移除节点的下一个节点，使没有任何其他节点指向这个被移除的节点，以达到移除节点的目的。但这里有个问题，就是当我们循环链表到当前节点时，没法回退回去操作当前节点的前一个节点。所以，为了解决这个问题，在遍历链表时，除了需要记录当前指向的节点current外，还需要设置一个变量来记录当前节点的上一个节点previous，每次循环时，如果当前节点不是要被移除的节点，那么就将当前节点的值赋值给previous，而将下一个节点的引用赋值给当前节点，以达到向前移动的目的。同时，在找到了将被移除的节点后，我们会把found设置为true，停止遍历。\\n\\n另外，在删除节点时，可能会有三种情况：\\n\\n（1）被移除的节点就是链表中的开始节点，这时previous一定是None值，我们只需要将current.next赋值给head即可。（2）被移除的节点是链表中最后的节点。（3）被移除的节点是普通节点（即不是第一个也不是最后一个节点）。其中第（2）（3）种情况并不需要特殊处理，直接设置previous的next为current的next即可。\\n\\n`def remove(self, item):\\n    current = self.head\\n    previous = None\\n    found = False\\n    \\n    # 判断指定值是否存在于链表中 \\n    if not self.search(item):\\n        return\\n        \\n    while not found:\\n        if current.getData() == item:\\n            found = True\\n        else:\\n            previous = current\\n            current = current.getNext()\\n        \\n    if previous == None:\\n        self.head = current.getNext()\\n    else:\\n        previous.setNext(current.getNext())`\\n\\n- 获取链表中所有节点的值 getAllData()\\n\\n获取链表中的所有节点的值方便我们随时查看链表中究竟有哪些值。由于链表并不像普通的list一样可以直接打印出来看，所以一般我们需要借助于遍历链表把链表中的每个节点的值取出来放到一个列表中，然后再打印这个列表，从而取得链表中所有节点值的目的。\\n\\n`def getAllData(self):    # 得到链表中所有的值\\n    data = []\\n    current = self.head\\n    while current:\\n        data.append(current.getData())\\n        current = current.getNext()\\n    return data`\\n\\n这些就是我们在链表中常用的操作方法及对应的代码实现，接下来我们可以尝试来操作并测试一下我们写的这些方法对不对。\\n\\n`linkList = MySingleLinkList()\\nfor i in range(10, 50, 5):\\n    linkList.add(i)\\nprint(linkList.size())  # output: 8\\nprint(linkList.getAllData()) # output: [45, 40, 35, 30, 25, 20, 15, 10]\\n\\nlinkList.remove(25)\\nprint(linkList.getAllData()) # output: [45, 40, 35, 30, 20, 15, 10]\\n\\nlinkList.search(25)  # output: False\\nlinkList.isEmpyt()   # output: False`\\n\\n---\\n\\n##########################\\n\\n示例：操作链表\\n\\n```python\\n**if** __name__ **==** \\'__main__\\':\\n    link_list **=** SingleLinkList()\\n    *# 向链表尾部添加数据***for** i **in** range(5):\\n        link_list**.**append(i)\\n    *# 向头部添加数据*link_list**.**add(6)\\n    *# 遍历链表数据***for** i **in** link_list**.**items():\\n        print(i, end**=**\\'\\\\t\\')\\n    *# 链表数据插入数据*link_list**.**insert(3, 9)\\n    print(\\'\\\\n\\', list(link_list**.**items()))\\n    *# 删除链表数据*link_list**.**remove(0)\\n    *# 查找链表数据*print(link_list**.**find(4))\\n```\\n\\n#插入单链表\\n\\n\\n\\n```python\\nListValue = [1, 4, 5, 2]\\nListRight = [3, 2, -1, 1]\\nhead = 0\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000 #初始化头指针\\nnum = 3\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000#num为要插入的元素\\nnext,last = head,head\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000#初始化表示插入位置的下一个元素和上一个元素的指针\\ndef Output():\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000 #定义一个函数用于输出链表\\n    next = head\\n    while next != -1:\\n        print(ListValue[next])\\n        next = ListRight[next]\\nOutput()\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000 #输出列表查看插入前的顺序\\n    while ListValue[next] <= num and next != -1:\\u3000\\u3000 #找到适合插入元素的位置\\n        last = next\\n        next = ListRight[next]\\nListValue.append(num)\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000 #向数组末尾加上新元素的值\\nListRight.append(ListRight[last])\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000 #加上新元素指针指向的位置（下一个元素）\\nListRight[last] = len(ListValue)-1\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000#上一个元素的指针指向新元素\\nOutput()\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000 #输出列表查看结果\\n```\\n\\n节点类定义如下：\\n\\nclass Node:\\ndef **init**(self,cargo = None, next = None):\\nself.cargo = cargo\\nself.next = next\\ndef **str**(self):\\n#测试基本功能，输出字符串\\nreturn str(self.cargo)\\nprint Node(\"text\")\\n#输出text\\n\\n因为任何值都能通过str函数，且能存储下来。\\n\\n链表怎么定义呢？\\n我们可以先定义一个一个节点，如下：\\n\\nnode1 = Node(1)\\nnode2 = Node(2)\\nnode3 = Node(3)\\n1\\n2\\n3\\n然后再把每个节点的关系表示出来，就OK了\\n\\nnode1.next = node2\\nnode2.next = node3\\n————————————————\\n2.1 计算链表长度\\n\\nclass Node(object):\\n#节点类\\n#功能：输入一个值data，将值变为一个节点\\ndef **init**(self, data, next = None):\\nself.data = data\\nself.next = next\\n\\n```\\nclass Node(object):\\n#节点类\\n    #功能：输入一个值data，将值变为一个节点\\n    def __init__(self, data, next = None):\\n        self.data = data\\n        self.next = next\\n\\n    def __str__(self):\\n        return self.data\\n\\nclass LinkedList(object):\\n\\n    def __init__(self, head = None):\\n        self.head = head\\n    def __len__(self):\\n        #功能：输入头节点，返回链表长度\\n        curr = self.head\\n        counter = 0\\n        while curr is not None:\\n            counter += 1\\n            curr = curr.next\\n        return counter\\n————————————————\\n版权声明：本文为CSDN博主「黄小猿」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/qq_39422642/article/details/78988976\\n```\\n\\n> 2.2 从前，后插入\\n> \\n\\n从前插入：\\n\\n- 被插入数据为空，返回\\n- 使用该输入数据创建一个节点，并将该节点指向原来头节点\\n- 设置该节点为头节点\\n\\n\\n\\n从后：append\\n\\n- 若输入数据为空，返回None\\n- 若头节点为空，直接将输入数据作为头节点\\n- 遍历整个链表，直到当前节点的下一个节点为None时，将当前节点的下一个节点设置为输入数据\\n\\n\\n\\n**查找**\\n\\n- 若查找的数据为空，返回\\n- 设置头节点为当前节点，若当前节点不为None,遍历整个链表\\n- 若当前节点的data与输入的data相同，但会当前节点，否则轮到下一个节点\\n\\n\\n\\n删除1\\n申请两个变量，如果遇到匹配的，不用删除，直接将匹配节点的前一节点指向匹配节点的下一节点，因此需要定义一个前节点和一个当前节点，当前节点用来判断是否与输入数据匹配，前节点用来更改链表的指向。\\n\\n若输入数据为None,返回\\n将头节点设置为前节点，头节点的下一个节点设置为当前节点\\n判断前节点是否与输入数据匹配，若匹配，将头节点设置为当前节点\\n遍历整个链表，若当前节点与输入数据匹配，将前节点的指针指向当前节点的下一个节点，否则，移到下一个节点\\n\\n时间复杂度为O(n),空间复杂度为O(1).\\n————————————————\\n版权声明：本文为CSDN博主「黄小猿」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/qq_39422642/article/details/78988976\\n\\n\\n\\n**删除2**第二种解决办法就是只定义一个变量作为当前节点，使用它的下一个节点去判断是否与数据数据匹配，若匹配，直接将当前节点指向下下一个节点。\\n\\n时间复杂度为O(n),空间复杂度为O(1).\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e0530c65-1c0a-4352-b3d2-9f3b22e54e32', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nbacktracking\\n\\nVIPVIP this explains backtrack by putting recur function before vs. after the execution of the function  \\n\\nL2. Problems on Recursion - YouTube\\n\\n[[]] at master · [[]]\\n\\n回溯法是一种复杂度很高的暴力搜索算法，实现简单且有固定模板，常被用于搜索排列组合问题的所有可行性解。不同于普通的暴力搜索，回溯法会在每一步判断状态是否合法，而不是等到状态全部生成后再进行确认。当某一步状态非法时，它将回退到上一步中正确的位置，然后继续搜索其他不同的状态。前进和后退是回溯法的关键动作，因此可以使用递归去模拟整个过程，即使用递归实现回溯法\\n\\n判断回溯很简单，拿到一个问题，你感觉如果不穷举一下就没法知道答案，那就可以开始回溯了。一般回溯的问题有三种：\\n\\n1. Find a path to success 有没有解\\n2. Find all paths to success 求所有解\\n    - 求所有解的个数\\n    - 求所有解的具体信息\\n3. Find the best path to success 求最优解\\n\\n理解回溯：给一堆选择, 必须从里面选一个. 选完之后我又有了新的一组选择. This procedure is repeated over and over until you reach a final state. If you made a good sequence of choices, your final state is a goal state; if you didn\\'t, it isn\\'t.\\n\\n回溯是一种算法思想，主要是通过递归来构建并求解问题，当发现不满足问题的条件时，就回溯寻找其他的满足条件的解。\\n\\n这类问题之所以难是因为通常运用回溯思想求解的问题，通常是递归和循环同时存在，是的思考问题和求解的过程变得比较复杂。\\n\\n通过几道典型问题的求解，大致可以将回溯问题的求解步骤分为三个：\\n\\n1. choices 即在这个问题中，我们可以做哪些事情。例如在数独空格中我们的选择有0 - 9是个数字。\\n2. constraint 即我们并不能随心所欲的选择，在做选择的同时，有一定的约束会限制我们的选择。比如数独空格中我们需要依据数独的规则，每个空格选填一个数字，并不是每个空格随心所欲的填0-9 任意一个都可以。\\n3. goals 或者我们叫做递归的停止条件。 即随着选择的进行，我们什么时候就不用再做选择了，即问题求解完成\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nhttps://www.cis.upenn.edu/~matuszek/cit594-2012/Pages/backtracking.html\\n\\nBacktracking is a form of recursion.\\n\\nThe usual scenario is that you are faced with a number of options, and you must choose one of these. After you make your choice you will get a new set of options; just what set of options you get depends on what choice you made. This procedure is repeated over and over until you reach a final state. If you made a good sequence of choices, your final state is a\\xa0goal state;\\xa0if you didn\\'t, it isn\\'t.\\n\\nConceptually, you start at the root of a tree; the tree probably has some good leaves and some bad leaves, though it may be that the leaves are all good or all bad. You want to get to a good leaf. At each node, beginning with the root, you choose one of its children to move to, and you keep this up until you get to a leaf.\\n\\nSuppose you get to a bad leaf. You can\\xa0backtrack\\xa0to continue the search for a good leaf by revoking your\\xa0most recent\\xa0choice, and trying out the next option in that set of options. If you run out of options, revoke the choice that got you here, and try another choice at that node. If you end up at the root with no options left, there are no good leaves to be found.\\n\\nThis needs an example.\\n\\n!Untitled\\n\\n1. Starting at Root, your options are A and B. You choose A.\\n2. At A, your options are C and D. You choose C.\\n3. C is bad. Go back to A.\\n4. At A, you have already tried C, and it failed. Try D.\\n5. D is bad. Go back to A.\\n6. At A, you have no options left to try. Go back to Root.\\n7. At Root, you have already tried A. Try B.\\n8. At B, your options are E and F. Try E.\\n9. E is good. Congratulations!\\n\\nIn this example we drew a picture of a tree. The tree is an abstract model of the possible sequences of choices we could make. There is also a data structure called a tree, but usually we don\\'t have a data structure to tell us what choices we have. (If we do have an actual tree data structure, backtracking on it is called\\xa0depth-first tree searching.)\\n\\n**The backtracking algorithm.**\\n\\nHere is the algorithm (in pseudocode) for doing backtracking from a given node n:\\n\\n> boolean solve(Node n) {\\n    if n is a leaf node {\\n        if the leaf is a goal node, return true\\n        else return false\\n    } else {\\n        for each child c of n {\\n            if solve(c) succeeds, return true\\n        }\\n        return false\\n    }\\n}\\n> \\n\\nNotice that the algorithm is expressed as a\\xa0boolean\\xa0function. This is essential to understanding the algorithm. If\\xa0`solve(n)`\\xa0is true, that means node\\xa0`n`\\xa0is part of a solution--that is, node\\xa0`n`\\xa0is one of the nodes on a path from the root to some goal node. We say that\\xa0`n`\\xa0is\\xa0solvable. If\\xa0`solve(n)`\\xa0is false, then there is\\xa0no\\xa0path that includes\\xa0`n`\\xa0to any goal node.\\n\\nHow does this work?\\n\\n- If any child of\\xa0`n`\\xa0is solvable, then\\xa0`n`\\xa0is solvable.\\n- If no child of\\xa0`n`\\xa0is solvable, then\\xa0`n`\\xa0is not solvable.\\n\\nHence, to decide whether any non-leaf node\\xa0`n`\\xa0is solvable (part of a path to a goal node), all you have to do is test whether any child of\\xa0`n`\\xa0is solvable. This is done recursively, on each child of\\xa0`n`. In the above code, this is done by the lines\\n\\n```\\n        for each child c of n {\\n            if solve(c) succeeds, return true\\n        }\\n        return false\\n```\\n\\nEventually the recursion will \"bottom\" out at a leaf node. If the leaf node is a goal node, it is solvable; if the leaf node is not a goal node, it is not solvable. This is our base case. In the above code, this is done by the lines\\n\\n> if n is a leaf node {\\n    if the leaf is a goal node, return true\\n    else return false\\n}\\n> \\n\\nThe backtracking algorithm is simple but important. You should understand it thoroughly. Another way of stating it is as follows:\\n\\nTo search a tree:\\n1. If the tree consists of a single leaf, test whether it is a goal node,\\n2. Otherwise, search the subtrees until you find one containing a goal node, or until you have searched them all unsuccessfully.\\n\\n**Non-recursive backtracking, using a stack**\\n\\nBacktracking is a rather typical recursive algorithm, and any recursive algorithm can be rewritten as a stack algorithm. In fact, that is how your recursive algorithms are translated into machine or assembly language.\\n\\n> boolean solve(Node n) {\\n    put node n on the stack;\\n    while the stack is not empty {\\n        if the node at the top of the stack is a leaf {\\n            if it is a goal node, return true\\n            else pop it off the stack\\n        }\\n        else {\\n            if the node at the top of the stack has untried children\\n                push the next untried child onto the stack\\n            else pop the node off the stack\\n\\n    }\\n    return false\\n}\\n> \\n\\nStarting from the root, the only nodes that can be pushed onto the stack are the children of the node currently on the top of the stack, and these are only pushed on one child at a time; hence, the nodes on the stack at all times describe a valid path in the tree. Nodes are removed from the stack only when it is known that they have no goal nodes among their descendents. Therefore, if the root node gets removed (making the stack empty), there must have been no goal nodes at all, and no solution to the problem.\\n\\nWhen the stack algorithm terminates successfully, the nodes on the stack form (in reverse order) a path from the root to a goal node.\\n\\nSimilarly, when the recursive algorithm finds a goal node, the path information is embodied (in reverse order) in the sequence of recursive calls. Thus as the recursion unwinds, the path can be recovered one node at a time, by (for instance) printing the node at the current level, or storing it in an array.\\n\\nHere is the recursive backtracking algorithm, modified slightly to print (in reverse order) the nodes along the successful path:\\n\\n> boolean solve(Node n) {\\n    if n is a leaf node {\\n        if the leaf is a goal node {\\n           print n\\n           return true\\n        }\\n        else return false\\n    } else {\\n        for each child c of n {\\n            if solve(c) succeeds {\\n                print n\\n                return true\\n            }\\n        }\\n        return false\\n    }\\n}\\n> \\n\\n**Keeping backtracking simple**\\n\\nAll of these versions of the backtracking algorithm are pretty simple, but when applied to a real problem, they can get pretty cluttered up with details. Even determining whether the node is a leaf can be complex: for example, if the path represents a series of moves in a chess endgame problem, the leaves are the checkmate and stalemate solutions.\\n\\nTo keep the program clean, therefore, tests like this should be buried in methods. In a chess game, for example, you could test whether a node is a leaf by writing a\\xa0`gameOver`\\xa0method (or you could even call it\\xa0`isLeaf`). This method would encapsulate all the ugly details of figuring out whether any possible moves remain.\\n\\nNotice that the backtracking altorithms require us to keep track, for each node on the current path, which of its children have been tried already (so we don\\'t have to try them again). In the above code we made this look simple, by just saying\\xa0`for\\xa0each\\xa0child\\xa0c\\xa0of\\xa0n`. In reality, it may be difficult to figure out what the possible children are, and there may be no obvious way to step through them. In chess, for example, a node can represent one arrangement of pieces on a chessboard, and each child of that node can represent the arrangement after some piece has made a legal move. How do you find these children, and how do you keep track of which ones you\\'ve already examined?\\n\\nThe most straightforward way to keep track of which children of the node have been tried is as follows: Upon initial entry to the node (that is, when you first get there from above), make a list of all its children. As you try each child, take it off the list. When the list is empty, there are no remaining untried children, and you can return \"failure.\" This is a simple approach, but it may require quite a lot of additional work.\\n\\nThere is an easier way to keep track of which children have been tried,\\xa0**if**\\xa0you can define an ordering on the children. If there is an ordering, and you know which child you just tried, you can determine which child to try next.\\n\\nFor example, you might be able to number the children\\xa0`1`\\xa0through\\xa0`n`, and try them in numerical order. Then, if you have just tried child\\xa0`k`, you know that you have already tried children\\xa0`1`\\xa0through\\xa0`k-1`, and you have not yet tried children\\xa0`k+1`\\xa0through\\xa0`n`. Or, if you are trying to color a map with just four colors, you can always try red first, then yellow, then green, then blue. If child yellow fails, you know to try child green next. If you are searching a maze, you can try choices in the order left, straight, right (or perhaps north, east, south, west).\\n\\nIt isn\\'t always easy to find a simple way to order the children of a node. In the chess game example, you might number your pieces (or perhaps the squares of the board) and try them in numerical order; but in addition each piece may also have several moves, and these must also be ordered.\\n\\nYou can probably find some way to order the children of a node. If the ordering scheme is simple enough, you should use it; but if it is too cumbersome, you are better off keeping a list of untried children.\\n\\n**Example: TreeSearch**\\n\\nFor starters, let\\'s do the simplest possible example of backtracking, which is searching an actual tree. We will also use the simplest kind of tree, a binary tree.\\n\\nA\\xa0**binary tree**\\xa0is a data structure composed of\\xa0**nodes**. One node is designated as the\\xa0**root node**. Each node can reference (point to) zero, one, or two other nodes, which are called its\\xa0**children**. The children are referred to as the\\xa0**left child**\\xa0and/or the\\xa0**right child**. All nodes are reachable (by one or more steps) from the root node, and there are no cycles. For our purposes, although this is not part of the definition of a binary tree, we will say that a node might or might not be a goal node, and will contain its name. The first example in this paper (which we repeat here) shows a binary tree.\\n\\n!Untitled\\n\\nHere\\'s a definition of the BinaryTree class:\\n\\n> public class BinaryTree {\\n    BinaryTree leftChild = null;\\n    BinaryTree rightChild = null;\\n    boolean isGoalNode = false;\\n    String name;\\n    \\n    BinaryTree(String name, BinaryTree left, BinaryTree right, boolean isGoalNode) {\\n        this.name = name;\\n        leftChild = left;\\n        rightChild = right;\\n        this.isGoalNode = isGoalNode;\\n    }\\n}\\n> \\n\\nNext we will create a\\xa0`TreeSearch`\\xa0class, and in it we will define a method\\xa0`makeTree()`\\xa0which constructs the above binary tree.\\n\\n> static BinaryTree makeTree() {\\n    BinaryTree root, a, b, c, d, e, f;\\n    c = new BinaryTree(\"C\", null, null, false);\\n    d = new BinaryTree(\"D\", null, null, false);\\n    e = new BinaryTree(\"E\", null, null, true);\\n    f = new BinaryTree(\"F\", null, null, false);\\n    a = new BinaryTree(\"A\", c, d, false);\\n    b = new BinaryTree(\"B\", e, f, false);\\n    root = new BinaryTree(\"Root\", a, b, false);\\n    return root;\\n}\\n> \\n\\nHere\\'s a main program to create a binary tree and try to solve it:\\n\\n> public static void main(String args[]) {\\n    BinaryTree tree = makeTree();\\n    System.out.println(solvable(tree));\\n}\\n> \\n\\nAnd finally, here\\'s the recursive backtracking routine to \"solve\" the binary tree by finding a goal node.\\n\\n> static boolean solvable(BinaryTree node) {\\n/* 1 */  if (node == null) return false;\\n/* 2 */  if (node.isGoalNode) return true;\\n/* 3 */  if (solvable(node.leftChild)) return true;\\n/* 4 */  if (solvable(node.rightChild)) return true;\\n/* 5 */  return false;\\n}\\n> \\n\\nHere\\'s what the numbered lines are doing:\\n\\n1. If we are given a null node, it\\'s not solvable. This statement is so that we can call this method with the children of a node, without first checking whether those children actually exist.\\n2. If the node we are given is a goal node, return success.\\n3. See if the left child of\\xa0`node`\\xa0is solvable, and if so, conclude that\\xa0`node`\\xa0is solvable. We will only get to this line if\\xa0`node`\\xa0is non-null and is not a goal node, says to\\n4. Do the same thing for the right child.\\n5. Since neither child of\\xa0`node`\\xa0is solvable,\\xa0`node`\\xa0itself is not solvable.\\n\\nThis program runs correctly and produces the unenlightening result\\xa0`true`.\\n\\nEach time we ask for another node, we have to check if it is\\xa0`null`. In the above we put that check as the first thing in\\xa0`solvable`. An alternative would be to check first whether each child exists, and recur only if they do. Here\\'s that alternative version:\\n\\n> static boolean solvable(BinaryTree node) {\\n    if (node.isGoalNode) return true;\\n    if (node.leftChild != null && solvable(node.leftChild)) return true;\\n    if (node.rightChild != null && solvable(node.rightChild)) return true;\\n    return false;\\n}\\n> \\n\\nI think the first version is simpler, but the second version is slightly more efficient.\\n\\n**What are the children?**\\n\\nOne of the things that simplifies the above binary tree search is that, at each choice point, you can ignore all the previous choices. Previous choices don\\'t give you any information about what you should do next; as far as you know, both the left and the right child are possible solutions. In many problems, however, you may be able to eliminate children immediately, without recursion.\\n\\nConsider, for example, the problem of four-coloring a map. It is a theorem of mathematics that any map on a plane, no matter how convoluted the countries are, can be colored with at most four colors, so that no two countries that share a border are the same color.\\n\\nTo color a map, you choose a color for the first country, then a color for the second country, and so on, until all countries are colored. There are two ways to do this:\\n\\n- **Method 1.**\\xa0Try each of the four possible colors, and recur. When you run out of countries, check whether you are at a goal node.\\n- **Method 2.**\\xa0Try only those colors that have not already been used for an adjacent country, and recur. If and when you run out of countries, you have successfully colored the map.\\n\\nLet\\'s apply each of these two methods to the problem of coloring a checkerboard. This should be easily solvable; after all, a checkerboard only needs two colors.\\n\\nIn both methods, the colors are represented by integers, from\\xa0`RED=1`\\xa0to\\xa0`BLUE=4`. We define the following helper methods. The helper method code isn\\'t displayed here because it\\'s not important for understanding the method that does the backtracking.\\n\\n`boolean mapIsOK()`Used by method 1 to check (at a leaf node) whether the entire map is colored correctly.`boolean okToColor(int row, int column, int color)`Used by method 2 to check, at every node, whether there is an adjacent node already colored with the given color.`int[] nextRowAndColumn(int row, int column)`Used by both methods to find the next \"country\" (actually, the row and column of the next square on the checkerboard).\\n\\nHere\\'s the code for method 1:\\n\\n> boolean explore1(int row, int column, int color) {\\n    if (row >= NUM_ROWS) return mapIsOK();\\n    map[row][column] = color;\\n    for (int nextColor = RED; nextColor <= BLUE; nextColor++) {\\n        int[] next = nextRowAndColumn(row, column);\\n        if (explore1(next[0], next[1], nextColor)) return true;\\n    }\\n    return false;\\n}\\n> \\n\\nAnd here\\'s the code for method 2:\\n\\n> boolean explore2(int row, int column, int color) {\\n    if (row >= NUM_ROWS) return true;\\n    if (okToColor(row, column, color)) {\\n        map[row][column] = color;\\n        for (int nextColor = RED; nextColor <= BLUE; nextColor++) {\\n            int[] next = nextRowAndColumn(row, column);\\n            if (explore2(next[0], next[1], nextColor)) return true;\\n        }\\n    }\\n    return false;\\n}\\n> \\n\\nThose appear pretty similar, and you might think they are equally good. However, the timing information suggests otherwise:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dccd2d41-7207-409f-9db2-0f1da2d35a4e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='|Name|Tags|note|rep1|rep2|rep3|Retention|\\n|---|---|---|---|---|---|---|\\n|1. Two Sum|\"array, hash\"|,No|No|No,|\\n|100. Same Tree|\"recur, tree\"|在每个node上，比较当前node，和他的left right child是不是一样|Yes|Yes|No|perfect|\\n|1005.\\xa0Maximize Sum Of Array After K Negations|\"array, greedy\"|巧用index找到min value，不用重复reorder|No|No|No,|\\n|101 Symmetric tree|\"VIP, bfs, dfs, recur, tree\"|\"关键是不能直接递归主函数; check my code 我的错误主要是，递归的处理晚了一步，导致无法take care of null，又不能加一堆if is not none； 观察可以发现不能从第一步就开始递归，要建立一个left, right为input的函数; 这题其实用bfs比较好理解\"|Yes|Yes|No|bad|\\n|1013.\\xa0Partition Array Into Three Parts With Equal Sum|\"array, greedy\"|,No|No|No,|\\n|102. Binary Tree Level Order Traversal|\"VIP, bfs, tree\"|node object and value object 注意要用不同的list来存不要弄混，所以这里需要多出一个level_result list; VIP for i in stack 是错的，deque object不能这样循环，看code; BFS 才是通解|Yes|Yes|Yes|mid|\\n|103. Binary Tree Zigzag Level Order Traversal|tree|\"记住appendleft, deque的使用方式: import collections, level_result = collections.deque()\"|No|No|No,|\\n|104 max depth of binary tree|\"VIP, recur, tree\"|\"递归类似一个人不知道自己在电影院的第几排，他问前面的人，前面的人也不知道在第几排，就继续往前问，直到问到第一排的人才知道答案，那第二排的人就知道自己在No2, 然后依次把答案传递回来\"|Yes|Yes|No|mid|\\n|1046.\\xa0Last Stone Weight|\"VIP, heap\"|,No|No|No,|\\n|105. Construct Binary Tree from Preorder and Inorder Traversal|tree|学lucifer的手动画一下图才能懂 https://lucifer.ren/blog/2020/02/08/%E6%9E%84%E9%80%A0%E4%BA%8C%E5%8F%89%E6%A0%91%E4%B8%93%E9%A2%98/|Yes|No|No|mid|\\n|106. Construct Binary Tree from Inorder and Postorder Traversal|tree|,No|No|No,|\\n|107. Binary Tree Level Order Traversal II|\"VIP, linked list, stack/queue, tree\"|注意update current level这种循环方式; 注意循环的层级; result[::-1]逆向list ; 递归和循环都要掌握|Yes|No|No|mid|\\n|108 Convert Sorted Array to Binary Search Tree|\"recur, tree\"|注意nums is none 和 not nums 的区别|Yes|Yes|No|mid|\\n|109.\\xa0Convert Sorted List to Binary Search Tree|\"linked list, recur, tree\"|,No|No|No,|\\n|109.\\xa0Convert Sorted List to Binary Search Tree|tree|,No|No|No,|\\n|11.\\xa0Container With Most Water|\"greedy, two_pointer\"|关键是理解，左右指针如何决定移哪个|No|No|No,|\\n|110. Balanced Binary Tree|\"VIP, recur, tree\"|check my own code to see how to reduce time complexity；学会这个-1的写法|Yes|Yes|No|bad|\\n|111. Minimum Depth of Binary Tree|\"recur, tree\"|,Yes|Yes|No|mid|\\n|112. Path Sum|\"VIP, bfs, dfs, recur, tree\"|递归的本质是，找到一个basic case，然后让后面的case全部可以用这个基础算式来算；bfs 比较简单的题目比如这种binary tree的，后面的node之间不会有交集，不需要marked set来记录是否已访问|Yes|Yes|No|bad|\\n|113.\\xa0Path Sum II|\"VIP, backtracking, dfs, recur, tree\"|,No|No|No,|\\n|113.\\xa0Path Sum II|\"dfs, tree\"|,Yes|No|No|perfect|\\n|114. Flatten Binary Tree to Linked List|\"VIP, tree\"|\"note that the recur function does not explicity return anything. It only changes the root in every round \"|No|No|No,|\\n|1146.\\xa0Snapshot Array|\"array, binary search, design, hash\"|,No|No|No,|\\n|116. Populating Next Right Pointers in Each Node|tree|\"难度不大，想清楚link怎么连的就行; 思考为何用preorder, 不用别的\"|Yes|No|No|good|\\n|118 triangle|\"array, dp, recur\"|,Yes|No|No|bad|\\n|121. Best Time to Buy and Sell Stock|\"dp, sliding_window\"|,Yes|No|No|bad|\\n|122. Best Time to Buy and Sell Stock II|\"VIP, dp\"|这道题是双变量的dp典型|Yes|No|No|mid|\\n|123. Best time to buy and sell stock 3|dp|,No|No|No,|\\n|124.\\xa0Binary Tree Maximum Path Sum|\"VIP, dfs, tree\"|\"vip的是这种，借助一个辅助函数的循环，在辅助函数里update res的思路；这个思路和110 check for balance binary tree 一样 \"|No|No|No,|\\n|125. Valid Palindrome|\"VIP, string, two_pointer\"|\"双指针是正常的办法；最好不要用python函数取巧； 注意除掉非字符或数字的元素的写法很关键要记住; a[::-1] 注意要用到这个方法反转一个string; 要用到x.isalnum() 来判断是否是字符; string = \\'\\'.join([x for x in s.lower() if x.isalnum()]) #先去除掉非字符或数字的元素， 要记住; 另一种方法是re.sub(r\\'\\\\W+\\', \\'\\', x); 注意默认return是true or false很重要\"|Yes|Yes|No|mid|\\n|128.\\xa0Longest Consecutive Sequence|\"VIP, array, hash\"|注意如何create a set of a list: myset = set(nums)|No|No|No,|\\n|1290. Convert Binary Number in a Linked List to Integer|\"VIP, recur\"|理解二进制的转换； 二进制转化的题一般都是递归|Yes|No|No|mid|\\n|13. Roman to Integer|\"hash, string\"|关键是花一点时间找到规律比如1994; 动笔写下来规律; range()的用法要熟练； #这个用n-2 来避免循环里出现空值的方法很关键|Yes|No|No|bad|\\n|130.\\xa0Surrounded Regions|\"VIP, graph, matrix\"|反向思维; 只有找邻居的步骤需要dfs|No|No|No,|\\n|131. Palindrome Partitioning|\"VIP, backtracking, dp\"|,No|No|No|bad|\\n|133.\\xa0Clone Graph|\"bfs, dfs, graph\"|,No|No|No,|\\n|\"1333.\\xa0Filter Restaurants by Vegan-Friendly, Price and Distance\"|\"array, sort\"|\"new_list.sort( key = lambda x: (-x[1], -x[0]))\"|No|No|No,|\\n|134.\\xa0Gas Station|greedy|,No|No|No,|\\n|135. Candy|greedy|greedy的题好多都要分情况讨论|No|No|No,|\\n|1351. Count Negative Numbers in a Sorted Matrix|\"VIP, binary search\"|binary search的灵活运用|Yes|No|No|bad|\\n|136 Single_Numbe|hash|try except keyerror的用法； set用法|Yes|No|No|perfect|\\n|138.\\xa0Copy List with Random Pointer|\"hash, linked list\"|\"有点奇怪的题，要熟悉; 问题的关键是，比如node2 的random指向了node 5, 但node5还不存在；解决办法是copy两轮\"|No|No|No,|\\n|139. Word Break|dp|典型的双重循环 dp|Yes|Yes|Yes|bad|\\n|14. Longest Common Prefix|\"VIP, string\"|strs[0][0:i] != strs[j][0:i] 比较一截string是否相等不用一个一个元素来比; 注意prefix一定是从第一个字母开始的|Yes|Yes|No|bad|\\n|141. Linked List Cycle|\"linked list, two_pointer\"|有快慢指针的思路在里面；写node的时候要保证note存在，所以要考虑用try except的写法; try的用法没懂|Yes|No|No|good|\\n|143.\\xa0Reorder List|\"VIP, linked list\"|\"一道很综合的题目，包含了反转等操作； VIP 因为这里fast, slow并没有指向同一个class object，所以他们不会改变head的位置；\"|No|No|No,|\\n|1436.\\xa0Destination City|hash|要记得用set; difference的用法|No|No|No,|\\n|1448.\\xa0Count Good Nodes in Binary Tree|\"bfs, dfs, tree\"|,No|No|No,|\\n|146.\\xa0LRU Cache|\"hash, linked list\"|,No|No|No,|\\n|147. Insertion Sort List|\"VIP, linked list, sort\"|,Yes|No|No,|\\n|15.\\xa03Sum|\"VIP, array, sort, two_pointer\"|\"sort 之后用two pointer，就可以让搜索两次n^2 变为搜索一次; tuple is essential to use set on list of list \"|No|No|No,|\\n|150. Evaluate Reverse Polish Notation|stack/queue|,No|No|No,|\\n|152. Maximum Product Subarray|dp|reversed 的用法要记住|Yes|No|No|mid|\\n|153 Find Minimum in Rotated Sorted Array|\"VIP, binary search\"|if left == right 作为搜索的最终点，说明只剩一个元素了； 而现在缺少直接判断的条件（mid==target）。幸运的是，我们还是能够通过二分法不断地缩小最终答案可能存在的区间，当区间只剩下一个元素时（l==h），那么它就是最终答案; 至今还没见过left = mid 的只见过right = mid 的，left 永远是 mid + 1|Yes|Yes|No|mid|\\n|155 min stack|stack/queue|注意全局变量所有地方都要加self; 用空间复杂度换时间复杂度|Yes|No|No,|\\n|160. Intersection of Two Linked Lists|\"hash, linked list, two_pointer\"|,Yes|No|No|mid|\\n|1603.\\xa0Design Parking System|design|,No|No|No,|\\n|162 find peak element|binary search|,Yes|No|No|mid|\\n|167.\\xa0Two Sum II - Input Array Is Sorted|two_pointer|如果是按大小排列，似乎用two pointer就很合理|No|No|No,|\\n|169. Majority Element|array|可用hash count|Yes|No|No|good|\\n|17. Letter Combinations of a Phone Number|\"VIP, backtracking, string\"|从空开始循环的方法很少见; 循环加递归的模板题目|No|No|No,|\\n|171. Excel Sheet Column Number|math|\"用类似二进制的操作，把A, AA, AAA都转化成数字; ord 的用法； 循环的写法；本质上要得出基于s每个字符加一位之后的递推公式就能写出来了，得出通项公式没用\"|Yes|No|No|bad|\\n|172. Factorial Trailing Zeroes|\"math, recur\"|,Yes|No|No|bad|\\n|19.Remove Nth Node From End of List|\"VIP, linked list, two_pointer\"|#print out所有步骤就会发现，fast = fast.next 这类操作不会改变linked list本身，只是head处在的位置不同了， 只有slow.next = slow.next.next会改变|Yes|No|No|mid|\\n|190. Reverse Bits|string|把数字变成string的常用方法|No|No|No,|\\n|198. House Robber|\"VIP, array, dp\"|注意base case需要考虑两个case|Yes|No|No|good|\\n|199. Binary Tree Right Side View|tree|\"when to add a new function besides the main function? when your new function needs to add more parameter than the original one \"|No|No|No,|\\n|2. Add Two Numbers|\"linked list, recur\"|\"#为何是return added不是current node? 因为added完全没有被修改过; reversed 函数的用法要掌握, 这函数返回的是一个iterator，不是一个简单的reversed list\"|Yes|No|No|good|\\n|20. Valid Parentheses|\"VIP, stack/queue, string\"|continue的用法要掌握; 用stack的题目，往往都是需要随着循环把前面的东西remove掉，露出顶部的|Yes|Yes|No|mid|\\n|200.\\xa0Number of Islands|\"VIP, array, bfs, dfs, matrix\"|[[]]|No|No|No,|\\n|202. Happy Number|\"VIP, hash, math\"|关键是想清楚啥时候会无限循环啥时候不会；set hash; 这类不知道循环次数的循环只能用while; 注意拆分数字为str的写法tmp = sum([int(c) ** 2 for c in str(n)])|Yes|No|No|bad|\\n|203. Remove Linked List Elements|\"VIP, linked list, recur\"|\"基础linked list 关于pre, curr 的设定\"|Yes|No|No|mid|\\n|204. Count Primes|\"hash, math\"|先全设为true，再通过条件筛选出false的值|Yes|No|No|bad|\\n|205.\\xa0Isomorphic Strings|\"hash, string\"|,No|No|No,|\\n|206. Reverse Linked List|\"VIP, linked list, recur\"|,Yes|No|No|mid|\\n|207.\\xa0Course Schedule|\"bfs, dfs, graph\"|topological sort|No|No|No,|\\n|209. Minimum Size Subarray Sum|sliding_window|\"if requires contagious numbers, always consider sliding window\"|No|No|No,|\\n|21. Merge Two Sorted Lists|\"VIP, linked list, recur, two_pointer\"|递归的方法is better; 掌握如何建立一个空链表，就像建立空list一样|Yes|No|No|mid|\\n|213. House robber 2|dp|,No|No|No,|\\n|215.\\xa0Kth Largest Element in an Array|,,No|No|No,|\\n|217_Contains_Duplicate|hash|\"要掌握set add的用法: set|\\n|.add|\\n|(elem)\"|Yes|No|No|perfect|\\n|218.\\xa0The Skyline Problem|heap|,No|No|No,|\\n|22. Generate Parentheses|\"array, backtracking\"|,No|No|No,|\\n|226. Invert Binary Tree|\"bfs, dfs, recur, tree\"|注意赋值时写成一排和两排的关键区别，这个区别在linked list里很重要|Yes|Yes|No|perfect|\\n|227.\\xa0Basic Calculator II|\"math, stack/queue, string\"|,No|No|No,|\\n|23.\\xa0Merge k Sorted Lists|\"heap, sort\"|merge sort 的经典|No|No|No,|\\n|230. Kth Smallest Element in a BST|\"VIP, dfs, tree\"|这种match一个target value的，都可以考虑update这个value; VIP inorder travesal creates a sorted list|Yes|No|No|bad|\\n|231. Power of Two|recur|连续乘积的都可以尝试递归|Yes|Yes|No|perfect|\\n|232.\\xa0Implement Queue using Stacks|stack/queue|,No|No|No,|\\n|234. Palindrome Linked List|\"linked list, two_pointer\"|linked list放到list里来解|Yes|No|No,|\\n|235. Lowest Common Ancestor of a Binary Search Tree|\"recur, tree\"|注意bst的性质； while 和 if区别； 用了递归的题一般不用while，因为递归就是在循环操作了；可以用递归和非递归的方式来写|Yes|Yes|No|perfect|\\n|236. Lowest Common Ancestor of a Binary Tree|\"VIP, tree\"|反直觉的dfs; 需要先想清楚规律|Yes|No|No|bad|\\n|237. Delete Node in a Linked List|linked list|,Yes|No|No|bad|\\n|238.\\xa0Product of Array Except Self|\"VIP, array, dp, prefix_sum\"|prefix 是一个词的前缀；suffix是一个词的后缀; VIP 本质是在循环前通过构建prefix把复杂度降低一级； reversed(range(n - 1))的用法|No|No|No,|\\n|240. Search a 2D Matrix II|binary search|为何这题不需要mid了；选取右上角或者左下角开始循环，才可以做到一增一减|Yes|No|No|mid|\\n|242. Valid Anagram|\"hash, string\"|dic.get 的用法要掌握，用来取代counter|Yes|Yes|No|perfect|\\n|251. Flatten 2D Vector|\"design, two_pointer\"|,No|No|No,|\\n|252: Meeting Rooms|greedy|intervals.sort(key=lambda x: x.start)|No|No|No,|\\n|253. Meeting Rooms II|,interval的题画图很重要|No|No|No,|\\n|257.\\xa0Binary Tree Paths|\"backtracking, dfs\"|,No|No|No,|\\n|258. Add Digits|recur|,Yes|No|No|bad|\\n|259. 3Sum Smaller|sliding_window|list.sort()要记住；不是list = list.sort(|No|No|No,|\\n|26 Remove Duplicates from Sorted Array|\"VIP, array, two_pointer\"|感觉duplicate相关的题都可以用指针|Yes|No|No|mid|\\n|268 Missing Number|array|,Yes|No|No|mid|\\n|270: Closest Binary Search Tree Value II | Leetcode lock|tree|关键是gap的定义在无穷大; BST的题目用interation比recurssion一般更直观|Yes|No|No|mid|\\n|273.\\xa0Integer to English Words|\"array, recur, string\"|,No|No|No,|\\n|276 paint fence|\"VIP, dp\"|dp 入门题目|Yes|Yes|No|bad|\\n|278.\\xa0First Bad Version|\"VIP, binary search\"|key to binary search is the boundary conditions; check ipad|No|No|No,|\\n|28. Implement strStr()|\"sliding_window, string, two_pointer\"|,Yes|No|No|good|\\n|283 Move Zeroes|\"array, stack/queue\"|注意在循环最后迭代index的写法；stack永远是不准创造新space的好选择; 注意最好不要用nums.remove(0)因为他会打乱loop|Yes|No|No|mid|\\n|287. Find the Duplicate Number|\"binary search, graph, linked list, two_pointer\"|,Yes|No|No|bad|\\n|3. Longest Substring Without Repeating Characters|\"sliding_window, string, 有疑问\"|双指针都是用while，不用if; 双指针关键是分别找到移动两个point的条件； set add的用法很关键|Yes|No|No|mid|\\n|3. Longest Substring Without Repeating Characters|\"VIP, sliding_window\"|\"add, remove, set 的用法要记住\"|No|No|No,|\\n|300. Longest Increasing Subsequence|\"VIP, binary search, dp, greedy\"|非常经典的题目；两层循环的dp|Yes|Yes|No|mid|\\n|310.\\xa0Minimum Height Trees|\"graph, tree\"|,No|No|No,|\\n|322. Coin Change|\"VIP, dp\"|尤其注意记忆化递归的易错点|No|Yes|No,|\\n|326. Power of Three|\"math, recur\"|这种需要连续乘积的，都可以试下用递归来做|Yes|No|No|perfect|\\n|33. Search in Rotated Sorted Array|\"VIP, binary search, middle\"|核心是判断啥时候target在mid的左边or右边；mid< target时有两种情况；#看做一个recursion，判断break在MR 还是LM之间，需要很多轮|Yes|No|No|mid|\\n|337. House Robber III|\"VIP, tree\"|,No|No|No,|\\n|34 Find First and Last Position of Element in Sorted Array|\"VIP, array, binary search\"|https://www.youtube.com/watch?v=bPdnC5X5xDw binary search的灵活运用|Yes|No|No|mid|\\n|341.\\xa0Flatten Nested List Iterator|\"design, dfs, stack/queue\"|,No|No|No,|\\n|344. Reverse String|\"recur, string, two_pointer\"|所有可以从前后两端考虑的问题，都可以用双指针; left < right 就是一个极佳的stopping condition，比recurssion的stop condition容易理解; 这道题的recur 写法有点binary search的思想在里面|Yes|Yes|No|mid|\\n|347.\\xa0Top K Frequent Elements|\"array, hash, heap\"|\"dd= sorted((dic[ip], ip) for ip in dic)  #VIP how to sort dic by their keys\"|No|No|No,|\\n|350 Intersection of Two Arrays II|\"hash, two_pointer\"|先把一部分data放到hash，再用另一部分去match|Yes|No|No|perfect|\\n|355.\\xa0Design Twitter|\"VIP, hash, heap\"|,No|No|No,|\\n|36.\\xa0Valid Sudoku|\"hash, matrix\"|注意set的写法：collections.defaultdict(set)|No|No|No,|\\n|362. Design Hit Counter|,,No|No|No,|\\n|367. Valid Perfect Square|binary search|\"注意考虑1, 0 等特殊情况； 注意取mid的时候的取整与否\"|Yes|No|No|perfect|\\n|376. Wiggle subsequence|dp|,No|No|No,|\\n|380.\\xa0Insert Delete GetRandom O(1)|\"array, design, hash\"|index = self.hmap[val] 关键是找到这个index的操作|No|No|No,|\\n|383.\\xa0Ransom Note|hash|,No|No|No|perfect|\\n|387. First Unique Character in a String|\"hash, string\"|掌握如何得到list的index: index = animals.index(\\'dog\\')|Yes|No|No|perfect|\\n|39. Combination Sum|\"VIP, backtracking\"|可以当模板; sort array before u do anything can be helpful； 循环加递归的套路在backtrack里极其常见，比如78 subsets; VIP 注意ans.append(path[:]) 不能直接append path|No|No|No,|\\n|40. Combination Sum II|\"VIP, backtracking\"|注意不能写candidates = candidates.sort()； #my work correct!!!  #VIP check my own solution 这个才是符合模板式解法的办法 https://www.youtube.com/watch?v=GBKI9VSKdGg|No|No|No,|\\n|404 Sum of Left Leaves|\"VIP, bfs, dfs, tree\"|注意这种自己加参数的写法|Yes|No|No|bad|\\n|409.\\xa0Longest Palindrome|,,No|No|No,|\\n|412. Fizz Buzz|string|记住整除的写法|Yes|No|No|perfect|\\n|416.\\xa0Partition Equal Subset Sum|dp|,No|No|No,|\\n|417.\\xa0Pacific Atlantic Water Flow|\"dfs, graph, matrix\"|,No|No|No,|\\n|42.\\xa0Trapping Rain Water|\"dp, stack/queue, two_pointer\"|,No|No|No,|\\n|424.\\xa0Longest Repeating Character Replacement|sliding_window|,No|No|No,|\\n|437. Path Sum III|\"VIP, tree\"|本质上是两层dfs，遍历整个tree两次|No|No|No,|\\n|438.\\xa0Find All Anagrams in a String|\"VIP, array, hash, sliding_window\"|vip用hashmap的话可以避免先求出所有的anagram; Counter的用法； 注意scount.pop(s[l]) 用来remove a key from a dict|No|No|No,|\\n|445. Add Two Numbers II|linked list|掌握linked list的反向遍历方法|No|No|No,|\\n|45.\\xa0Jump Game II|\"bfs, dp, greedy\"|看视频，很像bfs的思路，也是求最小值|No|No|No,|\\n|450.\\xa0Delete Node in a BST|\"dfs, tree\"|,No|No|No,|\\n|455 assign cookies|\"VIP, greedy\"|贪心问题的特点是，前面的决策会影响后面的，比如这题，被用过的饼干就不能再用来满足后面的kid了，所以要保证前面决策的时候就是可以实现全局最优的局部解; 每一步的最优解一定包含上一步的最优解; 注意break 的用法; 注意g.sort()，不是g = g.sort()|Yes|No|No|bad|\\n|46. permutations|\"VIP, backtracking\"|把code和树状图结合起来看就容易理解了; # 回溯法的含义是对每个可能的结果进行遍历，如果某个数字已经使用则跳过，如果没有使用则放入path中。这个“回溯”怎么理解？我认识是在递归的过程中使用了一个数组path来保存自己走过的路，# 如果沿着这条路走完了全部的解，则需要弹出path中的最后一个元素，相当于向后回退，于是叫做回溯法。|No|No|No,|\\n|463.\\xa0Island Perimeter|\"bfs, dfs, matrix\"|,No|No|No,|\\n|487 Max consecutive ones 2|\"dp, two_pointer\"|,No|No|No,|\\n|49.\\xa0Group Anagrams|\"VIP, array, hash\"|注意anagram的题很多都可以用上sort； 注意need to change list to tuple because list cant be keys in python； defaultdict(list) 不是defaultdict(tuple)|No|No|No,|\\n|5. Longest Palindromic Substring|\"VIP, dp, string, two_pointer\"|对称性的问题，都可以试试two pointer; 函数套函数是这题双指针写法的关键; 注意python 里string subset s[1:2]只能取到一个字母|Yes|No|No|bad|\\n|\"50. Pow(x, n)\"|recur|通过一些数字单双数之类的小trick来减少计算量；注意shift的操作>>|Yes|No|No|mid|\\n|51. N-Queens|backtracking|,No|No|No,|\\n|513.\\xa0Find Bottom Left Tree Value|\"bfs, dfs, tree\"|bfs 按每一环的遍历方式，本质上就是层序遍历|No|No|No,|\\n|513.\\xa0Find Bottom Left Tree Value|\"bfs, tree\"|,No|No|No,|\\n|525.\\xa0Contiguous Array|\"array, hash, prefix_sum\"|,No|No|No,|\\n|528.\\xa0Random Pick with Weight|\"VIP, binary search\"|本质上是一个mapping from index to value； if seed <= self.w[mid]:  # 最后输出的l是满足这个条件的最小index; 等号是关键|No|No|No,|\\n|53. Maximum Subarray|\"VIP, array, dp, greedy\"|\"key is definition of dp: dp[i] is the max sum ending at index i, so any dp[i] might be the global max; sorting an array 的 time complexity is nlogn \"|Yes|No|No|mid|\\n|54.\\xa0Spiral Matrix|\"matrix, simulation\"|,No|No|No,|\\n|542.\\xa001 Matrix|\"bfs, dp, matrix\"|[[]]|No|No|No,|\\n|543. Diameter of Binary Tree|\"VIP, tree\"|get depth; 注意用self.result来储存结果，不能直接用result; return的结果不是拿来作为最后输出的，而是为了recur到下一轮|Yes|No|No|mid|\\n|55. Jump Game|\"VIP, array, dp, greedy\"|\"greedy 经常可以做到linear time; dp 一般都是square time 因为dp要遍历所有path，只是可以用memory存一下； greedy 从末尾挪动goal position是关键, 看neetcode视频\"|Yes|No|No,|\\n|56.\\xa0Merge Intervals|\"VIP, array, sort\"|\"classic sorting question;lambda function intervals = sorted(intervals, key=lambda x:x[0])\"|No|No|No,|\\n|560.\\xa0Subarray Sum Equals K|\"VIP, dp, hash, prefix_sum\"|\"掌握cumulative sum 的用法，记住下面的模板，可以有效减少复杂度; 注意要用mymap = collections.defaultdict(int)，不能直接用普通的dic = {}定义, The difference is that a\\xa0defaultdict|\\n|will \"\"default\"\" a value if that key has not been set yet. If you didn\\'t use a\\xa0defaultdict|\\n|you\\'d have to check to see if that key exists, and if it doesn\\'t, set it to what you want.\"|No|No|No,|\\n|567.\\xa0Permutation in String|\"VIP, hash, sliding_window\"|掌握A = [ord(x) - ord(\\'a\\') for x in s1]的操作方式；同时在字典里提前加26个为0的key的操作也很经典；不一定要用dic来当hashtable用array也行|No|No|No,|\\n|57.\\xa0Insert Interval|VIP|\"先想res是什么样子的，再想怎么组合出res来；extend is the key; res.extend(intervals[i:])  # equal to for j in range(i, len(intervals)):  res.append(intervals[j])\"|Yes|No|No,|\\n|572.\\xa0Subtree of Another Tree|tree|,No|No|No,|\\n|59.\\xa0Spiral Matrix II|\"matrix, simulation\"|,No|No|No,|\\n|599.\\xa0Minimum Index Sum of Two Lists|\"array, hash, string\"|,No|No|No,|\\n|617. Merge Two Binary Trees|tree|,Yes|No|No|perfect|\\n|62. Unique Paths|dp|\"memory =[[0] * n for _ in range(m)]; 注意二维list的写法memo[row][col] 不是memo[row, col]\"|Yes|No|No,|\\n|621. Task Scheduler|\"greedy, heap\"|,No|No|No,|\\n|64.\\xa0Minimum Path Sum|\"dp, matrix\"|\"dp = [[0] * columns for _ in range|\\n|(rows)]\"|No|No|No,|\\n|66 plus one|\"array, math\"|看情况要选择倒转循环顺序; 提前return避免循环走完的写法|Yes|No|No|bad|\\n|674. Longest Continuous Increasing Subsequence|\"array, dp\"|\"How to create a list of empty lists: For arbitrary length lists, you can use [ [] for _ in range(N) ]|\\n|Do not use [ [] ] * N, as that will result in the list containing the same list object N times\"|Yes|No|No|mid|\\n|678.\\xa0Valid Parenthesis String|,,No|No|No,|\\n|680.\\xa0Valid Palindrome II|\"greedy, recur, two_pointer\"|,No|No|No,|\\n|684.\\xa0Redundant Connection|\"bfs, dfs, graph\"|,No|No|No,|\\n|69 sqrt|\"VIP, binary search\"|循环改变左右边界； 注意这里用while的循环，因为不知道for loop的次数； 循环停止的边界要自己举例来判断; 最后return left or right是关键|Yes|No|No|mid|\\n|692.\\xa0Top K Frequent Words|\"VIP, hash, sort\"|,No|No|No,|\\n|695.\\xa0Max Area of Island|\"bfs, dfs, graph\"|,No|No|No,|\\n|7. Reverse Integer|math|,No|No|No,|\\n|70. climbing stairs|\"VIP, dp\"|之所以要套两层函数，是因为memory function needs to be defined outside the recurssion; 一定要用memory不然会超时|Yes|No|No|mid|\\n|703.\\xa0Kth Largest Element in a Stream|\"design, heap\"|,No|No|No,|\\n|704 binary search|binary search|while left <= right: #这个等号是关键。 #取等号的时候，low = high = mid，就一个数了，判断他是否== target就可以，不等的话left 就比right 大了，就跳出循环返回-1|Yes|Yes|No|good|\\n|705.\\xa0Design HashSet|design|,No|No|No,|\\n|706. Design HashMap|hash|比较特殊的取余数法|No|No|No,|\\n|714. Best Time to Buy and Sell Stock with Transaction Fee|dp|这道题和122是双变量的dp典型|No|No|No,|\\n|721.\\xa0Accounts Merge|\"VIP, array, bfs, dfs, string\"|dfs也可以不用recur的写法|No|No|No,|\\n|733.\\xa0Flood Fill|\"bfs, dfs, matrix, recur\"|edge cases; # 注意这题不像 200 number of island那样，要loop 整个matrix，只需要loop 和起始点相邻的island就够了|Yes|No|No|mid|\\n|739.\\xa0Daily Temperatures|\"VIP, stack/queue\"|\"monotonic decreasing stack problem; 这是一类经典问题; stack经常用在，可以一边循环一边删掉一部分list ele的问题上, 比如只用考虑相邻元素的关系，不用管其他\"|No|No|No,|\\n|74.\\xa0Search a 2D Matrix|\"binary search, matrix\"|,No|No|No,|\\n|75.\\xa0Sort Colors|\"sort, two_pointer\"|,Yes|No|No|perfect|\\n|76.\\xa0Minimum Window Substring|\"hash, sliding_window\"|,No|No|No,|\\n|763.\\xa0Partition Labels|\"greedy, hash, two_pointer\"|last = {s[i]: i for i in range(L)} 的写法得出每个字母的last position|No|No|No,|\\n|78.\\xa0Subsets|\"VIP, backtracking, dfs, dp\"|\"dp 的难点在于不知道怎么定义状态; backtrack的写法注意用 backtrack(i + 1, path + [nums[i]])，不能写backtrack(i + 1, path.append(nums[i])\"|No|No|No,|\\n|79.\\xa0Word Search|\"backtracking, matrix\"|,No|No|No,|\\n|807. Max Increase to Keep City Skyline|greedy|max_j = max([row[j] for row in grid]) 这个求每列最大值的地方是关键；|No|No|No,|\\n|82. Remove Duplicates from Sorted List 2|\"VIP, linked list, two_pointer\"|如果简单遍历一遍linked list没法完成任务，就可以考虑双指针之类的办法; 双指针经典|Yes|No|No|bad|\\n|83. Remove Duplicates from Sorted List|linked list|,Yes|No|No|good|\\n|846. Hand of Straights|\"VIP, greedy, hash\"|collections.Counter(hand) 直接变成hashtable的操作十分关键; min(cards.keys()) 注意这个写法； dic.pop(key1)的写法|No|No|No,|\\n|853.\\xa0Car Fleet|\"VIP, stack/queue\"|zip的用法|No|No|No,|\\n|860.\\xa0Lemonade Change|\"array, greedy\"|没必要用哈希表来做counter，随便用一个int做counter也行，反正最后是判断这个counter是否大于零|No|No|No,|\\n|863.\\xa0All Nodes Distance K in Binary Tree|\"VIP, bfs, dfs, tree\"|\"defaultdict 的用法是关键 https://stackoverflow.com/questions/5900578/how-does-collections-defaultdict-work; has to create a graph first because we need to traverse parent nodes； vip is creating a graph from a tree, 彻底记住这个操作\"|No|No|No,|\\n|875. Koko Eating Bananas|binary search|这里的判断mid==target的条件要自己写函数定义; 注意余数怎么求|Yes|No|No|mid|\\n|876. Middle of the Linked List|\"linked list, two_pointer\"|所有找中点的题目都可以用双指针|Yes|No|No|good|\\n|88 Merge Sorted Array|\"VIP, array, sort, two_pointer\"|\"为了不打乱前面的顺序，从list末尾开始往前面填充；If you simply consider one element each at a time from the two arrays and make a decision and proceed accordingly, you will arrive at the optimal solution.； #VIP nums1[:]值整个list的每个值\"|Yes|No|No|mid|\\n|90.\\xa0Subsets II|\"array, backtracking\"|\"为了避免重复，关键是移动i的时候一次移动多个，跳过相等的数字比如[1|2,2]\"|No|No|No,|\\n|904. Fruit Into Baskets|sliding_window|,No|No|No,|\\n|929. Unique Email Addresses|\"VIP, hash, string\"|set用法；string的基本处理，split|Yes|No|No|mid|\\n|933.\\xa0Number of Recent Calls|\"design, stack/queue\"|,No|No|No,|\\n|94. Binary Tree Inorder Traversal|\"VIP, dfs, tree\"|掌握用另一个function存下result的方法； #需要新造一个function，不能直接用inorderTraversal的原因是，需要把res定义在函数外面，否则每次循环都会把res清空|Yes|No|No|perfect|\\n|96. Unique Binary Search Trees|\"dp, tree\"|,No|No|No,|\\n|973. K Closest Points to Origin|\"VIP, array, hash, heap\"|\"heap 只要On, sort takes Onlogn\"|No|No|No,|\\n|98. Validate Binary Search Tree|tree|update boundary; 要记住inf的写法|Yes|Yes|No|bad|\\n|981.\\xa0Time Based Key-Value Store|\"VIP, binary search, hash\"|VIP不要用普通的dic要用defaultdict; use turple to store pairs of values; 字典get的用法; 字典里一个key是可以对应好几个值的！！！|No|No|No,|\\n|N meetings in one room|greedy|\"核心是理解为何一个阶段的greedy可以实现总体最优解：我们希望meeting越短越好，这样才能排下更多meeting所以按end time 来排列更合理，如果按start time排列可能第一个meeting就很长. While choosing the meetings greedily with a minimum ending time first, we are left with more time to schedule more meetings in one room. Hence, as a result, we are able to allocate maximum meetings in one room.\"|No|No|No,|\\n|smallest difference|two_pointer|,No|No|No,|\\n|validate subsequence|\"array, two_pointer\"|理清楚思路，什么时候可以return True，就是sequence被耗完到最后的时候，所以要在sequence index下手; 指针问题|No|No|No,|\\n|剑指 Offer 22. 链表中倒数第k个节点|linked list|,No|No|No,|\\n|,hash|,No|No|No,|\\n|,linked list|,No|No|No||', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c906f695-6520-4816-9679-bdb591a3c4f9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n**1011. Capacity To Ship Packages Within D Days [Medium]**\\n\\nA conveyor belt has packages that must be shipped from one port to another within\\xa0`D`\\xa0days. The\\xa0`i`-th package on the conveyor belt has a weight of\\xa0`weights[i]`. Each day, we load the ship with packages on the conveyor belt (in the order given by\\xa0`weights`). We may not load more weight than the maximum weight capacity of the ship.\\n\\nReturn the least weight capacity of the ship that will result in all the packages on the conveyor belt being shipped within\\xa0`D`\\xa0days.\\n\\n**Example :**\\n\\n```\\nInput: weights = [1,2,3,4,5,6,7,8,9,10], D = 5\\nOutput: 15\\nExplanation:\\nA ship capacity of 15 is the minimum to ship all the packages in 5 days like this:\\n1st day: 1, 2, 3, 4, 5\\n2nd day: 6, 7\\n3rd day: 8\\n4th day: 9\\n5th day: 10\\n\\nNote that the cargo must be shipped in the order given, so using a ship of capacity 14 and splitting the packages into parts like (2, 3, 4, 5), (1, 6, 7), (8), (9), (10) is not allowed.\\n\\n```\\n\\nBinary search probably would not come to our mind when we first meet this problem. We might automatically treat\\xa0`weights`\\xa0as search space and then realize we've entered a dead end after wasting lots of time. In fact, we are looking for the minimal one among all feasible capacities. We dig out the monotonicity of this problem: if we can successfully ship all packages within\\xa0`D`\\xa0days with capacity\\xa0`m`, then we can definitely ship them all with any capacity larger than\\xa0`m`. Now we can design a\\xa0`condition`\\xa0function, let's call it\\xa0`feasible`, given an input\\xa0`capacity`, it returns whether it's possible to ship all packages within\\xa0`D`\\xa0days. This can run in a greedy way: if there's still room for the current package, we put this package onto the conveyor belt, otherwise we wait for the next day to place this package. If the total days needed exceeds\\xa0`D`, we return\\xa0`False`, otherwise we return\\xa0`True`.\\n\\nNext, we need to initialize our boundary correctly. Obviously\\xa0`capacity`\\xa0should be at least\\xa0`max(weights)`, otherwise the conveyor belt couldn't ship the heaviest package. On the other hand,\\xa0`capacity`\\xa0need not be more than`sum(weights)`, because then we can ship all packages in just one day.\\n\\nNow we've got all we need to apply our binary search template:\\n\\n```\\ndef shipWithinDays(weights: List[int], D: int) -> int:\\n    def feasible(capacity) -> bool:\\n        days = 1\\n        total = 0\\n        for weight in weights:\\n            total += weight\\n            if total > capacity:  # too heavy, wait for the next day\\n                total = weight\\n                days += 1\\n                if days > D:  # cannot ship within D days\\n                    return False\\n        return True\\n\\n    left, right = max(weights), sum(weights)\\n    while left < right:\\n        mid = left + (right - left) // 2\\n        if feasible(mid):\\n            right = mid\\n        else:\\n            left = mid + 1\\n    return left\\n```\\n\\nThe following code is the solution for\\xa0leetcode 287 find the duplicate number\\n\\n```\\nclass Solution {\\npublic:\\n    int findDuplicate(vector& nums) {\\n        int low = 1, high = nums.size() - 1;\\n        while (low < high) {\\n            int mid = low + (high - low) * 0.5;\\n            int cnt = 0;\\n            for (auto a : nums) {\\n                if (a <= mid) ++cnt;\\n            }\\n            if (cnt <= mid) low = mid + 1;\\n            else high = mid;\\n        }\\n        return low;\\n    }\\n};\\n\\n```\\n\\nThere are several places I am confused about:\\n\\n1.the condition for the while loop\\xa0`low<high or low<=high`\\n\\n2.`a<=mid or a<mid`\\xa0(specific for this example)\\n\\n3.`cnt<= mid or cnt<mid`\\n\\n4.`low=mid+1 or low=mid`\\n\\n5.`high=mid or high=mid-1`\\n\\n6.which value do I return?\\n\\nWhen writing a binary search there are a couple of things to consider. The first is what interval range you are searching over and specifically, how you are defining it.\\n\\nFor example, it could be inclusive of both\\xa0`low`\\xa0and\\xa0`high`, meaning\\xa0`[low, high]`, but it could also be exclusive of\\xa0`high`,\\xa0`[low, high)`. Which of these you choose will change the rest of your algorithm.\\n\\nThe obvious implication is the initial values. Generally,\\xa0`high`\\xa0should be the length of the array if it is exclusive and it should be one less if it's inclusive, but it could be something entirely different depending on the problem you're solving.\\n\\nFor the while loop you want it to terminate when the search interval is empty, meaning there are no more candidates to check. If you are using the interval\\xa0`[low, high]`, then this will be empty when\\xa0`low`\\xa0is strictly greater than\\xa0`high`\\xa0(for example,\\xa0`[5,5]`\\xa0contains 5, but\\xa0`[6,5]`\\xa0contains nothing), so the while loop will check for the opposite,\\xa0`low <= high`. However, if you use the interval\\xa0`[low, high)`, then this interval is empty when\\xa0`low`\\xa0is equal to\\xa0`high`, so the while loop needs to check for\\xa0`low < high`.\\n\\nWithin the while loop, after checking\\xa0`mid`, you want to remove it from the interval so you don't check it again. If\\xa0`high`\\xa0is inclusive, then you have to use one less than\\xa0`mid`\\xa0as the new\\xa0`high`\\xa0in order to exclude it from the interval. But if\\xa0`high`\\xa0is exclusive, then setting\\xa0`high`\\xa0equal to\\xa0`mid`\\xa0is enough to exclude it.\\n\\nAs for when to update\\xa0`low`\\xa0vs\\xa0`high`, this depends on what you're searching for. Besides the basic binary search where you just want to know if something exists exactly in the collection, you will have to consider what to do when you are as close as you can get.\\n\\nIn C++ for example, the more useful versions of\\xa0`binary_search`\\xa0are called\\xa0`lower_bound`\\xa0and\\xa0`upper_bound`. If the value being searched for doesn't exist in the container, then these both return the same position, namely the first position which is larger than the search value. This is convenient since this is the position you should insert that value if you want to keep the container sorted. However, if the value is in the container, possibly multiple times, then\\xa0`lower_bound`\\xa0will return the first occurrence of the value, whereas\\xa0`upper_bound`\\xa0will still return the first position larger than the value (or in other words, a right bound to the location of the values).\\n\\nTo get these different behaviors you update either the\\xa0`low`\\xa0or\\xa0`high`\\xa0bound when\\xa0`mid`\\xa0is equal to the search value. If you want the lower bound, then you want to continue searching the lower half of your search range, so you bring\\xa0`high`\\xa0down. If you want the high bound, then you bring\\xa0`low`\\xa0up. In your example, it brings\\xa0`low`\\xa0up when\\xa0`cnt == mid`, so it will find an upper bound.\\n\\nAs for what to return, it depends on both your search interval and what you're looking for. In your example, the while loop is checking\\xa0`(low < high)`, so\\xa0`low`\\xa0and\\xa0`high`\\xa0will be equal when it breaks and it doesn't matter which you use, but even then you may want to return\\xa0`left - 1`\\xa0or\\xa0`left + 1`\\xa0depending on the problem. If the while loop is\\xa0`(low <= high)`\\xa0then when it breaks\\xa0`low == high + 1`, so it will depend on what you're looking for. When in doubt you can always think through an example.\\n\\nSo to put this all to use, here is a version of the solution you mentioned, but using an interval of [low, high] rather than [low, high):\\n\\n```\\nclass Solution {\\n    public:\\n        int findDuplicate(vector& nums) {\\n            int low = 1, high = nums.size() - 2;\\n            while (low <= high) {\\n                int mid = low + (high - low) * 0.5;\\n                int cnt = 0;\\n                for (auto a : nums) {\\n                    if (a <= mid) ++cnt;\\n                }\\n                if (cnt <= mid) low = mid + 1;\\n                else high = mid - 1;\\n            }\\n            return low;\\n        }\\n    };\\n\\n```\\n\\nPS: The reason I didn't mention the interval\\xa0`(low, high]`\\xa0or\\xa0`(low, high)`\\xa0is because it messes with the math around calculating the\\xa0`mid`\\xa0index. Because int math will round down, you can end up in a situation where\\xa0`mid`\\xa0is searched again. For example, if\\xa0`low`\\xa0is 7 and\\xa0`high`\\xa0is 9, then\\xa0`low + (high - low) * 0.5`\\xa0will be 8. After updating low to 8 (since it's exclusive you wouldn't add one),\\xa0`low + (high - low) * 0.5`\\xa0will still be 8 and your loop will never terminate. You can get around this by adding 1 to the part being divided by 2, but generally it's cleaner to go with an interval where\\xa0`low`\\xa0is inclusive.\\n\\nI had exactly the same issue until I figured out loop invariants along with predicates are the best and most consistent way of approaching all binary problems.\\n\\nPoint 1:\\xa0**Think of predicates**In general for all these 4 cases (and also the normal binary search for equality), imagine them as a predicate. So what this means is that some of the values are meeting the predicate and some some failing. So consider for example this array with a target of 5: [1, 2, 3, 4, 6, 7, 8]. Finding the first number greater than 5 is basically equivalent of finding the first one in this array: [0, 0, 0, 0, 1, 1, 1].\\n\\nPoint 2:\\xa0**Search boundaries inclusive**I like to have both ends always inclusive. But I can see some people like start to be inclusive and end exclusive (on len instead of len -1). I like to have all the elements inside of the array, so when referring to a[mid] I don't think whether that will give me an array out of bound. So my preference: Go inclusive!!!\\n\\nPoint 3:\\xa0**While loop condition <=**So we even want to process the subarray of size 1 in the while loop, and when the while loop finishes there should be no unprocessed element. I really like this logic. It's always solid as a rock. Initially all the elements are not inspected, basically they are unknown. Meaning that everything in the range of [st = 0, to end = len - 1] are not inspected. Then when the while loop finishes, the range of uninspected elements should be array of size 0!\\n\\nPoint 4:\\xa0**Loop invariants**Since we defined start = 0, end = len - 1, invariants will be like this: Anything left of start is smaller than target. Anything right of end is greater than or equal to the target.\\n\\nPoint 5:\\xa0**The answer**Once the loop finishes, basically based on the loop invariants anything to the left of start is smaller. So that means that start is the first element greater than or equal to the target. Equivalently, anything to the right of end is greater than or equal to the target. So that means the answer is also equal to end + 1.\\n\\nThe code:\\n\\n```\\npublic int find(int a[], int target){\\n  int start = 0;\\n  int end = a.length - 1;\\n  while (start <= end){\\n    int mid = (start + end) / 2; // or for no overflow start + (end - start) / 2\\n    if (a[mid] < target)\\n       start = mid + 1;\\n    else // a[mid] >= target\\n       end = mid - 1;\\n  }\\n  return start; // or end + 1;\\n}\\n\\n```\\n\\nvariations:**<**It's equivalent of finding the first 0. So basically only return changes.\\n\\n```\\nreturn end; // or return start - 1;\\n```\\n\\n**>**change the if condition to . No other change.\\n\\n**,\\xa0`return end; // or return start - 1;`\\n\\nSo in general with this model for all the 5 variations (, >=, normal binary search) only the condition in the if changes and the return statement. And figuring those small changes is super easy when you consider the invariants (point 4) and the answer (point 5).\\n\\nHope this clarifies for whoever reads this. If anything is unclear of feels like magic please ping me to explain. After understanding this method, everything for binary search should be as clear as day!\\n\\nExtra point: It would be a good practice to also try including the start but excluding the end. So the array would be initially [0, len). If you can write the invariants, new condition for the while loop, the answer and then a clear code, it means you learnt the concept.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='78d5bac0-a727-448a-9c29-32140222a0bd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n算法课 拉勾\\n\\n\\n\\ncomplexity是啥意思？how does time and space scale with the size of input N\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nsparse data有时候可以用别的data structure来存来省内存\\n\\n\\n\\nspace resource is a sunk cost on your computer already. Time is flexible cost so its more important\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n链表只能按位置条件查找，所以时间复杂度是o(n)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n需要完成的操作包括，按照位置的查找、新增和按照数据数值的查找、删除\\n\\n\\n\\n数据处理的基本操作只有 3 个，分别是增、删、查。其中，增和删又可以细分为在数据结构中间的增和删，以及在数据结构最后的增和删。区别就在于原数据的位置是否发生改变。查找又可以细分为按照位置条件的查找和按照数据数值特征的查找。几乎所有的数据处理，都是这些基本操作的组合和叠加\\n\\n我们需要从时间复杂度和空间复杂度两个维度来考虑。常用的降低时间复杂度的方法有递归、二分法、排序算法、动态规划等，这些知识我们都会在后续课程中逐一学习，这里我先不讲。而降低空间复杂度的方法，就要围绕数据结构做文章了。\\n\\n降低空间复杂度的核心思路就是，能用低复杂度的数据结构能解决问题，就千万不要用高复杂度的数据结构\\n\\n根据这里的分析不难发现，链表在新增、删除数据都比较容易，可以在 O(1) 的时间复杂度内完成。但对于查找，不管是按照位置的查找还是按照数值条件的查找，都需要对全部数据进行遍历。这显然就是 O(n) 的时间复杂度。\\n\\n虽然链表在新增和删除数据上有优势，但仔细思考就会发现，这个优势并不实用。这主要是因为，在新增数据时，通常会伴随一个查找的动作。例如，在第五个结点后，新增一个新的数据结点，那么执行的操作就包含两个步骤：\\n\\n第一步，查找第五个结点；\\n\\n第二步，再新增一个数据结点。整体的复杂度就是 O(n) + O(1)\\n\\n根据我们前面所学的复杂度计算方法，这也等同于 O(n) 的时间复杂度。线性表真正的价值在于，它对数据的存储方式是按照顺序的存储。如果数据的元素个数不确定，且需要经常进行数据的新增和删除时，那么链表会比较合适。如果数据元素大小确定，删除插入的操作并不多，那么数组可能更适合些\\n\\n栈是什么\\n你需要牢记一点，栈是一种特殊的线性表。栈与线性表的不同，体现在增和删的操作。具体而言，栈的数据结点必须后进先出。后进的意思是，栈的数据新增操作只能在末端进行，不允许在栈的中间某个结点后新增数据。先出的意思是，栈的数据删除操作也只能在末端进行，不允许在栈的中间某个结点后删除数据\\n\\n浏览器都有页面前进和后退功能，这就是个很典型的后进先出的场景。假设你先后访问了五个页面，分别标记为 1、2、3、4、5。当前你在页面 5，如果执行两次后退，则退回到了页面 3，如果再执行一次前进，则到了页面 4。处理这里的页面链接存储问题，栈就应该是我们首选的数据结构\\n\\n栈的基本操作\\n如何通过栈这个后进先出的线性表，来实现增删查呢？初始时，栈内没有数据，即空栈。此时栈顶就是栈底。当存入数据时，最先放入的数据会进入栈底。接着加入的数据都会放入到栈顶的位置。如果要删除数据，也只能通过访问栈顶的数据并删除。对于栈的新增操作，通常也叫作 push 或压栈。对于栈的删除操作，通常也叫作 pop 或出栈\\n\\n通过分析你会发现，不管是顺序栈还是链栈，数据的新增、删除、查找与线性表的操作原理极为相似，时间复杂度完全一样，都依赖当前位置的指针来进行数据对象的操作。区别仅仅在于新增和删除的对象，只能是栈顶的数据结点\\n\\n**接下来，我们一起来看两个栈的经典案例，从中你可以更深切地体会到栈所发挥出的价值。**\\n\\n例 1，给定一个只包括 \\'(\\'，\\')\\'，\\'{\\'，\\'}\\'，\\'[\\'，\\']\\' 的字符串，判断字符串是否有效。有效字符串需满足：左括号必须与相同类型的右括号匹配，左括号必须以正确的顺序匹配。例如，{ [ ( ) ( ) ] } 是合法的，而 { ( [ ) ] } 是非法的。\\n\\n这个问题很显然是栈发挥价值的地方。原因是，在匹配括号是否合法时，左括号是从左到右依次出现，而右括号则需要按照“后进先出”的顺序依次与左括号匹配。因此，实现方案就是通过栈的进出来完成。\\n\\n具体为，从左到右顺序遍历字符串。当出现左括号时，压栈。当出现右括号时，出栈。并且判断当前右括号，和被出栈的左括号是否是互相匹配的一对。如果不是，则字符串非法。当遍历完成之后，如果栈为空。则合法。如下图所示：\\n\\n```python\\npublic static void main(String[] args) {\\nString s = \"{[()()]}\";\\nSystem.out.println(isLegal(s));\\n}\\nprivate static int isLeft(char c) {\\nif (c == \\'{\\' || c == \\'(\\' || c == \\'[\\') {\\nreturn 1;\\n} else {\\nreturn 2;\\n}\\n}\\nprivate static int isPair(char p, char curr) {\\nif ((p == \\'{\\' && curr == \\'}\\') || (p == \\'[\\' && curr == \\']\\') || (p == \\'(\\' && curr == \\')\\')) {\\nreturn 1;\\n} else {\\nreturn 0;\\n}\\n}\\nprivate static String isLegal(String s) {\\nStack stack = new Stack();\\nfor (int i = 0; i < s.length(); i++) {\\nchar curr = s.charAt(i);\\nif (isLeft(curr) == 1) {\\nstack.push(curr);\\n} else {\\nif (stack.empty()) {\\nreturn \"非法\";\\n}\\nchar p = (char) stack.pop();\\nif (isPair(p, curr) == 0) {\\nreturn \"非法\";\\n}\\n}\\n}\\nif (stack.empty()) {\\nreturn \"合法\";\\n} else {\\nreturn \"非法\";\\n}\\n}\\n```\\n\\n例 2，浏览器的页面访问都包含了后退和前进功能，利用栈如何实现？\\n\\n我们利用浏览器上网时，都会高频使用后退和前进的功能。比如，你按照顺序先后访问了 5 个页面，分别标记为 1、2、3、4、5。现在你不确定网页 5 是不是你要看的网页，需要回退到网页 3，则需要使用到两次后退的功能。假设回退后，你发现网页 4 有你需要的信息，那么就还需要再执行一次前进的操作。\\n\\n为了支持前进、后退的功能，利用栈来记录用户历史访问网页的顺序信息是一个不错的选择。此时需要维护两个栈，分别用来支持后退和前进。当用户访问了一个新的页面，则对后退栈进行压栈操作。当用户后退了一个页面，则后退栈进行出栈，同时前进栈执行压栈。当用户前进了一个页面，则前进栈出栈，同时后退栈压栈。我们以用户按照 1、2、3、4、5、4、3、4 的浏览顺序为例，两个栈的数据存储过程，如下图所示\\n\\n队列是什么\\n与栈相似，队列也是一种特殊的线性表，与线性表的不同之处也是体现在对数据的增和删的操作上。\\n\\n队列的特点是先进先出：\\n\\n先进，表示队列的数据新增操作只能在末端进行，不允许在队列的中间某个结点后新增数据;\\n\\n先出，队列的数据删除操作只能在始端进行，不允许在队列的中间某个结点后删除数据。也就是说队列的增和删的操作只能分别在这个队列的队尾和队头进行\\n\\n\\n\\n\\n\\n循环队列进行新增数据元素操作时，首先判断队列是否为满。如果不满，则可以将新元素赋值给队尾，然后让 rear 指针向后移动一个位置。如果已经排到队列最后的位置，则 rea r指针重新指向头部。\\n\\n循环队列进行删除操作时，即出队列操作，需要判断队列是否为空，然后将队头元素赋值给返回值，front 指针向后移一个位置。如果已经排到队列最后的位置，就把 front 指针重新指向到头部。这个过程就好像钟表的指针转到了表盘的尾部 12 点的位置后，又重新回到了表盘头部 1 点钟的位置。这样就能在不开辟大量存储空间的前提下，不采用 O(n) 的操作，也能通过移动数据来完成频繁的新增和删除数据\\n\\n\\n\\n数组的基本操作\\n数组在存储数据时是按顺序存储的，并且存储数据的内存也是连续的，这就造成了它具有增删困难、查找容易的特点。同时，栈和队列是加了限制的线性表，只能在特定的位置进行增删操作。相比之下，数组并没有这些限制，可以在任意位置增删数据，所以数组的增删操作会更为多样\\n\\n数组的新增操作\\n数组新增数据有两个情况：\\n\\n第一种情况，在数组的最后增加一个新的元素。此时新增一条数据后，对原数据产生没有任何影响。可以直接通过新增操作，赋值或者插入一条新的数据即可。时间复杂度是 O(1)。\\n\\n第二种情况，如果是在数组中间的某个位置新增数据，那么情况就完全不一样了。这是因为，新增了数据之后，会对插入元素位置之后的元素产生影响，具体为这些数据的位置需要依次向后挪动 1 个位置。\\n\\n例如，对于某一个长度为 4 的数组，我们在第 2 个元素之后插入一个元素，那么修改后的数组中就包含了 5 个元素，其中第 1、第 2 个元素不发生变化，第 3 个元素是新来的元素，第 4、第 5 个元素则是原来第 3、第 4 个元素。这一波操作，就需要对一般的数据进行重新搬家。而这个搬家操作，与数组的数据量线性相关，因此时间复杂度是 O(n)\\n\\n数组的删除操作\\n数组删除数据也有两种情况：\\n\\n第一种情况，在这个数组的最后，删除一个数据元素。由于此时删除一条数据后，对原数据没有产生任何影响。我们可以直接删除该数据即可，时间复杂度是 O(1)。\\n\\n第二种情况，在这个数组的中间某个位置，删除一条数据。同样的，这两种情况的区别在于，删除数据之后，其他数据的位置是否发生改变。由于此时的情况和新增操作高度类似，我们就不再举例子了。\\n\\n数组的查找操作\\n相比于复杂度较高的增删操作，数组的查找操作就方便一些了。由于索引的存在，数组基于位置的查找操作比较容易实现。我们可以索引值，直接在 O(1) 时间复杂度内查找到某个位置的元素。\\n\\n例如，查找数组中第三个位置的元素，通过 a[2] 就可以直接取出来。但对于链表系的数据结构，是没有这个优势的。\\n\\n不过，另外一种基于数值的查找方法，数组就没有什么优势了。例如，查找数值为 9 的元素是否出现过，以及如果出现过，索引值是多少。这样基于数值属性的查找操作，也是需要整体遍历一遍数组的。和链表一样，都需要 O(n) 的时间复杂度\\n\\n实际上数组是一种相当简单的数据结构，其增删查的时间复杂度相对于链表来说整体上是更优的。那么链表存在的价值又是什么呢？\\n\\n首先，链表的长度是可变的，数组的长度是固定的，在申请数组的长度时就已经在内存中开辟了若干个空间。如果没有引用 ArrayList 时，数组申请的空间永远是我们在估计了数据的大小后才执行，所以在后期维护中也相当麻烦。\\n\\n其次，链表不会根据有序位置存储，进行插入数据元素时，可以用指针来充分利用内存空间。数组是有序存储的，如果想充分利用内存的空间就只能选择顺序存储，而且需要在不取数据、不删除数据的情况下才能实现\\n\\n数组的案例\\n例题，假设数组存储了 5 个评委对 1 个运动员的打分，且每个评委的打分都不相等。现在需要你：\\n\\n用数组按照连续顺序保存，去掉一个最高分和一个最低分后的 3 个打分样本；\\n\\n计算这 3 个样本的平均分并打印。\\n\\n要求是，不允许再开辟 O(n) 空间复杂度的复杂数据结构。\\n\\n我们先分析一下题目：第一个问题，要输出删除最高分和最低分后的样本，而且要求是不允许再开辟复杂空间。因此，我们只能在原数组中找到最大值和最小值并删除。第二个问题，基于删除后得到的数组，计算平均值。所以解法如下：\\n\\n数组一次遍历，过程中记录下最小值和最大值的索引。对应下面代码的第 7 行到第 16 行。时间复杂度是 O(n)。\\n\\n执行两次基于索引值的删除操作。除非极端情况，否则时间复杂度仍然是 O(n)。对应于下面代码的第 18 行到第 30 行。\\n\\n计算删除数据后的数组元素的平均值。对应于下面代码的第 32 行到第 37 行。时间复杂度是 O(n)。\\n\\n因此，O(n) + O(n) + O(n) 的结果仍然是 O(n)。\\n\\n代码如下：\\n\\n```python\\n\\npublic void getScore() {\\nint a[] = { 2, 1, 4, 5, 3 };\\nmax_inx = -1;\\nmax_val = -1;\\nmin_inx= -1;\\nmin_val = 99;\\nfor (int i = 0; i < a.length; i++) {\\nif (a[i] > max_val) {\\nmax_val = a[i];\\nmax_inx = i;\\n}\\nif (a[i] < min_val) {\\nmin_val = a[i];\\nmin_inx = i;\\n}\\n}\\n```\\n\\n```\\ninx1 = max_inx;\\ninx2 = min_inx;\\nif (max_inx < min_inx){\\n    inx1 = min_inx;\\n    inx2 = max_inx;\\n}\\nfor (int i = inx1; i < a.length-1; i++) {\\n    a[i] = a[i+1];\\n}\\nfor (int i = inx2; i < a.length-1; i++) {\\n    a[i] = a[i+1];\\n}\\nsumscore = 0;\\nfor (int i = 0; i < a.length-2; i++) {\\n    sumscore += a[i];\\n}\\navg = sumscore/3.0;\\nSystem.out.println(avg);\\n}\\n```\\n\\n\\n\\n字符串的新增操作\\n字符串的新增操作和数组非常相似，都牵涉对插入字符串之后字符的挪移操作，所以时间复杂度是 O(n)。\\n\\n例如，在字符串 s1 = \"123456\" 的正中间插入 s2 = \"abc\"，则需要让 s1 中的 \"456\" 向后挪移 3 个字符的位置，再让 s2 的 \"abc\" 插入进来。很显然，挪移的操作时间复杂度是 O(n)。不过，对于特殊的插入操作时间复杂度也可以降低为 O(1)。这就是在 s1 的最后插入 s2，也叫作字符串的连接，最终得到 \"123456abc\"。\\n\\n字符串的删除操作\\n字符串的删除操作和数组同样非常相似，也可能会牵涉删除字符串后字符的挪移操作，所以时间复杂度是 O(n)。\\n\\n例如，在字符串 s1 = \"123456\" 的正中间删除两个字符 \"34\"，则需要删除 \"34\" 并让 s1 中的 \"56\" 向前挪移 2 个字符的位置。很显然，挪移的操作时间复杂度是 O(n)。不过，对于特殊的插入操作时间复杂度也可以降低为 O(1)。这就是在 s1 的最后删除若干个字符，不牵涉任何字符的挪移。\\n\\n字符串的查找操作\\n字符串的查找操作，是反映工程师对字符串理解深度的高频考点，这里需要你格外注意。\\n\\n例如，字符串 s = \"goodgoogle\"，判断字符串 t = \"google\" 在 s 中是否存在。需要注意的是，如果字符串 t 的每个字符都在 s 中出现过，这并不能证明字符串 t 在 s 中出现了。当 t = \"dog\" 时，那么字符 \"d\"、\"o\"、\"g\" 都在 s 中出现过，但他们并不连在一起\\n\\n子串查找（字符串匹配）\\n首先，我们来定义两个概念，主串和模式串。我们在字符串 A 中查找字符串 B，则 A 就是主串，B 就是模式串。我们把主串的长度记为 n，模式串长度记为 m。由于是在主串中查找模式串，因此，主串的长度肯定比模式串长，n>m。因此，字符串匹配算法的时间复杂度就是 n 和 m 的函数。\\n\\n假设要从主串 s = \"goodgoogle\" 中找到 t = \"google\" 子串。根据我们的思考逻辑，则有：\\n\\n首先，我们从主串 s 第 1 位开始，判断 s 的第 1 个字符是否与 t 的第 1 个字符相等。\\n\\n如果不相等，则继续判断主串的第 2 个字符是否与 t 的第1 个字符相等。直到在 s 中找到与 t 第一个字符相等的字符时，然后开始判断它之后的字符是否仍然与 t 的后续字符相等。\\n\\n如果持续相等直到 t 的最后一个字符，则匹配成功。\\n\\n如果发现一个不等的字符，则重新回到前面的步骤中，查找 s 中是否有字符与 t 的第一个字符相等。\\n\\n如下图所示，s 的第1 个字符和 t 的第 1 个字符相等，则开始匹配后续。直到发现前三个字母都匹配成功，但 s 的第 4 个字母匹配失败，则回到主串继续寻找和 t 的第一个字符相等的字符\\n\\n在二叉树中，有下面两个特殊的类型，如下图所示：\\n\\n满二叉树，定义为除了叶子结点外，所有结点都有 2 个子结点。\\n\\n完全二叉树，定义为除了最后一层以外，其他层的结点个数都达到最大，并且最后一层的叶子结点都靠左排列\\n\\n\\n\\n\\n\\n\\n\\n遍历一棵树，有非常经典的三种方法，分别是前序遍历、中序遍历、后序遍历。这里的序指的是父结点的遍历顺序，前序就是先遍历父结点，中序就是中间遍历父结点，后序就是最后遍历父结点。不管哪种遍历，都是通过递归调用完成的。如下图所示：\\n\\n前序遍历，对树中的任意结点来说，先打印这个结点，然后前序遍历它的左子树，最后前序遍历它的右子树。\\n\\n中序遍历，对树中的任意结点来说，先中序遍历它的左子树，然后打印这个结点，最后中序遍历它的右子树。\\n\\n后序遍历，对树中的任意结点来说，先后序遍历它的左子树，然后后序遍历它的右子树，最后打印它本身。\\n\\n\\n\\n不难发现，二叉树遍历过程中，每个结点都被访问了一次，其时间复杂度是 O(n)。接着，在找到位置后，执行增加和删除数据的操作时，我们只需要通过指针建立连接关系就可以了。对于没有任何特殊性质的二叉树而言，抛开遍历的时间复杂度以外，真正执行增加和删除操作的时间复杂度是 O(1)。树数据的查找操作和链表一样，都需要遍历每一个数据去判断，所以时间复杂度是 O(n)。\\n\\n我们上面讲到二叉树的增删查操作很普通，时间复杂度与链表并没有太多差别。但当二叉树具备一些特性的时候，则可以利用这些特性实现时间复杂度的降低。接下来，我们详细介绍二叉查找树的特性。\\n\\n二叉查找树的特性\\n二叉查找树（也称作二叉搜索树）具备以下几个的特性：\\n\\n在二叉查找树中的任意一个结点，其左子树中的每个结点的值，都要小于这个结点的值。\\n\\n在二叉查找树中的任意一个结点，其右子树中每个结点的值，都要大于这个结点的值。\\n\\n在二叉查找树中，会尽可能规避两个结点数值相等的情况。\\n\\n对二叉查找树进行中序遍历，就可以输出一个从小到大的有序数据队列。如下图所示，中序遍历的结果就是 10、13、15、16、20、21、22、26。\\n\\n二叉查找树的查找操作\\n在利用二叉查找树执行查找操作时，我们可以进行以下判断：\\n\\n首先判断根结点是否等于要查找的数据，如果是就返回。\\n\\n如果根结点大于要查找的数据，就在左子树中递归执行查找动作，直到叶子结点。\\n\\n如果根结点小于要查找的数据，就在右子树中递归执行查找动作，直到叶子结点。\\n\\n这样的“二分查找”所消耗的时间复杂度就可以降低为 O(logn)。关于二分查找，我们会在后续的分治法一讲中详细讲述。\\n\\n二叉查找树的插入操作\\n在二叉查找树执行插入操作也很简单。从根结点开始，如果要插入的数据比根结点的数据大，且根结点的右子结点不为空，则在根结点的右子树中继续尝试执行插入操作。直到找到为空的子结点执行插入动作。\\n\\n如下图所示，如果要插入数据 X 的值为 14，则需要判断 X 与根结点的大小关系：\\n\\n由于 14 小于 16，则聚焦在其左子树，继续判断 X 与 13 的关系。\\n\\n由于 14 大于 13，则聚焦在其右子树，继续判断 X 与15 的关系。\\n\\n由于 14 小于 15，则聚焦在其左子树。\\n\\n因为此时左子树为空，则直接通过指针建立 15 结点的左指针指向结点 X 的关系，就完成了插入动作。\\n\\n二叉查找树插入数据的时间复杂度是 O(logn)。但这并不意味着它比普通二叉树要复杂。原因在于这里的时间复杂度更多是消耗在了遍历数据去找到查找位置上，真正执行插入动作的时间复杂度仍然是 O(1)。\\n\\n二叉查找树的删除操作会比较复杂，这是因为删除完某个结点后的树，仍然要满足二叉查找树的性质。我们分为下面三种情况讨论。\\n\\n情况一，如果要删除的结点是某个叶子结点，则直接删除，将其父结点指针指向 null 即可\\n\\n情况二，如果要删除的结点只有一个子结点，只需要将其父结点指向的子结点的指针换成其子结点的指针即可\\n\\n情况三，如果要删除的结点有两个子结点，则有两种可行的操作方式。\\n\\n第一种，找到这个结点的左子树中最大的结点，替换要删除的结点。\\n\\n第二种，找到这个结点的右子树中最小的结点，替换要删除的结点。\\n\\n总结\\n本课时的内容围绕着不同种类树的原理、二叉树对于数据的增删查操作展开。要想利用二叉树实现增删查操作，你需要熟练掌握二叉树的三种遍历方式。遍历的时间复杂度是 O(n)。有了遍历方式之后，你可以完成在指定位置的数据增删操作。增删操作的时间复杂度都是 O(1)。\\n\\n对于查找操作，如果是普通二叉树，则查找的时间复杂度和遍历一样，都是 O(n)。如果是二叉查找树，则可以在 O(logn) 的时间复杂度内完成查找动作。树结构在存在“一对多”的数据关系中，可被高频使用，这也是它区别于链表系列数据结构的关键点\\n\\n对于数据处理它们彼此之间各有千秋，例如：\\n\\n线性表中的栈和队列对增删有严格要求，它们会更关注数据的顺序。\\n\\n数组和字符串需要保持数据类型的统一，并且在基于索引的查找上会更有优势。\\n\\n树的优势则体现在数据的层次结构上。\\n\\n但它们普遍都存在这样的缺陷，那就是数据数值条件的查找，都需要对全部数据或者部分数据进行遍历。那么，有没有一种方法可以省去数据比较的过程，从而进一步提升数值条件查找的效率呢？答案当然是：有。这一课时我们就来介绍这样一种高效率的查找神器，哈希表。\\n\\n什么是哈希表\\n哈希表名字源于 Hash，也可以叫作散列表。哈希表是一种特殊的数据结构，它与数组、链表以及树等我们之前学过的数据结构相比，有很明显的区别。\\n\\n哈希表的核心思想\\n在我们之前学过的数据结构里，数据的存储位置和数据的具体数值之间不存在任何关系。因此，在面对查找问题时，这些数据结构必须采取逐一比较的方法去实现。\\n\\n而哈希表的设计采用了函数映射的思想，将记录的存储位置与记录的关键字关联起来。这样的设计方式，能够快速定位到想要查找的记录，而且不需要与表中存在的记录的关键字比较后再来进行查找。\\n\\n我们回顾一下数组的查找操作。数组是通过数据的索引（index）来取出数值的，例如要找出 a 数组中，索引值为 1 的元素。在前面的课时中，我们讲到索引值是数据存储的位置，因此，直接通过 a[1] 就可以取出这个数据。通过这样的方式，数组实现了“地址 = f (index)”的映射关系。\\n\\n如果用哈希表的逻辑来理解的话，这里的 f () 就是一个哈希函数。它完成了索引值到实际地址的映射，这就让数组可以快速完成基于索引值的查找。然而，数组的局限性在于，它只能基于数据的索引去查找，而不能基于数据的数值去查找。\\n\\n如果有一种方法，可以实现“地址 = f (关键字)”的映射关系，那么就可以快速完成基于数据的数值的查找了。这就是哈希表的核心思想。 下面我们通过一个例子来体会一下。\\n\\n假如，我们要对一个手机通讯录进行存储，并要根据姓名找出一个人的手机号码，如下所示：\\n\\n张一：155555555\\n\\n张二：166666666\\n\\n张三：177777777\\n\\n张四：188888888\\n\\n一个可行的方法是，定义包含姓名、手机号码的结构体，再通过链表把 4 个联系人的信息存起来。当要判断“张四”是否在链表中，或者想要查找到张四的手机号码时，就需要从链表的头结点开始遍历。依次将每个结点中的姓名字段，同“张四”进行比较。直到查找成功或者全部遍历一次为止。显然，这种做法的时间复杂度为 O(n)。\\n\\n如果要降低时间复杂度，就需要借助哈希表的思路，构建姓名到地址的映射函数“地址 = f (姓名)”。这样，我们就可以通过这个函数直接计算出”张四“的存储位置，在 O(1) 时间复杂度内就可以完成数据的查找。\\n\\n通过这个例子，不难看出 Hash 函数设计的好坏会直接影响到对哈希表的操作效率。假如对上面的例子采用的 Hash 函数为，姓名的每个字的拼音开头大写字母的 ASCII 码之和。即：\\n\\naddress (张一) = ASCII (Z) + ASCII (Y) = 90 + 89 = 179；\\n\\naddress (张二) = ASCII (Z) + ASCII (E) = 90 + 69 = 159；\\n\\naddress (张三) = ASCII (Z) + ASCII (S) = 90 + 83 = 173；\\n\\naddress (张四) = ASCII (Z) + ASCII (S) = 90 + 83 = 173；\\n\\n我们发现这个哈希函数存在一个非常致命的问题，那就是 f ( 张三) 和 f (张四) 都是 173。这种现象称作哈希冲突，是需要在设计哈希函数时进行规避的。\\n\\n从本质上来看，哈希冲突只能尽可能减少，不能完全避免。这是因为，输入数据的关键字是个开放集合。只要输入的数据量够多、分布够广，就完全有可能发生冲突的情况。因此，哈希表需要设计合理的哈希函数，并且对冲突有一套处理机制\\n\\n如何设计哈希函数\\n我们先看一些常用的设计哈希函数的方法：\\n\\n第一，直接定制法\\n\\n哈希函数为关键字到地址的线性函数。如，H (key) = a*key + b。\\xa0这里，a 和 b 是设置好的常数。\\n\\n第二，数字分析法\\n\\n假设关键字集合中的每个关键字 key 都是由 s 位数字组成（k1,k2,…,Ks），并从中提取分布均匀的若干位组成哈希地址。上面张一、张二、张三、张四的手机号信息存储，就是使用的这种方法。\\n\\n第三，平方取中法\\n\\n如果关键字的每一位都有某些数字重复出现，并且频率很高，我们就可以先求关键字的平方值，通过平方扩大差异，然后取中间几位作为最终存储地址。\\n\\n第四，折叠法\\n\\n如果关键字的位数很多，可以将关键字分割为几个等长的部分，取它们的叠加和的值（舍去进位）作为哈希地址。\\n\\n第五，除留余数法\\n\\n预先设置一个数 p，然后对关键字进行取余运算。即地址为 key mod p\\n\\n如何解决哈希冲突\\n上面这些常用方法都有可能会出现哈希冲突。那么一旦发生冲突，我们该如何解决呢？\\n\\n常用的方法，有以下两种：\\n\\n第一，开放定址法\\n\\n即当一个关键字和另一个关键字发生冲突时，使用某种探测技术在哈希表中形成一个探测序列，然后沿着这个探测序列依次查找下去。当碰到一个空的单元时，则插入其中。\\n\\n常用的探测方法是线性探测法。 比如有一组关键字 {12，13，25，23}，采用的哈希函数为 key mod 11。当插入 12，13，25 时可以直接插入，地址分别为 1、2、3。而当插入 23 时，哈希地址为 23 mod 11 = 1。然而，地址 1 已经被占用，因此沿着地址 1 依次往下探测，直到探测到地址 4，发现为空，则将 23 插入其中。如下图所示\\n\\n第二，链地址法\\n\\n将哈希地址相同的记录存储在一张线性链表中。\\n\\n例如，有一组关键字 {12,13,25,23,38,84,6,91,34}，采用的哈希函数为 key mod 11。如下图所示\\n\\n哈希表相对于其他数据结构有很多的优势。它可以提供非常快速的插入-删除-查找操作，无论多少数据，插入和删除值需要接近常量的时间。在查找方面，哈希表的速度比树还要快，基本可以瞬间查找到想要的元素。\\n\\n哈希表也有一些不足。哈希表中的数据是没有顺序概念的，所以不能以一种固定的方式（比如从小到大）来遍历其中的元素。在数据处理顺序敏感的问题时，选择哈希表并不是个好的处理方法。同时，哈希表中的 key 是不允许重复的，在重复性非常高的数据中，哈希表也不是个好的选择\\n\\n哈希表的基本操作\\n在很多高级语言中，哈希函数、哈希冲突都已经在底层完成了黑盒化处理，是不需要开发者自己设计的。也就是说，哈希表完成了关键字到地址的映射，可以在常数级时间复杂度内通过关键字查找到数据。\\n\\n至于实现细节，比如用了哪个哈希函数，用了什么冲突处理，甚至某个数据记录的哈希地址是多少，都是不需要开发者关注的。接下来，我们从实际的开发角度，来看一下哈希表对数据的增删查操作。\\n\\n哈希表中的增加和删除数据操作，不涉及增删后对数据的挪移问题（数组需要考虑），因此处理就可以了。\\n\\n哈希表查找的细节过程是：对于给定的 key，通过哈希函数计算哈希地址 H (key)。\\n\\n如果哈希地址对应的值为空，则查找不成功。\\n\\n反之，则查找成功。\\n\\n虽然哈希表查找的细节过程还比较麻烦，但因为一些高级语言的黑盒化处理，开发者并不需要实际去开发底层代码，只要调用相关的函数就可以了\\n\\n哈希表的案例\\n下面我们来讲解两个案例，帮助你进一步理解哈希表的操作过程。\\n\\n例 1，将关键字序列 {7, 8, 30, 11, 18, 9, 14} 存储到哈希表中。哈希函数为： H (key) = (key * 3) % 7，处理冲突采用线性探测法。\\n\\n接下来，我们分析一下建立哈希表和查找关键字的细节过程。\\n\\n首先，我们尝试建立哈希表，求出这个哈希地址：\\n\\nH (7) = (7 * 3) % 7 = 0\\n\\nH (8) = (8 * 3) % 7 = 3\\n\\nH (30) = 6\\n\\nH (11) = 5\\n\\nH (18) = 5\\n\\nH (9) = 6\\n\\nH (14) = 0\\n\\n按关键字序列顺序依次向哈希表中填入，发生冲突后按照“线性探测”探测到第一个空位置填入\\n\\n\\n\\n接着，有了这个表之后，我们再来看一下查找的流程：\\n\\n查找 7。输入 7，计算得到 H (7) = 0，根据哈希表，在 0 的位置，得到结果为 7，跟待匹配的关键字一样，则完成查找。\\n\\n查找 18。输入 18，计算得到 H (18) = 5，根据哈希表，在 5 的位置，得到结果为 11，跟待匹配的关键字不一样（11 不等于 18）。因此，往后挪移一位，在 6 的位置，得到结果为 30，跟待匹配的关键字不一样（11 不等于 30）。因此，继续往后挪移一位，在 7 的位置，得到结果为 18，跟待匹配的关键字一样，完成查找\\n\\n例 2，假设有一个在线系统，可以实时接收用户提交的字符串型关键字，并实时返回给用户累积至今这个关键字被提交的次数。\\n\\n例如，用户输入\"abc\"，系统返回 1。用户再输入\"jk\"，系统返回 1。用户再输入\"xyz\"，系统返回 1。用户再输入\"abc\"，系统返回 2。用户再输入\"abc\"，系统返回 3。\\n\\n一种解决方法是，用一个数组保存用户提交过的所有关键字。当接收到一个新的关键字后，插入到数组中，并且统计这个关键字出现的次数。\\n\\n根据数组的知识可以计算出，插入到最后的动作，时间复杂度是 O(1)。但统计出现次数必须要全部数据遍历一遍，时间复杂度是 O(n)。随着数据越来越多，这个在线系统的处理时间将会越来越长。显然，这不是一个好的方法。\\n\\n如果采用哈希表，则可以利用哈希表新增、查找的常数级时间复杂度，在 O(1) 时间复杂度内完成响应。预先定义好哈希表后（可以采用 Map  d = new HashMap  (); ）对于关键字（用变量 key_str 保存），判断 d 中是否存在 key_str 的记录。\\n\\n如果存在，则把它对应的value（用来记录出现的频次）加 1；\\n\\n如果不存在，则把它添加到 d 中，对应的 value 赋值为 1。最后，打印处 key_str 对应的 value，即累积出现的频次。\\n\\n代码如下：\\n\\n复制代码\\nif (d.containsKey(key_str) {\\nd.put(key_str, d.get(key_str) + 1);\\n}\\nelse{\\nd.put(key_str, 1);\\n}\\nSystem.out.println(d.get(key_str));\\n\\n哈希表在我们平时的数据处理操作中有着很多独特的优点，不论哈希表中有多少数据，查找、插入、删除只需要接近常量的时间，即 O(1）的时间级。\\n\\n实际上，这只需要几条机器指令。哈希表运算得非常快，在计算机程序中，如果需要在一秒钟内查找上千条记录通常使用哈希表（例如拼写检查器)，哈希表的速度明显比树快，树的操作通常需要 O(n) 的时间级。哈希表不仅速度快，编程实现也相对容易。如果不需要有序遍历数据，并且可以提前预测数据量的大小。那么哈希表在速度和易用性方面是无与伦比的\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='56b23ee7-04dc-421c-bcd8-9e310beebdb9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**15. Pattern: Topological Sort (Graph)，拓扑排序类型**\\n\\n拓扑排序模式用来寻找一种线性的顺序，这些元素之间具有依懒性。比如，如果事件B依赖于事件A，那A在拓扑排序顺序中排在B的前面。\\n\\n这种模式定义了一种简单方式来理解拓扑排序这种技术。\\n\\n这种模式是这样奏效的：\\n\\n1. 初始化a) 借助于HashMap将图保存成邻接表形式。b) 找到所有的起点，用HashMap来帮助记录每个节点的入度\\n2. 创建图，找到每个节点的入度a) 利用输入，把图建好，然后遍历一下图，将入度信息记录在HashMap中\\n3. 找所有的起点a) 所有入度为0的节点，都是有效的起点，而且我们讲他们都加入到一个队列中\\n4. 排序a) 对每个起点，执行以下步骤—i) 把它加到结果的顺序中— ii)将其在图中的孩子节点取到— iii)将其孩子的入度减少1— iv)如果孩子的入度变为0，则改孩子节点成为起点，将其加入队列中b) 重复（a）过程，直到起点队列为空。\\n\\n拓扑排序模式识别：\\n\\n- 待解决的问题需要处理无环图\\n- 你需要以一种有序的秩序更新输入元素\\n- 需要处理的输入遵循某种特定的顺序\\n\\n**经典题目：**\\n\\nTopological Sort (medium)\\n\\nTasks Scheduling (medium)\\n\\nTasks Scheduling Order (medium)\\n\\nAll Tasks Scheduling Orders (hard)\\n\\nAlien Dictionary (hard)\\n\\n大家好好练练这些题目，面试中遇到中高等难度的题目，应该就能解得不错了。\\n\\n!Untitled\\n\\n作者：编程指北\\n\\n链接：https://www.zhihu.com/question/28580777/answer/1961384750\\n\\n来源：知乎\\n\\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\\n\\n**排序类（Sort）：**\\n\\n- 基础知识：快速排序（Quick Sort）， 归并排序（Merge Sort）的原理与代码实现。需要能讲明白代码中每一行的目的。快速排序时间复杂度平均状态下O（NlogN），空间复杂度O（1），归并排序最坏情况下时间复杂度O（NlogN），空间复杂度O（N）\\n- 入门题目：\\n    - Leetcode 148. Sort List\\n    - Leetcode 56. Merge Intervals\\n- 进阶题目：\\n    - Leetcode 179. Largest Number\\n    - Leetcode 75. Sort Colors\\n    - Leetcode 215. Kth Largest Element\\n    - Leetcode 4. Median of Two Sorted Arrays\\n\\n注意：后两题是与快速排序非常相似的快速选择（Quick Select）算法，面试中很常考\\n\\n**链表类（Linked List）：**\\n\\n- 基础知识：链表如何实现，如何遍历链表。链表可以保证头部尾部插入删除操作都是O（1），查找任意元素位置O（N）\\n- 基础题目：\\n    - Leetcode 206. Reverse Linked List\\n    - Leetcode 876. Middle of the Linked List\\n\\n注意：快慢指针和链表反转几乎是所有链表类问题的基础，尤其是反转链表，代码很短，建议直接背熟。\\n\\n- 进阶题目:\\n    - Leetcode 160. Intersection of Two Linked Lists\\n    - Leetcode 141. Linked List Cycle (Linked List Cycle II)\\n    - Leetcode 92. Reverse Linked List II\\n    - Leetcode 328. Odd Even Linked List\\n\\n**堆（Heap or Priority Queue）、栈（Stack）、队列（Queue）、哈希表类（Hashmap、Hashset）：**\\n\\n- 基础知识：各个数据结构的基本原理，增删查改复杂度。\\n- Queue题目：\\n    - Leetcode 225. Implement Stack using Queues\\n    - Leetcode 346. Moving Average from Data Stream\\n    - Leetcode 281. Zigzag Iterator\\n    - Leetcode 1429. First Unique Number\\n    - Leetcode 54. Spiral Matrix\\n    - Leetcode 362. Design Hit Counter\\n- Stack题目：\\n    - Leetcode 232. Implement Queue using Stacks\\n    - Leetcode 150. Evaluate Reverse Polish Notation\\n    - Leetcode 224. Basic Calculator II (I, II, III, IV)\\n    - Leetcode 20. Valid Parentheses\\n    - Leetcode 1472. Design Browser History\\n    - Leetcode 1209. Remove All Adjacent Duplicates in String II\\n    - Leetcode 1249. Minimum Remove to Make Valid Parentheses\\n    - Leetcode 735. Asteroid Collision\\n- Hashmap/ Hashset题目：\\n    - Leetcode 1. Two Sum\\n    - Leetcode 146. LRU Cache (Python中可以使用OrderedDict来代替)\\n    - Leetcode 128. Longest Consecutive Sequence\\n    - Leetcode 73. Set Matrix Zeroes\\n    - Leetcode 380. Insert Delete GetRandom O(1)\\n    - Leetcode 49. Group Anagrams\\n    - Leetcode 350. Intersection of Two Arrays II\\n    - Leetcode 299. Bulls and Cows\\n    - Leetcode 348 Design Tic-Tac-Toe\\n- Heap／Priority Queue题目：\\n    - Leetcode 973. K Closest Points\\n    - Leetcode 347. Top k Largest Elements\\n    - Leetcode 23. Merge K Sorted Lists\\n    - Leetcode 264. Ugly Number II\\n    - Leetcode 1086. High Five\\n    - Leetcode 68. Merge Sorted Arrays\\n    - Leetcode 692. Top K Frequent Words\\n    - Leetcode 378. Kth Smallest Element in a Sorted Matrix\\n    - Leetcode 295. Find Median from Data Stream\\n    - Leetcode 767. Reorganize String\\n    - Leetcode 1438. Longest Continuous Subarray With Absolute Diff Less Than or Equal to Limit\\n    - Leetcode 895. Maximum Frequency Stack\\n\\n**二分法（Binary Search）：**\\n\\n- 基础知识：二分法是用来解法基本模板，时间复杂度logN；常见的二分法题目可以分为两大类，显式与隐式，即是否能从字面上一眼看出二分法的特点：要查找的数据是否可以分为两部分，前半部分为X，后半部分为O\\n- 显式二分法：\\n    - Leetcode 34. Find First and Last Position of Element in Sorted Array\\n    - Leetcode 33. Search in Rotated Sorted Array\\n    - Leetcode 1095. Find in Mountain Array\\n    - Leetcode 162. Find Peak Element\\n    - Leetcode 278. First Bad Version\\n    - Leetcode 74. Search a 2D Matrix\\n    - Leetcode 240. Search a 2D Matrix II\\n- 隐式二分法：\\n    - Leetcode 69. Sqrt(x)\\n    - Leetcode 540. Single Element in a Sorted Array\\n    - Leetcode 644. Maximum Average Subarray II\\n    - Leetcode 528. Random Pick with Weight\\n    - Leetcode 1300. Sum of Mutated Array Closest to Target\\n    - Leetcode 1060. Missing Element in Sorted Array\\n\\n**双指针（2 Pointer）：**\\n\\n- 基础知识：常见双指针算法分为三类，同向（即两个指针都相同一个方向移动），背向（两个指针从相同或者相邻的位置出发，背向移动直到其中一根指针到达边界为止），相向（两个指针从两边出发一起向中间移动直到两个指针相遇）\\n- 背向双指针：(基本上全是回文串的题)\\n    - Leetcode 409. Longest Palindrome\\n    - Leetcode 125. Valid Palindrome\\n    - Leetcode 5. Longest Palindromic Substring\\n- 相向双指针：(以two sum为基础的一系列题)\\n    - Leetcode 1. Two Sum (这里使用的是先排序的双指针算法，不同于hashmap做法)\\n    - Leetcode 167. Two Sum II - Input array is sorted\\n    - Leetcode 15. 3Sum\\n    - Leetcode 16. 3Sum Closest\\n    - Leetcode 18. 4Sum\\n    - Leetcode 454. 4Sum II\\n    - Leetcode 277. Find the Celebrity\\n- 同向双指针：（个人觉得最难的一类题）\\n    - Leetcode 283. Move Zeroes\\n    - Leetcode 26. Remove Duplicate Numbers in Array\\n    - Leetcode 395. Longest Substring with At Least K Repeating Characters\\n    - Leetcode 340. Longest Substring with At Most K Distinct Characters\\n    - Leetcode 76. Minimum Window Substring\\n    - Leetcode 3. Longest Substring Without Repeating Characters\\n\\n**宽度优先搜索（BFS）：面试中与DFS都为几乎必考的题目**\\n\\n- 基础知识：\\n    - 常见的BFS用来解决什么问题？(1) 简单图（有向无向皆可）的最短路径长度（2）拓扑排序 （3） 遍历一个图（或者树）\\n- BFS基本模板（需要记录层数或者不需要记录层数）\\n- 多数情况下时间复杂度空间复杂度都是O（N+M），N为节点个数，M为边的个数\\n- 基于树的BFS：不需要专门一个set来记录访问过的节点\\n    - Leetcode 102 Binary Tree Level Order Traversal\\n    - Leetcode 103 Binary Tree Zigzag Level Order Traversal\\n    - Leetcode 297 Serialize and Deserialize Binary Tree （很好的BFS和双指针结合的题）\\n- Leetcode 314 Binary Tree Vertical Order Traversal\\n- 基于图的BFS：（一般需要一个set来记录访问过的节点）\\n    - Leetcode 200. Number of Islands\\n    - Leetcode 133. Clone Graph\\n    - Leetcode 127. Word Ladder\\n    - Leetcode 490. The Maze\\n    - Leetcode 323. Connected Component in Undirected Graph\\n    - Leetcode 130. Surrounded Regions\\n    - Leetcode 752. Open the Lock\\n    - Leetcode 815. Bus Routes\\n    - Leetcode 1091. Shortest Path in Binary Matrix\\n    - Leetcode 542. 01 Matrix\\n    - Leetcode 1293. Shortest Path in a Grid with Obstacles Elimination\\n- 拓扑排序：（https://zh.wikipedia.org/wiki/%E6%8B%93%E6%92%B2%E6%8E%92%E5%BA%8F）\\n    - Leetcode 207 Course Schedule （I, II）\\n    - Leetcode 444 Sequence Reconstruction\\n    - Leetcode 269 Alien Dictionary\\n\\n**深度优先搜索（DFS）：面试中与BFS都为几乎必考的题目**\\n\\n- 基础知识：\\n    - 常见的DFS用来解决什么问题？(1) 图中（有向无向皆可）的符合某种特征（比如最长）的路径以及长度（2）排列组合（3） 遍历一个图（或者树）（4）找出图或者树中符合题目要求的全部方案\\n    - DFS基本模板（需要记录路径，不需要返回值 and 不需要记录路径，但需要记录某些特征的返回值）\\n    - 除了遍历之外多数情况下时间复杂度是指数级别，一般是O(方案数×找到每个方案的时间复杂度)\\n    - 递归题目都可以用非递归迭代的方法写，但一般实现起来非常麻烦\\n- 基于树的DFS：需要记住递归写前序中序后序遍历二叉树的模板\\n    - Leetcode 543 Diameter of Binary Tree\\n    - Leetcode 226 Invert Binary Tree\\n    - Leetcode 124 Binary Tree Maximum Path Sum\\n    - Leetcode 236 Lowest Common Ancestor of a Binary Tree\\n    - Leetcode 101 Symmetric Tree\\n    - Leetcode 105 Construct Binary Tree from Preorder and Inorder Traversal\\n    - Leetcode 104 Maximum Depth of Binary Tree\\n    - Leetcode 951 Flip Equivalent Binary Trees\\n    - Leetcode 987 Vertical Order Traversal of a Binary Tree\\n    - Leetcode 1485 Clone Binary Tree With Random Pointer\\n    - Leetcode 572 Subtree of Another Tree\\n    - Leetcode 863 All Nodes Distance K in Binary Tree\\n- 二叉搜索树（BST）：BST特征：中序遍历为单调递增的二叉树，换句话说，根节点的值比左子树任意节点值都大，比右子树任意节点值都小，增删查改均为O（h）复杂度，h为树的高度；注意不是所有的BST题目都需要递归，有的题目只需要while循环即可\\n    - Leetcode 230 Kth Smallest element in a BST\\n    - Leetcode 98 Validate Binary Search Tree\\n    - Leetcode 270 Cloest Binary Search Tree Value\\n    - Leetcode 235 Lowest Common Ancestor of a Binary Search Tree\\n    - Leetcode 669 Trim a Binary Search Tree\\n    - Leetcode 700 Search Range in Binary Search Tree\\n    - Leetcode 108 Convert Sorted Array to Binary Search Tree\\n    - Leetcode 333 Largest BST Subtree\\n    - Leetcode 510 Inorder Successor in BST II\\n- 基于图的DFS: 和BFS一样一般需要一个set来记录访问过的节点，避免重复访问造成死循环\\n    - Leetcode 341 Flatten Nested List Iterator\\n    - Leetcode 394 Decode String\\n    - Leetcode 51 N-Queens\\n    - Leetcode 291 Word Pattern II (I为简单的Hashmap题)\\n    - Leetcode 126 Word Ladder II （I为BFS题目）\\n    - Leetcode 1110 Delete Nodes And Return Forest\\n    - Leetcode 93 Restore IP Addresses\\n    - Leetcode 22 Generate Parentheses\\n    - Leetcode 37 Sodoku Solver\\n    - Leetcode 301 Remove Invalid Parentheses\\n    - Leetcode 212 Word Search II （I, II）\\n    - Leetcode 1087 Brace Expansion\\n    - Leetcode 399 Evaluate Division\\n    - Leetcode 1274 Number of Ships in a Rectangle\\n    - Leetcode 1376 Time Needed to Inform All Employees\\n    - Leetcode 694 Number of Distinct Islands\\n    - Leetcode 586 Score of Parentheses\\n- 基于排列组合的DFS: 其实与图类DFS方法一致，但是排列组合的特征更明显\\n    - Leetcode 17 Letter Combinations of a Phone Number\\n    - Leetcode 39 Combination Sum （I, II, III, IV）\\n    - Leetcode 90 Subsets II （重点在于如何去重）\\n    - Leetcode 47 Permutation II\\n    - Leetcode 77 Combinations\\n    - Leetcode 526 Beautiful Arrangement\\n- 记忆化搜索（DFS + Memoization Search）：算是动态规划的一种，递归每次返回时同时记录下已访问过的节点特征，避免重复访问同一个节点，可以有效的把指数级别的DFS时间复杂度降为多项式级别\\n    - Leetcode 139 Word Break II\\n    - Leetcode 131 Palindrome Partitioning\\n    - Leetcode 72 Edit Distance\\n    - Leetcode 377 Combination Sum IV\\n    - Leetcode 1335 Minimum Difficulty of a Job Schedule\\n\\n**前缀和（Prefix Sum）**\\n\\n- 基础知识：前缀和本质上是在一个list当中，用O（N）的时间提前算好从第0个数字到第i个数字之和，在后续使用中可以在O（1）时间内计算出第i到第j个数字之和，一般很少单独作为一道题出现，而是很多题目中的用到的一个小技巧\\n- 常见题目：\\n    - Leetcode 53 Maximum Subarray\\n    - Leetcode 1423 Maximum Points You Can Obtain from Cards\\n    - Leetcode 1031 Maximum Sum of Two Non-Overlapping Subarrays\\n    - Leetcode 523 Continuous Subarray Sum\\n\\n---\\n\\n以上内容皆为面试中高频的知识点，以下知识点和题目在面试中属于中等频率（大概面10道题会遇到一次），时间不足的情况下，请以准备上面的知识点为主。\\n\\n**并查集（Union Find）：把两个或者多个集合合并为一个集合**\\n\\n- 基础知识：如果数据不是实时变化，本类问题可以用BFS或者DFS的方式遍历，如果数据实时变化（data stream）则并查集每次的时间复杂度可以视为O（1）；需要牢记合并与查找两个操作的模板\\n- 常见题目：\\n    - Leetcode 721 Accounts Merge\\n    - Leetcode 547 Number of Provinces\\n    - Leetcode 737 Sentence Similarity II\\n    - Leetcode 434 Number of Islands II\\n\\n**字典树（Trie）**\\n\\n- 基础知识：（https://zh.wikipedia.org/wiki/Trie）；多数情况下可以通过用一个set来记录所有单词的prefix来替代，时间复杂度不变，但空间复杂度略高\\n- 常见题目：\\n    - Leetcode 208 Implement Trie (Prefix Tree)\\n    - Leetcode 211 Design Add and Search Words Data Structure\\n    - Leetcode 1268 Search Suggestions System\\n    - Leetcode 79 Word Search\\n\\n**单调栈与单调队列（Monotone Stack／Queue）**\\n\\n- 基础知识：单调栈一般用于解决数组中找出每个数字的第一个大于／小于该数字的位置或者数字；单调队列只见过一道题需要使用；不论单调栈还是单调队列，单调的意思是保留在栈或者队列中的数字是单调递增或者单调递减的\\n- 常见题目：\\n    - Leetcode 85 Maximum Rectangle\\n    - Leetcode 84 Largest Rectangle in Histogram\\n    - Leetcode 739 Daily Temperatures\\n    - Leetcode 901 Online Stock Span\\n    - Leetcode 503 Next Greater Element II\\n    - Leetcode 239 Sliding Window Maximum （唯一的单调队列题）\\n\\n**扫描线算法（Sweep Line）**\\n\\n- 基础知识：一个很巧妙的解决时间安排冲突的算法，本身比较容易些也很容易理解\\n- 常见题目：\\n    - Leetcode 253 Meeting Room II（Meeting Room I也可以使用）\\n    - Leetcode 218 The Skyline Problem\\n    - Leetcode 759 Employee Free Time\\n\\n**TreeMap**\\n\\n- 基础知识：基于红黑树（平衡二叉搜索树）的一种树状 hashmap，增删查改、找求大最小均为logN复杂度，Python当中可以使用SortedDict替代\\n- 常见题目：\\n- Leetcode 729 My Calendar I\\n- Leetcode 981 Time Based Key-Value Store\\n- Leetcode 846 Hand of Straights\\n- Leetcode 826 Most Profit Assigning Work\\n\\n**动态规划（Dynamic Programming）**\\n\\n- 基础知识：这里指的是用for循环方式的动态规划，非Memoization Search方式。DP可以在多项式时间复杂度内解决DFS需要指数级别的问题。常见的题目包括找最大最小，找可行性，找总方案数等，一般结果是一个Integer或者Boolean。动态规划有很多分支，暂时还没想好怎么去写这部分，后面想好了再具体写吧。\\n- 常见题目：\\n    - Leetcode 674 Longest Continuous Increasing Subsequence\\n    - Leetcode 62 Unique Paths II\\n    - Leetcode 70 Climbing Stairs\\n    - Leetcode 64 Minimum Path Sum\\n    - Leetcode 368 Largest Divisible Subset\\n    - Leetcode 300 Longest Increasing Subsequence\\n    - Leetcode 354 Russian Doll Envelopes\\n    - Leetcode 256 Paint House\\n    - Leetcode 121 Best Time to Buy and Sell Stock\\n    - Leetcode 55 Jump Game\\n    - Leetcode 45 Jump Game II\\n    - Leetcode 403 Frog Jump\\n    - Leetcode 132 Palindrome Partitioning II\\n    - Leetcode 312 Burst Balloons\\n    - Leetcode 1143 Longest Common Subsequence\\n    - Leetcode 115 Distinct Subsequences\\n    - Leetcode 72 Edit Distance\\n    - Leetcode 91 Decode Ways\\n    - Leetcode 639 Decode Ways II\\n    - Leetcode 712 Minimum ASCII Delete Sum for Two Strings\\n    - Leetcode 221 Maximal Square\\n    - Leetcode 198 House Robber\\n    - Leetcode 213 House Robber II\\n    - Leetcode 87 Scramble String\\n    - Leetcode 1062 Longest Repeating Substring\\n    - Leetcode 1140 Stone Game II\\n    - Leetcode 322 Coin Change\\n    - Leetcode 518 Coin Change II\\n    - Leetcode 97 Interleaving String\\n    \\n    作者：编程指北\\n    \\n    链接：https://www.zhihu.com/question/28580777/answer/1961384750\\n    \\n    来源：知乎\\n    \\n    著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\\n    \\n    ### 1. 线性表：\\n    \\n    **1.1 Remove Duplicates from Sorted Array**\\n    \\n    **1.2 Remove Duplicates from Sorted Array II**\\n    \\n    **1.3 Search in Rotated Sorted Array II**\\n    \\n    **1.4 Median of Two Sorted Arrays**\\n    \\n    **1.5 Longest Consecutive Sequence**\\n    \\n    **1.6 Two Sum**\\n    \\n    **1.7 Valid Sudoku**\\n    \\n    **1.8 Trapping Rain Water**\\n    \\n    **1.9 Swap Nodes in Pairs**\\n    \\n    **1.10 Reverse Nodes in k-Group**\\n    \\n    ### 2. 字符串\\n    \\n    ### 2.1 **Valid Palindrome**\\n    \\n    **2.2 Implement strStr()**\\n    \\n    **3.3 String to Integer (atoi)**\\n    \\n    **3.4 Add Binary**\\n    \\n    **3.5 Longest Palindromic Substring**\\n    \\n    **3.6 Regular Expression Matching**\\n    \\n    **3.7 Wildcard Matching**\\n    \\n    **3.8 Longest Common Prefix**\\n    \\n    **3.9 Valid Number**\\n    \\n    **3.10 Integer to Roman**\\n    \\n    ### 3. 树、二叉树\\n    \\n    3.1 **Binary Tree Preorder Traversal**\\n    \\n    **3.2 Binary Tree Inorder Traversal**\\n    \\n    **3.3 Binary Tree Postorder Traversal**\\n    \\n    **3.4 Binary Tree Level Order Traversal II**\\n    \\n    **3.5 Binary Tree Zigzag Level Order Traversal**\\n    \\n    **3.6 Construct Binary Tree from Preorder and Inorder Traversal**\\n    \\n    **3.7 Unique Binary Search Trees**\\n    \\n    3.8 **Validate Binary Search Tree**\\n    \\n    **3.9 Convert Sorted Array to Binary Search Tree**\\n    \\n    ### 4. 排序\\n    \\n    4.1 **Merge Sorted Array**\\n    \\n    **4.2 Merge Two Sorted Lists** **4.3 Merge k Sorted Lists**\\n    \\n    **4.4 Insertion Sort List**\\n    \\n    **4.5 Sort List**\\n    \\n    **4.6 First Missing Positive**\\n    \\n    ### 5. 暴力枚举\\n    \\n    5.1 **Subsets**\\n    \\n    **5.1 Subsets II**\\n    \\n    **5.3 Permutations**\\n    \\n    **5.4 Letter Combinations of a Phone Number**\\n    \\n    ### 6. 深度优先搜索\\n    \\n    6.1 **Palindrome Partitioning**\\n    \\n    **6.2 Unique Paths**\\n    \\n    **6.3 Unique Paths II**\\n    \\n    ### 7. 回溯\\n    \\n    ### 8. 深搜与递归的区别\\n    \\n    ### 9. 分治法\\n    \\n    ### 10. 贪心法\\n    \\n    10.1 **Jump Game**\\n    \\n    **10.2 Best Time to Buy and Sell Stock**\\n    \\n    **10.3 Best Time to Buy and Sell Stock II**\\n    \\n    **10.4 Longest Substring Without Repeating Characters**\\n    \\n    **10.5 Container With Most Water**\\n    \\n    ### 11. 动态规划\\n    \\n    11.1 **Triangle**\\n    \\n    **11.2 Maximum Subarray**\\n    \\n    **11.3 Palindrome Partitioning II**\\n    \\n    **11.4  Maximal Rectangle**\\n    \\n    **11.5** **Best Time to Buy and Sell Stock III**\\n    \\n    **11.6 Interleaving String**\\n    \\n    ## **①基本数据类型**\\n    \\n    https://www.lintcode.com/problem/1\\n    \\n    https://www.lintcode.com/problem/37\\n    \\n    https://www.lintcode.com/problem/764\\n    \\n    https://www.lintcode.com/problem/1300\\n    \\n    ## **②判断语句**\\n    \\n    https://www.lintcode.com/problem/23\\n    \\n    https://www.lintcode.com/problem/766\\n    \\n    https://www.lintcode.com/problem/145\\n    \\n    https://www.lintcode.com/problem/1141\\n    \\n    https://www.lintcode.com/problem/478\\n    \\n    https://www.lintcode.com/problem/283\\n    \\n    ## **③数组与循环**\\n    \\n    https://www.lintcode.com/problem/25\\n    \\n    https://www.lintcode.com/problem/214\\n    \\n    https://www.lintcode.com/problem/485\\n    \\n    https://www.lintcode.com/problem/539\\n    \\n    https://www.lintcode.com/problem/297\\n    \\n    https://www.lintcode.com/problem/484\\n    \\n    https://www.lintcode.com/problem/9\\n    \\n    https://www.lintcode.com/problem/220\\n    \\n    https://www.lintcode.com/problem/407\\n    \\n    https://www.lintcode.com/problem/807\\n    \\n    https://www.lintcode.com/problem/463\\n    \\n    https://www.lintcode.com/problem/298\\n    \\n    https://www.lintcode.com/problem/479\\n    \\n    https://www.lintcode.com/problem/46\\n    \\n    https://www.lintcode.com/problem/768\\n    \\n    https://www.lintcode.com/problem/1334\\n    \\n    https://www.lintcode.com/problem/767\\n    \\n    https://www.lintcode.com/problem/235\\n    \\n    https://www.lintcode.com/problem/53\\n    \\n    https://www.lintcode.com/problem/50\\n    \\n    ## **④字符串与循环**\\n    \\n    https://www.lintcode.com/problem/8\\n    \\n    https://www.lintcode.com/problem/491\\n    \\n    https://www.lintcode.com/problem/146\\n    \\n    https://www.lintcode.com/problem/422\\n    \\n    https://www.lintcode.com/problem/353\\n    \\n    https://www.lintcode.com/problem/936\\n    \\n    https://www.lintcode.com/problem/241\\n    \\n    https://www.lintcode.com/problem/13\\n    \\n    https://www.lintcode.com/problem/1535\\n    \\n    https://www.lintcode.com/problem/1343\\n    \\n    https://www.lintcode.com/problem/133\\n    \\n    ## **⑤栈与队列**\\n    \\n    https://www.lintcode.com/problem/263\\n    \\n    https://www.lintcode.com/problem/423\\n    \\n    https://www.lintcode.com/problem/495\\n    \\n    https://www.lintcode.com/problem/492\\n    \\n    https://www.lintcode.com/problem/771\\n    \\n    ## **⑥简单递归**\\n    \\n    https://www.lintcode.com/problem/366\\n    \\n    https://www.lintcode.com/problem/66\\n    \\n    https://www.lintcode.com/problem/67\\n    \\n    https://www.lintcode.com/problem/68\\n    \\n    ## **链表：**\\n    \\n    https://www.lintcode.com/problem/35\\n    \\n    https://www.lintcode.com/problem/36\\n    \\n    https://www.lintcode.com/problem/450\\n    \\n    https://www.lintcode.com/problem/228\\n    \\n    https://www.lintcode.com/problem/102\\n    \\n    https://www.lintcode.com/problem/103\\n    \\n    98 · Sort List - LintCode\\n    \\n    ## **二分法：**\\n    \\n    https://www.lintcode.com/problem/14\\n    \\n    https://www.lintcode.com/problem/28\\n    \\n    https://www.lintcode.com/problem/75\\n    \\n    https://www.lintcode.com/problem/457\\n    \\n    https://www.lintcode.com/problem/458\\n    \\n    ## **二分答案：**\\n    \\n    https://www.lintcode.com/problem/183\\n    \\n    https://www.lintcode.com/problem/437\\n    \\n    https://www.lintcode.com/problem/319\\n    \\n    https://www.lintcode.com/problem/963\\n    \\n    ## **相向双指针：**\\n    \\n    https://www.lintcode.com/problem/56\\n    \\n    https://www.lintcode.com/problem/57\\n    \\n    https://www.lintcode.com/problem/58\\n    \\n    https://www.lintcode.com/problem/363\\n    \\n    https://www.lintcode.com/problem/539\\n    \\n    https://www.lintcode.com/problem/6\\n    \\n    https://www.lintcode.com/problem/32\\n    \\n    https://www.lintcode.com/problem/521\\n    \\n    https://www.lintcode.com/problem/1870\\n    \\n    https://www.lintcode.com/problem/328\\n    \\n    https://www.lintcode.com/problem/547\\n    \\n    https://www.lintcode.com/problem/406\\n    \\n    ## **宽度优先搜索：**\\n    \\n    https://www.lintcode.com/problem/433\\n    \\n    https://www.lintcode.com/problem/615\\n    \\n    https://www.lintcode.com/problem/630\\n    \\n    https://www.lintcode.com/problem/120\\n    \\n    https://www.lintcode.com/problem/178/\\n    \\n    https://www.lintcode.com/problem/278\\n    \\n    https://www.lintcode.com/problem/787\\n    \\n    ## **二叉树遍历：**\\n    \\n    https://www.lintcode.com/problem/66\\n    \\n    https://www.lintcode.com/problem/67\\n    \\n    https://www.lintcode.com/problem/68\\n    \\n    https://www.lintcode.com/problem/69\\n    \\n    https://www.lintcode.com/problem/73\\n    \\n    https://www.lintcode.com/problem/72\\n    \\n    ## **二叉树＆分治法：**\\n    \\n    https://www.lintcode.com/problem/468\\n    \\n    https://www.lintcode.com/problem/854\\n    \\n    https://www.lintcode.com/problem/596\\n    \\n    https://www.lintcode.com/problem/628\\n    \\n    https://www.lintcode.com/problem/597\\n    \\n    ## **二叉搜索树：**\\n    \\n    https://www.lintcode.com/problem/902\\n    \\n    https://www.lintcode.com/problem/915\\n    \\n    https://www.lintcode.com/problem/85\\n    \\n    https://www.lintcode.com/problem/95\\n    \\n    https://www.lintcode.com/problem/689\\n    \\n    ## **深度优先搜索：**\\n    \\n    https://www.lintcode.com/problem/1909\\n    \\n    https://www.lintcode.com/problem/634\\n    \\n    https://www.lintcode.com/problem/802\\n    \\n    https://www.lintcode.com/problem/652\\n    \\n    https://www.lintcode.com/problem/169\\n    \\n    https://www.lintcode.com/problem/425\\n    \\n    https://www.lintcode.com/problem/33\\n    \\n    ## **坐标型动态规划：**\\n    \\n    https://www.lintcode.com/problem/114\\n    \\n    https://www.lintcode.com/problem/115\\n    \\n    https://www.lintcode.com/problem/1861\\n    \\n    https://www.lintcode.com/problem/1827\\n    \\n    https://www.lintcode.com/problem/76\\n    \\n    https://www.lintcode.com/problem/109\\n    \\n    https://www.lintcode.com/problem/1702\\n    \\n    ## **背包型动态规划：**\\n    \\n    https://www.lintcode.com/problem/669\\n    \\n    https://www.lintcode.com/problem/564\\n    \\n    https://www.lintcode.com/problem/92\\n    \\n    https://www.lintcode.com/problem/1915\\n    \\n    https://www.lintcode.com/problem/1800\\n    \\n    https://www.lintcode.com/problem/125\\n    \\n    https://www.lintcode.com/problem/440\\n    \\n    https://www.lintcode.com/problem/562\\n    \\n    https://www.lintcode.com/problem/563\\n    \\n    https://www.lintcode.com/problem/724', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0025ffba-8caa-44e1-a6cf-b2ec5e2eaf41', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nDrawing\\n```json\\n{\\n\\t\"type\": \"excalidraw\",\\n\\t\"version\": 2,\\n\\t\"source\": \"https://github.com/zsviczian/obsidian-excalidraw-plugin/releases/tag/1.9.9\",\\n\\t\"elements\": [\\n\\t\\t{\\n\\t\\t\\t\"type\": \"freedraw\",\\n\\t\\t\\t\"version\": 36,\\n\\t\\t\\t\"versionNonce\": 67521280,\\n\\t\\t\\t\"isDeleted\": false,\\n\\t\\t\\t\"id\": \"n5qri4pZQTumqhSLwVflx\",\\n\\t\\t\\t\"fillStyle\": \"hachure\",\\n\\t\\t\\t\"strokeWidth\": 1,\\n\\t\\t\\t\"strokeStyle\": \"solid\",\\n\\t\\t\\t\"roughness\": 1,\\n\\t\\t\\t\"opacity\": 100,\\n\\t\\t\\t\"angle\": 0,\\n\\t\\t\\t\"x\": -215.12890625,\\n\\t\\t\\t\"y\": -162.390625,\\n\\t\\t\\t\"strokeColor\": \"#1e1e1e\",\\n\\t\\t\\t\"backgroundColor\": \"transparent\",\\n\\t\\t\\t\"width\": 194.9375,\\n\\t\\t\\t\"height\": 78.2109375,\\n\\t\\t\\t\"seed\": 2001151232,\\n\\t\\t\\t\"groupIds\": [],\\n\\t\\t\\t\"frameId\": null,\\n\\t\\t\\t\"roundness\": null,\\n\\t\\t\\t\"boundElements\": [],\\n\\t\\t\\t\"updated\": 1690073550610,\\n\\t\\t\\t\"link\": null,\\n\\t\\t\\t\"locked\": false,\\n\\t\\t\\t\"points\": [\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0.2421875,\\n\\t\\t\\t\\t\\t-0.4921875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0.2421875,\\n\\t\\t\\t\\t\\t-0.73828125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t3.015625,\\n\\t\\t\\t\\t\\t-4.21484375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t10.390625,\\n\\t\\t\\t\\t\\t-13.4375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t14.48046875,\\n\\t\\t\\t\\t\\t-19.578125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t25.64453125,\\n\\t\\t\\t\\t\\t-31.4453125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t41.68359375,\\n\\t\\t\\t\\t\\t-47.48828125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t62.78125,\\n\\t\\t\\t\\t\\t-62.68359375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t73.23046875,\\n\\t\\t\\t\\t\\t-68.21875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t92.23828125,\\n\\t\\t\\t\\t\\t-74.3046875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t113.328125,\\n\\t\\t\\t\\t\\t-78.2109375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t126.0390625,\\n\\t\\t\\t\\t\\t-78.2109375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t148.49609375,\\n\\t\\t\\t\\t\\t-77.41015625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t154.87890625,\\n\\t\\t\\t\\t\\t-75.9375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t171.40625,\\n\\t\\t\\t\\t\\t-70.91015625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t179.14453125,\\n\\t\\t\\t\\t\\t-66.48828125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t185.77734375,\\n\\t\\t\\t\\t\\t-60.41015625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t190.56640625,\\n\\t\\t\\t\\t\\t-53.4921875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t192.28125,\\n\\t\\t\\t\\t\\t-49.19921875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t194.40625,\\n\\t\\t\\t\\t\\t-41.21875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t194.9375,\\n\\t\\t\\t\\t\\t-33.23828125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t194.9375,\\n\\t\\t\\t\\t\\t-27.59375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t194.19921875,\\n\\t\\t\\t\\t\\t-25.01953125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t192.7265625,\\n\\t\\t\\t\\t\\t-22.8125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t190.8828125,\\n\\t\\t\\t\\t\\t-20.97265625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t184.7421875,\\n\\t\\t\\t\\t\\t-16.8828125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t177.2890625,\\n\\t\\t\\t\\t\\t-13.69140625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t172.9921875,\\n\\t\\t\\t\\t\\t-11.9765625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t162.890625,\\n\\t\\t\\t\\t\\t-8.4140625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t151.20703125,\\n\\t\\t\\t\\t\\t-5.34375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t142.359375,\\n\\t\\t\\t\\t\\t-3.1328125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t138.0625,\\n\\t\\t\\t\\t\\t-2.27734375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t134.953125,\\n\\t\\t\\t\\t\\t-2.27734375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t134.953125,\\n\\t\\t\\t\\t\\t-2.27734375\\n\\t\\t\\t\\t]\\n\\t\\t\\t],\\n\\t\\t\\t\"lastCommittedPoint\": null,\\n\\t\\t\\t\"simulatePressure\": true,\\n\\t\\t\\t\"pressures\": []\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\"type\": \"freedraw\",\\n\\t\\t\\t\"version\": 50,\\n\\t\\t\\t\"versionNonce\": 678685440,\\n\\t\\t\\t\"isDeleted\": false,\\n\\t\\t\\t\"id\": \"MSJXLz6mp9TO2BptJMSt-\",\\n\\t\\t\\t\"fillStyle\": \"hachure\",\\n\\t\\t\\t\"strokeWidth\": 1,\\n\\t\\t\\t\"strokeStyle\": \"solid\",\\n\\t\\t\\t\"roughness\": 1,\\n\\t\\t\\t\"opacity\": 100,\\n\\t\\t\\t\"angle\": 0,\\n\\t\\t\\t\"x\": -153.6328125,\\n\\t\\t\\t\"y\": -153.70703125,\\n\\t\\t\\t\"strokeColor\": \"#1e1e1e\",\\n\\t\\t\\t\"backgroundColor\": \"transparent\",\\n\\t\\t\\t\"width\": 265.28515625,\\n\\t\\t\\t\"height\": 96.640625,\\n\\t\\t\\t\"seed\": 1587012864,\\n\\t\\t\\t\"groupIds\": [],\\n\\t\\t\\t\"frameId\": null,\\n\\t\\t\\t\"roundness\": null,\\n\\t\\t\\t\"boundElements\": [],\\n\\t\\t\\t\"updated\": 1690073551890,\\n\\t\\t\\t\"link\": null,\\n\\t\\t\\t\"locked\": false,\\n\\t\\t\\t\"points\": [\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0.2421875,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0.484375,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t1.015625,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t4.6953125,\\n\\t\\t\\t\\t\\t1.6328125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t11.078125,\\n\\t\\t\\t\\t\\t4.0859375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t19.3671875,\\n\\t\\t\\t\\t\\t7.40234375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t30.05859375,\\n\\t\\t\\t\\t\\t10.96484375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t59.08203125,\\n\\t\\t\\t\\t\\t20.03125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t96.54296875,\\n\\t\\t\\t\\t\\t29.140625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t119,\\n\\t\\t\\t\\t\\t33.1484375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t128.74609375,\\n\\t\\t\\t\\t\\t34.8671875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t149.8359375,\\n\\t\\t\\t\\t\\t38.76953125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t159.58203125,\\n\\t\\t\\t\\t\\t41.63671875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t160.44140625,\\n\\t\\t\\t\\t\\t41.921875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t160.44140625,\\n\\t\\t\\t\\t\\t42.1640625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t160.1953125,\\n\\t\\t\\t\\t\\t42.1640625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t159.66015625,\\n\\t\\t\\t\\t\\t42.6953125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t157.921875,\\n\\t\\t\\t\\t\\t44.08203125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t154.0546875,\\n\\t\\t\\t\\t\\t46.65625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t151.33203125,\\n\\t\\t\\t\\t\\t48.59765625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t146.625,\\n\\t\\t\\t\\t\\t52.359375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t138.59375,\\n\\t\\t\\t\\t\\t58.6640625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t126.40625,\\n\\t\\t\\t\\t\\t67.46484375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t114.58984375,\\n\\t\\t\\t\\t\\t74.68359375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t101.72265625,\\n\\t\\t\\t\\t\\t82.8046875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t91.26953125,\\n\\t\\t\\t\\t\\t88.94921875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t81.76171875,\\n\\t\\t\\t\\t\\t94.29296875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t74.59765625,\\n\\t\\t\\t\\t\\t96.3359375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t73.3671875,\\n\\t\\t\\t\\t\\t96.640625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t73.12109375,\\n\\t\\t\\t\\t\\t96.39453125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t73.12109375,\\n\\t\\t\\t\\t\\t96.1484375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t73.12109375,\\n\\t\\t\\t\\t\\t95.90234375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t73.65234375,\\n\\t\\t\\t\\t\\t95.3671875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t75.4921875,\\n\\t\\t\\t\\t\\t93.5234375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t77.69921875,\\n\\t\\t\\t\\t\\t91.6796875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t82.1953125,\\n\\t\\t\\t\\t\\t88.9765625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t94.6640625,\\n\\t\\t\\t\\t\\t82.41015625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t106.73828125,\\n\\t\\t\\t\\t\\t77.32421875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t137.36328125,\\n\\t\\t\\t\\t\\t68.04296875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t152.2578125,\\n\\t\\t\\t\\t\\t64.65625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t182.1875,\\n\\t\\t\\t\\t\\t59.2109375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t200.671875,\\n\\t\\t\\t\\t\\t57.73046875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t230.6015625,\\n\\t\\t\\t\\t\\t55.9140625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t238.58203125,\\n\\t\\t\\t\\t\\t55.37890625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t257.06640625,\\n\\t\\t\\t\\t\\t55.37890625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t262.7109375,\\n\\t\\t\\t\\t\\t55.37890625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t265.28515625,\\n\\t\\t\\t\\t\\t55.37890625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t265.28515625,\\n\\t\\t\\t\\t\\t55.37890625\\n\\t\\t\\t\\t]\\n\\t\\t\\t],\\n\\t\\t\\t\"lastCommittedPoint\": null,\\n\\t\\t\\t\"simulatePressure\": true,\\n\\t\\t\\t\"pressures\": []\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\"id\": \"iMby4zSS\",\\n\\t\\t\\t\"type\": \"text\",\\n\\t\\t\\t\"x\": -100.75,\\n\\t\\t\\t\"y\": 24.2578125,\\n\\t\\t\\t\"width\": 13.760000228881836,\\n\\t\\t\\t\"height\": 25,\\n\\t\\t\\t\"angle\": 0,\\n\\t\\t\\t\"strokeColor\": \"#1e1e1e\",\\n\\t\\t\\t\"backgroundColor\": \"transparent\",\\n\\t\\t\\t\"fillStyle\": \"hachure\",\\n\\t\\t\\t\"strokeWidth\": 1,\\n\\t\\t\\t\"strokeStyle\": \"solid\",\\n\\t\\t\\t\"roughness\": 1,\\n\\t\\t\\t\"opacity\": 100,\\n\\t\\t\\t\"groupIds\": [],\\n\\t\\t\\t\"frameId\": null,\\n\\t\\t\\t\"roundness\": null,\\n\\t\\t\\t\"seed\": 1986857474,\\n\\t\\t\\t\"version\": 2,\\n\\t\\t\\t\"versionNonce\": 1886984130,\\n\\t\\t\\t\"isDeleted\": false,\\n\\t\\t\\t\"boundElements\": null,\\n\\t\\t\\t\"updated\": 1690077945135,\\n\\t\\t\\t\"link\": null,\\n\\t\\t\\t\"locked\": false,\\n\\t\\t\\t\"text\": \"0\",\\n\\t\\t\\t\"rawText\": \"0\",\\n\\t\\t\\t\"fontSize\": 20,\\n\\t\\t\\t\"fontFamily\": 1,\\n\\t\\t\\t\"textAlign\": \"left\",\\n\\t\\t\\t\"verticalAlign\": \"top\",\\n\\t\\t\\t\"baseline\": 17,\\n\\t\\t\\t\"containerId\": null,\\n\\t\\t\\t\"originalText\": \"0\",\\n\\t\\t\\t\"lineHeight\": 1.25\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\"id\": \"KrVz1ElIGpyKrxog9to-N\",\\n\\t\\t\\t\"type\": \"freedraw\",\\n\\t\\t\\t\"x\": 50.962063033404434,\\n\\t\\t\\t\"y\": -4.4973520730414975,\\n\\t\\t\\t\"width\": 73.69803630675119,\\n\\t\\t\\t\"height\": 35.85309874382489,\\n\\t\\t\\t\"angle\": 0,\\n\\t\\t\\t\"strokeColor\": \"#1e1e1e\",\\n\\t\\t\\t\"backgroundColor\": \"transparent\",\\n\\t\\t\\t\"fillStyle\": \"hachure\",\\n\\t\\t\\t\"strokeWidth\": 1,\\n\\t\\t\\t\"strokeStyle\": \"solid\",\\n\\t\\t\\t\"roughness\": 1,\\n\\t\\t\\t\"opacity\": 100,\\n\\t\\t\\t\"groupIds\": [],\\n\\t\\t\\t\"frameId\": null,\\n\\t\\t\\t\"roundness\": null,\\n\\t\\t\\t\"seed\": 1092009310,\\n\\t\\t\\t\"version\": 37,\\n\\t\\t\\t\"versionNonce\": 105578974,\\n\\t\\t\\t\"isDeleted\": false,\\n\\t\\t\\t\"boundElements\": null,\\n\\t\\t\\t\"updated\": 1690077948976,\\n\\t\\t\\t\"link\": null,\\n\\t\\t\\t\"locked\": false,\\n\\t\\t\\t\"points\": [\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-0.9959194095507655,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0.9959194095506518,\\n\\t\\t\\t\\t\\t-1.9918388191013605\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t3.983677638202721,\\n\\t\\t\\t\\t\\t-3.983677638202778\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t9.95919409550686,\\n\\t\\t\\t\\t\\t-7.967355276405556\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t15.934710552810998,\\n\\t\\t\\t\\t\\t-11.951032914608277\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t21.910227010115136,\\n\\t\\t\\t\\t\\t-16.930629962361763\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t28.88166287697004,\\n\\t\\t\\t\\t\\t-20.91430760056454\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t35.85309874382483,\\n\\t\\t\\t\\t\\t-21.910227010115193\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t40.83269579157832,\\n\\t\\t\\t\\t\\t-22.9061464196659\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t44.81637342978104,\\n\\t\\t\\t\\t\\t-21.910227010115193\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t48.80005106798387,\\n\\t\\t\\t\\t\\t-20.91430760056454\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t50.791889887085176,\\n\\t\\t\\t\\t\\t-18.922468781463124\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t51.78780929663594,\\n\\t\\t\\t\\t\\t-16.930629962361763\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t50.791889887085176,\\n\\t\\t\\t\\t\\t-12.946952324158985\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t48.80005106798387,\\n\\t\\t\\t\\t\\t-7.967355276405556\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t45.8122928393318,\\n\\t\\t\\t\\t\\t-4.97959704775343\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t42.82453461067962,\\n\\t\\t\\t\\t\\t-0.9959194095507087\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t39.83677638202755,\\n\\t\\t\\t\\t\\t1.9918388191013605\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t38.8408569724769,\\n\\t\\t\\t\\t\\t3.983677638202778\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t37.84493756292625,\\n\\t\\t\\t\\t\\t4.9795970477534865\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t38.8408569724769,\\n\\t\\t\\t\\t\\t3.983677638202778\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t41.82861520112897,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t46.808212248882455,\\n\\t\\t\\t\\t\\t-2.987758228652069\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t52.783728706186594,\\n\\t\\t\\t\\t\\t-5.975516457304138\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t57.76332575394008,\\n\\t\\t\\t\\t\\t-7.967355276405556\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t61.7470033921428,\\n\\t\\t\\t\\t\\t-7.967355276405556\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t64.73476162079487,\\n\\t\\t\\t\\t\\t-5.975516457304138\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t66.72660043989629,\\n\\t\\t\\t\\t\\t-3.983677638202778\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t67.72251984944694,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t64.73476162079487,\\n\\t\\t\\t\\t\\t5.975516457304138\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t63.73884221124422,\\n\\t\\t\\t\\t\\t10.955113505057625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t62.74292280169345,\\n\\t\\t\\t\\t\\t12.946952324158985\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t65.73068103034552,\\n\\t\\t\\t\\t\\t11.951032914608277\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t71.70619748764977,\\n\\t\\t\\t\\t\\t8.963274685956208\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t72.70211689720043,\\n\\t\\t\\t\\t\\t7.967355276405556\\n\\t\\t\\t\\t]\\n\\t\\t\\t],\\n\\t\\t\\t\"pressures\": [\\n\\t\\t\\t\\t0.07999999821186066,\\n\\t\\t\\t\\t0.07999999821186066,\\n\\t\\t\\t\\t0.14249108731746674,\\n\\t\\t\\t\\t0.16876882314682007,\\n\\t\\t\\t\\t0.17254318296909332,\\n\\t\\t\\t\\t0.1880166381597519,\\n\\t\\t\\t\\t0.17874497175216675,\\n\\t\\t\\t\\t0.18084609508514404,\\n\\t\\t\\t\\t0.17159868776798248,\\n\\t\\t\\t\\t0.16680264472961426,\\n\\t\\t\\t\\t0.15366637706756592,\\n\\t\\t\\t\\t0.148329496383667,\\n\\t\\t\\t\\t0.14571578800678253,\\n\\t\\t\\t\\t0.15379293262958527,\\n\\t\\t\\t\\t0.16824303567409515,\\n\\t\\t\\t\\t0.17132166028022766,\\n\\t\\t\\t\\t0.1722126454114914,\\n\\t\\t\\t\\t0.16361545026302338,\\n\\t\\t\\t\\t0.1489483267068863,\\n\\t\\t\\t\\t0.14314965903759003,\\n\\t\\t\\t\\t0.1397760659456253,\\n\\t\\t\\t\\t0.1379593163728714,\\n\\t\\t\\t\\t0.14086319506168365,\\n\\t\\t\\t\\t0.14766232669353485,\\n\\t\\t\\t\\t0.1495460569858551,\\n\\t\\t\\t\\t0.15658633410930634,\\n\\t\\t\\t\\t0.15987975895404816,\\n\\t\\t\\t\\t0.152239128947258,\\n\\t\\t\\t\\t0.13482604920864105,\\n\\t\\t\\t\\t0.12245431542396545,\\n\\t\\t\\t\\t0.11606164276599884,\\n\\t\\t\\t\\t0.12594124674797058,\\n\\t\\t\\t\\t0.10662301629781723,\\n\\t\\t\\t\\t0.06340881437063217,\\n\\t\\t\\t\\t0.010039123706519604,\\n\\t\\t\\t\\t0\\n\\t\\t\\t],\\n\\t\\t\\t\"simulatePressure\": false,\\n\\t\\t\\t\"lastCommittedPoint\": [\\n\\t\\t\\t\\t72.70211689720043,\\n\\t\\t\\t\\t7.967355276405556\\n\\t\\t\\t]\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\"id\": \"rnDA67ZopFTU3uxsbuEzA\",\\n\\t\\t\\t\"type\": \"freedraw\",\\n\\t\\t\\t\"x\": 56.93757949070857,\\n\\t\\t\\t\"y\": 52.27005427134793,\\n\\t\\t\\t\"width\": 12.946952324158929,\\n\\t\\t\\t\"height\": 4.97959704775343,\\n\\t\\t\\t\"angle\": 0,\\n\\t\\t\\t\"strokeColor\": \"#1e1e1e\",\\n\\t\\t\\t\"backgroundColor\": \"transparent\",\\n\\t\\t\\t\"fillStyle\": \"hachure\",\\n\\t\\t\\t\"strokeWidth\": 1,\\n\\t\\t\\t\"strokeStyle\": \"solid\",\\n\\t\\t\\t\"roughness\": 1,\\n\\t\\t\\t\"opacity\": 100,\\n\\t\\t\\t\"groupIds\": [],\\n\\t\\t\\t\"frameId\": null,\\n\\t\\t\\t\"roundness\": null,\\n\\t\\t\\t\"seed\": 423957250,\\n\\t\\t\\t\"version\": 10,\\n\\t\\t\\t\"versionNonce\": 1285035806,\\n\\t\\t\\t\"isDeleted\": false,\\n\\t\\t\\t\"boundElements\": null,\\n\\t\\t\\t\"updated\": 1690077949850,\\n\\t\\t\\t\"link\": null,\\n\\t\\t\\t\"locked\": false,\\n\\t\\t\\t\"points\": [\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0.9959194095506518,\\n\\t\\t\\t\\t\\t-0.9959194095507087\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t1.9918388191013037,\\n\\t\\t\\t\\t\\t-0.9959194095507087\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t2.987758228652069,\\n\\t\\t\\t\\t\\t-1.9918388191014174\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t5.975516457304138,\\n\\t\\t\\t\\t\\t-2.987758228652069\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t8.963274685956208,\\n\\t\\t\\t\\t\\t-2.987758228652069\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t11.951032914608277,\\n\\t\\t\\t\\t\\t-2.987758228652069\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t12.946952324158929,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t11.951032914608277,\\n\\t\\t\\t\\t\\t1.9918388191013605\\n\\t\\t\\t\\t]\\n\\t\\t\\t],\\n\\t\\t\\t\"pressures\": [\\n\\t\\t\\t\\t0.07999999821186066,\\n\\t\\t\\t\\t0.07999999821186066,\\n\\t\\t\\t\\t0.09253799170255661,\\n\\t\\t\\t\\t0.10853799432516098,\\n\\t\\t\\t\\t0.13899774849414825,\\n\\t\\t\\t\\t0.15639950335025787,\\n\\t\\t\\t\\t0.14868274331092834,\\n\\t\\t\\t\\t0.05903305113315582,\\n\\t\\t\\t\\t0\\n\\t\\t\\t],\\n\\t\\t\\t\"simulatePressure\": false,\\n\\t\\t\\t\"lastCommittedPoint\": [\\n\\t\\t\\t\\t11.951032914608277,\\n\\t\\t\\t\\t1.9918388191013605\\n\\t\\t\\t]\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\"id\": \"h3__rzCeg5dKTzJ7OMIb_\",\\n\\t\\t\\t\"type\": \"freedraw\",\\n\\t\\t\\t\"x\": 44.98654657610018,\\n\\t\\t\\t\"y\": 80.15579773876726,\\n\\t\\t\\t\"width\": 50.791889887085176,\\n\\t\\t\\t\"height\": 64.73476162079498,\\n\\t\\t\\t\"angle\": 0,\\n\\t\\t\\t\"strokeColor\": \"#1e1e1e\",\\n\\t\\t\\t\"backgroundColor\": \"transparent\",\\n\\t\\t\\t\"fillStyle\": \"hachure\",\\n\\t\\t\\t\"strokeWidth\": 1,\\n\\t\\t\\t\"strokeStyle\": \"solid\",\\n\\t\\t\\t\"roughness\": 1,\\n\\t\\t\\t\"opacity\": 100,\\n\\t\\t\\t\"groupIds\": [],\\n\\t\\t\\t\"frameId\": null,\\n\\t\\t\\t\"roundness\": null,\\n\\t\\t\\t\"seed\": 2045702594,\\n\\t\\t\\t\"version\": 61,\\n\\t\\t\\t\"versionNonce\": 1622118978,\\n\\t\\t\\t\"isDeleted\": false,\\n\\t\\t\\t\"boundElements\": null,\\n\\t\\t\\t\"updated\": 1690077950987,\\n\\t\\t\\t\"link\": null,\\n\\t\\t\\t\"locked\": false,\\n\\t\\t\\t\"points\": [\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-0.9959194095506518,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-1.9918388191013037,\\n\\t\\t\\t\\t\\t0.9959194095506518\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-1.9918388191013037,\\n\\t\\t\\t\\t\\t1.9918388191014174\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-0.9959194095506518,\\n\\t\\t\\t\\t\\t1.9918388191014174\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t2.987758228652069,\\n\\t\\t\\t\\t\\t-0.9959194095506518\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t9.959194095506973,\\n\\t\\t\\t\\t\\t-2.987758228652069\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t16.930629962361763,\\n\\t\\t\\t\\t\\t-4.9795970477534865\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t21.91022701011525,\\n\\t\\t\\t\\t\\t-5.975516457304138\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t24.89798523876732,\\n\\t\\t\\t\\t\\t-5.975516457304138\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t25.89390464831797,\\n\\t\\t\\t\\t\\t-4.9795970477534865\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t25.89390464831797,\\n\\t\\t\\t\\t\\t-1.9918388191014174\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t22.9061464196659,\\n\\t\\t\\t\\t\\t4.9795970477534865\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t15.934710552811111,\\n\\t\\t\\t\\t\\t12.946952324159042\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t6.971435866854904,\\n\\t\\t\\t\\t\\t20.914307600564484\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t1.9918388191014174,\\n\\t\\t\\t\\t\\t25.89390464831797\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-0.9959194095506518,\\n\\t\\t\\t\\t\\t25.89390464831797\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-1.9918388191013037,\\n\\t\\t\\t\\t\\t21.91022701011525\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-0.9959194095506518,\\n\\t\\t\\t\\t\\t15.934710552811111\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t2.987758228652069,\\n\\t\\t\\t\\t\\t10.955113505057625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t7.967355276405556,\\n\\t\\t\\t\\t\\t6.971435866854904\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t12.946952324159042,\\n\\t\\t\\t\\t\\t5.975516457304138\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t16.930629962361763,\\n\\t\\t\\t\\t\\t6.971435866854904\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t19.918388191013833,\\n\\t\\t\\t\\t\\t11.951032914608277\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t20.914307600564598,\\n\\t\\t\\t\\t\\t12.946952324159042\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t22.9061464196659,\\n\\t\\t\\t\\t\\t14.938791143260346\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t23.902065829216667,\\n\\t\\t\\t\\t\\t14.938791143260346\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t24.89798523876732,\\n\\t\\t\\t\\t\\t13.942871733709694\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t25.89390464831797,\\n\\t\\t\\t\\t\\t10.955113505057625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t26.889824057868736,\\n\\t\\t\\t\\t\\t9.959194095506973\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t27.88574346741939,\\n\\t\\t\\t\\t\\t5.975516457304138\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t30.873501696071457,\\n\\t\\t\\t\\t\\t1.9918388191014174\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t32.865340515172875,\\n\\t\\t\\t\\t\\t-0.9959194095506518\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t33.86125992472353,\\n\\t\\t\\t\\t\\t-0.9959194095506518\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t32.865340515172875,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t32.865340515172875,\\n\\t\\t\\t\\t\\t0.9959194095506518\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t32.865340515172875,\\n\\t\\t\\t\\t\\t1.9918388191014174\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t33.86125992472353,\\n\\t\\t\\t\\t\\t2.987758228652069\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t34.85717933427429,\\n\\t\\t\\t\\t\\t3.983677638202721\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t35.853098743824944,\\n\\t\\t\\t\\t\\t2.987758228652069\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t36.849018153375596,\\n\\t\\t\\t\\t\\t1.9918388191014174\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t37.84493756292636,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t39.836776382027665,\\n\\t\\t\\t\\t\\t-3.983677638202721\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t40.83269579157843,\\n\\t\\t\\t\\t\\t-8.963274685956208\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t42.824534610679734,\\n\\t\\t\\t\\t\\t-14.938791143260346\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t42.824534610679734,\\n\\t\\t\\t\\t\\t-20.914307600564484\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t43.8204540202305,\\n\\t\\t\\t\\t\\t-25.89390464831797\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t44.81637342978115,\\n\\t\\t\\t\\t\\t-28.88166287697004\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t46.80821224888257,\\n\\t\\t\\t\\t\\t-31.86942110562211\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t47.80413165843322,\\n\\t\\t\\t\\t\\t-31.86942110562211\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t47.80413165843322,\\n\\t\\t\\t\\t\\t-30.8735016960714\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t48.80005106798387,\\n\\t\\t\\t\\t\\t-27.88574346741933\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t47.80413165843322,\\n\\t\\t\\t\\t\\t-20.914307600564484\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t46.80821224888257,\\n\\t\\t\\t\\t\\t-5.975516457304138\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t46.80821224888257,\\n\\t\\t\\t\\t\\t0.9959194095506518\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t45.8122928393318,\\n\\t\\t\\t\\t\\t13.942871733709694\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t44.81637342978115,\\n\\t\\t\\t\\t\\t24.89798523876732\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t44.81637342978115,\\n\\t\\t\\t\\t\\t31.86942110562211\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t46.80821224888257,\\n\\t\\t\\t\\t\\t32.865340515172875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t46.80821224888257,\\n\\t\\t\\t\\t\\t31.86942110562211\\n\\t\\t\\t\\t]\\n\\t\\t\\t],\\n\\t\\t\\t\"pressures\": [\\n\\t\\t\\t\\t0.07999999821186066,\\n\\t\\t\\t\\t0.07999999821186066,\\n\\t\\t\\t\\t0.07999999821186066,\\n\\t\\t\\t\\t0.0922422781586647,\\n\\t\\t\\t\\t0.1250520646572113,\\n\\t\\t\\t\\t0.14411777257919312,\\n\\t\\t\\t\\t0.15346874296665192,\\n\\t\\t\\t\\t0.1688670665025711,\\n\\t\\t\\t\\t0.17451907694339752,\\n\\t\\t\\t\\t0.1685582846403122,\\n\\t\\t\\t\\t0.1554294377565384,\\n\\t\\t\\t\\t0.1426158994436264,\\n\\t\\t\\t\\t0.12471560388803482,\\n\\t\\t\\t\\t0.09776122868061066,\\n\\t\\t\\t\\t0.08797091990709305,\\n\\t\\t\\t\\t0.08787216246128082,\\n\\t\\t\\t\\t0.10124929994344711,\\n\\t\\t\\t\\t0.10661477595567703,\\n\\t\\t\\t\\t0.10849160701036453,\\n\\t\\t\\t\\t0.13088442385196686,\\n\\t\\t\\t\\t0.15850116312503815,\\n\\t\\t\\t\\t0.17213410139083862,\\n\\t\\t\\t\\t0.18254505097866058,\\n\\t\\t\\t\\t0.1739979237318039,\\n\\t\\t\\t\\t0.16745318472385406,\\n\\t\\t\\t\\t0.14641784131526947,\\n\\t\\t\\t\\t0.11774850636720657,\\n\\t\\t\\t\\t0.0873769223690033,\\n\\t\\t\\t\\t0.07325021177530289,\\n\\t\\t\\t\\t0.08028052002191544,\\n\\t\\t\\t\\t0.08954119682312012,\\n\\t\\t\\t\\t0.09533966332674026,\\n\\t\\t\\t\\t0.10918218642473221,\\n\\t\\t\\t\\t0.1555793136358261,\\n\\t\\t\\t\\t0.1618737131357193,\\n\\t\\t\\t\\t0.16119931638240814,\\n\\t\\t\\t\\t0.15661807358264923,\\n\\t\\t\\t\\t0.14226864278316498,\\n\\t\\t\\t\\t0.12615841627120972,\\n\\t\\t\\t\\t0.11801333725452423,\\n\\t\\t\\t\\t0.10189080983400345,\\n\\t\\t\\t\\t0.0855678990483284,\\n\\t\\t\\t\\t0.06881728768348694,\\n\\t\\t\\t\\t0.0602109357714653,\\n\\t\\t\\t\\t0.05756954848766327,\\n\\t\\t\\t\\t0.062178224325180054,\\n\\t\\t\\t\\t0.07693600654602051,\\n\\t\\t\\t\\t0.09691091626882553,\\n\\t\\t\\t\\t0.12771853804588318,\\n\\t\\t\\t\\t0.13554473221302032,\\n\\t\\t\\t\\t0.15531519055366516,\\n\\t\\t\\t\\t0.16409802436828613,\\n\\t\\t\\t\\t0.17790883779525757,\\n\\t\\t\\t\\t0.21996301412582397,\\n\\t\\t\\t\\t0.2279004156589508,\\n\\t\\t\\t\\t0.22937725484371185,\\n\\t\\t\\t\\t0.2008998692035675,\\n\\t\\t\\t\\t0.14772313833236694,\\n\\t\\t\\t\\t0.01260598748922348,\\n\\t\\t\\t\\t0\\n\\t\\t\\t],\\n\\t\\t\\t\"simulatePressure\": false,\\n\\t\\t\\t\"lastCommittedPoint\": [\\n\\t\\t\\t\\t46.80821224888257,\\n\\t\\t\\t\\t31.86942110562211\\n\\t\\t\\t]\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\"id\": \"nmh56ErqLie4gPWc2FAL_\",\\n\\t\\t\\t\"type\": \"freedraw\",\\n\\t\\t\\t\"x\": 125.65601874970628,\\n\\t\\t\\t\"y\": 46.29453781404379,\\n\\t\\t\\t\"width\": 4.9795970477534865,\\n\\t\\t\\t\"height\": 18.922468781463124,\\n\\t\\t\\t\"angle\": 0,\\n\\t\\t\\t\"strokeColor\": \"#1e1e1e\",\\n\\t\\t\\t\"backgroundColor\": \"transparent\",\\n\\t\\t\\t\"fillStyle\": \"hachure\",\\n\\t\\t\\t\"strokeWidth\": 1,\\n\\t\\t\\t\"strokeStyle\": \"solid\",\\n\\t\\t\\t\"roughness\": 1,\\n\\t\\t\\t\"opacity\": 100,\\n\\t\\t\\t\"groupIds\": [],\\n\\t\\t\\t\"frameId\": null,\\n\\t\\t\\t\"roundness\": null,\\n\\t\\t\\t\"seed\": 1903897310,\\n\\t\\t\\t\"version\": 10,\\n\\t\\t\\t\"versionNonce\": 1520560386,\\n\\t\\t\\t\"isDeleted\": false,\\n\\t\\t\\t\"boundElements\": null,\\n\\t\\t\\t\"updated\": 1690077951439,\\n\\t\\t\\t\"link\": null,\\n\\t\\t\\t\"locked\": false,\\n\\t\\t\\t\"points\": [\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t0.9959194095506518\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t1.9918388191013605\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t4.97959704775343\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-0.9959194095507655,\\n\\t\\t\\t\\t\\t7.967355276405499\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-1.9918388191014174,\\n\\t\\t\\t\\t\\t11.95103291460822\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-2.987758228652069,\\n\\t\\t\\t\\t\\t15.934710552811055\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-3.9836776382028347,\\n\\t\\t\\t\\t\\t18.922468781463124\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-4.9795970477534865,\\n\\t\\t\\t\\t\\t18.922468781463124\\n\\t\\t\\t\\t]\\n\\t\\t\\t],\\n\\t\\t\\t\"pressures\": [\\n\\t\\t\\t\\t0.07999999821186066,\\n\\t\\t\\t\\t0.13944903016090393,\\n\\t\\t\\t\\t0.18283599615097046,\\n\\t\\t\\t\\t0.19746887683868408,\\n\\t\\t\\t\\t0.17992430925369263,\\n\\t\\t\\t\\t0.15612246096134186,\\n\\t\\t\\t\\t0.09196163713932037,\\n\\t\\t\\t\\t0,\\n\\t\\t\\t\\t0\\n\\t\\t\\t],\\n\\t\\t\\t\"simulatePressure\": false,\\n\\t\\t\\t\"lastCommittedPoint\": [\\n\\t\\t\\t\\t-4.9795970477534865,\\n\\t\\t\\t\\t18.922468781463124\\n\\t\\t\\t]\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\"id\": \"5rOqHAGH7nkc-i7xqwI1m\",\\n\\t\\t\\t\"type\": \"freedraw\",\\n\\t\\t\\t\"x\": 114.70090524464865,\\n\\t\\t\\t\"y\": 64.22108718595626,\\n\\t\\t\\t\"width\": 60.75108398259215,\\n\\t\\t\\t\"height\": 39.836776382027665,\\n\\t\\t\\t\"angle\": 0,\\n\\t\\t\\t\"strokeColor\": \"#1e1e1e\",\\n\\t\\t\\t\"backgroundColor\": \"transparent\",\\n\\t\\t\\t\"fillStyle\": \"hachure\",\\n\\t\\t\\t\"strokeWidth\": 1,\\n\\t\\t\\t\"strokeStyle\": \"solid\",\\n\\t\\t\\t\"roughness\": 1,\\n\\t\\t\\t\"opacity\": 100,\\n\\t\\t\\t\"groupIds\": [],\\n\\t\\t\\t\"frameId\": null,\\n\\t\\t\\t\"roundness\": null,\\n\\t\\t\\t\"seed\": 126787614,\\n\\t\\t\\t\"version\": 92,\\n\\t\\t\\t\"versionNonce\": 938087810,\\n\\t\\t\\t\"isDeleted\": false,\\n\\t\\t\\t\"boundElements\": null,\\n\\t\\t\\t\"updated\": 1690077953127,\\n\\t\\t\\t\"link\": null,\\n\\t\\t\\t\"locked\": false,\\n\\t\\t\\t\"points\": [\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t-0.9959194095507655\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-0.9959194095507655,\\n\\t\\t\\t\\t\\t-0.9959194095507655\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-0.9959194095507655,\\n\\t\\t\\t\\t\\t-1.9918388191014174\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-0.9959194095507655,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-1.9918388191014174,\\n\\t\\t\\t\\t\\t3.983677638202721\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-1.9918388191014174,\\n\\t\\t\\t\\t\\t6.97143586685479\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-1.9918388191014174,\\n\\t\\t\\t\\t\\t7.967355276405442\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-0.9959194095507655,\\n\\t\\t\\t\\t\\t7.967355276405442\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t1.9918388191013037,\\n\\t\\t\\t\\t\\t7.967355276405442\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t5.975516457304138,\\n\\t\\t\\t\\t\\t5.975516457304138\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t10.955113505057625,\\n\\t\\t\\t\\t\\t1.9918388191013037\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t14.938791143260346,\\n\\t\\t\\t\\t\\t-0.9959194095507655\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t18.922468781463067,\\n\\t\\t\\t\\t\\t-2.987758228652183\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t18.922468781463067,\\n\\t\\t\\t\\t\\t-1.9918388191014174\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t17.926549371912415,\\n\\t\\t\\t\\t\\t0.9959194095506518\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t15.934710552810998,\\n\\t\\t\\t\\t\\t4.979597047753373\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t13.942871733709694,\\n\\t\\t\\t\\t\\t8.963274685956208\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t11.951032914608277,\\n\\t\\t\\t\\t\\t12.946952324158929\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t9.95919409550686,\\n\\t\\t\\t\\t\\t16.93062996236165\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t6.97143586685479,\\n\\t\\t\\t\\t\\t19.91838819101372\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t6.97143586685479,\\n\\t\\t\\t\\t\\t20.914307600564484\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t7.967355276405556,\\n\\t\\t\\t\\t\\t18.922468781463067\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t8.963274685956208,\\n\\t\\t\\t\\t\\t16.93062996236165\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t10.955113505057625,\\n\\t\\t\\t\\t\\t15.934710552810998\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t12.946952324158929,\\n\\t\\t\\t\\t\\t14.938791143260346\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t14.938791143260346,\\n\\t\\t\\t\\t\\t14.938791143260346\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t15.934710552810998,\\n\\t\\t\\t\\t\\t14.938791143260346\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t15.934710552810998,\\n\\t\\t\\t\\t\\t15.934710552810998\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t15.934710552810998,\\n\\t\\t\\t\\t\\t17.926549371912415\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t14.938791143260346,\\n\\t\\t\\t\\t\\t20.914307600564484\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t13.942871733709694,\\n\\t\\t\\t\\t\\t23.902065829216554\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t12.946952324158929,\\n\\t\\t\\t\\t\\t24.897985238767205\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t11.951032914608277,\\n\\t\\t\\t\\t\\t25.89390464831797\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t10.955113505057625,\\n\\t\\t\\t\\t\\t26.889824057868623\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t10.955113505057625,\\n\\t\\t\\t\\t\\t27.885743467419275\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t10.955113505057625,\\n\\t\\t\\t\\t\\t28.88166287697004\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t9.95919409550686,\\n\\t\\t\\t\\t\\t30.873501696071344\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t9.95919409550686,\\n\\t\\t\\t\\t\\t32.86534051517276\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t9.95919409550686,\\n\\t\\t\\t\\t\\t34.85717933427418\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t9.95919409550686,\\n\\t\\t\\t\\t\\t35.85309874382483\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t10.955113505057625,\\n\\t\\t\\t\\t\\t35.85309874382483\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t12.946952324158929,\\n\\t\\t\\t\\t\\t34.85717933427418\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t15.934710552810998,\\n\\t\\t\\t\\t\\t32.86534051517276\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t17.926549371912415,\\n\\t\\t\\t\\t\\t28.88166287697004\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t20.914307600564484,\\n\\t\\t\\t\\t\\t21.910227010115136\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t23.902065829216554,\\n\\t\\t\\t\\t\\t14.938791143260346\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t26.889824057868623,\\n\\t\\t\\t\\t\\t10.955113505057511\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t29.877582286520692,\\n\\t\\t\\t\\t\\t5.975516457304138\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t30.873501696071457,\\n\\t\\t\\t\\t\\t3.983677638202721\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t30.873501696071457,\\n\\t\\t\\t\\t\\t1.9918388191013037\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t30.873501696071457,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t31.86942110562211,\\n\\t\\t\\t\\t\\t-0.9959194095507655\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t32.86534051517276,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t32.86534051517276,\\n\\t\\t\\t\\t\\t0.9959194095506518\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t32.86534051517276,\\n\\t\\t\\t\\t\\t1.9918388191013037\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t33.86125992472353,\\n\\t\\t\\t\\t\\t3.983677638202721\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t33.86125992472353,\\n\\t\\t\\t\\t\\t6.97143586685479\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t32.86534051517276,\\n\\t\\t\\t\\t\\t10.955113505057511\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t31.86942110562211,\\n\\t\\t\\t\\t\\t16.93062996236165\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t29.877582286520692,\\n\\t\\t\\t\\t\\t23.902065829216554\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t28.88166287697004,\\n\\t\\t\\t\\t\\t26.889824057868623\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t28.88166287697004,\\n\\t\\t\\t\\t\\t27.885743467419275\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t28.88166287697004,\\n\\t\\t\\t\\t\\t26.889824057868623\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t28.88166287697004,\\n\\t\\t\\t\\t\\t24.897985238767205\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t28.88166287697004,\\n\\t\\t\\t\\t\\t20.914307600564484\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t28.88166287697004,\\n\\t\\t\\t\\t\\t14.938791143260346\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t28.88166287697004,\\n\\t\\t\\t\\t\\t8.963274685956208\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t30.873501696071457,\\n\\t\\t\\t\\t\\t3.983677638202721\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t33.86125992472353,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t36.849018153375596,\\n\\t\\t\\t\\t\\t-1.9918388191014174\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t39.836776382027665,\\n\\t\\t\\t\\t\\t-2.987758228652183\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t42.824534610679734,\\n\\t\\t\\t\\t\\t-3.9836776382028347\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t46.808212248882455,\\n\\t\\t\\t\\t\\t-3.9836776382028347\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t49.795970477534524,\\n\\t\\t\\t\\t\\t-3.9836776382028347\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t51.78780929663594,\\n\\t\\t\\t\\t\\t-3.9836776382028347\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t51.78780929663594,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t49.795970477534524,\\n\\t\\t\\t\\t\\t5.975516457304138\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t46.808212248882455,\\n\\t\\t\\t\\t\\t13.94287173370958\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t44.81637342978104,\\n\\t\\t\\t\\t\\t21.910227010115136\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t43.820454020230386,\\n\\t\\t\\t\\t\\t25.89390464831797\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t43.820454020230386,\\n\\t\\t\\t\\t\\t27.885743467419275\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t45.8122928393318,\\n\\t\\t\\t\\t\\t28.88166287697004\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t47.80413165843311,\\n\\t\\t\\t\\t\\t29.877582286520692\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t49.795970477534524,\\n\\t\\t\\t\\t\\t28.88166287697004\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t52.783728706186594,\\n\\t\\t\\t\\t\\t27.885743467419275\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t54.77556752528801,\\n\\t\\t\\t\\t\\t24.897985238767205\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t57.76332575394008,\\n\\t\\t\\t\\t\\t19.91838819101372\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t58.75924516349073,\\n\\t\\t\\t\\t\\t17.926549371912415\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t58.75924516349073,\\n\\t\\t\\t\\t\\t13.94287173370958\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t58.75924516349073,\\n\\t\\t\\t\\t\\t12.946952324158929\\n\\t\\t\\t\\t]\\n\\t\\t\\t],\\n\\t\\t\\t\"pressures\": [\\n\\t\\t\\t\\t0.07999999821186066,\\n\\t\\t\\t\\t0.07999999821186066,\\n\\t\\t\\t\\t0.06320296972990036,\\n\\t\\t\\t\\t0.10915341228246689,\\n\\t\\t\\t\\t0.1811860054731369,\\n\\t\\t\\t\\t0.18042361736297607,\\n\\t\\t\\t\\t0.15798428654670715,\\n\\t\\t\\t\\t0.1465887427330017,\\n\\t\\t\\t\\t0.14284701645374298,\\n\\t\\t\\t\\t0.1309598982334137,\\n\\t\\t\\t\\t0.12439867854118347,\\n\\t\\t\\t\\t0.12318631261587143,\\n\\t\\t\\t\\t0.12910830974578857,\\n\\t\\t\\t\\t0.15082882344722748,\\n\\t\\t\\t\\t0.1758720725774765,\\n\\t\\t\\t\\t0.18134182691574097,\\n\\t\\t\\t\\t0.18000583350658417,\\n\\t\\t\\t\\t0.16857904195785522,\\n\\t\\t\\t\\t0.1520618349313736,\\n\\t\\t\\t\\t0.12154809385538101,\\n\\t\\t\\t\\t0.09022567421197891,\\n\\t\\t\\t\\t0.08023639023303986,\\n\\t\\t\\t\\t0.07979943603277206,\\n\\t\\t\\t\\t0.09863708168268204,\\n\\t\\t\\t\\t0.13005325198173523,\\n\\t\\t\\t\\t0.1643577218055725,\\n\\t\\t\\t\\t0.18455109000205994,\\n\\t\\t\\t\\t0.18869909644126892,\\n\\t\\t\\t\\t0.17340891063213348,\\n\\t\\t\\t\\t0.15141043066978455,\\n\\t\\t\\t\\t0.1262923926115036,\\n\\t\\t\\t\\t0.12135089188814163,\\n\\t\\t\\t\\t0.11789422482252121,\\n\\t\\t\\t\\t0.13694052398204803,\\n\\t\\t\\t\\t0.1450185924768448,\\n\\t\\t\\t\\t0.1304534673690796,\\n\\t\\t\\t\\t0.12285357713699341,\\n\\t\\t\\t\\t0.13362182676792145,\\n\\t\\t\\t\\t0.14701925218105316,\\n\\t\\t\\t\\t0.15868359804153442,\\n\\t\\t\\t\\t0.16355951130390167,\\n\\t\\t\\t\\t0.15933364629745483,\\n\\t\\t\\t\\t0.1502024233341217,\\n\\t\\t\\t\\t0.14284588396549225,\\n\\t\\t\\t\\t0.13931000232696533,\\n\\t\\t\\t\\t0.14129230380058289,\\n\\t\\t\\t\\t0.1379031389951706,\\n\\t\\t\\t\\t0.1314718872308731,\\n\\t\\t\\t\\t0.126432865858078,\\n\\t\\t\\t\\t0.11115927249193192,\\n\\t\\t\\t\\t0.0861562192440033,\\n\\t\\t\\t\\t0.04663082957267761,\\n\\t\\t\\t\\t0.021255889907479286,\\n\\t\\t\\t\\t0.09039077907800674,\\n\\t\\t\\t\\t0.1072980985045433,\\n\\t\\t\\t\\t0.1327335238456726,\\n\\t\\t\\t\\t0.15709514915943146,\\n\\t\\t\\t\\t0.1928785741329193,\\n\\t\\t\\t\\t0.22216199338436127,\\n\\t\\t\\t\\t0.22787213325500488,\\n\\t\\t\\t\\t0.21927663683891296,\\n\\t\\t\\t\\t0.20952355861663818,\\n\\t\\t\\t\\t0.15570572018623352,\\n\\t\\t\\t\\t0.10941454768180847,\\n\\t\\t\\t\\t0.1000286266207695,\\n\\t\\t\\t\\t0.09653158485889435,\\n\\t\\t\\t\\t0.10343005508184433,\\n\\t\\t\\t\\t0.12496615946292877,\\n\\t\\t\\t\\t0.1418415904045105,\\n\\t\\t\\t\\t0.15852588415145874,\\n\\t\\t\\t\\t0.17555394768714905,\\n\\t\\t\\t\\t0.1838352084159851,\\n\\t\\t\\t\\t0.17751239240169525,\\n\\t\\t\\t\\t0.16881689429283142,\\n\\t\\t\\t\\t0.149704709649086,\\n\\t\\t\\t\\t0.13695739209651947,\\n\\t\\t\\t\\t0.13103735446929932,\\n\\t\\t\\t\\t0.13976992666721344,\\n\\t\\t\\t\\t0.1574166864156723,\\n\\t\\t\\t\\t0.17939318716526031,\\n\\t\\t\\t\\t0.17816564440727234,\\n\\t\\t\\t\\t0.17347152531147003,\\n\\t\\t\\t\\t0.1427186280488968,\\n\\t\\t\\t\\t0.1203295886516571,\\n\\t\\t\\t\\t0.10864010453224182,\\n\\t\\t\\t\\t0.09333465248346329,\\n\\t\\t\\t\\t0.10012930631637573,\\n\\t\\t\\t\\t0.10919537395238876,\\n\\t\\t\\t\\t0.09066644310951233,\\n\\t\\t\\t\\t0.020901642739772797,\\n\\t\\t\\t\\t0\\n\\t\\t\\t],\\n\\t\\t\\t\"simulatePressure\": false,\\n\\t\\t\\t\"lastCommittedPoint\": [\\n\\t\\t\\t\\t58.75924516349073,\\n\\t\\t\\t\\t12.946952324158929\\n\\t\\t\\t]\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\"id\": \"uDs_VncJChAcEIS8Jh0UL\",\\n\\t\\t\\t\"type\": \"freedraw\",\\n\\t\\t\\t\"x\": 187.40302214184908,\\n\\t\\t\\t\"y\": 48.28637663314515,\\n\\t\\t\\t\"width\": 162.33486375676273,\\n\\t\\t\\t\"height\": 63.738842211244275,\\n\\t\\t\\t\"angle\": 0,\\n\\t\\t\\t\"strokeColor\": \"#1e1e1e\",\\n\\t\\t\\t\"backgroundColor\": \"transparent\",\\n\\t\\t\\t\"fillStyle\": \"hachure\",\\n\\t\\t\\t\"strokeWidth\": 1,\\n\\t\\t\\t\"strokeStyle\": \"solid\",\\n\\t\\t\\t\"roughness\": 1,\\n\\t\\t\\t\"opacity\": 100,\\n\\t\\t\\t\"groupIds\": [],\\n\\t\\t\\t\"frameId\": null,\\n\\t\\t\\t\"roundness\": null,\\n\\t\\t\\t\"seed\": 596474782,\\n\\t\\t\\t\"version\": 79,\\n\\t\\t\\t\"versionNonce\": 2121621854,\\n\\t\\t\\t\"isDeleted\": false,\\n\\t\\t\\t\"boundElements\": null,\\n\\t\\t\\t\"updated\": 1690077954708,\\n\\t\\t\\t\"link\": null,\\n\\t\\t\\t\"locked\": false,\\n\\t\\t\\t\"points\": [\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-0.9959194095506518,\\n\\t\\t\\t\\t\\t0.9959194095507087\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-0.9959194095506518,\\n\\t\\t\\t\\t\\t3.983677638202778\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t4.97959704775343\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t5.975516457304138\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t1.9918388191014174,\\n\\t\\t\\t\\t\\t7.967355276405499\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t2.987758228652069,\\n\\t\\t\\t\\t\\t8.963274685956208\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t3.9836776382028347,\\n\\t\\t\\t\\t\\t9.95919409550686\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t4.9795970477534865,\\n\\t\\t\\t\\t\\t9.95919409550686\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t4.9795970477534865,\\n\\t\\t\\t\\t\\t7.967355276405499\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t6.971435866854904,\\n\\t\\t\\t\\t\\t4.97959704775343\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t10.955113505057625,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t16.930629962361763,\\n\\t\\t\\t\\t\\t-4.9795970477534865\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t23.902065829216554,\\n\\t\\t\\t\\t\\t-7.967355276405556\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t28.88166287697004,\\n\\t\\t\\t\\t\\t-7.967355276405556\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t29.877582286520806,\\n\\t\\t\\t\\t\\t-7.967355276405556\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t28.88166287697004,\\n\\t\\t\\t\\t\\t-3.983677638202778\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t22.9061464196659,\\n\\t\\t\\t\\t\\t2.987758228652069\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t11.951032914608277,\\n\\t\\t\\t\\t\\t11.951032914608277\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t2.987758228652069,\\n\\t\\t\\t\\t\\t21.91022701011525\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-2.987758228652069,\\n\\t\\t\\t\\t\\t26.889824057868623\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-5.975516457304138,\\n\\t\\t\\t\\t\\t26.889824057868623\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-6.97143586685479,\\n\\t\\t\\t\\t\\t21.91022701011525\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-5.975516457304138,\\n\\t\\t\\t\\t\\t13.942871733709694\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-0.9959194095506518,\\n\\t\\t\\t\\t\\t7.967355276405499\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t4.9795970477534865,\\n\\t\\t\\t\\t\\t3.983677638202778\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t9.959194095506973,\\n\\t\\t\\t\\t\\t2.987758228652069\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t13.942871733709694,\\n\\t\\t\\t\\t\\t1.9918388191013605\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t16.930629962361763,\\n\\t\\t\\t\\t\\t2.987758228652069\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t17.926549371912415,\\n\\t\\t\\t\\t\\t5.975516457304138\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t14.938791143260346,\\n\\t\\t\\t\\t\\t10.955113505057625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t7.967355276405556,\\n\\t\\t\\t\\t\\t19.918388191013833\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-2.987758228652069,\\n\\t\\t\\t\\t\\t28.88166287697004\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-12.946952324158929,\\n\\t\\t\\t\\t\\t36.849018153375596\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-21.910227010115136,\\n\\t\\t\\t\\t\\t41.82861520112908\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-30.873501696071457,\\n\\t\\t\\t\\t\\t43.820454020230386\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-38.8408569724769,\\n\\t\\t\\t\\t\\t43.820454020230386\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-47.80413165843322,\\n\\t\\t\\t\\t\\t42.824534610679734\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-54.77556752528801,\\n\\t\\t\\t\\t\\t40.83269579157832\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-59.7551645730415,\\n\\t\\t\\t\\t\\t38.84085697247701\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-63.73884221124422,\\n\\t\\t\\t\\t\\t36.849018153375596\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-64.73476162079487,\\n\\t\\t\\t\\t\\t33.86125992472353\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-65.73068103034564,\\n\\t\\t\\t\\t\\t30.873501696071457\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-65.73068103034564,\\n\\t\\t\\t\\t\\t27.88574346741939\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-68.7184392589977,\\n\\t\\t\\t\\t\\t26.889824057868623\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-76.68579453540326,\\n\\t\\t\\t\\t\\t27.88574346741939\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-91.6245856786636,\\n\\t\\t\\t\\t\\t31.86942110562211\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-103.57561859327188,\\n\\t\\t\\t\\t\\t38.84085697247701\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-113.53481268877874,\\n\\t\\t\\t\\t\\t43.820454020230386\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-121.5021679651843,\\n\\t\\t\\t\\t\\t46.808212248882455\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-128.4736038320392,\\n\\t\\t\\t\\t\\t47.80413165843322\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-132.45728147024192,\\n\\t\\t\\t\\t\\t47.80413165843322\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-131.46136206069127,\\n\\t\\t\\t\\t\\t46.808212248882455\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-122.49808737473506,\\n\\t\\t\\t\\t\\t38.84085697247701\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-105.5674574123733,\\n\\t\\t\\t\\t\\t25.89390464831797\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-81.66539158315663,\\n\\t\\t\\t\\t\\t12.946952324158929\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-55.77148693483866,\\n\\t\\t\\t\\t\\t1.9918388191013605\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-30.873501696071457,\\n\\t\\t\\t\\t\\t-6.971435866854847\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-7.967355276405556,\\n\\t\\t\\t\\t\\t-9.959194095506916\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t8.963274685956208,\\n\\t\\t\\t\\t\\t-9.959194095506916\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t17.926549371912415,\\n\\t\\t\\t\\t\\t-6.971435866854847\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t18.92246878146318,\\n\\t\\t\\t\\t\\t-0.9959194095507087\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t12.946952324159042,\\n\\t\\t\\t\\t\\t5.975516457304138\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0.9959194095506518,\\n\\t\\t\\t\\t\\t14.938791143260346\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-16.930629962361763,\\n\\t\\t\\t\\t\\t22.9061464196659\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-38.8408569724769,\\n\\t\\t\\t\\t\\t29.877582286520692\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-59.7551645730415,\\n\\t\\t\\t\\t\\t37.84493756292625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-77.68171394495391,\\n\\t\\t\\t\\t\\t44.81637342978115\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-94.61234390731568,\\n\\t\\t\\t\\t\\t49.795970477534524\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-105.5674574123733,\\n\\t\\t\\t\\t\\t52.783728706186594\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-111.54297386967744,\\n\\t\\t\\t\\t\\t53.77964811573736\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-110.54705446012667,\\n\\t\\t\\t\\t\\t51.78780929663594\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-99.59194095506916,\\n\\t\\t\\t\\t\\t46.808212248882455\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-78.67763335450456,\\n\\t\\t\\t\\t\\t40.83269579157832\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-54.77556752528801,\\n\\t\\t\\t\\t\\t35.85309874382483\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-30.873501696071457,\\n\\t\\t\\t\\t\\t28.88166287697004\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-11.951032914608277,\\n\\t\\t\\t\\t\\t20.914307600564484\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-6.97143586685479,\\n\\t\\t\\t\\t\\t18.92246878146318\\n\\t\\t\\t\\t]\\n\\t\\t\\t],\\n\\t\\t\\t\"pressures\": [\\n\\t\\t\\t\\t0.07999999821186066,\\n\\t\\t\\t\\t0.07999999821186066,\\n\\t\\t\\t\\t0.1090639978647232,\\n\\t\\t\\t\\t0.11759048700332642,\\n\\t\\t\\t\\t0.127610445022583,\\n\\t\\t\\t\\t0.1494656354188919,\\n\\t\\t\\t\\t0.13932812213897705,\\n\\t\\t\\t\\t0.1264503449201584,\\n\\t\\t\\t\\t0.07199783623218536,\\n\\t\\t\\t\\t0.07556182891130447,\\n\\t\\t\\t\\t0.09087585657835007,\\n\\t\\t\\t\\t0.11792977899312973,\\n\\t\\t\\t\\t0.15368500351905823,\\n\\t\\t\\t\\t0.17648300528526306,\\n\\t\\t\\t\\t0.17781402170658112,\\n\\t\\t\\t\\t0.18531586229801178,\\n\\t\\t\\t\\t0.14834906160831451,\\n\\t\\t\\t\\t0.12049063295125961,\\n\\t\\t\\t\\t0.07403720170259476,\\n\\t\\t\\t\\t0.06293389946222305,\\n\\t\\t\\t\\t0.08030901849269867,\\n\\t\\t\\t\\t0.10696588456630707,\\n\\t\\t\\t\\t0.09294155985116959,\\n\\t\\t\\t\\t0.09678619354963303,\\n\\t\\t\\t\\t0.0900837704539299,\\n\\t\\t\\t\\t0.12043420225381851,\\n\\t\\t\\t\\t0.16941504180431366,\\n\\t\\t\\t\\t0.19701339304447174,\\n\\t\\t\\t\\t0.20666848123073578,\\n\\t\\t\\t\\t0.20913344621658325,\\n\\t\\t\\t\\t0.19611236453056335,\\n\\t\\t\\t\\t0.14732769131660461,\\n\\t\\t\\t\\t0.09921478480100632,\\n\\t\\t\\t\\t0.06488580256700516,\\n\\t\\t\\t\\t0.059745848178863525,\\n\\t\\t\\t\\t0.08588150143623352,\\n\\t\\t\\t\\t0.10220865160226822,\\n\\t\\t\\t\\t0.11737481504678726,\\n\\t\\t\\t\\t0.12219372391700745,\\n\\t\\t\\t\\t0.11954028159379959,\\n\\t\\t\\t\\t0.1177532970905304,\\n\\t\\t\\t\\t0.11425948888063431,\\n\\t\\t\\t\\t0.11477605998516083,\\n\\t\\t\\t\\t0.11529559642076492,\\n\\t\\t\\t\\t0.11550057679414749,\\n\\t\\t\\t\\t0.09895990043878555,\\n\\t\\t\\t\\t0.05332806333899498,\\n\\t\\t\\t\\t0.05016760155558586,\\n\\t\\t\\t\\t0.05966867133975029,\\n\\t\\t\\t\\t0.0985812395811081,\\n\\t\\t\\t\\t0.1396407186985016,\\n\\t\\t\\t\\t0.17888760566711426,\\n\\t\\t\\t\\t0.21238180994987488,\\n\\t\\t\\t\\t0.21392938494682312,\\n\\t\\t\\t\\t0.22956372797489166,\\n\\t\\t\\t\\t0.22757042944431305,\\n\\t\\t\\t\\t0.215678870677948,\\n\\t\\t\\t\\t0.18348000943660736,\\n\\t\\t\\t\\t0.173598513007164,\\n\\t\\t\\t\\t0.1730438768863678,\\n\\t\\t\\t\\t0.18371808528900146,\\n\\t\\t\\t\\t0.22672131657600403,\\n\\t\\t\\t\\t0.2308312952518463,\\n\\t\\t\\t\\t0.19925054907798767,\\n\\t\\t\\t\\t0.11754519492387772,\\n\\t\\t\\t\\t0.08259066939353943,\\n\\t\\t\\t\\t0.09804791212081909,\\n\\t\\t\\t\\t0.14002612233161926,\\n\\t\\t\\t\\t0.1726100742816925,\\n\\t\\t\\t\\t0.19601498544216156,\\n\\t\\t\\t\\t0.21659402549266815,\\n\\t\\t\\t\\t0.22422964870929718,\\n\\t\\t\\t\\t0.2436126470565796,\\n\\t\\t\\t\\t0.2614494264125824,\\n\\t\\t\\t\\t0.23918844759464264,\\n\\t\\t\\t\\t0.16303803026676178,\\n\\t\\t\\t\\t0.060110319405794144,\\n\\t\\t\\t\\t0\\n\\t\\t\\t],\\n\\t\\t\\t\"simulatePressure\": false,\\n\\t\\t\\t\"lastCommittedPoint\": [\\n\\t\\t\\t\\t-6.97143586685479,\\n\\t\\t\\t\\t18.92246878146318\\n\\t\\t\\t]\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\"id\": \"0G6qQG2Td9dv0i4veDcZw\",\\n\\t\\t\\t\"type\": \"freedraw\",\\n\\t\\t\\t\"x\": 210.30916856151498,\\n\\t\\t\\t\"y\": -171.81181287755766,\\n\\t\\t\\t\"width\": 94.61234390731568,\\n\\t\\t\\t\"height\": 99.59194095506916,\\n\\t\\t\\t\"angle\": 0,\\n\\t\\t\\t\"strokeColor\": \"#1e1e1e\",\\n\\t\\t\\t\"backgroundColor\": \"transparent\",\\n\\t\\t\\t\"fillStyle\": \"hachure\",\\n\\t\\t\\t\"strokeWidth\": 1,\\n\\t\\t\\t\"strokeStyle\": \"solid\",\\n\\t\\t\\t\"roughness\": 1,\\n\\t\\t\\t\"opacity\": 100,\\n\\t\\t\\t\"groupIds\": [],\\n\\t\\t\\t\"frameId\": null,\\n\\t\\t\\t\"roundness\": null,\\n\\t\\t\\t\"seed\": 1460824962,\\n\\t\\t\\t\"version\": 35,\\n\\t\\t\\t\"versionNonce\": 958261058,\\n\\t\\t\\t\"isDeleted\": false,\\n\\t\\t\\t\"boundElements\": null,\\n\\t\\t\\t\"updated\": 1690077956094,\\n\\t\\t\\t\"link\": null,\\n\\t\\t\\t\"locked\": false,\\n\\t\\t\\t\"points\": [\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t1.9918388191014174,\\n\\t\\t\\t\\t\\t-0.9959194095506518\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t3.9836776382028347,\\n\\t\\t\\t\\t\\t-1.9918388191013605\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t7.967355276405556,\\n\\t\\t\\t\\t\\t-2.987758228652069\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t11.951032914608277,\\n\\t\\t\\t\\t\\t-3.983677638202778\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t24.89798523876732,\\n\\t\\t\\t\\t\\t-8.963274685956208\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t36.849018153375596,\\n\\t\\t\\t\\t\\t-13.942871733709694\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t50.79188988708529,\\n\\t\\t\\t\\t\\t-19.918388191013833\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t64.73476162079498,\\n\\t\\t\\t\\t\\t-23.902065829216554\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t76.68579453540326,\\n\\t\\t\\t\\t\\t-26.88982405786868\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t86.64498863091012,\\n\\t\\t\\t\\t\\t-26.88982405786868\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t90.62866626911295,\\n\\t\\t\\t\\t\\t-23.902065829216554\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t90.62866626911295,\\n\\t\\t\\t\\t\\t-16.930629962361763\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t84.65314981180882,\\n\\t\\t\\t\\t\\t-6.971435866854847\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t73.69803630675119,\\n\\t\\t\\t\\t\\t3.983677638202778\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t57.76332575394008,\\n\\t\\t\\t\\t\\t15.934710552811055\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t41.82861520112908,\\n\\t\\t\\t\\t\\t26.88982405786868\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t25.89390464831797,\\n\\t\\t\\t\\t\\t35.85309874382489\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t16.930629962361763,\\n\\t\\t\\t\\t\\t41.828615201129026\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t15.934710552811111,\\n\\t\\t\\t\\t\\t41.828615201129026\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t22.9061464196659,\\n\\t\\t\\t\\t\\t37.844937562926305\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t30.873501696071457,\\n\\t\\t\\t\\t\\t31.86942110562211\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t42.824534610679734,\\n\\t\\t\\t\\t\\t22.9061464196659\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t52.78372870618671,\\n\\t\\t\\t\\t\\t17.926549371912472\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t66.72660043989629,\\n\\t\\t\\t\\t\\t14.938791143260403\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t77.68171394495391,\\n\\t\\t\\t\\t\\t14.938791143260403\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t88.63682745001154,\\n\\t\\t\\t\\t\\t19.918388191013833\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t93.61642449776502,\\n\\t\\t\\t\\t\\t29.87758228652075\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t94.61234390731568,\\n\\t\\t\\t\\t\\t40.832695791578374\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t91.6245856786636,\\n\\t\\t\\t\\t\\t51.78780929663594\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t85.64906922135947,\\n\\t\\t\\t\\t\\t62.74292280169357\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t80.66947217360598,\\n\\t\\t\\t\\t\\t70.71027807809912\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t81.66539158315675,\\n\\t\\t\\t\\t\\t72.70211689720048\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t84.65314981180882,\\n\\t\\t\\t\\t\\t70.71027807809912\\n\\t\\t\\t\\t]\\n\\t\\t\\t],\\n\\t\\t\\t\"pressures\": [\\n\\t\\t\\t\\t0.07999999821186066,\\n\\t\\t\\t\\t0.07999999821186066,\\n\\t\\t\\t\\t0.13836000859737396,\\n\\t\\t\\t\\t0.14543798565864563,\\n\\t\\t\\t\\t0.15853051841259003,\\n\\t\\t\\t\\t0.1723373681306839,\\n\\t\\t\\t\\t0.17169290781021118,\\n\\t\\t\\t\\t0.17217181622982025,\\n\\t\\t\\t\\t0.17130127549171448,\\n\\t\\t\\t\\t0.16030846536159515,\\n\\t\\t\\t\\t0.1522316038608551,\\n\\t\\t\\t\\t0.14880673587322235,\\n\\t\\t\\t\\t0.1445712000131607,\\n\\t\\t\\t\\t0.16054610908031464,\\n\\t\\t\\t\\t0.16493242979049683,\\n\\t\\t\\t\\t0.15076379477977753,\\n\\t\\t\\t\\t0.15401895344257355,\\n\\t\\t\\t\\t0.1494321972131729,\\n\\t\\t\\t\\t0.15311475098133087,\\n\\t\\t\\t\\t0.16898642480373383,\\n\\t\\t\\t\\t0.15161310136318207,\\n\\t\\t\\t\\t0.1442108154296875,\\n\\t\\t\\t\\t0.13550527393817902,\\n\\t\\t\\t\\t0.13199694454669952,\\n\\t\\t\\t\\t0.1559293270111084,\\n\\t\\t\\t\\t0.1685316413640976,\\n\\t\\t\\t\\t0.1701705902814865,\\n\\t\\t\\t\\t0.16214044392108917,\\n\\t\\t\\t\\t0.16235555708408356,\\n\\t\\t\\t\\t0.1736508160829544,\\n\\t\\t\\t\\t0.18561020493507385,\\n\\t\\t\\t\\t0.19134090840816498,\\n\\t\\t\\t\\t0.10439816117286682,\\n\\t\\t\\t\\t0\\n\\t\\t\\t],\\n\\t\\t\\t\"simulatePressure\": false,\\n\\t\\t\\t\"lastCommittedPoint\": [\\n\\t\\t\\t\\t84.65314981180882,\\n\\t\\t\\t\\t70.71027807809912\\n\\t\\t\\t]\\n\\t\\t}\\n\\t],\\n\\t\"appState\": {\\n\\t\\t\"theme\": \"light\",\\n\\t\\t\"viewBackgroundColor\": \"#ffffff\",\\n\\t\\t\"currentItemStrokeColor\": \"#1e1e1e\",\\n\\t\\t\"currentItemBackgroundColor\": \"transparent\",\\n\\t\\t\"currentItemFillStyle\": \"hachure\",\\n\\t\\t\"currentItemStrokeWidth\": 1,\\n\\t\\t\"currentItemStrokeStyle\": \"solid\",\\n\\t\\t\"currentItemRoughness\": 1,\\n\\t\\t\"currentItemOpacity\": 100,\\n\\t\\t\"currentItemFontFamily\": 1,\\n\\t\\t\"currentItemFontSize\": 20,\\n\\t\\t\"currentItemTextAlign\": \"left\",\\n\\t\\t\"currentItemStartArrowhead\": null,\\n\\t\\t\"currentItemEndArrowhead\": \"arrow\",\\n\\t\\t\"scrollX\": 652.1570401093837,\\n\\t\\t\"scrollY\": 455.6488445995047,\\n\\t\\t\"zoom\": {\\n\\t\\t\\t\"value\": 1.0040973098929256\\n\\t\\t},\\n\\t\\t\"currentItemRoundness\": \"round\",\\n\\t\\t\"gridSize\": null,\\n\\t\\t\"currentStrokeOptions\": null,\\n\\t\\t\"previousGridSize\": null\\n\\t},\\n\\t\"files\": {}\\n}\\n```\\n%%', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ec1ffcd5-9ed4-407d-bd72-36e3c10d858e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nThesis project\\n\\n[[meeting note 2021 Feb]]\\n\\n[[intro part about stereotyping]]\\n\\n[[graduation procedure]]\\n\\n[[trait structure]]\\n\\n[[motives]]\\n\\n[[一些social perception theories]]\\n\\n[[study 7 and 13 , 17 scale]]\\n\\n[[整合两篇essay]]\\n\\n[[dual identity]]\\n\\n[[self concept 相关理论]]\\n\\n[[english 改动]]\\n\\n[[杂 thesis]]\\n\\n1. study 5\\n\\nversatility definition as  \\n\\nmultifaceted; versatile; well-rounded \\n\\ndefine versatility as adaptive and specialization in different social roles; \\n\\nnew kind of implicit theory; \\n\\nimplicit theory on; unitary vs. multiple selves;  \\n\\nimplicit theory \\n\\nbiased \\n\\nadd more figures; to explain the mediation \\n\\nlow probability of two things exist on the same person; \\n\\n3. 没有一个clear recommendation on poster or perceiver\\n\\n4. exploration (finding new) vs. exploitation (finding coherence) in impression formation \\n\\nmind perception; complex mind -> impression formation ; presumption of impression formation\\n\\n6. big data results\\n\\nRegression analysis showed that inconsistency was negatively correlated to impression favorability (i.e., number of followers and shares) for influencers’ accounts, while the correlation was not significant for regular users.\\n\\n7. brand image presentation 可以在general discussion讨论\\n\\n  \\n\\n加上essentialist belief 作为另一个observer trait \\n\\n我的manipulation 是说，view himself more flexible  -> better coping ability in different social context -> better social impression \\n\\n versatility coming after entertainment value?\\n\\nquant part要不要写一个study 6\\n\\nfragmentation 咋就不能导致 better coping ability in different social context \\n\\n讨论breadth and Depth 这一层\\n\\nI guess the inconsistency can come from both within a level and between levels. But from the pretest I had, it seems that most people can only tell the inconsistency across public vs. personal levels.\\n\\n如果是SNS上signal的post，让人judge 他的authenticity，可能有两层意思，一层是他表现的trait是不是他的true trait；另一层是他这俩conflict trait能否体现他的authenticity\\n\\nDilute this person’s identity? identity dilusion?\\n\\ninconsistency dilute your understanding of the person; diversity strengthen your understanding\\n\\n如何处理self concept 这个词的问题？我能否用identity 来代替他\\n\\nwould be positive when perceivers view inconsistent attributes as a sign of flexibility of the poster’s identity\\n\\n如何做power analysis，在我无法predict effect of inconsistency的情况下\\n\\nStudy 3 showed inconsistency has an effect size at r = .18. Given the 2 by 2 ANOVA design, a priori power analysis suggested a sample size of 245 to replicate the effect. Since I expect a strong effect from interpretation valence that reverses the effect of inconsistency (i.e., cross-over pattern),\\n\\n一个人的社会属性，是其他所有人对他的impression的总和\\n\\nInterpersonal perception can be analyzed with three components: actor effect, perceiver effect, and (Kenny and Albright, 1987).\\n\\nDepending on perceivers’ own traits and believes, it is possible that some perceivers are more likely to feel inauthentic about the poster from his inconsistent attributes, while others are more likely to perceive diversity. It is their subjective weighting on the two competing paths determined the favorability perception. Study 3 has shown a moderation effect from the perceivers’ essentialist belief. Study 4b extended this exploration into several other perceiver traits and discovered a similar moderation effect of social media usage frequency. Study 5 demonstrated that perceivers’ motives of using social media did not impact the process of inconsistency. The perceiver effect is always an essential component of interpersonal perception (Back and Kenny, 2010). The present investigation demonstrated perceivers’ belief on the .\\n\\n讨论：啥时候两个path共存\\n\\nAs a result, the choice of *how* to send messages (a letter, a phone call, an email) warrants just as much\\n\\nconsideration as *what* should comprise the content of those messages.\\n\\nPrior conceptual\\n\\nwork informs this\\n\\nGiven that a range of .\\n\\nHow much effort did you put in to understand [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue] by the posts?\\n\\nHow much attention did you paid to understand [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue] by the posts?\\n\\nI adopted four items from the scale from Clatterbuck (1979), which was often used to measure relational uncertainty for initial social impressions (Antheunis, Valkenburg, and Peter, 2010).\\n\\nnegatively correlated to warmth (t(134) = -2.94; p<.01; β = -.20) and competence (t(134) = -3.04; p<.01; β = -.21),\\n\\nFor close friends, perceivers probably know them well and are expected to explain the friend’s attribute inconsistencies they observed from the posts. In other words, they are likely to believe close friends to integrate inconsistent attributes better than distant friends. Therefore, perceivers might react less negatively to inconsistencies for close friends than to strangers. Alternatively,\\n\\nThe scale on inconsistency was focused on measuring cross-situational inconsistency, which was different from those in other studies. In personality psychology, the issue of whether a person’s traits remain stable across situations is fundamental (Underwood and Moore, 1981). However, researchers mostly focused on the objective fact that whether a person reveals different attributes in different contexts (Diener and Larsen, 1984). They ignored the subject perception of attribute difference in the observers’ eyes.\\n\\nDiversity 是因为人们思索了attribute的关系之后，才得出的结论。只给两个毫不相干的attribute，不会让人思考attribute之间的关系，所以就不会又diversity perception\\n\\nPositive connection 之后也不会有diversity perception, 因为理应有这些\\n\\nNeutral 的和positive的一样，不会让人觉得有啥不对的，不会影响inauthenticity，但可能会影响diversity；不会影响diversity，因为being tall, likes cooking，不会让人infer this person is multifaceted;\\n\\nRelational uncertainty is often defined as “the lack of confidence a person feels in his or her ability to predict or explain issues associated with a given relationship”( Redmond, 2015).\\n\\nformatting and study 1;\\n\\nbe hold in the skillset rather than the outcome; \\n\\n 一个人的social self和private self overlap有多少？这个决定了authenticity。人们可能会把private self等同于authenticity；但这不见得是对的；social self也是一个人authentic的一部分\\n\\nThat is, the adjectives people use to describe their qualities\\n\\n(e.g., shy, philanthropic) will generally be their attributes,\\n\\nwhereas the nouns that establish contexts binding those attributes\\n\\n(e.g., Mike’s girlfriend, student) typically reflect their self-aspects\\n\\n关于identity conflict\\n\\ncausing internal conflict and tension. More recently, terms such as “clashing identities” and “identity interference” (Settles, 2004; Van Sell, Brief, & Schuler, 1981) convey the notion that a person’s multiple identities can conflict with one another. In research on biculturals and immigrants, as highlighted in the section on social psychological approaches, people have been shown to experience opposition and conflict between their ethnic (e.g. Asian) and national (e.g. American) identities (Benet-Martı´nez & Haritatos, 2005; La Framboise, Coleman, & Gerton, 1993).\\n\\nSimilarly, research has shown that people experience conflict between multiple work identities or between work and non-work identities to the extent they feel they cannot satisfy their own and others’ expectations or requirements (Creed et al., 2010; Greenhaus & Beutell, 1985; Kreiner et al., 2006; Settles, 2004).\\n\\nThus, identity conflict arises when individuals feel they must give precedence to one set of meanings, values, and behaviors over another in order to satisfy particular identity-based expectations, and therefore cannot express or validate the other identities she may hold (Ashforth et al., 2008; Burke & Stets, 2009; Hewlin, 2009; Horton, Bayerl, & Jacobs, 2014; Stryker & Burke, 2000)\\n\\napa manual; \\n\\nlook up formatting; \\n\\nfirst part: importance of my dissertation; no jargons; \\n\\nsocial cognition; group valuation; impression formation; \\n\\nuse propose as if it\\'s something u want to test; \\n\\nI given all the evidences, it\\'s clear that we can work with the assumption that online \\n\\n**change to difference;** \\n\\n**diff on the same topic will be a strong test of inauthenticity;** \\n\\nrewriting ; \\n\\n**track all the important changes;** \\n\\nbreak out the hypoethsis\\n\\n**all hypothesis up front; not in the study 3 part; leave the original theory graph and add other graph; can mention prerestration at the studies again;** \\n\\nstudy intro: conceptually what is the study about in theory; \\n\\nres to change r; \\n\\nAgentic高的人更可能造假\\n\\n). Agentics and\\n\\ncommunals may present themselves differently online. We propose that\\n\\nagentic individuals tend to share experiences that highlight their individuality\\n\\nand superiority, whereas communal individuals’ posts reflflect their emotional\\n\\nexpressivity and warmth. This tendency may in turn affect the traffific and\\n\\nattention their profifiles attract, determining the success of their self-presentation\\n\\nefforts.\\n\\nSimilar to the effects of the physical presence of friends, the participation in\\n\\nsocial networking sites may alter consumer behavior. Agentic individuals may\\n\\nbe more likely to self-promote through their spending decisions (e.g., going on\\n\\nan expensive trip, buying a new car, dining at a fancy restaurant, etc.).\\n\\ncompensation; \\n\\nwhy the measures were included; need to say it in study design section; \\n\\ndo not \"suggested\" when u report direct results; \\n\\nsay that positive direct effect in study is ; suggesting that inconsistency has a ; \\n\\n通过改变perceiver goal来改变对不同trait的重视程度；如果在sm上有一个entertainment goal，可能就更重视diversity不重视inauthenticity, 如果有一个functional goal，可能就更重视inauthenticity; 但是inauthenticity和comminum 联系更紧密\\n\\nsimilar to interpersonal interaction with actors and observers,\\n\\nthe self can also be distinguished into an “actor” self (how do I define myself )\\n\\nand an “observer” self (how do others see me).\\n\\nAbele and Hauke (2019), for instance, people had to assess own\\n\\nAgency-competence, Agency-assertiveness, Communion-friendliness and\\n\\nCommunion-morality, and also rated their self-esteem\\n\\nAs illustrative evidence: Abele and Brack (2013)\\n\\npresented their participants with situations in which the self was either inde\\n\\npendent of the other person (accidental contact without any consequences),\\n\\ndependent on the other person (own goal attainment was dependent on\\n\\nother’s behavior), or interdependent with the other person (goal attainment\\n\\n12\\n\\nAlex Koch et al.\\n\\nARTICLE IN PRESSdue to joint effort) and asked them to choose which out of an equal number of\\n\\nAgency and Communion traits (pretested for valence) the other person should\\n\\nhave. The findings clearly revealed that the importance of Agency varied with\\n\\ndependence/interdependence: The more dependence/interdependence, the\\n\\nhigher the importance of Agency traits. In a field study in which workers had\\n\\nto assess their supervisor’s Agency and Communion and also had to state their\\n\\nliking of the supervisor, results showed that liking was always related to the\\n\\nsupervisor’s Communion, but when the workers’ outcomes were dependent\\n\\non the supervisor’s competence, then liking was also related to Agency\\n\\n(Wojciszke & Abele, 2008).\\n\\nRegarding power and status, Cislak (2013) showed that people in high\\n\\npower positions were more interested in prospective subordinates’ Agency\\n\\nthan Communion. Even temporary increases in status, such as winning a game\\n\\nof tennis, resulted in a heightened use of Agency as opposed to Communion\\n\\ntraits, when deciding who is similar to whom and when describing one’s own\\n\\ncurrent affective state (Baryła & Wojciszke, 2019).\\n\\n\\n\\nspike about the two essays; \\n\\ndiversity as an higher-level attribute derived from inconsistency\\n\\n从没见过一个paper这样提两个competing hypothesis的\\n\\nessentialist belief is it neccessary? 后面的study没找到类似的结果 \\n\\nentertainment and diversity   \\n\\nPeople often engage in self-repetition—repeating the same story, joke, or presentation across \\n3 different audiences. While behaving consistently has generally been found to enhance \\n4 perceptions of authenticity, ten studies demonstrate that performers who are revealed to be self5 repeating are perceived as less authentic. We find convergent evidence that this effect is driven \\n6 by observers’ implicit assumption that social interactions are unique. Self-repetitions violate this \\n7 assumption, leading observers to judge performers as inauthentic because they are thought to be \\n8 falsely presenting their performance as unique when it is not.\\n\\n\\n\\nPersistent need for consistency and stability,” according to\\nMarkus et al. (1997, p. 24), is one of the key characterizations of\\nthe European American self.\\n\\nentertainment 和diversity如何取舍？serial mediation model with two mediators 怎么弄？SEM \\n\\npos path论据可能不足，有何建议? 可以 moderate diversity pos path; \\n\\ngeneral discusssion; \\n\\nmediator comments; study 3 everything can mediate it; \\n\\nstay as a student beyond summer; if I do, summer date; \\n\\n加进去big data results\\n\\ngeneral discussion\\n\\n手动找一千个user就好\\n\\n\\n\\ninauthenticity 解决的是一个cue validity的问题\\n\\ndiversity解决的是什么问题？cue tilization的问题？因为\\n\\n\\n\\n\\n\\n---\\n\\n**remove alternative explanation;** \\n\\n**explain inconsistencies across studies**\\n\\n**empirically differentiate diversity and inconsistency**\\n\\n**spike说的，通过大数据证明不同的attribute coexist几率是不同的来避免政治正确问题**\\n\\n**如果把difference 当IV；diversity perception vs. inauthenticity perception as two consequence paths**\\n\\nvegan study; 可以解释为intrapersonal difference; different attitude： **one positive and one neg path**\\n\\njumping: 可以解释为intrapersonal difference; cross-situational diff: 只有Inauthenticity一个neg path\\n\\n**是否要去掉jumping study;** \\n\\n7+8 可以分出一个diff; 或者直接把Incons 当成diff: **inauthenticity 出现了意想不到的pos effect**\\n\\n13+14 可以分出一个diff; 或者直接把Incons 当成diff：**Only one positive diversity path，没有inauth effect** \\n\\n**study 17: 两个path都有，可以用Inc，也可以用diff当IV，也有div > entertainment的sequential** \\n\\n \\n\\n**如果把inconsistency当IV，优势是all study works;** \\n\\n但主要问题有什么？在于theory 不太 make sense，如何argue diff does not lead to diversity perception; \\n\\ndifferent posts 投射到attribute上之后，人们要判断how likely are the attributes going to coexist; 如果他觉得coexist 几率高，觉得这是一个普通人；如果coexist几率低，说明是个diverse的人或者虚假的人\\n\\n**关键是这个likelihood加工的过程，我要如何argue，是通过authenticity lit 来，还是通过complexity lit来，或者both**\\n\\n**我没有测这个likelihood; 只能在reason里argue，不能放在Definition**\\n\\n**different posts 投射到attribute上之后，人们判断how different/inconsistent are they. 如果very different，人们会觉得这个人diverse 或者 虚假，如果very similar，就没有反应**\\n\\n**能否直接用inconsistent 取代different?**\\n\\n**是否要argue posts diff is not enough; 我的measure都是集中在attribute level, 不在post level**\\n\\n是否要argue bookclub and nightclub inconsistent的本质原因是，一个reflect了introvert, 一个reflect了extrovert, 这俩是对立的\\n\\n **entertainment 背后的theory**\\n\\n**如果用entertainment替代diversity，前面几个study咋办？**\\n\\n**如何解决diversity and inconsistency的区分问题**\\n\\n**halo effect 如何解决; 除了show they are not sig mediator in a competing model之外，还能如何解决; show EFA results to say they are different/independent factors**\\n\\n**reversing the order of varaibels; show it only on one study;** \\n\\n**do a EFA to show they are different; obliq rotation; max likelihood, egen values above one;** \\n\\n**shared higher order factors;** \\n\\n**run mediation model with covariates; robust effect can exist with controlling;** \\n\\n**average uncertainty and authenticity together;** \\n\\nred box; to the two pathes; \\n\\nattributes different from each other; \\n\\nwe don\\'t have to talk about attribtues that cannot exist together; \\n\\ngenral discussiion talk about the whole model; general G part has lot to write; \\n\\n ppl have a process of trait inference; then they judge \\n\\n**如何把两篇paper连起来？**\\n\\nthey are both about social perception of consumer behavior; \\n\\nargue that consumer behavior is a huge part of consumer\\'s social lives; but it lacks research in the field; review lit on everything about social perception of consumer behavior\\n\\nprevious research on WOM focuses on what drives WOM and how it influence the perceivers\\' behavior; no research on how WOM influence consumer himself\\n\\nsocial media is new thing to study multiple identity; personal brand theory  \\n\\nessay 1 is about using multiple self aspects theory to study social perception on social media \\n\\nessay 2 is about social perception of consumer attitude/WOM\\n\\ninconsistency按理说可以推出diversity perception，但如果感到了inauthenticity，就不觉得这个人diverse了，因为是假的；这里的default path要看被试的lay belief\\n\\n两个path如何解释同时存在？\\n\\ndifference = inconsistency because the definition on likelihood of coexistence \\n\\n**Intrapersonal difference = inconsistency**\\n\\nHigh likelihood of coexistence\\xa0= low inconsistency - > high overlap (difference) among attributes\\n\\nLow likelihood of coexistence\\xa0= high inconsistency -> low overlap (difference) among attributes\\n\\nLoves cooking and swimming: high difference; low overlap; low inconsistency\\n\\n**Inconsistency is one source of intrapersonal difference; it reminds people to think about how diverse the user is**\\n\\n**difference is one level;** \\n\\n**diversity perception is another level, it happens only when ppl sense inconsistency**\\n\\n**the posts (traits) are different  -> this is a diverse/multifaceted person**\\n\\n**the posts (traits) are different  -> they are unlikely to exist on the same person -> this is a diverse/multifaceted person -> positive impression**\\n\\n**the posts (traits) are different  -> they are likely to exist on the same person -> this is an average person  -> neutral impression**\\n\\n**why difference to diversity is not direct?**  \\n\\ndifference >inconsistency -> lower similarity -> bad impression\\n\\ndifference > diversity > higher similarity > good impression\\n\\n**interpersonal similarity 是关键**\\n\\n**inconsistency -> inauthentic > lower similarity -> bad impression**\\n\\n**inconsistency -> diversity > higher similarity -> bad impression**\\n\\nCairo, there might be data about trait-behaviour and behaviour-trait or behavoiur-behaviour with adults. I don’t know if this would be helpful or not, but I do remember reading some papers suggesting that adults generalize trait inferences.\\n\\nspontaneous trait inference (Jim Uleman)\\n\\nJust to touch on what Cecilia said earlier, I feel like inauthenticity may have to do more about traits than hobbies/interests, which is what the current manipulation focuses on, so it might be worth shifting the emphasis onto trait-based inconsistencies on top of behaviour-based inconsistencies\\n\\nFor the results on favorability. The left figure is expected and the right one is not. Because the negative interpretation manipulation required participants to view inconsistency negatively. People in the inconsistent (vs. consistent) condition observe greater inconsistency, so they should report lower favorability.\\n\\n\\n\\n解释为何inconsistent moderate effect of diversity on information\\n\\nIV: intrapersonal difference\\n\\nDV: impression favorability\\n\\nThree competing mechanisms: informativeness,  inconsistency, and \\n\\nviolation: more attention/effort on information processing; more individualized/non-categorized processing;  \\n\\nno violation: informativenss; \\n\\nhow to find negative effect? Is it neccesary to find negative effect? \\n\\n \\n\\n**If we aim to find both the negative and positive effect of intrapersonal difference:**\\n\\nnegative effect:  intrapersonal difference ->  confusion or inauthenticity -> favorability\\n\\npositive effect: informativeness \\n\\n**If we aim to explain the positive effect and not show the negative effect, there are two potential mechanisms for the positive effect:**\\n\\nmore different posts increases informativeness\\n\\nmore different posts increases expectation violation, which leads to more attention/effort on information processing; more individualized/non-categorized information processing\\n\\n**Study design**\\n\\nviolation two types: interesting vs. I don\\'t know how to label him\\n\\nExpectation violation and \\n\\ncombine the measures of diversity and inconsistency\\n\\n我的初始的想法是：\\n\\nadd measures to test meaningful knowledge: my understanding of him is deeper\\n\\nhttps://journals.sagepub.com/doi/pdf/10.1177/0093650212466257?casa_token=e_mpnB5QUhgAAAAA:9raS8aLt66TX86neYs1fCrdfc-XJqyJml1nydP7_lbOIhXFgJHqDDhpa1bky8hcVqyXJHi0I63IQ4-A\\n\\n1. What I see are a list of models. It’s not obvious to me this data-driven approach makes sense. I get the sense you’re looking at your data for consistency. But that is just one way to approach your studies, and it can be very messy, given that each study creates a context within which to interpret the data. That means one of your models might make more sense for some studies, but not others.\\n2. The concept of inconsistency – one thing to ask is it operationalized similarly or differently across studies? Even if it is operationalized the same, it an be interpreted differently within the context of the study.\\n\\n**在vegan and jumping study, I believe this person has conflicted attributes/identities 会非常low，**\\n\\n1. Have you thought about your original reasoning for doing the studies you did? Recall why you chose to do the experiments first and why conduct those particular ones. I’d like you to refresh my memory on the original intentions when we meet Tuesday.\\n    \\n    **inconsistency is the core construct; it is not; intuition is that diversity is good**\\n    \\n    diversity has a positive effect; inconsistency is a moderator\\n    \\n     \\n    \\n\\n**diversity lead to lower inconsistency or no significant results from the two studies;  If I want to find a positive correlation, need a moderator** \\n\\n**informativenss is generally negatively correlated with inconsistency**\\n\\n加了一层attitude differ or not 的factor，导致了\\n\\n**我可以直接manipulate inconsistency and informative往不同的方向。可以让一个人inconsistent or not, 让同样的diversity more or less informative**\\n\\n**only manipulate diversity OR manipulate two factors**\\n\\n**manipulate diversity and inconsistency. Informativeness is hard to manipulate; inconsistency can be manipulated by the likelihood induction**  \\n\\n现在的inconsistency is too vague，我需要更精确的manipulation on inconsistency  \\n\\nlow inconsis + high    inform = high diverse and consist\\n\\nhigh inconsis + high inform = highly diverse and inconsistent\\n\\nlow inconsis + low inform   = low diversity\\n\\n**study 2 主要是创造了一个自然的same consistency, different informativeness的情况；如果我可以让liked activity has no effect on liking,这就是一个好的manipulation**\\n\\n如何说服geoff: ; \\n\\n可以提供一个why avery is inconsistent的理由来do moderation\\n\\nwhether they anticipate or motivated for future involvement, whether they are oriented toward\\n\\ns using SNSs for relationship initiation\\n\\n测量\\n\\ndisclosure intimacy， appropriateness and, desirability\\n\\nnarrativity, information valence (very negative- positive)\\n\\ndecategorization: to explain positive effect of inconsistency \\n\\nexplain w**hy the effect became negative when both factors are high?** \\n\\ninconsistency alone is negative, only after controlling diversity it became positive \\n\\n \\n\\n为何inconsistent 可以有正效用：因为conceptually, some ppl think inconsistent is good, other pp think its bad\\n\\n**How to explain the negative main effect of inconsistency of the first two studies:** it is possibly a design issue where I only asked about inconsistency and participants only thought about negative interpretations of intrapersonal differences. \\n\\n**balance measure: too divers; vs. too simple; very balanced**\\n\\nwhy do you think too diverse? How can it influence your impression on him?\\n\\nwhy do you think too diverse? How can it influence your impression on him?\\n\\n2(measure only diversity vs. inconsistency) design: \\n\\n- \\n- \\n\\n**a two stage survey to ask ppl who reported bad things** \\n\\n\\n\\ninformation is neutral, diversity is positive because it means learning more about the person\\n\\ndiversity has a positive effect because of lower uncertainty perception\\n\\ninconsistency is a moderator of the effect of diversity, because when inconsistency is high, uncertainty perception increases\\n\\n**Old theoretical structure:** \\n\\nmy original thought is: the main contribution of this paper is to bring the multiple identity/self complexity framework to study social perception. So grounded on the overlap of attributes/identities, the construct of diversity and inconsistency comes natural. Consistent overlap (i.e., diversity ) can be seen positive by the perceiver. Inconsistent overlap (i.e., inconsistency) can be seen negative. 对于不overlap的会怎么想？\\n\\nAlong this line, the focal goal is to find how diversity and inconsistency influence social perception. The current empirical findings suggest that diversity has a positive main effect. When both diversity and inconsistency is high, the effect become negative. The main hurdle is finding a way to explain this interaction. \\n\\n**potential new data collection to resolve the hurdle:** need to get a better understanding of inconsistency. Can try to manipulate inconsistency by leading people to think about the likelihood of the co-existence of two attributes; \\n\\nCan explore more dimensions of inconsistency, such as the appropriateness, intimacy, and effectiveness of the posts\\n\\nP**otential new theoretical framework:** intrapersonal difference can be interpreted positively or negatively. The focal goal is to find the moderator of the two interpretations; A major hurdle is that this theoretical framework cannot explain the current findings. \\n\\n**potential new data collection to achieve the goal:** \\n\\nFirst measure information gain from the posts; It will have a null/neutral effect. Then lead participants to interpret the information gain positively or negatively by adding only diversity vs. inconsistency perception measures\\n\\nNow the information gain measure is \"These posts on social media tell a lot about ___\". It can be changed to more neutral measures like: \"These posts contains a lot of information\"\\n\\n**new analysis to resolve the hurdle:**\\n\\nsince information gain mostly has a neutral/null effect from the current data, we can try to describe diversity as the positive interpretation and describe inconsistency as the negative interpretation. However, it is hard to explain the interaction between the two. \\n\\n1. In the analyses from below, you mostly focus on informativeness as a moderator, but I’m not sure that’s really what you will want to focus on in those studies. Those are experiments, which create a context for studying the effects. Notably, attitudinal inconsistency in Study 1, behavioral inconsistency in Study 2. So it’s not obvious to me the starting point will be looking at interactions between informativeness and inconsistency. Let’s keep in mind what you have here, but consider other aspects of the studies.\\n2. I didn’t see any tests of the experimental manipulations on informativeness. I think maybe Study 2 figure does that (in interaction with likejump), which is fine (although I would like to see the analysis without the jump variable too). Also, what did you find in Study 1 when you submitted it to analysis by the experimental manipulation?\\n3. I am not yet persuaded by the wording change to the informativeness items. I actually think the current wording is quite neutral as applied to the target (i.e. poster). That said, it can be positively related because in the context of your study, gaining more information about a poster can generally lead to a more positive impression of the poster. One direction you could go is stipulating why that is the case here, and when might informativeness lead to a more negative impression. We can talk more about this in the meeting.\\n\\nHow to explain the positive main effect of diversity of last two studies: higher authenticity, lower uncertainty, higher competence etc. Several factors can potentially explain it.\\n\\n1. **Current empirical results that are hard to explain** \\n\\nintrapersonal \\n\\nempricial studies; \\n\\ninformativeness can be diversity perception; neutral \\n\\nlow low\\n\\nupdate informativeness to all studies; attribute diversity correlation with them; \\n\\n INFORmativeness; \\n\\nintrapersonal diff can have both effect; how study 5 can solve the problem; maybe infor is super high for; inter for informative and closeness; include infor in all regression \\n\\ncorrelation table for all variabels; (APA table)\\n\\nliking; not impression label\\n\\n**2. The effect is  positive when either inconsistency or diversity is high. Why the effect became negative when both factors are high?** \\n\\nCurrently no factor follows the same trend (i.e., decrease only when **inconsistency and diversity are both high**) as the DVs. \\n\\n3. How to define inconsistency more precisely:  **the presence of one attribute decreases the likelihood of another** **attribute** \\n\\ndo you need more sample; use study 6 effect size for study 7\\n\\nis the interaction the way to go; \\n\\ninformativeness; deeper understanding of someone; \\n\\ninformativeness + closeness is good; \\n\\ninformativeness is the results of diversity; \\n\\ncan justify between \\n\\nassuming the interaction is worth explaining; \\n\\nis closeness the way to go?\\n\\ninformativness as the neutral diversity; \\n\\nis the eta squared output the semi partial?\\n\\npartial eta square; square root; \\n\\nrosenh; \\n\\nTo place effect size estimates into a common metric across studies and\\nanalysis procedures, we opted to use the “r family” of effect size estimates,\\nwhere we estimated rcontrast for ANOVAs (Rosenthal, Rosnow, & Rubin,\\n2000) and requivalent for logistic regression analyses (Rosenthal & Rubin,\\n2003). Estimates had unsigned (and thus positive) directions and could\\nrange from 0 to 1\\n\\nimpression management; \\n\\nchange snsb; \\n\\nwhat are DV; social media behaviors; interpersonal liking: attitudeual  and behaviorals on social media; main DV as interpersonal liking; \\n\\ninconsistency > informativeness; when u \\n\\npositive diff effect\\n\\n1. How to explain the positive effect of intrapersonal difference. The perceiver learned more about the poster, which reduced interpersonal uncertainty, find more interpersonal similarities, and find more meaning in the relationship. There can be various things behind the perception of greater intrapersonal difference, and several of them did emerge as potential mediators from current empirical results. In fact, I am thinking to combine all these factors into one \"More information is better\" heuristic, which suggest that people have a belief that in social life, getting more information about others is always better. This is a similar to the \"more option is better\" heuristics in the paradox of choice literature that people believe more option is always better when they make choices.\\n2. I feel it is hard to find neutral measures that can both reflect the concept of intrapersonal difference and have no correlation with the current dependent measures. My current measures on diversity looks already neutral to me (see below), but they are strongly positively correlated with most dependent measures. It seems that people\\'s default interpretation of intrapersonal difference is positive and it is hard to make it neutral.\\n3. If it is the different interpretation of a neutral construct that lead to both negative and positive effect , the empirical studies should produce two types of results: in one situation, intrapersonal difference would lead to positive effect (i.e., diversity effect) and in another situation, intrapersonal difference would lead to negative effect (i.e., inconsistency effect). Then the studies would not show interaction effect between diversity and inconsistency like the current data showed\\n\\nsend email with meeting notes; \\n\\nintegrate text with stats; table; \\n\\nstudy 1 table to show primary measures; apa tables format;\\n\\nstudy 2 testing everything with interaction; all measures interactions; \\n\\n1988 mgual self identity; 2008 pspb self;  \\n\\n  \\n\\nThe current measures on diversity are:\\n\\nConsider this user\\'s identities/attributes that you just identified, to what extent do you think this user is a multifaceted person?\\n\\nConsider this user\\'s identities/attributes that you just identified, to what extent do you think this user is a versatile person?\\n\\nDo you think this user\\'s identities/attributes belong to a single domain or multiple domains?\\n\\nshow 10 different posts on different political topics\\n\\nhow likely is he going to post something like this?\\n\\nshow ppl that he post the most likely posts and it\\'s opposite\\n\\nshow ppl that he post the least likely posts and it\\'s opposite\\n\\nBased\\xa0only\\xa0on Avery’s above post about bungee jumping/tornado, how\\xa0cautious or bold do you think he/she is? (1 = Very cautious; 7 = Very bold)\\n\\n\\n\\n\\n\\nstudy 5 里是不是更close的friend, inconsistency perception越强？\\n\\ninconsistency and diversity的interaction 我如何hypothesis?\\n\\n看看现有data里有没有出现有些人只有negative corr, 有些人只有negative corr   \\n\\n1. 读者的第一反应是什么？diversity是好还是坏事？\\n\\nspike lab meeting 觉得是好事\\n\\nppl\\'s first reaction is not about inconsistency\\n\\n现有的mechanism，authenticity, uncertainty, competence warmth都不能找到以inconsistency为主导的解释方式，相比diversity更不符合直觉\\n\\n**run a study about automatic reaction about diversity and inconsistency**\\n\\n只有2 个inconsistency study left，而且都under powered\\n\\n2. frame the whole paper with compet  ing hypothesis?\\n\\ncompeting hypothesis is hard. The only way is to use perceiver motive as moderator\\n\\ninterpretation depends on motive? \\n\\n现在的情况是，interpretation as moderator不存在。inconsistency and diversity interaction effect存在\\n\\n3. the current study only measured inconsistency, which lead ppl to only think about the negative effect of diversity\\n\\n4. **why inconsistency has positive effect when diversity is low? authenticity? (inconsistency does not lead to inauthenticity when its low)**\\n\\n是不是inconsistency只是diversity的另一种测量方式而已？被试觉得其实差不多\\n\\n**study design with only one measure on diversity, one measure on inconsistency, and measure on both**\\n\\n5. use authenticity as DV\\n\\n**6. inconsistency: the presence of one event inhibits the likelihood of the other; so when they cooccur, u believe they shouldn’t occur;**\\n\\n**when people see one post, they derive one attribute, they believe this attributes should increase and decrease the likelkihood of one type of posts.** \\n\\n**each post reflect an attribute: if some of the attributes are inconsistent, they feel inconsistenc**\\n\\n**manipulate inconsistency by leading people to think this way? Or test if ppl think this way**\\n\\nppl try to label a person with an attribute. \\n\\nwhen would ppl interpret it positively? when negatively?\\n\\ndifferent motives can moderate it. If ppl are looking for entertainment, diversity is a good thing; \\n\\nsince I only asked inconsistency, it lead ppl to think only about the negative part of diversity\\n\\nthat might be a \\n\\nif people sense authenticity \\n\\nintimates may be affected by such factors as\\nfamiliarity, emotionality. behavioral interdependence. and ideological beliefs\\nabout intimacy\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aeaeb75f-4156-410c-8cb9-ffb92c35f530', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\ndehumanization and mind perception\\n\\nThe findings from the two essays can be incorporated into the framework of mind perception. Previous research suggests that people understand others’ minds in two sets of capacities: agency and experience (Gray, Gray, & Wegner, 2007). Agency involves the capacity for reasoned planning, acting, and self-control. Experience refers to the capacity for experiencing emotions and possess desires.\\xa0 The findings in Essay 1 primarily reflect the agency aspect of mind perception. If a presenter has many inconsistent attributes, perceivers are likely to believe he is a versatile person with a complex mind, which leads to high agency perception. Alternatively, if perceivers believe having inconsistent attributes means the presenter is inauthentic, they are likely to attribute low agency to the person, because inauthenticity is found to negatively relate to agency and autonomy (Dobson, 2020). If a presenter behaves inauthentically, perceivers tend to think he is not acting on his true intentions, goals, and values, which reflects low agency. Essay 2 maps onto the experience aspect of mind perception. If a presenter has neutral attitudes to hedonic products and brands, perceivers tend to think he has low capacity of experiencing pleasure and pain, which leads to lower perception of mind on the presenter.\\n\\nObviously, impression formation often incorporates a broader range of attributes and dispositions of the presenter beyond mind perception, such as being tall and being Asian. Although a series of studies have explored perceivers’ understanding of others’ mind by studying perspective-taking (Epley, Caruso, & Bazerman, 2006) and the theory of mind (Baron-Cohen, Leslie, & Frith, 1985), the literatures of mind perception and impression formation are separated. I hereby propose that mind perception can be treated as an alternative framework to understand impression formation and social perception in general. Similar to other impression formation theories using conceptual dichotomies like warmth and competence (Fiske, Cuddy, & Glick, 2007), human nature and unique human (Haslam, 2006), the framework of mind perception provides a more fundamental layer of impression formation. As Epley and Waytz (2010) suggest, mind perception is a “preattributional process” that determines how perceivers see and interpret other people’s personal attributes.\\n\\n###############################\\n\\nThese chapters organize the dominant\\nresearch tradition on causal attribution that addresses the\\n\\nprocess by which people intuitively explain other people ’ s\\nbehavior to arrive at impressions of others ’ stable dispositions and enduring personalities. Intentional actions reveal\\nmore about someone ’ s enduring dispositions than accidental actions, meaning that inferences about others ’\\nmental states are often at the heart of the person perception process. But before an ordinary perceiver can decide\\nwhich mental states are responsible for a given action, an\\nordinary perceiver needs to at least implicitly determine if\\nanother agent has a mind in the first place, and then determine that agent ’ s state of mind in the second place\\n\\n**higher level traits; conclusive traits;** \\n\\nbecause \\n\\n**People ’ s attempts to\\nunderstand other agents can be organized conceptually\\ninto three questions: (1) Does it have a mind?, (2) What\\nstate might that mind be in?, and (3) Which states of mind\\nare responsible for the agent ’ s behavior? Research on mind\\nperception focuses on the first two questions, whereas\\nmuch of the work in person perception has focused on the\\nprocesses of integration, correction, and discounting that\\nguide the third question.**\\n\\npeople must get something especially useful out\\nof mind perception to warrant all of the additional neurons. This turns out to be wrong. People do not just get something useful, t**hey get at least three things useful — the\\nabilities to comprehend others ’ actions, to understand others ’ communication, and to coordinate one ’ s own behavior with others ’ behavior**\\n\\nAdopting what Dennett (1987) calls an “ intentional stance ”\\nby thinking of others in terms of mental states is a simple\\nand wide - ranging approximation that serves as a practical\\nguide for the intuitive psychologist, providing seemingly\\nadequate reasons that explain actions and events (Malle,\\n1999). **At the least, it works markedly better than an\\nexplanatory system without mental states.**\\n\\nThis inferential process is usually referred to as a\\ntheory of mind for two main reasons, “ first, because such\\n[mental] states are not directly observable, and second,\\nbecause the system can be used to make predictions, specifically about the behavior of other organisms ” (Premack &\\nWoodruff, 1978, p. 515)\\n\\nthe “problem of other minds” as a basic and punishing conundrum: people can never really be \\nsure that other people even have minds, not to mention the challenge of figuring out what might\\nbe going on in them\\n\\n**We define mind perception as the everyday inferential act of\\na perceiver ascribing mental states such as intentions, beliefs, desires, and feelings to others**\\n\\nUnderstanding the causes\\nand consequences of mind perception can explain when\\nthis most social of cognitive skills will be used, and why\\nit matters.\\n\\n\\n\\nConsiderable research has explored the capacity for understanding other minds, targeting theory of mind (Baron-Cohen, Leslie, & Frith, 1985; Gopnik & Wellman, 1992; Premack & Woodruff, 1978; Wimmer & Perner, 1983; Woodward, 1998), mentalizing (Frith & Frith, 2003), and perspective taking (Epley, Caruso, & Bazerman, 2006; Galinsky & Moskowitz, 2000; Stotland, 1969).\\n\\n**From me to you: Simple social projection. The most basic form of social projection\\nemerges when a perceiver ascribes or projects his or her own general attitudes and mental states to a target**\\n\\n. This “egocentric attribution” (Heider, 1958) or “attributive \\n19\\nprojection” (Holmes, 1968) habit leads people to overestimate the prevalence of their own mental\\nstates.\\n\\nA wealth of research suggests that mind reading is compromised by perceivers’ discomfort with ambiguity, their preexisting expectations, and the manner in which they collect information and test their intuitive theories about the causal forces behind others’ actions (e.g., Roese & Sherman, 2007)\\n\\nAmong the attributional distortions perceivers\\ndisplay, the tendency to overweight dispositional factors (e.g., “He’s so kind”) and underweight\\nsituational determinants (“His mother is watching”) when assigning causes to others’ behaviors—\\nthe correspondence bias—continues to be widely referenced and fiercely debated (e.g., Jones &\\nHarris, 1967; Gilbert & Malone, 1995; Malle, 2006; McClure, 2002)\\n\\n如何把inconsistent attribute 联系到 mind perception\\n\\nAgency refers to the capacity for planning and acting, including self-control, morality, memory, communication, thought, and reasoning. Experience refers to the capacity for desires and feelings, including emotions, awareness of the surrounding environment, and basic psychological states such as hunger, thirst, and pain.\\n\\n**Although other minds cannot be directly observed, they can be easily defined. People intuitively represent other minds in two broad sets of psychological capacities — conscious experience and intentional agency (Gray, Gray, & Wegner, 2007). Conscious experience involves metacognitive capacities, including secondary emotions (e.g., regret, rage, sympathy, pride, or joy; Demoulin et al., 2004; Leyens et al., 2003), conscious awareness of one ’ s environment, and basic psychological states (e.g., fear, hunger, thirst, or pain). Intentional agency is the capacity to engage in reasoned action, self - control, strategic planning, or goal - directed behavior and therefore to possess conscious preferences, beliefs, and explicit knowledge.**\\n\\n**mind perception 比dehumanization 概念要更大**\\n\\n**我可以把impression formation 和mind perception联系起来**\\n\\ninauthenticity是Exp and agency之外的另一个维度，他说明这个人很human，cognitive complex，但是不是好人\\n\\nEpley and Waytz (2012) proposed that when studying how perceivers make sense of others, the research on impression formation has been limited to the question on which trait or states of a person’s mind are responsible for his behavior. For instance, being a basketball fan and being introvert can both be seen as traits of a presenter’s mind that help perceiver to explain the presenter’s intention, cognition, and behaviors.\\n\\n**Linking the two dimensions of mind perception to Haslam’s (2006) two senses of humanness, agency can be understood as an element of UH, incorporating moral sensibility, rationality, and maturity as they all reflect the ability to think, reason, and act. On the other hand, experience can be understood as an element of HN, incorporating emotional responsiveness and \\nperhaps also cognitive openness as they both reflect the ability to feel and to sense the environment. This is not to suggest, however, that the mind perception theory can be reduced to an aspect of Haslam’s dehumanization theory. Mind perception can be applied to contexts much \\nbroader than perceptions of humanity, but integrating it with Haslam’s account of dehumanization can enrich our understanding of this particular phenomenon.**\\n\\nIntroduced by Gray and colleagues (2007), the mind perception theory postulates that people perceive other minds, human or non-human, in terms of two fundamental dimensions: agency \\nand experience. Agency refers to the capacity for planning and acting, including self-control, morality, memory, communication, thought, and reasoning. Experience refers to the capacity for desires and feelings, including emotions, awareness of the surrounding environment, and basic psychological states such as hunger, thirst, and pain. In Gray et al.’s, participants rated a variety of \\ntargets (e.g., animals, humans, supernatural entities, robots, dead people) on mental capacities \\nrepresenting either agency or experience. Their findings indicated that humans were perceived as \\nhaving considerably more agency compared to non-human animals (but less compared to God), \\nand having more experience as compared to non-living objects (e.g., dead man, robot, God).\\n\\nAs an aspect of human nature, agency in Haslam’s theory is \\nequivalent to individuality or fungibility — the extent to which the target is interchangeable. Denying others agency thus renders them “interchangeable (fungible) and passive” (Haslam, 2006, \\np. 258). In mind perception, however, agency is primarily concerned with higher-order cognitive \\nabilities rather than the traits that characterize the essence of humans (for a review on the role that \\nagency, as understood by Gray et al., plays in dehumanization see Tipler & Ruscher, 2014).\\n\\nNussbaum (1999) identified seven components\\nof this objectification: “instrumentality” and “ownership” involve treating others as tools and commodities;\\n“denial of autonomy” and “inertness” involve seeing\\nthem as lacking self-determination and agency; “fungibility” involves seeing people as interchangeable with\\nothers of their type; “violability” represents others as\\nlacking boundary integrity; and “denial of subjectivity” involves believing that their experiences and feelings can be neglected\\n\\nWhen HN is denied to others, they should be seen\\nas lacking in emotionality, warmth, cognitive openness, individual agency, and, because HN is essentialized, depth. As others are seen as lacking emotion\\nand warmth they will be perceived as inert and cold.\\nDenying them cognitive openness (e.g., curiosity, flexibility) will give them the appearance of rigidity, and\\ndenying them individual agency represents them as interchangeable (fungible) and passive, their behavior\\ncaused rather than propelled by personal will\\n\\nAlthough other minds cannot be directly observed, they \\ncan be easily defined. People intuitively represent other \\nminds in two broad sets of psychological capacities — \\nconscious experience and intentional agency (Gray, Gray, & \\nWegner, 2007). Conscious experience involves metacognitive capacities, including secondary emotions (e.g., regret, \\nrage, sympathy, pride, or joy; Demoulin et al., 2004; \\nLeyens et al., 2003), conscious awareness of one ’ s environment, and basic psychological states (e.g., fear, hunger, \\nthirst, or pain). Intentional agency is the capacity to engage \\nin reasoned action, self - control, strategic planning, or goal -\\n directed behavior and therefore to possess conscious preferences, beliefs, and explicit knowledge. Agents — entities \\nthat act — are attributed these mental capacities in varying \\ndegrees along a continuum rather than as a dichotomy, \\nwith agents being seen as having more or less of a capacity \\nrather than all or none of it\\n\\nPeople seem to have access to the workings of their own\\nminds but not to the workings of other minds. Philosophically\\nspeaking, this is supposed to create a problem for us.\\nBecause we do not have access to other minds, we cannot\\nconfidently conclude that other people have minds at all.\\nBut most of us do not speak philosophically and therefore\\nhave no trouble talking at length about other people ’ s\\ndesires, intentions, goals, attitudes, beliefs, and emotions.\\nSolving this other minds problem at all seems to be no problem at all. People worry about whether others like them or\\nnot, find them trustworthy or not, or find them attractive or\\nnot. People wonder whether others are being truthful\\nor deceptive, whether others are motivated by greed or generosity, and whether others behaved intentionally or accidentally. And people infer their own future mental states\\nwhen making decisions designed to create future versions\\nof themselves that are happy, such as whether to marry or\\nsave for retirement\\n\\n**Inferences about mental states are often at the very center\\nof social thought and behavior, and social psychologists\\nhave been working for a long time to understand how,\\nand how well, people make them.**\\n\\n**Social psychological research on mind perception borrows much from its intellectual foundations in person perception but differs from classic work because it is both \\nnarrower and broader than the research in person perception from which it grew. It is narrower because it focuses \\nonly on inferences about others ’ minds — what is often \\ncalled “ mentalizing ” (Frith & Frith, 2003) — rather than** \\n\\n**the broader host of traits, dispositions, and capacities that people might attribute to others.** \\n\\n**People ’ s attempts to \\nunderstand other agents can be organized conceptually \\ninto three questions: (1) Does it have a mind?, (2) What \\nstate might that mind be in?, and (3) Which states of mind \\nare responsible for the agent ’ s behavior? Research on mind \\nperception focuses on the first two questions, whereas \\nmuch of the work in person perception has focused on the \\nprocesses of integration, correction, and discounting that \\nguide the third question. In this way, work on mind perception may be considered a kind of preattributional process, \\nidentifying the kinds of causes that might explain or predict \\nanother ’ s behavior. Teenagers, for instance, are capable of \\nintentional deception whereas toddlers are not, and parents \\ntrying to explain their child ’ s behavior do well to know the \\ndifference.**\\n\\nIntentional agency is the capacity to engage\\nin reasoned action, self - control, strategic planning, or goal -\\n directed behavior and therefore to possess conscious preferences, beliefs, and explicit knowledge. Agents — entities\\nthat act — are attributed these mental capacities in varying\\ndegrees along a continuum rather than as a dichotomy,\\nwith agents being seen as having more or less of a capacity\\nrather than all or none of it. Agents can vary along these\\ntwo dimensions quite independently; some have a high\\ndegree of both intentional agency and experience (e.g., the\\nself)\\n\\nIn contrast, research on mind perception is also broader \\nthan existing research on person perception because it \\nexpands the scope of agents considered by ordinary perceivers. “ Social ” agents include any entity that acts interdependently with others, but research in “ social cognition ” \\nhas almost exclusively addressed how people think about \\nother people (Kwan & Fiske, 2008). This is unfortunate \\nbecause people seem readily able to attribute humanlike \\nmental states to all sorts of agents. The stock market can \\n “ flirt with 10,000 ” (Morris, Sheldon, Ames, & Young, \\n\\n2007), one ’ s crashing computer can seem to have a “ mind \\nof its own ” (Waytz, Morewedge, Epley, Monteleone, \\nGao, & Cacioppo, 2009), and one ’ s dog can be a loyal \\nand caring companion (Serpell, 2003). The vast majority \\nof people living today — and nearly all who have lived in \\npast centuries — believe in an omniscient God (or set of \\ngods) whose goals, intentions, and desires can be observed \\nin the world around them (Guthrie, 1993)\\n\\n**Mental states render an agent ’ s behavior understandable and predictable \\n(Baron - Cohen, 1995; Dennett, 1987; Hebb, 1946; Heider, \\n1958), whether those agents are people or not.** Research \\non mind perception shifts the focus of attention from the \\ntarget being perceived to the person perceiving and thereby \\nshifts the focus from a specific target to more domain -\\n general psychological processes involved in mental state \\nattributions.\\n\\nperson “perception” is not about physically objective reality. Modern psychology regards the properties we “see” in others as largely inferred, assumed, felt, and/or enacted (e.g., Uleman, Saribay, & Gonzalez, 2008)\\n\\n关于一个人更深入的了解，都可以归类为mind perception\\n\\nIn Japanese, there is no clear separation between mind and body, and a variety of terms ( kokoro, hara, seishim , and mi ) refer to various aspects of the mind-body (Lebra, 1993).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e7186ef5-8ead-4c1d-ba4a-1430864e9d53', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nstudy 7 and 13 , 17 scale\\n\\n[[study 17]]\\n\\n[[Untitled Database]]\\n\\n**Study 7**\\n\\ndiff\\n\\nDo you think [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue]'s identities/attributes belong to a single domain or multiple domains?\\n\\nAmong [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue]'s identities/attributes that you just identified, how would you rate them in terms of similarity and consistency.\\t\\n\\n[QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue]'s roles/identities are segmented\\n\\ninc\\n\\nAmong [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue]'s identities/attributes that you just identified, how would you rate them in terms of similarity and consistency.\\t\\n\\nI feel that [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue]'s identities/attributes are incompatible\\n\\ndiv\\n\\nto what extent do you think [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue] is a multifaceted person?\\t\\n\\nConsider [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue]'s identities/attributes that you just identified, to what extent do you think [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue] is a versatile person?\\n\\ndiversity\\n\\nConsider [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue]'s identities/attributes that you just identified, to what extent do you think [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue] is a multifaceted person?\\n\\nConsider [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue]'s identities/attributes that you just identified, do you think [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue] is a simple or a complex person?\\n\\ninconsistent\\n\\nAmong [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue]'s identities/attributes that you just identified, how would you rate them in terms of similarity and consistency.\\t\\n\\nI feel that [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue]'s identities/attributes are incompatible\\t\\n\\n[QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue]'s roles/identities are segmented\\n\\ninconsistent_a: diff + inc\\n\\n**Study 7b**\\n\\n应该把这个加到diversity上：[QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue]'s self-concept seamlessly blends among his/her identities\\n\\ndiff\\n\\nDo you think [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue]'s identities/attributes belong to a single domain or multiple domains?\\t\\n\\n[QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue]'s roles/identities are segmented\\t\\n\\nI feel that [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue]'s identities/attributes overlap with each other\\n\\ninc\\t\\n\\nI feel that [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue]'s identities/attributes are incompatible\\t\\n\\nI feel that [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue]'s identities/attributes are inconsistent\\n\\ndiv\\t\\n\\nto what extent do you think [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue] is a multifaceted person?\\t\\n\\nConsider [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue]'s identities/attributes that you just identified, to what extent do you think [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue] is a versatile person?\\n\\ninconsistent\\t\\n\\nI feel that [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue]'s identities/attributes are incompatible\\t\\n\\nI feel that [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue]'s identities/attributes are inconsistent\\t\\n\\nI feel that [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue]'s identities/attributes are different among each other\\n\\ndiversity\\t\\n\\nConsider [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue]'s identities/attributes that you just identified, to what extent do you think [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue] is a multifaceted person?\\t\\n\\nConsider [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue]'s identities/attributes that you just identified, do you think [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue] is a shallow or a deep person?\\n\\ninconsistent_a: diff + inc\\n\\n**combined 7 + 8**\\n\\ninconsistent_a: diff + inc\\n\\nincon_a  > diversity no\\n\\ninconsistent> diversity 能, div 不能\\n\\ninconsistent> div 接近可以\\n\\nincon_b > diversity or div 不能\\n\\n**combined 13 + 14**\\n\\ninconsistent_a: diff + inc\\n\\nincon_a  > diversity works; \\n\\nincon > diversity or div 不能work\\n\\nincon_b > diversity or div 不能work \\n\\ninc > diversity 不行C      \\n\\nstudy 17 \\n\\ndiversity 把multiple domain改成 flexible \\n\\n**Study 13**\\n\\ndiff\\t\\n\\nDo you think this user's identities/attributes belong to a single domain or multiple domains?\\n\\n应该加上：Consider this user's identities/attributes that you just identified, to what extent do you think these identities/attributes are similar among each other?\\n\\ninc\\t\\n\\nI feel that this users's identities/attributes are incompatible\\tI feel that this user's identities/attributes are inconsistent\\tI feel that this user's identities/attributes are conflicted\\n\\ndiv\\t\\n\\nConsider this user's identities/attributes that you just identified, to what extent do you think this user is a multifaceted person?\\tConsider this user's identities/attributes that you just identified, to what extent do you think this user is a versatile person?\\n\\ndiversity\\t\\n\\nConsider this user's identities/attributes that you just identified, to what extent do you think this user is a multifaceted person?\\tConsider this user's identities/attributes that you just identified, to what extent do you think this user is a versatile person?\\tDo you think this user's identities/attributes belong to a single domain or multiple domains?\\tConsider this user's identities/attributes that you just identified, do you think this user is a simple or a complex person?\\n\\ninconsistent\\t\\n\\nI feel that this users's identities/attributes are incompatible\\tI feel that this user's identities/attributes are inconsistent\\tI feel that this user's identities/attributes are conflicted\\n\\ninconsistent_a: diff + inc\\n\\n**Study 14**\\n\\ndiff\\t\\n\\nDo you think this user's identities/attributes belong to a single domain or multiple domains?\\tConsider this user's identities/attributes that you just identified, to what extent do you think these identities/attributes are different among each other?\\n\\ninc\\t\\n\\nI feel that this user's identities/attributes are incompatible\\tI feel that this user's identities/attributes are inconsistent\\tI feel that this user's identities/attributes are conflicted\\n\\ndiv\\t\\n\\nConsider this user's identities/attributes that you just identified, to what extent do you think this user is a multifaceted person?\\tConsider this user's identities/attributes that you just identified, to what extent do you think this user is a versatile person?\\n\\ndiversity\\t\\n\\nConsider this user's identities/attributes that you just identified, to what extent do you think this user is a multifaceted person?\\tConsider this user's identities/attributes that you just identified, to what extent do you think this user is a versatile person?\\tDo you think this user's identities/attributes belong to a single domain or multiple domains?\\tConsider this user's identities/attributes that you just identified, do you think this user is a simple or a complex person?\\n\\ninconsistent\\t\\n\\nI feel that this user's identities/attributes are incompatible\\tI feel that this user's identities/attributes are inconsistent\\tI feel that this user's identities/attributes are conflicted\\n\\nStudy 1\\n\\nTo what extent do you agree or disagree with the following statements? - Avery tends to show different sides of himself to different people\\n\\nTTo what extent do you agree or disagree with the following statements? - In different situations and with different people, Avery often acts like very different persons\\n\\nTo what extent do you agree or disagree with the following statements? - Different situations can make Avery behave like very different people\\t\\n\\nTo what extent do you agree or disagree with the following statements? - Different people tend to have different impressions about the type of person Avery is\\n\\n**Study 5a: incons**\\n\\nAmong [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue]'s identities that you just identified, how would you rate them in terms of similarity and consistency.\\t similar\\n\\nAmong [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue]'s identities that you just identified, how would you rate them in terms of similarity and consistency.\\t consistent\\n\\nI feel that [QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue]'s identities are incompatible\\t\\n\\n[QID1989-ChoiceTextEntryValue][QID1991-ChoiceTextEntryValue][QID2050-ChoiceTextEntryValue]'s roles/identities are segmented\\n\\n##################################\\n\\nDo you think this user's identities/attributes belong to a single domain or multiple domains?\\t\\n\\nConsider this user's identities/attributes that you just identified, to what extent do you think these identities/attributes are different among each other?\\n\\ninc\\t\\n\\nI feel that this user's identities/attributes are incompatible\\t\\n\\nI feel that this user's identities/attributes are inconsistent\\t\\n\\nI feel that this user's identities/attributes are conflicted\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e1d88590-557a-4d3d-9ad4-0e3ae1ccfd7b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nversatility issue\\n\\nstudy 5说明 flexibility 和 versatle 没关系\\n\\n \\n\\nflexibility and integration; \\n\\nit is fragmented, 就说明\\n\\nversatile, multifaceted, well-rounded person; \\n\\nrigid vs. flexible; \\n\\nalso involves the element of the number of attributes; \\n\\nversatile 说明的是：\\n\\nintegrated person is more likely to be flexible; functioning without disruption; better communication skill ; \\n\\n可以相互促进？\\n\\n**has one identity that\\'s consists of different elements; communication skill and adaptive skill**\\n\\n ****\\n\\nfragmented or divided person has one identity needs to wear masks;  -> inauthenticity; because it\\'s masks, it need to \\n\\n完全无链接，causes alienation and loneliness; \\n\\nalienation\\n\\n**These poorly integrated mental representations could lead to alienation and feelings of loneliness.**\\n\\nsuggests that complex individuals are more likely to experience higher levels of marital adjustment as indicated by superior communication skills and greater perceptual accuracy of high conflict scenarios (Denton, Burleson, & Sprenkle, 1995)\\n\\nAnother variable we explored in relation to self-concept structure was that of dissociation. The DSM-IV defines dissociative experiences as “a disruption in the usually integrated functions of consciousness, memory, identity, or perception of the environment (American Psychiatric Association, 1994, p. 477).” Given that inherent in this definition is iden- tity disruption, it stands to reason that this construct would be closely re- lated to fragmentation in one’s self-representation.\\n\\n**SCD: “divided self,” that is a self concept which lacks integration, is an important precursor to the development of psychological problems**\\n\\n**self-complexity, which is defined as “having more self-aspects and maintaining greater distinction among self-aspects” (Linville, 1987,**\\n\\nCalculation of multiple social identity integration. Following Bigler et al.\\n(2001), the average standard deviation of trait ratings across social identities\\nserved as the index of multiple social identity integration\\n\\nThe ability of integrating inconsistent attributes suggest the presenter can act adaptively and flexibly in different social roles (Bigler, Neimeyer, & Brown, 2001), which are likely to be viewed positively by others.\\n\\nThis distinction between\\nself-generated or artificially imposed roles is potentially important in that\\nit implies that these measures may be tapping two different underlying\\ndimensions of self-structure: flexibility (versus rigidity) and integration\\n(versus fragmentation)\\n\\n为何\\n\\nentertaining; similarity; disclosure; competence and autonomy\\n\\n到底是specilization, 还是integration？还是flexibility？\\n\\n本质上是，how to interpret different attributes in different roles \\n\\nwhy use the word: versatile? because flexibility is used in self perception; I want to differ from it; \\n\\nflexibility是across context; versatility 更多的多元化也specialization \\n\\nSCD: variation of traits across roles; \\n\\nnegatively correlated to well being\\n\\nprovided roles; \\n\\nlead to feeling of divided self  and fragmentation \\n\\nself complexity: \\n\\npositively correlated to well being \\n\\nrequires **one to independently generate the roles that are personally meaningful to himself or herself when provided with a set of personality descriptors.** An index of self-complexity is\\nthen calculated that reflects both the number and distinctiveness of roles\\npeople utilize to mentally represent themselves.\\n\\nlead to flexibility and specialization \\n\\nview self flexibly -> have many self identities -> perceiver think he can switch between different social roles -> versatile -> better impression；\\n\\nview self inflexibly -> have one core self identity -> perceiver think he “wear different masks” in different social roles-> inauthentic -> better impression；\\n\\nInconsistent attributes can be interpreted in different directions, which can also moderate the effect between inconsistency and impression favorability. I propose that it is possible for perceivers to interpret inconsistent attributes as a sign of flexibility or inflexibility of the presenter’s identity. Previous studies suggested that when differentiated self-aspects triggers an experience of fragmentation or ‘‘divided self’’ in different roles (Goffman, 1959), it often leads to negative self perception, such as lower self-esteem and greater depression (Donahue et al., 1993). However, when differentiated self-aspects lead people to view themselves to be versatile and specializing in different roles, it produces positive self- perception (Bigler, Neimeyer, and Brown, 2001; Koch and Shepperd, 2004; Diehl, Hastings, & Stanton, 2001). I propose that inflexibility is correlated to fragmentation in interpersonal perception because if the perceiver believe a presenter is unable to change his identity flexibly, it has to be divided into different pieces to fit the needs of different roles. In comparison, if the presenter can change his identity flexibly, he can switch between adaptive selves depending on contexts and does not need to maintain a divided self.\\n\\nI propose that inflexibility can indicate inauthenticity in interpersonal perception because if the perceiver believe a presenter is unable to change his identity flexibly, he has to “wear different masks” to hide his true self to fit in different social contexts (Goffman, 1959).\\xa0 In addition, the flexibility manipulation told participants interpretation is about how the poster view himself. Flexibility means the poster can switch between specialized selves in different contexts, which can strengthen the perception of versatility. Therefore, interpreting inconsistent attributes as flexible (vs. inflexible) identities of the poster might bring more negative impressions.\\n\\nBesides inauthenticity, I assume the perception of versatility as another consequence of perception of inconsistent attributes. I propose that when perceivers observe inconsistent attributes in a presenter, such as being a book lover and a nightclubber, they are likely to perceive versatility about the presenter, which I define as the perceivers’ perception of the presenter’s ability to integrate inconsistent attributes. \\n\\nThe ability of integrating inconsistent attributes suggest the presenter can act adaptively and flexibly in different social roles (Bigler, Neimeyer, and Brown, 2001), which are likely to be viewed positively by others. \\n\\nAttribute inconsistencies are perceived at the attribute level (i.e., To what extent are the attributes inconsistent among each other), while versatility is perceived at the individual level (i.e., To what extent is the presenter a versatile person). \\n\\nPerceived versatility tends to be high when the perceivers believe what the presenter shows on social media is true. If the perceivers interpret the presenter’s inconsistent attributes to be inauthentic, they will not judge the presenter to be versatile because the signaled attributes lose cue validity (Brunswik, 1956). Instead, the only attribute they learn about the presenter is his inauthenticity. If perceivers believe the perceiver is versatile enough to integrate inconsistent attributes, inconsistencies could be beneficial to impression favorability for several potential reasons.\\n\\nFirst, if the personal attributes revealed by the posts make the presenter look versatile, the perceivers might feel they know a lot about the presenter (Greene et al., 2006). According to the theories in self disclosure and relational uncertainty perception, perceivers tend to like the presenter if they feel they learn a lot about him (Berger, 1993; Collins & Miller, 1994). In other words, inconsistent (*vs*. consistent) attributes can reveal more diverse information about the presenter and improve impression favorability. Second, processing a diverse set of inconsistent attributes often reflects autonomy and competence of the presenter (Phillips & Zuckerman, 2001; Bellezza, Gino, & Keinan, 2013), which can improve impression favorability. Third, breaking norms and showing inconsistent attributes are often found to produce entertainment value (McGraw & Warren, 2010; Knop-Huelss, Rieger, & Schneider, 2020). Perceivers might believe that presenters with inconsistent (*vs.* consistent) attributes have a greater potential to produce interesting/entertaining content. Fourth, when perceivers believe the presenter is a versatile person, they might be less influenced by simple categorization or stereotyping. Instead, multiple categorization or decategorization are more likely to occur (Crisp & Hewstone, 2007; Crisp, Hewstone, & Rubin, 2001), so the perceiver can have a relatively individuated and effortful processing of the presenter (Stern, Marrs, Millar, and Cole, 1984). Consequently, individuated (*vs.* stereotyped) interpersonal perception often leads to positive impressions of the presenter (Kang & Bodenhausen, 2014). Finally, research on interpersonal similarity generally found that perceivers tend to like someone who is similar (*vs.* dissimilar) to themselves (Byrne, 1961). When perceivers feel the presenter is versatile, they might perceive him more favorably, because a diverse set of attributes makes it easier for the perceivers to find similarities between themselves and the presenter. As a result, diversity of the presenter might improve impression favorability by increasing interpersonal similarity perception.\\xa0 To sum, I propose:\\n\\n**Versatility hypothesis (H2):** *Inconsistent attributes make the perceiver believe the presenter to be a versatile person, which increases impression favorability.*\\n\\n!Untitled\\n\\n**Central to this line of\\nresearch is the dispute whether high differentiation can be viewed as an\\nadaptive measure of flexibility in one’s social roles and relationships\\n(i.e., “specialization”) or whether it signifies the lack of a core self (i.e.,\\n“fragmentation”) and thus predicts maladjustment (Donahue et al.,\\n1993).** \\n\\nThe argument bears directly on social constructionist accounts of\\nthe self where self-fluidity, flexibility, and differentiation are viewed as\\npositive predictors of an adaptive capacity that is functional within a\\npostmodern world (Gergen, 1991). The alternative viewpoint, advanced\\nby Donahue and colleagues (1993), is that such differentiation may be\\nbetter understood as reflecting fragmentation in the self-system and\\nmay better predict psychological maladjustment.\\n\\nIn contrasting SCD with self-complexity, researchers\\nhave suggested that measures of **SCD may tap the subjective (and\\nostensibly negative) experience of a ‘‘divided self,’’ whereas self-complexity measures may instead tap the (ostensibly positive) experience\\nof specializing within multiple roles. For example, an SCD measure\\nmay draw a person’s attention to the potentially unpleasant experience of wearing ‘‘multiple masks’’ (Goffman, 1959), as one begins to\\nrecognize how differently he or she behaves as a student, sibling,\\nfriend, or romantic partner (Harter, Bresnick, Bouchey, & Whitesell,\\n1997). In contrast, as we later describe in more detail, typical\\nself-complexity measures simply require that participants consider\\npersonally important roles and the traits inherent in each role (e.g.,\\nLinville, 1985, 1987).** Such a process may draw attention away from\\nthe sense of a ‘‘divided self’’ and instead tap flexibility in the manner\\nin which people view themselves (e.g., Diehl, Hastings, & Stanton,\\n2001). In support of the theoretical distinction between self-complexity and SCD, results of a recent study revealed that self-complexity\\n(as assessed by a standard card sort task) was uncorrelated with\\nSCD (assessed by Donahue et al.’s [1993] measure; Constantino &\\nPinel, 2000)\\n\\nSpecifically, SCD is frequently measured by\\nexamining the amount of deviation in one’s assessments of his or her own\\npersonality across a number of different predefined roles. In contrast, Linville’s measure of self-complexity requires **one to independently generate\\nthe roles that are personally meaningful to himself or herself when provided with a set of personality descriptors.** An index of self-complexity is\\nthen calculated that reflects both the number and distinctiveness of roles\\npeople utilize to mentally represent themselves. This distinction between\\nself-generated or artificially imposed roles is potentially important in that\\nit implies that these measures may be tapping two different underlying\\ndimensions of self-structure: flexibility (versus rigidity) and integration\\n(versus fragmentation)\\n\\nLeary\\'s (1957) method of assessing interpersonal flexibility\\ninvolves self-reports on the Interpersonal Check List\\n\\nchecklist comprises 128 items: 8 items for each of 16 interpersonal variables (e.g., aggressive, competitive, docile, rebellious).\\nFor each variable there is 1 item that reflects Level 1 intensity,\\n\"a mild or necessary amount of the trait\"; 3 items that reflect\\nLevel 2 intensity, \"a moderate or appropriate amount of the\\ntrait\"; 3 items that reflect Level 3 intensity, \"a marked or inappropriate amount of the trait\"; and 1 item that expresses Level\\n4 intensity, \"an extreme amount of the trait\" (Leary, 1957, p.\\n455). Flexibility in aggressiveness, for example, would be assessed by having individuals rate whether each of the levels of\\nthe trait (e.g., Level 1, can be frank and honest; Level 4, hardhearted) is self-descriptive. A profile of scores on each of the\\n16 interpersonal variables is derived from these item ratings.\\nFlexibility is indexed by the number of Level 2 responses.\\n\\nSnyder\\'s (1974) Self-Monitoring\\nScale, a 25-item true-false self-report scale containing questions about five aspects of self-monitoring. These aspects include (a) concern with the social appropriateness of one\\'s selfpresentation, (b) attention to social comparison information as\\ncues to appropriate self-expression, (c) the ability to control and\\nmodify one\\'s self-presentation and expressive behavior\\n\\nBriggs, Cheek, and Buss\\n(1980), found the scale to be a combination of three factors:\\nExtraversion, Other-Directedness, and Acting Skill\\n\\nIn a recent report (Paulhus & Martin, 1987), we described a\\nself-report inventory designed to assess a variety of interpersonal capabilities: the Battery of Interpersonal Capabilities\\n(BIC). Respondents are asked four questions about their ability\\nto enact each of a series of interpersonal attributes. For each\\nattribute, subjects are asked a direct capability question, for example, \"How capable are you of being dominant when the situation requires it?\" Three additional questions were asked to assess (a) the difficulty of performing each behavior, (b) anxiety\\nwhen performing each behavior, and (c) the tendency to avoid\\nsituations demanding such behavior. Responses to all questions\\nare rated on 7-point Likert scales anchored by very much (7)\\nandnorafo//(l).\\n\\nThis emergence of a positive manifold structure (i.e., no negative correlations) for capabilities is a critical finding for the\\nanalysis of interpersonal flexibility. Respondents do not have to\\ncontradict themselves to report a wide range of capabilities. An\\nindividual who claimed all 16 capabilities in the circumplex\\nwould be maximally flexible, and the individual who claimed\\nonly one would be maximally rigid. An appropriate index of\\nflexibility would be some composite of all 16 capabilities. In\\nfact, given that there are no negative intercorrelations among\\nthe capabilities, the simple sum seems psychometrically reasonable.\\nIn short, the BIC provides an ideal inventory for assessing\\nfunctional flexibility. It assesses the two criteria of flexibility:\\nthe breadth of the interpersonal repertoire and the ability to\\nadjust behavior in accordance with situational demands\\n\\n1. Does X deal well with social situations? Very poorly Poorly So-so Very well 2. Does X tend to avoid certain situations? Never Seldom Sometimes Often Very often 3. How easily does X adjust to new social situations? Very easily Easily So-so Has trouble A lot of trouble\\n\\n4. Is X a likable person? Not at all Not much Somewhat Likable Very likable 5. How often does X act inappropriately for the situation? Never Seldom Sometimes Often Very often', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='016de1a4-0f4f-468c-8b29-c51e53236793', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n杂 thesis\\n\\n总结所有的two type model 有啥区别，包括human nature; human unique; warmth competence; agency experience; \\n\\n人们为何不喜欢inconsistent stuff\\n\\n*Confifirmatory categorization*\\n\\nUpon receiving additional information, perceivers attempt to preserve the initial\\n\\ncategorization. Originally, Fiske and Neuberg (1990) reviewed evidence that\\n\\nconfifirmatory categorization was encouraged by certain information conditions\\n\\n—namely, a label plus category-consistent attributes; a label plus mixed attri -\\n\\nbutes; or a strong label plus judgment-irrelevant, category-irrelevant attributes.\\n\\nIn addition, Fiske and Neuberg speculated that certain motivators (such as self\\n\\nesteem threat) might *increase* the likelihood that a perceiver would view target\\n\\ninformation as consistent with the initial categorization, whereas other motivat -\\n\\nors (such as task outcome dependency, fear of invalidity, and account ability to\\n\\nan audience with unknown attitudes) might *reduce* the likelihood that a perceiver\\n\\nwould view target information as consistent with the initial categorization. Since\\n\\nthe publication of the original chapter, new evidence suggests not only that\\n\\nsuccessful confifirmatory categorization requires perceivers to attend effortfully\\n\\nto stereotype consistencies, but that such efforts indeed can be triggered by threats\\n\\nto oneself or one’s ingroup (Fiske & Leyens, 1996) and by the need to justify\\n\\none’s power position (Goodwin, Gubin, Fiske, & Yzerbyt, 1998).\\n\\n不管是presenter自己的label，还是perceive view。本质上是一个socially constructed concept; 所以说，整个研究都是highly norm dependent\\n\\n**要改叫connectionist models\\xa0 ；**\\n\\nConnectionist modeling (e.g., Hertz, Krogh, & Palmer, 1991; McClelland &\\n\\nRumelhart, 1986; Rumelhart & McClelland, 1986) treats the processing in[1]\\n\\nvolved in perceptual and cognitive tasks in terms of the passage of activa[1]\\n\\ntion, in parallel, among simple, neuron-like units. The most important com[1]\\n\\nponents of these models are: (a) simple processing units or nodes, which\\n\\nsum the incoming activation, following a specified equation, and then send\\n\\nthe resulting activation to the nodes to which they are connected, (b)\\n\\nequations that determine the activation of each node at each point in time,\\n\\nbased on the incoming activation from other nodes, previous activation, and\\n\\nthe decay rate, (c) weighted connections between the nodes, where the\\n\\nweights affect how activation is spread, and (d) a learning rule that specifies\\n\\nhow the weights change in response to experience (Bechtel & Abrahamsen,\\n\\n1991). Processing in a connectionist model proceeds solely by the spread\\n\\nof activation among nodes, where the pattern of connections affects how\\n\\nactivation spreads. There is no higher order executive or control process.\\n\\nMoreover, knowledge in a connectionist model is represented entirely in the pattern of weights among nodes.\\n\\nWe propose that making sense of people through conceptual integration,\\n\\nexplanation, and analogy can all be understood in terms of cognitive m echa[1]\\n\\nnisms for maximizing coherence. When integrating information about a\\n\\nperson, we attem pt to achieve coherence among concepts by reconciling\\n\\nconflicts among the different pieces of information that we have about an\\n\\nindividual (Kunda & Thagard, 1996). Knowing that someone who is a lawyer\\n\\nresponded meekly to an insult requires us to balance conflicting expecta[1]\\n\\ntions generated by the stereotype that lawyers are aggressive and the\\n\\nunaggressive behavior (Kunda, Sinclair, & Griffin, 1997).\\n\\n讨论几个核心theory Fisk, Susan T.; Neuberg, Steven L. (1990).\\n\\n(Freeman & Ambady, 2011; Kunda & Thagard, 1996; Read & Miller, 1998),\\n\\nThese critiques were taken quite seriously, and many espoused instead a\\n\\nmodern, information-processing version of the Gestalt approach. As the social\\n\\ncognitive literature developed, aspects of the Gestalt approach informed\\n\\nresearchers’ understanding of social categorization processes—a view repre -\\n\\nsented in schema, category, prototype, and stereotype models (for a review,\\n\\nsee Fiske & Taylor, 1991, Ch. 6). Theorists proposed both a richer role for\\n\\nperceivers’ prior knowledge in organizing their thinking about newly\\n\\nencountered people, and a more confifigural approach to understanding how\\n\\nelements of this prior knowledge might interact to create a more holistic\\n\\nmeaning. This approach, too, garnered signifificant empirical support.\\n\\nAnd so the debate between the two approaches raged, until it was declared\\n\\nunresolvable, given the theories and methods then available (Ostrom, 1977).\\n\\nSynthesizing the two approaches was the aim of the continuum model.\\n\\ngorized as a friendly person; or they can be verbally transmitted category labels, as when a friend introduces a new acquaintance as an “electrical engineer.” Many features can potentially elicit\\n\\nsocial categories, which then serve to organize and constrain the meaning and usefulness of subsequently identifified features or attributes\\n\\n能否把intention inference 和inauthenticity 结合起来；一个人inauthentic，不见得说明他的Intention is bad; 有可能他是无意识的，根据自己本性来show off，show off这个事他并不觉得自己虚假\\n\\nWalther and Parks (2002) proposed that the warranting value of information is “derived from the receiver’s perception about the extent to which the content of that information is immune to manipulation by the person to whom it refers” (p. 552).\\n\\n有可能发现intrapersonal diff之后觉得这个人更authentic，因为他没有试图塑造一个符合social norm的形象\\n\\nself as a multifaceted cognitive structure -》 social perceived identity as a multifaceted cognitive structure\\n\\n关于是否存在multifaceted cognitive structure in social perception\\n\\nThese ideas have found their way into modern psychology in that many social psychologists view the self as a multifaceted cognitive structure (Kihlstrom & Cantor, 1984; Markus & Nurius, 1986; Markus & Wurf, 1987; Strauman & Higgins, 1988).\\n\\n有可能发现intrapersonal diff之后觉得这个人更authentic，因为他没有试图塑造一个符合social norm的形象\\n\\n关于entity theorist\\n\\nAn array of research has shown that implicit theories\\ninfluence how people see others, themselves, and social\\ngroups. In typical experiments, participants’ implicit theories are measured by a questionnaire or are manipulated via\\ninstruction set. Studies have shown, for example, that entity\\ntheorists (compared to incremental theorists) draw stronger\\ninferences from behavior (e.g., Chiu, Hong, & Dweck,\\n1997; Hong, Chiu, Dweck, & Sacks, 1997), blame themselves more following failure (e.g., Erdley, Cain, Loomis,\\nDumas-Hines, & Dweck, 1997), and form and endorse more\\nextreme group stereotypes (e.g., Levy, Stroessner, &\\nDweck, 1998). Findings such as these have been obtained\\nfor men and women, children and adults, and members of\\nindependent and interdependent cultures. A consistent\\ntheme is that entity theorists are more ready to see others’\\nbehaviors as stable, consistent, and diagnostic of their underlying attributes than incremental theorists\\n\\n\"The kind of person someone is, is something basic about\\nthem, and it can\\'t be changed very much\"; \"People can do things\\ndifferently, but the important parts of who they are can\\'t really be\\nchanged\"; \"Everyone is a certain kind of person, and there is not much\\nthat they can do to really change that.\" Each item is accompanied by\\na scale ranging from 1 to 6 (1 = strongly agree, 2 = agree, 3 = mostly\\nagree, 4 = mostly disagree, 5 = disagree, 6 = strongly disagree)\\n\\n\\n\\n更深入的讨论positive and negative effect at same time；如何来理解这点\\n\\nIn Study 1, 87 undergraduates wrote descriptions of three separate memories: one in which they felt authentic, one in which they felt inauthentic, and a vivid, emotional memory. Thematic analysis identified five dimensions of authenticity (relational authenticity, resisting external pressures, expression of true self, contentment, owning one\\'s actions) and 4 dimensions of inauthenticity (phoniness, suppression, self-denigration, and conformity).\\n\\nPast research has shown that East Asians are more tolerant of apparent contradiction and tend to accept contradictory beliefs more readily than Americans. The present research examined through three studies whether such a dialectical tendency among East Asians also would be found in beliefs about the self. The results showed that in all three studies, Koreans displayed inconsistent beliefs about the self across contexts more than Americans. Unlike Americans, Koreans considered themselves more extroverted when asked how extroverted they were than when asked how introverted they were (Study 1). Koreans also exhibited less consistent beliefs about their relative standings on various personality dimensions than did Americans (Study 2). For example, Koreans evaluated their relative honesty differently when asked how honest they were than when asked how dishonest they were compared to their peers. Koreans also exhibited greater fluctuations in value preferences than did Americans (Study 3). Some implications of the present findings are discussed.\\n\\nself-presenters believed that their profiles portrayed them\\n\\nas better than reality on certain dimensions of self (e.g., ‘‘funny,’’ ‘‘adventurous,’’ ‘‘outgoing’’),\\n\\naccurately on other dimensions (e.g., ‘‘physically attractive,’’ ‘‘creative’’), and\\n\\nworse than reality on yet other dimensions (‘‘intelligent,’’ ‘‘polite,’’ ‘‘reliable’’). Participants\\n\\nbelieved that their own profile postings made them come across more positively\\n\\nthan reality, but Friends’ postings made them come across more negatively than reality.\\n\\nThe results are generally consistent with the Hyperpersonal model’s notion of selective\\n\\nself-presentation.\\n\\n1. When\\n\\ntwo representations fit together, there is a positive constraint between them:\\n\\nIf you apply one of the representations to someone, then you will tend to\\n\\napply the other representation as well. If two representations conflict, there\\n\\nis a negative constraint between them: If you apply one of the representations\\n\\nto someone, then you will tend not to apply the other representation. Coming\\n\\nup with a coherent interpretation of people is a matter of applying some\\n\\nrepresentations to them and not applying others. Generally, coherence is a\\n\\nm atter of accepting some representations and rejecting others in a way that\\n\\nmaximizes compliance with positive and negative constraints.\\n\\nThagard and Verbeurgt (1998) provide a general definition of coherence\\n\\nproblems. A coherence problem arises when one encounters a set of ele\\n\\nments that mutually constrain each other, and wishes to accept some of\\n\\nthese elements and reject the remaining ones. For example, one needs to\\n\\ndecide which of a set of interrelated traits are characteristic of John (the\\n\\naccepted elements) and which are not (the rejected elements). The con\\n\\nstraints among the elements may be positive or negative. A positive con\\n\\nstraint among two elements means that the two should go together—they\\n\\nshould both be accepted or both be rejected. For example, if John is loving,\\n\\nhe should be kind as well, and if he is not loving, he should not be kind\\n\\neither. A negative constraint means that the two elements should not go\\n\\ntogether—if one is accepted, the other should be rejected. For example, if\\n\\nJohn is loving, he should not be hateful; if he is hateful, he should not be\\n\\nloving. Each of the constraints carries a weight that reflects its importance.\\n\\nWhen partitioning the elements into the accepted set and the rejected\\n\\nset, it is often not possible to satisfy all of the constraints because they may\\n\\nconflict with each other.\\n\\n1. Each element is represented as a unit (node) in a network of units.\\n\\nThese units are very roughly analogous to neurons or groups of neurons in\\n\\nthe brain.\\n\\n1. A positive constraint between two elements is represented as an ex\\n\\ncitatory link between the corresponding units. Each link has a weight rep\\n\\nresenting the strength of the constraint, as determined, for exam pie, by the\\n\\nstrength of association between two concepts.\\n\\n1. A negative constraint between two elements is represented as an in\\n\\nhibitory link between the corresponding units.\\n\\n1. Each unit is assigned an equal initial activation, say .01. The activation\\n\\nof all the units is then updated in parallel. The updated activation of a unit\\n\\nis calculated on the basis of its current activation, the activation of the units\\n\\nto which it is linked, and the weights of these links. The activation of a given\\n\\nunit is increased with the activation of units to which it has excitatory links,\\n\\nand decreased with the activation of units to which it has inhibitory links.\\n\\nA number of equations are available for specifying how this updating is done\\n\\n(McClelland & Rumelhart, 1989). Typically, activation is constrained to re\\n\\nmain between a minimum (e.g., -1) and a maximum (e.g., +1).\\n\\n1. The network goes through many cycles in which the activation of all\\n\\nunits is updated. Updating is repeated until all units have settled, that is,\\n\\nachieved stable activation values that change only minimally from one cycle\\n\\nto another.\\n\\n1. If a unit’s final activation exceeds a specified threshold (e.g., 0), then\\n\\nthe element represented by that unit is deemed to be accepted. Otherwise,\\n\\nthat element is rej\\n\\nThe parallel constraint-satisfaction model of impression formation as\\n\\nsumes that a coherent impression of the person is achieved through parallel\\n\\nsatisfaction of the constraints imposed by the many concepts applied to the\\n\\nperson. This view of impression formation is quite different from the one\\n\\nadvocated by earlier serial models of impression formation (Brewer, 1988;\\n\\nFiske & Neuberg, 1990). These serial models assume that people first try to\\n\\nmake sense of other people by applying stereotypes. They may then use\\n\\nindividuating information such as traits and behaviors if they are strongly\\n\\nmotivated to understand the person or if they cannot successfully catego\\n\\nrize the person as belonging to any particular stereotype. Thus, the serial\\n\\nmodels give special, dominating status to stereotypes. In contrast, the par\\n\\nallel constraint-satisfaction model does not. It treats all kinds of information\\n\\nas equal in status, and assumes that their impact depends entirely on their\\n\\npatterns of association with other pieces of information.\\n\\nSuch asso\\n\\nciations may be learned through direct observation of nurses or Nazis as\\n\\nwell as through cultural transmission.\\n\\nSociological theory offers another perspective, suggesting that as the number of roles that a person holds increases, the commitment to each role decreases (Thoits, 1983). Thus, when people hold multiple roles, they may have a relatively low investment in any one role, so that a setback related to one role has little impact on the self as a whole (Niedenthal et al., 1992).\\n\\n在poster 的角度作为一个presentation strategy; slash person\\n\\n个体的diversity 对自己的意义是什么，就算有好处，也不见得需要po出来\\n\\nBecause of the\\n\\n有可能是自己没意识到inconsitent，有可能是故意post inconsistent\\n\\n在perceiver的角度能意识到自己的局限性。在你眼里是inauthentic的，在poster和其他人眼里可能并不是; 增强empathy\\n\\nOn a deeper level, the perception of attribute inconsistency could come from the relationship between the higher order personality attributes reflected by lower-level attributes (Hampson, John, and Goldberg, 1986; Markon, 2009). For instance, perceivers might feel high inconsistency about a person who is both a book lover and a frequent night clubber, because loving books suggests the person is likely to be introverted while nightclubbing suggests the person is likely to be extroverted. Given the opposite conception of extroversion and introversion, perceivers will naturally feel inconsistencies about the person.\\n\\nAs its centerpiece, the model tackled a contradiction between the litera -\\n\\nture on impression formation and social cognition—a contradiction pitting\\n\\nelemental, algebraic approaches to impression formation against Gestalt, holistic,\\n\\nconfifigural approaches (Asch, 1946). The elemental, piecemeal view of impres -\\n\\nsion forma tion posited that people form evaluative impressions of others by\\n\\ncomputing a weighted average of the isolated evaluations of the targets’ features\\n\\n(Anderson, 1981). For instance, a person known to be intelligent and altru -\\n\\nistic should be evaluated favorably, because intelligence and altruism are each\\n\\npositively viewed characteristics (for most perceivers). Indeed, Anderson’s\\n\\ninformation integration model—the standard bearer of the elemental approach\\n\\n—did quite well in predicting people’s evaluations of others.\\n\\nResearchers expressed two main concerns with this and similar models,\\n\\nhowever. First, many believed that the piecemeal processes articulated by\\n\\nsuch models are psychologically peculiar, and perhaps even impossible for\\n\\npeople to perform. Second, the elemental models viewed the meaning of each\\n\\ncharacteristic as fifixed, not inflfluenced by the other characteristics possessed\\n\\nby the target. For instance, regardless of whether it is paired with “altruistic”\\n\\nor “cruel”, the evaluation and meaning of “intelligent” is presumed to remain\\n\\nthe same. This assumption conflflicts with Gestalt, confifigural approaches, which\\n\\nposit that a characteristic’s meaning can change in light of a target’s other\\n\\ncharacteristics—that “intelligent” may mean something different and be valued\\n\\ndifferently, depending on whether it coexists with “altruistic” or “cruel” (for\\n\\na review of this controversy, see Leyens & Fiske, 1994)\\n\\nThere is an intriguing absence of literature documenting positive outcomes of norm transgressions, perhaps due to implicit assumptions that norm violations will be met with disapproval in society and will trigger negative reactions.\\n\\nTraditionally, adherence to norms was viewed as a way to enforce the social contract, under the threat of (perceived) formal or informal punishments for norm violations. Conformity is fostered by individuals’ fear of receiving sanctions in the event of breaking the norm, as well as the appeal of rewards for going along with the social norm. Moreover, conventional wisdom and previous literature suggest that when a social norm is challenged or broken, negative affective consequences (e.g., embarrassment, frustration, and discomfort) will occur for the person breaking the norm. The two prominent affective markers associated with norm breaking and norm compliance—as identified by Scheff (1988)—are the emotions of shame and pride, respectively\\n\\nPerhaps the most basic interpersonal goal facilitated by\\n\\nmind perception, however, is to affiliate and connect with\\n\\ndesirable others (Baumeister & Leary, 1995). This goal\\n\\nrequires people to think about others ’ existing impressions\\n\\nof the self, to behave in ways that are likely to maintain\\n\\nthose favorable impressions, and to consider how one ’ s\\n\\nown behavior is likely to be interpreted by others (Snyder,\\n\\n1974).\\n\\n[[other to-do]]\\n\\n[[所有人的日程]]', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cc191650-aa61-47f2-b868-4d7d20cb86c4', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Parameters**\\n\\n**`input_dict`**: dictDictionary that specifies \\'value_col\\', \\'id_cols\\', \\'pool_cols\\' and \\'date_col\\'. These parameters are constant among all the base estimators specified by \\'model_dict\\'.\\xa0**The allowable key-value pairs are as follows**:\\n\\n**`id_cols`**: list(str)List of strings that specifies the list of partition columns to compute base estimates for to e.g.: [\"p_id\", \"l_id\"]. \\'id_cols\\' should represent the partition cols in which \\'value_col\\' is given. Performing anomaly detection at the very bottom level of the product-location hierarchy, \\'id_cols\\' need to populate the product-location (SKU-store) sales values.\\n**`pool_cols`**: list(str)List of strings that specifies the list of partition cols to estimate uplift statistics at e.g.: [\\'line_idx\\', \\'location_group_id_idx\\'].\\n**`date_col`**: strStrings that specifies the date column to order history e.g.,: \"d_id\"\\n\\n**`model_dict`**: dictDictionary that specifies the method of anomaly detection to perform. Multiple methods can be input through the key-value pairs of \\'model_dict\\'. Each key-value represents an individual method of anomaly detector.\\xa0**The allowable key-value are as follows**:\\n\\n**`value_col`**: strSpecifies the value column to perform anomaly detection at e.g.,: \\'qty\\'.\\n**`base_col`**: strSpecifies the column containing the base estimate for the sales quantity given by value_col e.g.,: \\'qty_be\\'.\\n**`method`**: strSpecifies the method to perform base estimation. The supported methods include:\\n• stock-out\\n• mis-promo\\n• spike\\n• negative\\n• edge\\n**`method_params`**: dictDictionary that specifies the algorithm for anomaly method specified by \\'method\\' and the corresponding parameters.\\xa0**Supported key-value pairs are as follows**:\\n**`kind`**: strSpecifies the algorithm to perform the detection method specified by \\'method\\'.\\xa0**The supported algorithms are as follows**:\\n• **`stock-out`**: \\'consecutive-zero-count\\' and \\'consecutive-zero-prob\\'.\\n• **`mis-promo`**: \\'inter-quartile-range\\'.\\n**`consecutive_zero_count_th`**: intTriggers an anomaly if the number of consecutive zero values exceeds this value, consecutive-zero-count threshold. If \\'kind\\' is \\'consecutive-zero-prob\\': Trigger \\'stock-out\\' anomaly by evaluating the probability of consecutive zero sales values via hard thresholding. This algorithm fits a Negative-Binomial (NB) model to estimate zero sales probability. The maximum likelihood is currently used for parameter estimation of NB model.\\xa0**The supported kwargs are as follows**:\\n**`consecutive_zero_prob_th`**: intTriggers an anomaly if the probability of consecutive zero values is lower than this value, consecutive-zero-probability threshold.\\n• \\'mis-promo\\' if \\'kind\\' is \\'inter-quartile-range\\': Uses a lower (25%) and an upper (75%) inner percentile range to establish the distribution of the remainder. Limits are set by a certain factor above and below the inner percentile range. Trigger any remainders beyond the limits as anomalies.\\xa0**The supported kwargs are as follows**:\\n**`lower_percentile`**: floatSpecifies the lower percentile of inter-percentile-range.\\n**`upper_percentile`**: floatSpecifies the upper percentile of inter-percentile-range\\n**`iqr_factor`**:Floating value that the inner and the outer (outlying) ranges.\\n**`percentile_accuracy`**: floatSpecifies the accuracy parameter to pass to \\'approx_percentile\\' of apache spark. The accuracy parameter is a positive numeric literal, which controls approximation accuracy at the cost of memory. A higher value of accuracy yields better accuracy, 1.0/accuracy is the relative error of the approximation. When a percentage is an array, each value of the percentage array must be between 0.0 and 1.* In this case, returns the approximate percentile array of column col at the given percentage array. For more details, see\\xa0approx_percentile.\\n**`sigma_factor`**: floatSpecifies k factor in k-sigma-rule. \\'spike\\' if \\'kind\\' is \\'inter-quartile-range\\': Uses a lower (25%) and an upper (75%) inner percentile range to establish the distribution of the remainder. Limits are set by a certain factor above the inner percentile range. Trigger any remainders beyond the limits as anomalies.\\xa0**The supported kwargs are as follows**:\\n**`lower_percentile`**: floatSpecifies the lower percentile of inter-percentile-range.\\n**`upper_percentile`**: floatSpecifies the upper percentile of inter-percentile-range.\\n**`iqr_factor`**: floatFloating value that the inner and the outer (outlying) ranges.\\n**`percentile_accuracy`**: floatSpecifies the accuracy parameter to pass to \\'approx_percentile\\' of apache spark. The accuracy parameter is a positive numeric literal, which controls approximation accuracy at the cost of memory. A higher value of accuracy yields better accuracy, 1.0/accuracy is the relative error of the approximation. When a percentage is an array, each value of the percentage array must be between 0.0 and 1.* In this case, returns the approximate percentile array of column col at the given percentage array. For more details, see\\xa0approx_percentile.\\n**`sigma_factor`**: floatFloating value that specifies k factor in k-sigma-rule. \\'negative\\' Trigger any negative value and impute that value by zero (optional, if \\'impute_col\\' is specified). \\'edge\\' if \\'kind\\' is \\'zero\\': Trigger any zero value at the beginning or the end of time-series populated through \\'id_cols\\' and \\'date_col\\'.\\xa0**The supported kwargs are as follows**:\\n**`which`**: floatSpecifies the method to edge anomalies. The supported methods are \\'leading\\', \\'trailing\\' and \\'both\\'. If \\'leading\\', the leading anomalies are only detected. If \\'trailing\\', the trailing anomalies are only detected. If \\'both\\', both the leading and the trailing anomalies are detected. If None, the override operation is skipped. Default is None.\\n**`max_allowable_null_ratio`**: floatFloating value specifies the allowable ratio of null values to interpolate. If the null ratio is greater than \\'max_allowable_zero_ratio\\', the anomaly detection is skipped.\\n**`max_allowable_zero_ratio`**: floatSpecifies the allowable ratio of zero values to interpolate. If the zero ratio is greater than \\'max_allowable_zero_ratio\\', the anomaly detection is skipped.\\n**`min_allowable_history_length`**: floatInteger that specifies the minimum allowable number of samples to interpolate. If the number of samples is less than min_allowable_history_length\\', the anomaly detection is skipped.\\n**`override_anomalies`**: strSpecifies the method to override anomalies. The supported methods are \\'leading\\', \\'trailing\\', \\'both\\' and None. If \\'leading\\', the leading anomalies are overridden. If \\'trailing\\', the trailing anomalies are overridden. If \\'both\\', both the leading and the trailing anomalies are overridden. If None, the override operation is skipped. Default is None.\\n**`zero_offset`**: float (optional)Floating value to offset the value column. \\'zer_offset\\' parameter is only used in \\'mis-promo\\' and \\'spike\\' methods.\\n**`driver_cols`**: list(dict)List of dictionaries containing driver columns. \\'driver_cols\\' is only required for \\'mis-promo\\' and \\'spike\\' anomaly detection methods. \\'mis-promo\\' detection performs detection only observations driven by \\'driver-cols\\'. \\'spike\\' detection skips observations driven by \\'driver_cols\\'. Each dictionary must include the following key values:\\n**`col_name`**: strSpecifies driver column name. \\'operation\\' : str Specifies the logical operator to flag promotion points. The valid values for operation are \\'=\\', \\'\\', \\'==\\', \\'!=\\', \\'isNull\\' and \\'isNotNull\\'.\\n**`threshold`**: strInteger or floating value that specifies the value to flag promotion points.\\n**`output_cols`**\\xa0: dictDictionary that specifies the output column names.\\xa0**Supported keys for output_cols are as follows**: if \\'method\\' is \\'stock-out\\': \\'flag_col\\' : str Specifies the column name to store anomaly detection results. If 1, an anomaly has occurred, but not otherwise.\\n**`dist_params_col`**: str (optional)Specifies the column name to store the parameters of NB model in the anomaly detection by \\'consecutive-zero-prob\\'. If not specified, skipped.\\n**`exception_col`**: str (optional)Specifies the column name to store collected exceptions if anomaly detection is skipped. If not specified, skipped. If \\'method\\' is \\'mis-promo\\': \\'flag_col\\' : str Specifies the column name to store anomaly detection results. If 1, an upper limit anomaly has occurred, if -1 a lower limit anomaly has occurred, but not otherwise.\\n**`mask_col`**: str (optional)Specifies the column name to flag interpolant points. This column contains the disjunction of \\'driver_cols\\'. \\'mask_col\\' can only be passed to \\'mis-promo\\' and \\'spike\\' detection method. If not specified, skipped.\\n**`exception_col`**: str (optional)Specifies the column name to store collected exceptions if anomaly detection is skipped. If not specified, skipped. if \\'method\\' is \\'spike\\': \\'flag_col\\' : str Specifies the column name to store anomaly detection results. If 1, an anomaly has occurred, but not otherwise.\\n**`mask_col`**: str (optional)Specifies the column name to flag interpolant points. This column contains the disjunction of \\'driver_cols\\'. \\'mask_col\\' can only be passed to \\'mis-promo\\' and \\'spike\\' detection methods. If not specified, skipped.\\n**`exception_col`**: str (optional)Specifies the column name to store collected exceptions if anomaly detection is skipped. If not specified, skipped. if \\'method\\' is \\'negative\\': \\'flag_col\\' : str Specifies the column name to store anomaly detection results. If 1, an anomaly has occurred, but not otherwise.\\n**`impute_col`**: str (optional)Specifies the column name to store imputed \\'value_col\\' for negative values by zero. If not specified, skipped. if \\'method\\' is \\'edge\\': \\'flag_col\\' : str Specifies the column name to store anomaly detection results. If 1, an anomaly has occurred, but not otherwise.\\n**`exception_col`**: str (optional)Specifies the column name to store collected exceptions if anomaly detection is skipped. If not specified, skipped.\\n\\n**`flag_col`**: strSpecifies the column name to store aggregate anomaly detection results, inclusive of all the anomaly flags. If \\'flag_col\\' does not exist, a new column will be created.\\n\\n**`repartition_cols`**: list(str) (optional)List of strings that specifies the list of columns to re-partition after applying pandas-udf.\\n\\n**`compute_mode`**: str (optional)Specifies the computation mode. The supported modes are \\'spark\\' and \\'pandas-udf\\'. The default mode is \\'spark\\'.\\n\\n**The the dict below is an example that runs \\'stock-out\\' anomaly detection**:\\n\\n```\\n{\\n   \"flag_col\":\"ad_flag\",\\n   \"input_dict\":{\\n      \"id_cols\":[\\n         \"p_id\",\\n         \"l_id\"\\n      ],\\n      \"pool_cols\":[\\n         \"line_idx\",\\n         \"location_group_id_idx\"\\n      ],\\n      \"date_col\":\"d_id\"\\n   },\\n   \"model_dict\":{\\n      \"0\":{\\n         \"value_col\":\"qty\",\\n         \"method\":\"stock-out\",\\n         \"method_params\":{\\n            \"kind\":\"consecutive-zero-count\",\\n            \"consecutive_zero_count_th\":2\\n         },\\n         \"min_allowable_history_length\":10,\\n         \"max_allowable_null_ratio\":0.8,\\n         \"max_allowable_zero_ratio\":0.4,\\n         \"zero_offset\":1,\\n         \"output_cols\":{\\n            \"flag_col\":\"czvalue_f\",\\n            \"exception_col\":\"exception_ad_0\"\\n         }\\n      },\\n      \"1\":{\\n         \"value_col\":\"qty\",\\n         \"method\":\"stock-out\",\\n         \"method_params\":{\\n            \"kind\":\"consecutive-zero-prob\",\\n            \"consecutive_zero_prob_th\":0.02\\n         },\\n         \"min_allowable_history_length\":10,\\n         \"max_allowable_null_ratio\":0.8,\\n         \"max_allowable_zero_ratio\":0.4,\\n         \"zero_offset\":1,\\n         \"output_cols\":{\\n            \"flag_col\":\"pczvalue_f\",\\n            \"dist_params_col\":\"dist_params\",\\n            \"exception_col\":\"exception_ad_1\"\\n         }\\n      }\\n   }\\n}\\n\\n```\\n\\nText\\n\\nCopy\\n\\n**The the dict below is an example that runs \\'mis-promo\\' anomaly detection**:\\n\\n```\\n{\\n   \"flag_col\":\"ad_flag\",\\n   \"input_dict\":{\\n      \"id_cols\":[\\n         \"p_id\",\\n         \"l_id\"\\n      ],\\n      \"pool_cols\":[\\n         \"line_idx\",\\n         \"location_group_id_idx\"\\n      ],\\n      \"date_col\":\"d_id\"\\n   },\\n   \"model_dict\":{\\n      \"0\":{\\n         \"value_col\":\"qty\",\\n         \"method\":\"stock-out\",\\n         \"method_params\":{\\n            \"kind\":\"consecutive-zero-count\",\\n            \"consecutive_zero_count_th\":2\\n         },\\n         \"min_allowable_history_length\":10,\\n         \"max_allowable_null_ratio\":0.8,\\n         \"max_allowable_zero_ratio\":0.4,\\n         \"zero_offset\":1,\\n         \"output_cols\":{\\n            \"flag_col\":\"czvalue_f\",\\n            \"exception_col\":\"exception_ad_0\"\\n         }\\n      },\\n      \"1\":{\\n         \"value_col\":\"qty\",\\n         \"method\":\"stock-out\",\\n         \"method_params\":{\\n            \"kind\":\"consecutive-zero-prob\",\\n            \"consecutive_zero_prob_th\":0.02\\n         },\\n         \"min_allowable_history_length\":10,\\n         \"max_allowable_null_ratio\":0.8,\\n         \"max_allowable_zero_ratio\":0.4,\\n         \"zero_offset\":1,\\n         \"output_cols\":{\\n            \"flag_col\":\"pczvalue_f\",\\n            \"dist_params_col\":\"dist_params\",\\n            \"exception_col\":\"exception_ad_1\"\\n         }\\n      }\\n   }\\n}\\n\\n```\\n\\nText\\n\\nCopy\\n\\n**The the dict below is an example that runs \\'spike\\' anomaly detection**:\\n\\n```\\n{\\n   \"input_dict\":{\\n      \"id_cols\":[\\n         \"p_id\",\\n         \"l_id\"\\n      ],\\n      \"pool_cols\":[\\n         \"line_idx\",\\n         \"location_group_id_idx\"\\n      ],\\n      \"date_col\":\"d_id\"\\n   },\\n   \"model_dict\":{\\n      \"0\":{\\n         \"method\":\"mis-promo\",\\n         \"method_params\":{\\n            \"kind\":\"inter-quartile-range\",\\n            \"lower_percentile\":0.25,\\n            \"upper_percentile\":0.75,\\n            \"iqr_factor\":3.5,\\n            \"percentile_accuracy\":1000,\\n            \"sigma_factor\":2\\n         },\\n         \"value_col\":\"qty\",\\n         \"base_col\":\"qty_be_stl\",\\n         \"driver_cols\":[\\n            {\\n               \"col_name\":\"f_discount\",\\n               \"operator\":\">\",\\n               \"threshold\":0.1\\n            },\\n            {\\n               \"col_name\":\"m_discount\",\\n               \"operator\":\">\",\\n               \"threshold\":0.1\\n            },\\n            {\\n               \"col_name\":\"l_discount\",\\n               \"operator\":\">\",\\n               \"threshold\":0.1\\n            }\\n         ],\\n         \"output_cols\":{\\n            \"flag_col\":\"p2575_f\",\\n            \"mask_col\":\"mask_0\",\\n            \"exception_col\":\"exception_ad_0\"\\n         },\\n         \"min_allowable_history_length\":10,\\n         \"max_allowable_null_ratio\":0.8,\\n         \"max_allowable_zero_ratio\":0.4,\\n         \"override_anomalies\":\"None\",\\n         \"zero_offset\":1\\n      },\\n      \"1\":{\\n         \"method\":\"mis-promo\",\\n         \"method_params\":{\\n            \"kind\":\"inter-quartile-range\",\\n            \"lower_percentile\":0.10,\\n            \"upper_percentile\":0.90,\\n            \"iqr_factor\":3.5,\\n            \"percentile_accuracy\":1000,\\n            \"sigma_factor\":2\\n         },\\n         \"value_col\":\"qty\",\\n         \"base_col\":\"qty_be_stl\",\\n         \"driver_cols\":[\\n            {\\n               \"col_name\":\"f_discount\",\\n               \"operator\":\">\",\\n               \"threshold\":0.1\\n            },\\n            {\\n               \"col_name\":\"m_discount\",\\n               \"operator\":\">\",\\n               \"threshold\":0.1\\n            },\\n            {\\n               \"col_name\":\"l_discount\",\\n               \"operator\":\">\",\\n               \"threshold\":0.1\\n            }\\n         ],\\n         \"output_cols\":{\\n            \"flag_col\":\"p1090_f\",\\n            \"mask_col\":\"mask_1\",\\n            \"exception_col\":\"exception_ad_1\"\\n         },\\n         \"min_allowable_history_length\":10,\\n         \"max_allowable_null_ratio\":0.8,\\n         \"max_allowable_zero_ratio\":0.4,\\n         \"override_anomalies\":\"None\",\\n         \"zero_offset\":1\\n      }\\n   }\\n}\\n\\n```\\n\\nText\\n\\nCopy\\n\\n**The the dict below is an example that runs \\'negative\\' anomaly detection**:\\n\\n```\\n{\\n   \"input_dict\":{\\n      \"id_cols\":[\\n         \"p_id\",\\n         \"l_id\"\\n      ],\\n      \"pool_cols\":[\\n         \"line_idx\",\\n         \"location_group_id_idx\"\\n      ],\\n      \"date_col\":\"d_id\"\\n   },\\n   \"model_dict\":{\\n      \"0\":{\\n         \"method\":\"spike\",\\n         \"method_params\":{\\n            \"kind\":\"inter-quartile-range\",\\n            \"lower_percentile\":0.25,\\n            \"upper_percentile\":0.75,\\n            \"iqr_factor\":3.5,\\n            \"percentile_accuracy\":1000,\\n            \"sigma_factor\":2,\\n            \"rows_between\":\"(\"\"unbounded_preceding\",\\n            \"unbounded_following\"\")\"\\n         },\\n         \"value_col\":\"qty\",\\n         \"base_col\":\"qty_be_stl\",\\n         \"driver_cols\":[\\n            {\\n               \"col_name\":\"f_discount\",\\n               \"operator\":\">\",\\n               \"threshold\":0.1\\n            },\\n            {\\n               \"col_name\":\"m_discount\",\\n               \"operator\":\">\",\\n               \"threshold\":0.1\\n            },\\n            {\\n               \"col_name\":\"l_discount\",\\n               \"operator\":\">\",\\n               \"threshold\":0.1\\n            },\\n            {\\n               \"col_name\":\"events\",\\n               \"operator\":\">\",\\n               \"threshold\":0\\n            }\\n         ],\\n         \"output_cols\":{\\n            \"flag_col\":\"p75_f\",\\n            \"exception_col\":\"exception_ad_0\"\\n         },\\n         \"min_allowable_history_length\":24,\\n         \"max_allowable_null_ratio\":0.8,\\n         \"max_allowable_zero_ratio\":0.7,\\n         \"override_anomalies\":\"None\",\\n         \"zero_offset\":1\\n      },\\n      \"1\":{\\n         \"method\":\"spike\",\\n         \"method_params\":{\\n            \"kind\":\"inter-quartile-range\",\\n            \"lower_percentile\":0.10,\\n            \"upper_percentile\":0.90,\\n            \"iqr_factor\":3.5,\\n            \"percentile_accuracy\":1000,\\n            \"sigma_factor\":2,\\n            \"rows_between\":\"(\"\"unbounded_preceding\",\\n            \"unbounded_following\"\")\"\\n         },\\n         \"value_col\":\"qty\",\\n         \"base_col\":\"qty_be_stl\",\\n         \"driver_cols\":[\\n            {\\n               \"col_name\":\"f_discount\",\\n               \"operator\":\">\",\\n               \"threshold\":0.1\\n            },\\n            {\\n               \"col_name\":\"m_discount\",\\n               \"operator\":\">\",\\n               \"threshold\":0.1\\n            },\\n            {\\n               \"col_name\":\"l_discount\",\\n               \"operator\":\">\",\\n               \"threshold\":0.1\\n            },\\n            {\\n               \"col_name\":\"events\",\\n               \"operator\":\">\",\\n               \"threshold\":0\\n            }\\n         ],\\n         \"output_cols\":{\\n            \"flag_col\":\"p90_f\",\\n            \"exception_col\":\"exception_ad_1\"\\n         },\\n         \"min_allowable_history_length\":24,\\n         \"max_allowable_null_ratio\":0.8,\\n         \"max_allowable_zero_ratio\":0.7,\\n         \"override_anomalies\":\"None\",\\n         \"zero_offset\":1\\n      }\\n   }\\n}\\n\\n```\\n\\nText\\n\\nCopy\\n\\n**The the dict below is an example that runs \\'edge\\' anomaly detection**:\\n\\n```\\n{\\n   \"input_dict\":{\\n      \"id_cols\":[\\n         \"p_id\",\\n         \"l_id\"\\n      ],\\n      \"date_col\":\"d_id\"\\n   },\\n   \"model_dict\":{\\n      \"0\":{\\n         \"method\":\"negative\",\\n         \"value_col\":\"qty\",\\n         \"output_cols\":{\\n            \"flag_col\":\"negative_f\",\\n            \"impute_col\":\"qty_imp\"\\n         }\\n      },\\n      \"1\":{\\n         \"method\":\"negative\",\\n         \"value_col\":\"qty\",\\n         \"output_cols\":{\\n            \"flag_col\":\"negative_f_1\",\\n            \"impute_col\":\"qty_imp\"\\n         }\\n      }\\n   }\\n}\\n\\n```\\n\\nText\\n\\nCopy\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0b8d4f56-7a17-472a-a705-0565f1ddb59b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='|Name|Tags|question|answer|notes|\\n|---|---|---|---|---|\\n|\"1. Raining in Seattle|\\n|\"|\"VIP, prob\"|\"You\\'re about to get on a plane to Seattle. You want to know if you should bring an umbrella. You call 3 random friends of yours who live there and ask each independently if it\\'s raining. Each of your friends has a 2/3 chance of telling you the truth and a 1/3 chance of messing with you by lying. All 3 friends tell you that \"\"Yes\"\" it is raining. What is the probability that it\\'s actually raining in Seattle?\"|you should estimate the prior probability that it\\'s raining on any given day in Seattle. If you mention this or ask the interviewer will tell you to use 25%. Then it\\'s straight-forward,|\\n|2. Spaghetti|递推公式|\"A bowl contains N spaghetti noodles. You reach into the bowl and grab two free ends at random and attach them. You do this N times until there are no free ends left. On average, how many loops are formed by this process?\"|,|\\n|3. Birthday problem|prob|\"in a set of n randomly chosen people, some pair of them will have the same birthday\"|find the prob that none of the people in the group has the same birthday,|\\n|4. horse puzzle|,\"There are 25 horses. You can race any 5 of them at once, and all you get is the order they finished. How many races would you need to find the 3 fastest horses?\"|https://matt-croak.medium.com/google-interview-25-horses-c982d0a9b3af|画图来解|\\n|5. toss win|prob|A and B throw a Fair die one after another. Whoever throws 6 first wins. What is the probability that A wins ?|,等比数列求和公式要记住，尤其是无限长的时候|\\n|6 casino payout|binomial_prob|\"You\\'re at a casino with two dice, if you roll a 5 you win, and get paid $10. What is your expected payout? If you play until you win (however long that takes) then stop, what is your expected payout?\"|\"注意这是个二项分布的特种，几何分布； 几何分布的期望是E(x)=1/p。代表什么意思呢？|\\n|假如你每次表白的成功概率是60%，同时你也符合几何分布的特点，所以期望E(x)=1/p=1/0.6=1.67|\\n|所以你可以期望自己表白1.67次（约等于2次）会成功\",|\\n|7 Stones Game|prob|\"Consider a game with 2 players, A and B. Player A has 8 stones, player B has 6. Game proceeds as follows. First, A rolls a fair 6-sided die, and the number on the die determines how many stones A takes over from B. Next, B rolls the same die, and the exact same thing happens in reverse. This concludes the round. Whoever has more stones at the end of the round wins and the game is over. If players end up with equal # of stones at the end of the round, it is a tie and another round ensues. What is the probability that B wins in 1, 2, ..., n rounds?\"|,|\\n|8 prob basic|\"VIP, prob\"|What is the probability of getting a pair by drawing 2 cards in a 52 card deck?|\"P(E) = (favorable number of outcomes)/(total number of outcomes) favorable number of outcomes = selecting a pair In a deck of cards, there are 13 different cards and 4 cards of each. We can choose 2 cards from 4 cards as 4C2 (4! /2! * 2!) in 6 ways and we can do this in 13 ways. Therefore, favorable outcomes = 13*6 = 78 ways Total number of outcomes = Selecting 2 cards from a deck of 52 cards 52C2 = 1326 ways So P(E) = 78/1326 ~ 5.88%\",|\\n|9 cdf pdf|\"VIP, cdf_pdf\"|\"What is the Probability Density Function of Z = max(X, Y) where X, Y ~ U(0, 1)?\"|,|\\n|10 coupon use|binomial_prob|A discount coupon is given to N riders. The probability of using a coupon is P. What is the probability that one of the coupons will be used?|\"The probability that one (k=1) of the coupons will be used is np(1-p)^(n-1). For n =2, prob = p(1-p) The probability that two (k=2) of the coupons will be used is n/2(n-1) p^2 (1-p)^(n-2). For n =2, prob = 2p^2/2 = p^2 Likelihood that both person of will use the coupon, given that one has used can be computed by Bayes theorem. Let A = both use the coupon, B = one uses the coupon, therefore P(A|B) = P(B|A) P(A) / P(B) = (1 . p^2)/(p(1-p))= p/((1-p))\"|permutation and combination的基本计算要懂|\\n|11 prob basic 2|prob|\"Counting problem with numbered deck of cards, probability one card is exactly double another\"|,|\\n|12 order flip|prob|\"When flipping a coin infinitely, is the pattern HHT or HTH more likely to appear? What is the probability that one appears before the other?\"|,|\\n|\"12a \"|prob|\"You\\'re given a fair coin. You flip the coin until either Heads Heads Tails (HHT) or Heads Tails Tails (HTT) appears.|\\n|Is one more likely to appear first?|\\n|If so, which one and with what probability?\"|\"HT appears in both. Given that you have an HT, there\\'s a 50% chance of an H preceding it resulting in a HHT win, while for HTT to win you need a T as both a prefix and a suffix T - HT - T so that 1⁄2 * 1⁄2=25%. So the odds are 2⁄1 in favor of HHT , or 66% chance\",|\\n|13 prob basic 3|prob|What is probability of pulling a different color or shape card from a shuttled deck of 52 cards?|,|\\n|14 500 cards|VIP|\"Imagine a deck of 500 cards numbered from 1 to 500. If all the cards are shuffled randomly and you are asked to pick three cards, one at a time, what\\'s the probability of each subsequent card being larger than the previous drawn card?\"|,|\\n|15|prob|\"Without using a calculator, tell me the probability of getting at least 312 heads when you flip a fair coin 576 times.\"|\"It\\'s easy to calculate the expected number of heads as 576 * 0.5 = 288. Then unfortunately you just have to have memorized that the standard deviation of a binomial distribution is sqrt(np(1-p)) = sqrt(5760.50.5) = sqrt(144) = 12. You see that 312 is 2 standard deviations above the mean, and due to the binomial approximation to the normal distribution we have 5% on each tail, or 2.5% >= 312 H. So 2.5% chance of at least 312 H in 576 flips.\",|\\n|16|,What is the difference between likelihood and probability?|\"Probability = area under an interval of a distribution curve.|\\n|Probability: What is the probability of a measurement, given the distribution?|\\n|Probability example: what is the probability a mouse weighs 34 grams, given the normal distribution of weight?|\\n|Likelihood: What is the likelihood of the distribution, given our measurement?|\\n|Likelihood example: what is the likelihood that this distribution is normal(mean=30|std=2), given our mouse weight measurements?\",|\\n|17|,What is a confidence interval? What does it mean if we have a 0.95 confidence interval?|A confidence interval gives the PROBABILITY that our true value lies within the range of values. Bigger interval = higher probability.,|\\n|18|VIP|\"What is a p-value? Would your interpretation of p-value change if you had a different (much bigger, 3 mil records for ex.) data set?\"|\"Def. of p-value: probability of observed results if null hypothesis is true. (null hypothesis is true if samples are from same population).|\\n|With very large samples, p-values are not reliable because stderror = std/sqrt(n). It is best to measure differences in large samples using \\'Effect Size\\' because this uses standard deviation instead of standard error.\",|\\n|19|,\"What percentage of normal distribution lies within 1 std of mean? 2, 3 std?\"|\"68%, 95%, 99.7%\",|\\n|20|,\"Given n samples from a uniform distribution[0|d] (from 0 to d), how would you estimate d?\"|\"Assuming you sample many times, d would be close to the maximum observed value. The mean and expected value of the distribution would also equal d/2, so we could find d by taking 2*mean of our samples.\",|\\n|21|VIP|What is the Central Limit Theorem?|\"The sum of a large number of random variables is approximately normal. The more random variables we sum together, the more our distribution becomes normal.\",|\\n|22|logistic_reg|What is the relationship between the coefficient in logistic regression and \\'the odds ratio\\'?|\"LogReg coefficient = ln( Odds Ratio )|\\n|Odds Ratio = P(A) / (1 - P(A))|\\n|LogReg coefficient is high for a given variable when that variable has a major role in deciding classification boundary.|\\n|Odds Ratio tells us change in output variable for +1 unit change in input variable.\",|\\n|23|,\"In an A/B test, how can you check is assignment to the various buckets was truly random?\"|\"Plot the distributions of multiple features between A and B group. Distributions should match. Can perform t-tests, effect size tests between feature values between populations.\",|\\n|24|,\"In statistics, what is a \\'Type 1 error\\'?\"|\"Type 1 error = False Positive.|\\n|Predicting positive when obs is actually negative.Type 2 error = False Negative|\\n|Predicting negative when obs is actually positive.\",|\\n|25|,What is an ROC curve? What does area under the curve mean?|\"Receiver operator characteristic, used in evaluating binary classification model.|\\n|Y axis is true-positive rate, x-axis is false positive rate. Calculated for different values of Beta --> sensitivity versus specificity.\",|\\n|26|,What is an ANOVA? Does ANOVA assume normality?|\"When you want to compare 3+ groups, rather than 2 (with a t-test).|\\n|Core idea: How much of the total variance in all your samples is occuring WITHIN groups VS BETWEEN groups?|\\n|F = ratio of (between group variance)/(within group variance)|\\n|Larger F means that groups are different|\\n|b = degrees of freedom between groups = (number of groups - 1)|\\n|w = DoF within groups = (total number of observations - number of group)\",|\\n|27|,\"t test \"|Tests the null hypothesis: that two samples come from the same population.,|\\n|28|,What is the expected value and variance of: binomial distribution|\"Geometric dist: E(x) = 1/p Var(x) = (1-p) / p^2|\\n|Questions usually go: How many times do we expect to roll a dice before we see X?\",|\\n|29|prob|\"What is more likely: Getting at least one six in 6 rolls, at least two sixes in 12 rolls or at least 100 sixes in 600 rolls? (dice)\"|,|\\n|30|prob|\"Let\\'s say that there are six people trying to divide up into two equally separate teams. Because they want to pick random teams, on each round, each person shows either a face up or face down hand and if there are three of each, then they\\'ll split into teams.|\\n|What\\'s the expected number of rounds that everyone will have to pick a hand side before they split into teams?\"|,|\\n|31 elevator|prob|\"An elevator in a building starts with 5 passengers and stops at seven 7 floors. If each passenger is equally likely to get an any floor and all the passengers leave independently of each other, what is the probability that no two passengers will get off at the same floor?\"|,|\\n|32|prob|\"Amazon has a warehouse system where items on the website are located at different distribution centers across a city. Let\\'s say in one example city, the probability that a specific item X at location A is 0.6, and at location B the probability is 0.8.|\\n|Given you\\'re a customer in this example city and the items are only found on the website if they exist in the distribution centers, what is the probability that the item X would be found on Amazon\\'s website?\"|Probability of the item being present= 1- p(item NOT in A AND NOT in B) = 1-(0.4*0.2)=0.92|交集和并集|\\n|33 Perfectly Separable|logistic_reg|\"Say you are given a dataset of perfectly linearly separable data.|\\n|What would happen when you run logistic regression?\"|\"Simply put, logistic regression will have infinite number of decision boundaries and will have difficulty converging； that will result in very unstable beta paremter estimates. Since it is perfectly separable there will be infinitely many beta estimates that can give you the same loss. Thus, that will blow off your hypothesis testing on betas. But if you like to do just prediction and you do not care about the hypothesis testing or beta parameters, your model will still correctly predict your classes.\",|\\n|34 First to Six|prob|\"Amy and Brad take turns in rolling a fair six-sided die. Whoever rolls a “6” first wins the game. Amy starts by rolling first.|\\n|What’s the probability that Amy wins?\"|,|\\n|35 Fake Algorithm Reviews|prob|\"Let’s say we’re trying to determine fake reviews on our products.|\\n|Based on past data, 98% reviews are legitimate and 2% are fake. If a review is fake, there is 95% chance that the machine learning algorithm identifies it as fake. If a review is legitimate, there is a 90% chance that the machine learning algorithm identifies it as legitimate.|\\n|What is the percentage chance the review is actually fake when the algorithm detects it as fake?\"|,|\\n|36 Expected Tests|prob|\"Suppose there are one million users and we want to expose 1000 users per day to a test. The same user can be selected twice for the test.|\\n|1. What’s the expected value of how long someone will have to wait before they receive the test?|\\n|2. What is the likelihood they get selected after the first day? Is that closer to 0 or 1?\"|\"1. Its not a uniform distribution. The same user can be selected twice. So its geometric distribution with success probability = 1000/1M =\\xa01⁄1000\\xa0= 0.001. For a geometric distribution expectation of wait time = 1/p = 1000 days.|\\n|2. Other solutions are correct. Its 1-0.001 = 0.999\",|\\n|37 Three Zebras|,\"Three zebras are chilling in the desert. Suddenly a lion attacks.|\\n|Each zebra is sitting on a corner of an equally length triangle. Each zebra randomly picks a direction and only runs along the outline of the triangle to either edge of the triangle.|\\n|What is the probability that none of the zebras collide?\"|\"Each zebra has 2 possibilities (clockwise or counterclockwise). So the total is 2 x 2 x 2 = 8. The only possibilities are all 3 zebras going clockwise or counterclockwise (2). So\\xa02|\\n|⁄8|\\n|= 25%\",|\\n|38 Distribution of 2X - Y|,,,|\\n|39 Biased Random Number Generator|,\"Given an unfair coin with the probability of heads and tails not being equal, can you make an algorithm that could generate a list of uniformly distributed zeros and ones\\xa0only|\\n|using the results of the coin tosses?\"|\"Let’s say for the first round of flipping the coin we consider H=1 and T=0, next round we reverse the order and consider H=0 and T=1, and again in the next round of flipping the coin we consider H=1 and so on. In even numbers of flipping the coin the distribution of 1s and 0s will be exactly\\xa050|\\n|⁄50|\\n|.\",|\\n|40 Random Seed Function|,\"Let’s say you have a function that outputs a random integer between a minimum value,\\xa0N, and maximum value,\\xa0M.|\\n|Now let’s say\\xa0we take the output from the random integer function and place it into another random function as the max value with the same min value\\xa0N.|\\n|1. What would the distribution of the samples look like?|\\n|2. What would be the expected value?\"|\"The distribution of samples are expected to be uniform.|\\n|The expected value of the first output is (M + N)/2. Since this is the maximum value of the second function, its expected value is N + ((M + N)/2) = ((3N + M)/4).\",|\\n|41 Drawing Random Variable|,,,|\\n|42 Ride Coupon|,,,|\\n|43 Impression Reach|VIP|\"Let’s say we have a very naive advertising platform. Given an audience of size\\xa0A\\xa0and an impression size of\\xa0B, each user in the audience is given the same random chance of seeing an impression.|\\n|1. Compute the probability that a user sees exactly 0 impressions.|\\n|2. What’s the probability of each person receiving at least 1 impression?\"|,|\\n|44 Converted Sessions|,\"Let’s say there are two user sessions that both convert with probability 0.5.|\\n|1. What is the probability that they both converted?|\\n|2. Given that there are\\xa0NN\\xa0sessions and they convert with probability\\xa0qq, what is the expected number of converted sessions?\"|\"(1) Assuming independence, P(A and B) = P(A)P(B) = 0.5 * 0.5 = 0.25|\\n|(2) Assuming independence this is given by the expected value of the binomial distribution. The expected number of converted sessions is Nxq\",|\\n|45 Biased five out of six|,\"Let’s say we’re given a biased coin that comes up heads 30% of the time when tossed.|\\n|What is the probability of the coin landing as heads exactly 5 times out of 6 tosses?\"|,|\\n|46 Approximate Ad Views|,\"Let’s say you work for a social media website.|\\n|Users view 100 posts a day, and each post has a 10% chance of being an ad.|\\n|What is the probability that a user views more than 10 ads a day? How could you approximate this value using the standard normal distribution’s cdf?\"|,|\\n|47 Mutated Offspring|VIP|\"An animal appears normal if it has two normal genes or one normal and one mutated gene. If it has two mutated genes, it appears mutated. Any given animal has a 50% chance to contribute either of its genes to its offspring.|\\n|Animals\\xa0AA\\xa0and\\xa0BB\\xa0are parents of\\xa0CC\\xa0and\\xa0DD.\\xa0CC\\xa0and\\xa0DD\\xa0are parents of\\xa0EE.\\xa0AA\\xa0and\\xa0BB\\xa0both have one normal and one mutated gene. We know\\xa0CC\\xa0and\\xa0DD\\xa0both appear normal.|\\n|What is the probability that\\xa0DD\\xa0has one normal and one mutated gene given that\\xa0EE\\xa0appears normal?\"|,|\\n|48 Marble Bucket|,\"We have two buckets full of marbles. There are 30 red marbles and 10 black marbles in Bucket #1 and 20 red and 20 Black marbles in Bucket #2. Your friend secretly pulls a marble from one of the two buckets and shows you that the marble is red.|\\n|1. What is the probability that it was pulled from Bucket #1?|\\n|2. Let’s say your friend puts the marble back in and now picks two marbles. She draws one marble, puts it back in the same bucket, then draws a second. They both happen to be red. What is the probability that they both came from Bucket #1?\"|,|\\n|49 Fair Coin|VIP|\"Say you flip a coin 10 times.|\\n|It comes up tails 8 times and heads twice.|\\n|Is this a fair coin?\"|,|\\n|50 Stranded Miner|,,,|\\n|51 Best Slot Machine|VIP|\"You enter a room with 3 slot machines. One of the slot machines has a higher win-rate than the other two.|\\n|Given some finite amount of coins p, how would you identify the “best” slot machine while minimizing p in the discovery process?\"|,|\\n|52 Bounds for Coffee and Tea Drinkers|VIP|\"A survey asked 100 respondents two questions:|\\n|1. Do you like tea?|\\n|2. Do you like coffee?|\\n|70% of respondents said they liked coffee, and 80% of the respondents said they liked tea. Only 10% of respondents said they liked neither.|\\n|Based on this survey, what are the upper and lower bounds for the proportion of the population that like tea\\xa0and\\xa0coffee?\"|,|\\n|53 Jars and Coins|,\"A jar holds 1000 coins. Out of all of the coins, 999 are fair and one is double-sided with two heads. Picking a coin at random, you toss the coin ten times.|\\n|Given that you see 10 heads, what is the probability that the coin is double headed and the probability that the next toss of the coin is also a head?\"|,|\\n|54 All Tails Consecutive|VIP|\"Let’s say you flip a fair coin 10 times.|\\n|What is the probability that you only get three tails,\\xa0but, all the tails happen consecutively?|\\n|An example of this happening would be if the flips were\\xa0HHHHTTTHHH.|\\n|Bonus:\\xa0What would the probability of getting only\\xa0t\\xa0tails in\\xa0n\\xa0coin flips (t\\xa0≤\\xa0n), requiring that the tails all happen consecutively?\"|\"each permutation has a probability of: 0.5^10|\\n|number of valid permutations is: 8|\\n|so probability = 8 * 0.5^10 or (n - t + 1) * 0.5^n\",|\\n|55 Ad Raters: Part 2|VIP|\"Let’s say we use people to rate ads.|\\n|There are two types of raters. Random and independent from our point of view:|\\n|• 80% of raters are careful and they rate an ad as good (60% chance) or bad (40% chance).|\\n|• 20% of raters are lazy and they rate every ad as good (100% chance).|\\n|1. Suppose a rater rates just three ads, and rates them all as good. What’s probability the rater was lazy?|\\n|2.\\xa0Suppose a rater sees\\xa0N\\xa0ads and rates all of them as good. What happens to the probability the rater was lazy as\\xa0N\\xa0tends to infinity?|\\n|3.\\xa0Suppose we want to exclude lazy raters. Can you come up with a rule for classifying raters as careful or lazy?\"|\"1. Baye’s - (1.0)(0.2) / (1.0)(0.2)+(0.6^3)(0.8) = 53.6%|\\n|2. Trends toward 100%. Because the inverse probability gets lower and lower (0.6^N)(0.8)|\\n|3. Using confidence of 95%, need to rate 9 ads as good, then probability of lazy is 96%, can rule out\",|\\n|56 Coin Flip Probability|VIP|\"Let’s say you are playing a coin flip game. You start with 30 dollars. If you flip heads, you win one dollar, but if you get tails, you lose one dollar. You keep playing until you run out of money (have 0) or until you win 100 dollars.|\\n|What is the probability that you win 100 dollars?\"|\"E(X) = sum(f(x)*x) = 0.5 * 1 + 0.5 * (-1) = 0.|\\n|staring with $30, E(after one flip) = 30 + 0 = 30.|\\n|E(at the game end) is also equal to 30. In the end, we either lose and have 0 or win and have 100 Now, in the end, we will still have an expected value of $ 30 no matter what.|\\n|E(X) = 30 = p(win 100) * 100 + (1-p(win 100))* 0|\\n|=> 30 = p(win 100) * 100|\\n|=> p(win 100) =\\xa030⁄100\\xa0= 0.3\",|\\n|57 Estimated Rounds|,\"Let’s say that there are six people trying to divide up into two equally separate teams. Because they want to pick random teams, on each round, each person shows their hand in either a face-up or face-down position. If there are three of each position, then they’ll split into teams.|\\n|What’s the expected number of rounds that everyone will have to pick a hand side before they split into teams?\"|,|\\n|58 Swimmer Survival|,\"Let’s say there is man who is 5.8 feet tall who doesn’t know how to swim. Let’s say he wants to swim in a lake with an average depth of 5.6 feet with a standard deviation of 1 foot.|\\n|What’s the probability of his survival assuming that when the the lake depth exceeds his height, he dies?\"|\"Conceptually, it should be a bit more than 50% since he is taller than the mean depth.|\\n|We need to calculate the probability the depth of the lake is  58% survival\",|\\n|59 Median Probability|,\"Given three random variables independently and identically distributed from a uniform distribution of 0 to 4, what is the probability that the median of them is greater than 3?\"|\"1.Since each random variables has 4 outcomes (1-4), there are a total of 64 outcomes.|\\n|2.For the median to be greater than 3, there has to be at least 2 Fours. (144|244|344|444|414|424|434|441|442|443). These are the 10 combinations.|\\n|3.The probability is\\xa010⁄64\\xa0or\\xa05⁄32.\",|\\n|60 Drawing Random Variable|,,,|\\n|61 Secret Wins|VIP|\"There are 100 students that are playing a coin-tossing game.|\\n|The students are given a coin to toss. If a student tosses the coin and it turns up heads, they win. If it comes up tails, then they must flip again.|\\n|If the coin comes up heads the second time, the students will lie and say they have won when they didn’t. If it comes up tails then they will say they have lost.|\\n|If 30 students at the end say they won, how many students do we expect\\xa0actually won the game?\"|,|\\n|62 Coefficients of Logistic Regression|logistic_reg|How would you interpret coefficients of logistic regression for categorical and boolean variables?|,|\\n|63 lasso and ridge regression and elastic net|,,,|\\n|64 Rejection Reason|,\"Suppose we have a binary classification model that classifies whether or not an applicant should be qualified to get a loan.|\\n|Because we are a financial company, we have to provide each rejected applicant with a reason why.|\\n|Given we don’t have access to the feature weights, how would we give each rejected applicant a reason why they got rejected?\"|,|\\n|65 Bagging vs Boosting|,,,|\\n|,|Alice flips n＋1 coins and Bob flips n coins. What is the probability that Alice has more heads than Bob？|,|\\n|,|\"With some point in time T and a certain period dT, the conditional probability of company X going bankrupt between T and T＋dT, given that it doesn’t go bankrupt until T, is said to be equal to a constant K multiplied by dT. What is the probability that X goes bankrupt before T？\"|,|\\n|,|Calculate the odds of lining up 6 people around a round table in the order of their birthdays？|,|', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3a984bec-1dec-4650-99c8-7075a2e567e2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nj\\n\\nBrother! i found the greatest strategy for nofap. And i am starving to share it with all you guys.Here it is:\\n\\n\"THE BIGGEST CAUSE OF PMO ARE NOT YOUR URGES , NOT YOUR ADDICTION ,NOT TEMPETATION BUT SIMPLY \\'LACK OF DRIVE\\'\".\\n\\nLet me explain:\\n\\n[Part 1:\\n\\nA basic question:\"What cause you to relapse?\"\\n\\nAnswer:\"LACK OF DOPAMINE\"\\n\\nIts all dopamine youre craving , your brain dont give a shit about those tits or thigs.\\n\\nEvidence: when youre under stress whether of work , study ,Break up,loneliness or anything else i.e your dopamine levels are low. When youre are idle simply put \"no dopamine is lack of dopamine too\". Youre starving for pleasure , lets called it PLESTARVING.\\n\\nWhen youre plestarving you brain start searching folders to find easy pleasure. Here are few things it find effortlessly:\\n\\n1. Food: First desire of human.Eating food release dopamine , particularly sugar and fat. If you are on a diet for more than 6 active hours than you wont relapse.Because now your brain wont search other choices. Simply it is now fixated on hunger.But if youre more than full than,\\n2. Multiple : It includes habits that give burst of dopamine. Ex. video games, social media, youtube, narcotics or similar things.This is a very important stage that set the environment of further choices.\\n\\n3.Horny: Being horny release dopamine, your brain knows that. But for that it need trigger. Here are the types of trigger:# mind thought(simply dangerous), # visual triggers(through games, social media etc), #Mind patterns( habits of thinking similar things during a particular time). This led to browsing porn.\\n\\n4.Relapse: The final frontier is relapse. The ultimate joy and the ultimate despair.Because it exceed the brain baseline dopamine level which cause you to feel low even in average dopamine level.\\n\\npoints to be noted:\\n\\n- Brain primarily search for easy pleasure that exclude exercising, learning new things, making new things etc. Brain is lazy.\\n- You hate porn(fact youre here) , i.e your brain hate porn too.On a family dinner youll not watch it because its disgusting by nature. Sex is all disgusting by nature too.Thats why trigger are the most important aspect of relapse or sex.\\n- triggers decide what type of porn youll crave.\\n\\npart 2:\\n\\nQuestion:\"should i fight urges\"?\\n\\nAnswer:\"DONT FIGHT URGES , FIGHT THE FACTOR CAUSING IT\".\\n\\nExplanation: let say youre on diet , after some time you get hungry and brain fixated on hunger. Brain is desperate for food. Should i tell you fight the hunger? You can for several hours or with all might probably days but youre going to eat food.\\n\\nBut with pmo we have advantage that we may choose to never feel hungry in first place.i.e no urges. But for that there are several strategy you have to play with plestarving.\\n\\nRemember the point no.2 . That bring us to solution.\\n\\npart 3:\\n\\nTHE SOLUTION\\n\\nAll are necessary, work on which you lack.\\n\\n#The lifestyle change:\\n\\nYouve mind patterns. And if theyre not challenged , youre going to live the same day again and again. The first thing you can do is changing your sleep pattern ( I recommend 10-5 am). Must include a morning excercise routine.This will end the brain lethargic tendency throughout the day.And that is enough, really just that and nothing more.\\n\\n#The gluttony factor:\\n\\nRemember the first desire of human mind is food.And if youre full than will it go with next easy plestarving contender. But if youre never full than it will stay fixated on hunger. Thats it , you have to eat less than full everytime.Trust me you wont get weak , it only help with weight maintaing goals. More or less what you call full diet is exactly \" diet you need + Diet for extra dopamine\".Remember human efficient diet includes half hungry all time.\\n\\n#The hard rule:\\n\\nNow thats something hard. It directly hit on the second and most vicious plestarving contender after food(multiple). Remember brain only like easy pleasure. So the hard rule means to change easy pleasure giving things with hard and productive things.To do that, make easy things hard and hard things easy. Here are the few tricks,\\n\\n1.Make an accountability partner: lat say If you want tostudy for 4 hours than do it with your friends or make someone accountable(online or offline) to which youre answerable at end of day.\\n\\n2.Change easy with easy.Example , change social media with reading blogs.Change playing video games with playing musical instrument,change narcotics with praying spagghetti monster for enlightment, oh actually same.I have no idea what your life is, but you have.\\n\\n3.The brain is complex, once you started with browsing social media , it will continue doing it. But if you start with playing piano , it will continue doing it too. What nessecary is to give first push towards right things. With time that first push will become easier.\\n\\n4. Discover where trigger lies. They lie on some places of internet, on some games, on some tv series. Theyre nowhere to be found on piano, novels, fields, or canvas etc.\\n\\n#The breaking rule:\\n\\nSimple matter of your willpower to break the course of things that lead to relapse incase you neglect the hard rule.Ex. Put down the moblie , turn it off, shut it in a locker before it give a trigger.\\n\\n#The rule of divergence:\\n\\nIf you get a trigger or urge , you know where it lead into. There are a ton of things you can do here.Cold shower , walk outside , push ups etc.Nofap brothers knows it better. But always remember prevention is better than cure. Try not to fall into that stage often.\\n\\n#Rule of negligence:\\n\\nForget there is a thing called porn , pmo or nofap. You may find it suprising but it is the thing most common in all big streak guys. Apply the above rule and youll follow that one automatically.No sweat.\\n\\n#rule of habits:\\n\\nYour mind has a pattern of how you live your life. That pattern encourage to live same way. You have to break it. The problem is not pmo but what comes before it as quick as youre awake.To do that simply stick to productive things throughout day to make it new habit.Force yourself to do them atleast for 2 months.Make conditions that force you.Meditation may boost process.\\n\\n]\\n\\n[part 4:\\n\\nIn short: change the way you look at things and the things you look at will change. See porn as an enemy and you end up thinking more. See porn as a garbage dum on roadside called internet and youll ignore it to do better things.\\n\\nLife has many worse thing than porn. There are world hunger, wars, poverty etc or more personal things like breakups, midlife crisis, rejections, etc. pmo is just a part of fuckery that you have to rise above. A small bump on the mountain call life that we all have to ascend.\\n\\nDont think about it too much , rest your mind and focus on what makes you yourself.\\n\\n]\\n\\natLast qna:\\n\\nI am fighting this addiction since 7 years , discover nofap past 2 and a half year later. I have tried many things in between and have success many times. Currently have back to back multiple months streak. So i am a something of nofap veteran for say. Here , the list of challanging questions, i know youve in your mind.\\n\\n1. Will i ever get over it ?\\n\\nDEFINITELY MATE, BUT YOUVE TO CHANGE A LOT OF THINGS ABOUT YOURSELF AND YOUR LIFE TO ACHIEVE THAT GOAL.JUST START AND HEAVEN WILL HELP YOU.\\n\\n2. All that porn i have watched , i hate myself for enjoying that , i am bad, i had watched stuff so creepy and bloody that i cant forgive myself. will i able to forget it ever?\\n\\nYou will. In a sense , mind doesnt delete anything but we leart to live and rise above it instead. And trust me its better than deletion.Dont blame yourself, the stuff on internet make creep out of anyone.For all creepy porn youve watched, like i said porn is disgusting by nature. You dont want to eat anything while watching porn. There is a curtain of sexuality over our eyes that make it look interesting. But over time if you follow my advice and make something out of your life, youll rediscover your sexuality in a more healthy way , a more loving way.I promise you. Just dont pmo till then.\\n\\n3. Will nofap worth?\\n\\nOver time in your long-long life youll forget about nofap and pmo. That happens , trust me. You have a lot of time to care about things that matter , rather that choosing between shaking penis today or not. And that is benefit no less than a new life.\\n\\nI decided to make this post, because I see so many guys that dont really know why they cant quit porn, or why they keep relapsing. It\\'s so hard for a lot of people. And this people think it\\'s mostly a physical thing, not a psychical issue. Well.... that\\'s not true.\\n\\nWhen I started nofap I started to finally looking for a psychical reasons why all of this happened to me. Why people are getting addicted, or why cant they quit. Well I studied this topic for quite a lot time and I think the answer is\\xa0**shame**.\\n\\nSo... what is shame?\\n\\nPeople get confused about that and don\\'t really understand the meaning of feeling shameful. A lot of people say word \"shame\" and \" guilt\" interchangeably. It\\'s not correct.\\n\\n**Guilt**\\xa0means -\\xa0**I DID**\\xa0something bad/wrong/not ok etc.\\n\\n**Shame**\\xa0means -\\xa0**I AM**\\xa0bad/wrong/not ok/not lovable/ less than other people etc.\\n\\nAnd the first time I\\'ve heard this it striked me, becouse it\\'s actually really sad to feel that way about yourself. But the more and more I looked at my own feelings - the more confident I was that I actually feel this way.\\n\\nWhy do we feel shame?\\n\\nWell there are obviously a lot of reasons why we may feel that way, but\\xa0**there are 2 ways of feeling shameful:**\\n\\n**1st**\\xa0- You felt that way BEFORE you got addicted. Maybe you felt that you are not lovable because at your young age your parents got divorced. Maybe you got humiliated at a very young age and you seek for pornography about humuliation/bdsm etc.\\n\\n**2nd**\\xa0- You got addicted (for example at young age), and with time you got more and more shameful about your addiction and who you are.\\n\\nWhy you cant quit porn?\\n\\nYou can\\'t quit porn because your addiction is a way of you dealing with a lot of bad emotions. I suggest you to look into the situations when you relapsed. I am sure it was a time when you felt lonely / bad / tired / sad / rejected etc.\\xa0**You use porn as an escape.**\\xa0A lot of people can\\'t stop doing that, because if they do they find out how they really feel. And you will feel that way untill you finally find out THE SOURCE of this emotions. And the source is shameful behaviour and a lot of bad emotions about YOURSELF and WHO YOU REALLY ARE.\\n\\n**If you believe that you are worthless you can\\'t escape from porn. You believe that you are bad and then you automatically do bad things.**\\n\\nThe steps you need to do to stop feeling that way and finally quit addiction:\\n\\n**1)**\\xa0find out the type of shame you experienced - did you feel that way before you got addicted or during/after?\\n\\n**2)**\\xa0find out WHY you feel that way. You can also try to find the link between the porn you were watching and the accident that may coused you feel shameful.\\n\\n**For example**\\xa0*I was watching porn videos with humiliation motive, where I was the person who is getting humiliated. And I think the reason is that i trully belived that I cant be trully loved by other person. That I am less worth than another human beings and that the only love that I can experience is through being humiliated and used by another person. Thats a powerful shit...*\\n\\n**3)**\\xa0You need to understand that there isnt such a thing as being less worth than other people. Its a lie that you persuade yourself into. You need to deal with this through therapy or by trying to discover your own feelings. This will be very painfull to you - trying to visualise all the bad things that happened to you through your life, but I promise you - its going to be worth it.\\n\\n**Your PMO is actually a compensation for something deeper**\\n\\nCoping mechanisms are many, including playing games, meeting friends, and watching movies. We\\'re all technically broken inside, in hundreds of ways, if not thousands, and we utilise different means to cope. However, coping mechanisms seem to be okay so long as:\\n\\n1. You\\'re not hurting yourself\\n2. You\\'re not hurting anyone else\\n3. It\\'s legal (legislatively, religiously - if this applies to you, morally, etc.)\\n\\nFor example, eating is a coping mechanism for hunger. That\\'s socially acceptable though, so it\\'s okay. ;)\\n\\nThe problem with PMO is that the grounds on which it violates the above criteria are subtle and gradual. But I believe a true addict will understand how PMO is in fact, albeit long-term, very harmful.\\n\\nCoping mechanisms are created subconsciously over time, when your brain learns that an activity you recently engaged in soothed or otherwise subsided some internal calling or ache. It slowly makes the association between the action (let\\'s say PMO) and the problem it solves (say, boredom)...\\n\\nCoping mechanisms can develop into habits, and eventually, addictions. One might even argue that an addiction is nothing more than an unwanted or detrimental habit. :P\\n\\nI find the deeper the cause of what makes us require this coping mechanism, the more adamantly your brain will pursue it, and so the more challenging the \"streak\".\\xa0*Today I posit the argument that those who manage to \\'recover\\' or \\'abstain\\' are either in one of three states:*\\n\\n1. *Constant self-incongruency, i.e. in pain basically*\\n2. *They\\'re actually addressing the underlying issue behind this habit, whether they know it or not (e.g. some of us develop new habits, some of us distract ourselves, etc.)*\\n3. *They\\'re holding their emotional breath, and it is only a matter of time before they give in*\\n\\n**TL;DR**\\xa0- You engage in coping mechanisms when there\\'s an internal (usually emotional) issue that you don\\'t know how to process.\\n\\n**Why PMO?**\\n\\nI\\'ve learned and am continuing to learn that the nature of any given coping mechanism seems to reveal (with shocking accuracy) what the actual, underlying problem is. I don\\'t know them all. But look:\\n\\nSelf-harm is related to\\xa0**fear**\\xa0(especially of violence)**.**\\xa0Happens typically when a child witnesses domestic abuse between family or relatives. Ask your friends.\\n\\nSmoking is related to\\xa0**stress**\\xa0and\\xa0**anxiety.**\\xa0People who smoke usually have a \"my mother left me at a store\" story. I\\'m serious, ask your friends.\\n\\nPMO is related to\\xa0**intimacy**\\xa0and\\xa0**vulnerability**. Again, ask your friends about their mother\\'s emotional availability as a child, or their rejection history. Might not be those two, specifically, but you get the wider point.\\n\\nMost of us who are struggling with PMO are hooked because our brains have made the associations between the rush of feel-good chemicals our brains release when we engage in the act, and our challenges with intimacy and emotional vulnerability. It also doesn\\'t help that sex (therefore genital stimulation and ejaculation) are naturally incredibly pleasurable and biologically compelling. The second point applying more to men.\\n\\n**TL;DR**\\xa0- PMO is your coping mechanism because you\\'re likely struggling with deep intimacy and emotional vulnerability issues, which you subconsciously may have hidden away from your immediate sight.\\n\\nThese could be:\\n\\n1. Emotionally unavailable mother during childhood\\n2. Rejection from (or lack of pursuit of) an attractive mate, friend or companion, particularly while really young\\n\\n**Why we\\'ll remain addicted until we address this**\\n\\nAssociations created by your brain to \"cope\" and regulate your emotional welfare aren\\'t going to disappear because you want them to. You\\'re literally wired to survive, and your subconscious isn\\'t simply going to roll over and play along because you\\'re on a subreddit which socially disapproves of its coping mechanism. Nope. Your brain chooses to survive, still.\\n\\nUnless you heal, you\\'ll likely remain in the seemingly unending loop of tearing yourself apart. One side disapproving of this habit and dissociating from yourself, and the other side simply trying to survive by engaging in the act.\\n\\nThis is why abstaining doesn\\'t last long for some and will never work\\xa0**UNLESS**\\xa0(this is why some people win at NoFap) they do it long enough for their brains to forcibly undo that association. But it\\'s not the only way. Plus, long-term abstainers seem to relapse frequently, even on this very subreddit - strange huh? ;)\\n\\n**TL;DR -**\\xa0Curing symptoms doesn\\'t imply curing the root cause. Abstaining long enough to not be dependent on PMO is the emotional equivalent of putting a band-aid on a cat-scratch wound long enough for it to heal over. If you\\'ve still got the cat, you might as well buy a box of band-aids.\\n\\n**Appreciate the depth of this**\\n\\nThere is literally books-worth more of content to say on this, but I\\'ll stop after this.\\n\\nUnderstand that being \"bored\" or \"unmotivated\" is absolutely not the reason for your PMO. Understand how complex and calculated your nervous system is. Realise that there must be a reason for why your brain keeps compelling you to do this action SPECIFICALLY, as opposed to something like washing a car. What are the differences, emotionally? What do you gain from one and not the other? The answer for us dudes is\\xa0**intimacy**. Raw, sexual intimacy.\\n\\nDo you relish the idea of staring into an attractive mate sitting opposite you, and having them maybe touch you gently and say reassuring words? Yes, there\\'s a environmentally general application to this, but if you find that this scenario would be extremely healing for you, then please consider what I\\'ve posited.\\n\\n**Is there a solution**\\n\\nOf course... :D', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d62d68da-cc5f-4df4-8b03-88a0e91e15a0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n**主动引领** \\n主导谈话，做主角；主动去交谈，说笑话，控场  \\n**容纳冷遇**  \\n和不和人主动说话，是我们的事；回不回答，是对方的事。不必会因为对方没有回复而觉得尴尬，说不定对方正期待有人来找自己说话。\\n\\n\\n\\n能够坦然地面对别人的优点，才能平和地接受自己的缺点\\n\\n男朋友遇到女朋友的负面情绪，马上跳入到“解决问题”中。\\n\\n但是女朋友在遇到负面情绪时，她首先需要的是“肯定情绪”，然后才是“解决问题”。\\n\\n**很多时候，如果我们可以给予对方所需要的“肯定情绪”，可能对方可以自行去“解决问题”，并不见得需要我们帮助对方去“解决问题”。最有可能的情况是，对方其实有“解决问题”的能力，但因为当下对方体验到强烈的负面情绪，他的脑前额叶（prefrontal lobe）活动遭到抑制，他没办法进行有效的逻辑思维，去解决问题。**\\n\\n**请注意：肯定别人的情绪（emotions），并不代表我们要肯定别人的认知（cognitions）。我们需要肯定和认可的是“对方现在体验到了强烈的情绪”，但是针对这些情绪背后的一些并不符合事实的认知，我们不需要去肯定和认可。**\\n\\n打个比方：某人给好朋友发信息，但是过了一天还没有收到回复。某人认为“我的好朋友不在乎我”，进而体验到“抑郁”、“羞耻”、“愤怒””焦虑”等情绪。我们可以肯定对方体验到了这些情绪，对方现在很不好受，但不代表我们要认同他的认知，我们并不需要说“是的，你的好朋友的确不在乎你”\\n\\n哲学家：关于权力斗争，还有一点。就算自认为站得住脚的情况下，也不要刁难对方。这是大多数人在人际关系种容易掉入的陷井。\\n\\n年轻人：为什么？\\n\\n哲学家：人啊，一旦人际关系中确信“我是对的”，那瞬间就已经一脚踏入了权力斗争。\\n\\n年轻人：光凭认为自己是对的？天啊，这太夸张了吧！\\n\\n**哲学家：我是对的，就代表对方是错的。当你这样想的同时，讨论的焦点就已经从“意见的正确性”转变成“对待别人的方式”。总之，确信“自己是对的”会和“这个人错了”的想法连结起来，进而变成“所以我必须赢过他才行”这种胜负争夺战。完全就是权力斗争。**\\n\\n年轻人：喔……原来。\\n\\n哲学家：事实上，意见的正确性和胜负完全没有关系的。如果你认为自己是对的，那么不管其他人的意见是什么，都应该在这里画下据点。但大多数的人会进入权力斗争中，想让对方屈服，才会把“承认自己的错误”当成是“承认自己的失败”。\\n\\n年轻人：没错，的确有这种状况。\\n\\n哲学家：因为一心只想着不要失败而无法承认错误，反而害自己选错路。事实上，承认错误、表达歉意的言词，还有脱离权力斗争，这些都不是“挫败”。\\n\\n所谓追求卓越，并不是透过跟其他人的竞争来实现的。\\n\\n年轻人：您的意思是，执着于胜负的话，就没办法作出正确的选择？\\n\\n哲学家：嗯。就像模糊的眼镜让你只看到眼前的胜负，结果却走错路。所以我们要先摘掉这个竞争或胜负的眼镜，才能修正自己、改变自己\\n\\n**冷静表达的规则**\\n\\n**该做的**\\n\\n1.告诉对方你的感受。\\n\\n2.一次只着重解决一个问题。\\n\\n3.倾听。\\n\\n4.直言你想要对方做出哪些改变。\\n\\n5.和对方保持目光接触（注意不是紧盯）。\\n\\n6.要灵活——乐于改变自己的想法。\\n\\n7.平稳呼吸，保持放松，坦诚地协商和解决矛盾。\\n\\n8.对自己说的每一句话负责。\\n\\n9.关注问题的解决方案，而不是谁输谁赢。\\n\\n10.意见无法统一时，休息一下。\\n\\n**不该做的**\\n\\n1.取笑对方。\\n\\n2.纠结往事，逃避当下的问题。\\n\\n3.在话茬被打断的时候坚持说完。\\n\\n4.看对方不顺眼就出言羞辱。\\n\\n5.扮鬼脸。\\n\\n6.说“你总是”“你从未”或其他绝对概括性的话。\\n\\n7.站起来大喊大叫。\\n\\n8.说“忘了吧”“我不在乎”“那又怎样”或其他出尔反尔的话。\\n\\n9.结果稍有不如意，就攻击对方的人格。\\n\\n10.一直纠缠对方，或击打、推搡、紧抓或威胁对方。\\n\\n消极想法：“我永远也不会改变爱发脾气的毛病，所以何必徒劳？”\\n\\n挑战：“嘿，为什么我就不能少发点儿脾气呢？我和那些脾气好的人没什么分别。”\\n\\n充满敌意的想法：“我会痛恨前任直到他（她）死的那一天。”\\n\\n挑战：“这些已是陈年往事，我发火能伤害到的唯一一个人就是自己。是时候忘掉憎恨了。”\\n\\n偏执的想法：“他们不盼着我成功，他们都希望看到我一败涂地。”\\n\\n挑战：“‘他们’到底指的是谁？停止假想敌人，我不愿再偏执下去了。”\\n\\n替代原则在这里也是适用的。要想摆脱愤怒，你不能单靠摆脱掉愤怒的想法，还需要一些积极的想法替代。三个适合上述情况的新想法是：\\n\\n“我能够做到这些，我也能够拥有一个摆脱愤怒的生活。”\\n\\n“我会以理智的态度原谅前任。”\\n\\n“是时候重拾对他人的信任了。”\\n\\n完整版的心理愤怒问题管理应该是这样的，首先你发觉自己有了一个愤怒的想法，之后你去挑战这个错误的想法而不是轻信它。用新的和更积极的想法来代替之前旧的和消极的想法。你如此做的次数越多，就越容易做到，最后你将达到重新训练大脑的目的\\n\\n**忽视真正的情感**\\n\\n（悲伤—恐惧—高兴—孤独）——狂怒——（羞愧—内疚—骄傲—尴尬）\\n\\n这个小小的图解展示了愤怒是如何掩盖其他情感的，这就是易怒人士的情感世界。“狂怒”主宰一切，其他情感只能瑟瑟发抖，把话筒交给愤怒。所以，易怒人士只会注意到自己的愤怒情绪，把问题变得过于简单化\\n\\n有两种类型的冷静，每一个都很重要，你最好二者兼备。第一种冷静是指几乎从不动怒；第二种冷静是指学会在愤怒时冷静地表达自己想法的技能。\\n\\n**冷静=几乎从不发怒**\\n\\n保持冷静是指对所有的愤怒邀请函都说不\\n\\n下面是易怒人士的想法链条：\\n\\n1.我看不惯他们，他们也一定看不惯我。\\n\\n2.不管他们看起来多么和善，我都不能信任他们。\\n\\n3.找寻他们试图伤害我的证据。\\n\\n4.时刻准备好保护自己。\\n\\n5.进攻就是最好的防守。\\n\\n第二个原则是在对话过程中始终注意维护安全感，即让对方毫无心理压力地和你展开沟通。这个原则涉及很多具体做法，也是本书重点讨论的内容，作者从共同目标和互相尊重两大角度出发进行了细致的说明。实际上，只要你能做到在对话中拉紧“安全感”这根弦，能够设身处地地考虑对方的感受，能够三思而后言，可以说消除对方心理包袱的方法是无穷无尽的。它可以是一句道歉、一段解释、一个积极的目光交流，甚至是你在举手投足中流露的信号或是一个关注表情展示的信息。将心比心，如果在对话过程中对方不断采用这些方式化解我们的戒备心理，我们肯定会感到他们是真诚的、动机纯正的，认为这是一次希望以解决问题为目标的对话，而不是令人讨厌的一言堂或让人摸不着头脑的兜圈子。因此，当你做到了时刻关注对方的安全感时，关键对话问题自然也就迎刃而解了\\n\\n既想改变别人，又想尊重别人，最重要的一点是，把改变他人看成是一个过程，而不是一个结果。在这个过程中，你密切注意的不是他是否会改变，而是你们之间的关系如何进展，你怎么在真诚地表达自己的同时，不给彼此双方带来太多负面影响。作为一个普通的室友，你不要因为“无法改变”他的行为而对别人产生敌意，许多人在“改变别人”的时候，往往增加了自己的焦躁情绪，这就是因为他们错误的以为只有改变成功才是胜利……并非如此，成功的是对自己的一种情绪管理，以及对共赢的诚意展示。改变别人的时候，话语要直接，目的要明确，自身要有参与性，有一定的奖励措施，也可以辅之相应的消极重复性对抗，比如说，在对方打游戏的时候，说，别打了，我去自习，和我一起去吧。这个是直接的话语，自身的参与，邀请……当然了，一般人是不会答应的，那么你就自己去，不用再碎碎念。继续说服是一种积极对抗，而下次再进行邀约是对对方的消极对抗……什么时候是积极说服的时候，那是对方开始对你产生好奇：你干嘛每次都叫我一起去啊，自己去不行嘛？这个时候，你可与巴拉巴拉地说一些自己想说的，宿舍说也行，喝酒说也行，总之，找机会说出你自己想说的话，这是对自己的一种情感疏通。\\n\\xa0\\n\\nD——Dominance（支配型），行动力强、以结果为导向的性格特征；\\nI——Influence（影响型），性格温和乐观、以人为主的性格特征；\\nS——Steadiness（稳健型），以程序化工作为主、做事严谨、精细的性格特征；\\nC——Compliance（支持型），以服从规则为主、乐于支持他人的性格特征。\\n\\xa0\\n当你的上司是支配型上司时，你需要关注以下几点。\\n1.服从和尊重。\\n2.直截了当：\\xa0支配型上司把效率和结果放在最重要的位置，所以你的汇报越直接越好，否则会被视为耽误他的时间。\\n3.随时让他掌握状况：\\xa0如果你的上司事无巨细都要了解、经常要求你向他汇报现状，那么他一定是D型上司，D型上司对掌控自己和他人的权力非常执着。\\n4.拿出结果：\\xa0不要带着问题去找他，而要带着结果去找他。任何工作你都要卓有成效地完成，他喜欢能够直接带给他结果的下属。\\n5.能够处理他照顾不到的细节：\\xa0D型上司的开创性使他做决策时非常果断，但是这种果断常常会使他疏忽细节、忽视弊端。作为D型上司的得力助手，你需要在他做决策的时候，帮助他准确衡量利弊得失，并提醒他注意可能忽视的细节。但是注意，你需要做的是辅佐他，而不是反对他。\\n6.不要虚饰：\\xa0如果没有实际的成果，那么就直接承认，不要用好听的话来修饰，例如，“在这个过程中我们学到了……这个失败的教训给我们带来了……”，等等。与其说这种话，不如直接说“下次我一定会拿出结果”，这样反而会因为冲劲十足而更容易得到D型上司的欣赏。\\n7.抗压能力强：\\xa0D型上司的优点在于敢于勇往直前、抗压能力强，在工作遇到困难时表现优秀的永远是D型上司。作为D型上司的下属，你也需要有很强的抗压能力。D型上司的缺点在于非常容易发脾气，他可能会对下属讲粗话。如果D型上司骂你，你不要放在心上，他只是在发泄自己的情绪。\\n8.通过时间获取信任：\\xa0D型上司不容易信任他人，你要获得D型上司的认可，只能通过时间和一次次的优秀表现。只有时间会证明一切\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n当你的上司是I型上司时，你需要关注以下几点。\\n1.尊重和认同他：\\xa0友善是I型上司的标签，和I型上司一起工作你会感到很舒服，但是I型上司喜欢在讨好别人的同时获得他人的认同。在DISC四型中，I型上司是最渴求别人认同的。\\n因此，在I型上司手下工作，你需要多多赞美他，经常表达你对他的认可和欣赏。虽然他是你的上司，但是他仍然非常需要下属的肯定。\\n2.能够找到重点和充分思考：\\xa0I型上司的脑子非常灵活，但是容易流于表面。他们善于提出问题的解决办法，尽管有时他们的解决办法并不完美，因此能够补充他的解决方案的下属是被他喜欢的。I型上司同时具备充满激情的特征。\\n3.能够把工作计划贯彻到底：\\xa0I型上司性格自由、不喜拘束，有时他的工作风格会显得缺乏动力和行动力，优秀的下属能够弥补他的缺陷，协助他将工作计划贯彻到底。I型上司还有一个特质是略显不严谨，因为I型上司容易兴奋，很容易忘记事情、忽视细节。与I型上司沟通，你需要格外照顾他的情绪，注意他的情绪起伏，并照顾他的荣誉感和被认同的需要。\\n4.能够自我管理：\\xa0I型上司友善乐观的性格使他们不会像D型上司那样要求随时关注和掌控一切，在I型上司手下工作你会很轻松，但是这就更要求你有很好的自我管理能力，你必须自觉、主动。\\n5.忠诚可靠：\\xa0I型上司比其他类型的上司更缺乏安全感，他需要可靠的伙伴。因此你需要时刻让他感到安全，不要使他觉得自己的地位不稳固\\n\\n!Untitled\\n\\n当你的上司是S型上司时，你需要关注以下几点。\\n1.有勇气改变现状：\\xa0因为S型上司追求稳定，所以工作上有时会显得犹疑而死板。如果下属能够适当弥补他的这一缺点，灵活应变，在有突发情况时能较快地做出反应，那就会受到上司的欢迎。\\n2.灵活的工作方式：\\xa0S型上司本身不是创新型人才，他们工作时专注而有恒心，但是难免会显得保守。S型上司需要能够一心多用的下属协助他全面发展，灵活的下属能够和他形成优势互补，使效率最大化。\\n3.愿意代替他在人群中冲锋陷阵：\\xa0S型上司大多不太擅长和人打交道，作为S型上司的下属，如果你能主动替他与人交流，他会非常感激你\\n\\n!Untitled\\n\\nC型上司同时又被称为“思考者”，他们的特质有以下几点。\\n1.讲究做事精准，重视流程。\\n2.对品质有非常高的要求。\\n3.真正就事论事，对人的因素并不关心。\\n4.非常严肃和理性。虽然\\xa0S型和\\xa0C型一样理性，但是\\xa0C型上司更加严肃，而不平易近人。\\n5.缺乏变通。\\xa0C型上司的忠诚度高，源于其性格中的稳定性，基本不会自主创业，因为开创事业不符合他们喜欢服从的性格。\\nC型上司很适合做财务工作。\\n当你的上司是C型上司时，你需要关注以下几点。\\n1.能够团队协作：\\xa0C型上司是守规矩的人，其个人会表现得专业性强且有自制力，但是其下属最好能够实现团队协作。\\n2.能够给出明确的规则和完善的资料：\\xa0D型上司需要的是工作的结果，而C型上司更重视工作中的规则、流程和过程（不是说其不重视结果，而是更强调做事符合规则）。\\n向C型上司汇报工作时，一方面，你要能够给他提供完善的资料，另一方面，你要抓住重点。这就需要你灵活应变了。\\n3.精确、完美的品质：\\xa0C型上司注重品质，他不喜欢马马虎虎的行为模式。如果你要递交工作结果给C型上司，那么最好做到完美再给他，不然会被要求重做。\\n4.能够提供大量的数据和资料供他决策：\\xa0C型上司是团队中解决问题的高手，他们擅长分析、对比问题，在出现问题的时候，他们会花大量时间搜集和对比数据资料。作为C型上司的下属，你最好能够在他要求之前就协助他做到这一点\\n\\n!Untitled\\n\\n!Untitled\\n\\nLet other ppl to see things through my eyes, and learn to see through other ppl's eyes. 最终达到一个thoughtful disagreement的状态\\n\\n所以,聊天的「黄金思维公式」=共情+认可+适当表达。\\n\\n共情,简而言之,设身处地的理解他人的感受\\n认可,即情绪认可,认可他人传递出的情绪,才能进行有效的沟通。\\n适当表达,说对方想听的而不是你想说的。\\n\\n因此，在李松蔚眼中，和与你意见不统一的人相处既容易也简单。第一，理解他眼中的世界；第二，不求一致，只求合作；第三，让他看到好处。\\n\\n不要看对方说了什么，而是思考他为什么说这个\\n\\n决定一个人对你态度的，不是你是什么样的人，而是你们的关系\\n\\n我观察到+我感觉+是因为+我请求=非暴力沟通\\n\\nanswer a question not to get it right, but to be less wrong\\n\\n沟通前多付出一些脑力准备\\n\\nLet other ppl to see things through my eyes, and learn to see through other ppl's eyes. 最终达到一个thoughtful disagreement的状态\\n\\n表达情绪，不是用情绪表达\\n\\nunderstanding with a person; not about them\\n\\ncomminucation is not showing you are correct； the only goal is after conversation, we are getter better\\n\\n**赞美人的三类：work; energy; apperance**\\n\\nenergy: I like to be around ppl like you, its infectious; \\n\\nI know u feel like crap now. if there is anything I can do, let me know\\n\\n“是否将自己的观点强加在对方身上了呢？”常常这么问自己，就能避免这种疏忽\\n\\ntransition between topics: remind of me of sth\\n\\nwhy do you live in xxx; go deeper to show their value\\n\\nwhy do u decide to do this job?\\n\\nwhat do you do; do u like it; why; \\n\\nrepeat 1 to 3 words that the person just said to make them feel they are heard \\n\\n一个人会聊天，多半不是因为他和所有人的观点都相同。那是因为他能在不违反自己原则的前提下，找到和对方聊下去的共同话题\\n\\n1）乐观的人更容易成为他人的贵\\n\\n2）贵人更容易遇到贵人\\n\\n3）能帮助他人进步的才是真正的贵人\\n\\n4）优秀的人，值得尊重的人更容易获得帮助\\n\\n5）乐于分享的人更容易获得帮助\\n\\n6）不给他人制造负担的人更容易获得帮助\\n\\n7）不耻于求助的人更容易获得帮助\\n\\n8）求助、提供帮助不宜用金钱\\n\\n9）贵人不一定是牛人\\n\\n10）很多时候，人们的成功，源自于大量的人希望看到他成功。\\n\\n11）做正确的事情，更容易获得贵人相助\\n\\n12）活在未来的人更容易遇到贵人，因为别人能在他身上看到未来\\n\\n\\n\\n1.如果有人一直说个不停，你完全插不上话，掉个东西到地上（钥匙，笔之类的），你弯腰下去捡，然后开始说话。这样，你就可以神不知鬼不觉的打断对方，并且不被他发现。\\n\\n2.如果有人一直来你办公桌旁边烦你，你继续跟他讲话，但是起身，一起走回他的办公桌。之前有个老板就很会这招。他会把你带回你的办公桌，然后你还纳闷，我怎么回来了。\\n\\n3.如果有人对你发火，而你保持镇定，他们可能会更生气。但之后他们会为自己感到羞愧。\\n\\n4.在面试之前，提前到达面试地点，在外面跟陌生人聊20分钟天。这会让你的大脑感觉自己对这个环境是熟悉的，从而让你更有自信。\\n\\n5.如果你突然想起N年前出的丑，并且感觉很丢脸的时候。停下来想想看到那件事的一个人，你能想起来发生在他身上的那些丢脸的事吗？可能不会。同样的，没人会记得你的那些丢脸的时刻。\\n\\n6.人们会把你用在别人身上的形容词跟你联系在一起。这种现象叫“无意识特征转移”。也就是说，如果你说一个人很真诚，很善良，人们会把这些特质联系到你身上。如果你总是在背地里说别人的坏话，人们也会把这些负面的评价联系到你身上。\\n\\n7.如果你感觉有人在看你，你可以看下自己的手表或者戴手表的位置。如果那人在看你，他也会下意识的看看自己的手腕/手表。\\n\\n8.还有一种方法是，你可以打哈欠。然后看看那人有没有同样打哈欠。如果他也打了，那他可能真的在看你。因为哈欠是会传染的。\\n\\n9.如果有人想在很多人面前让你出丑，你可以对他非常有礼貌，这样会让他停止这种行为或者让他自己看起来很傻。\\n\\n10.如果你感觉上司要在开会的时候对你开火了，坐的离他近一点。说一个离你很近的人的坏话或对他发飙是非常不舒服的。结果？他的语气可能会柔和一点。\\n\\n11.这是一个有趣的小实验，当你和某人聊天的时候，挑选他们说的其中一个词。每次说到这个词的时候，你就点头，或者做一些积极的动作。然后你就会发现，他开始经常说这个词。\\n\\n12.想要在刚认识的人面前表现的友好而自信？尝试着记住他们瞳孔的颜色。当然，你不需要跟他们提起这事。当你尝试这种方法的时候，你跟他眼神交流的频率刚好会让你显得友好而自信。\\n\\n13.如果你希望某人对你坦诚，但他只对你说了其中一部分的时候，保持沉默并看着他的眼睛一会。\\n\\n14.如果你在某个活动或行动之前很紧张，不要去咬指甲或抽烟，可以尝试嚼口香糖，我们的大脑很奇怪，当我们在吃东西的时候，它会感觉我们是安全的。\\n\\n15.如果你现在有些焦虑，眼神交流让你感觉压力山大，你可以尝试看对方双眼的中间。这同样会让你显得友好，自信。\\n\\n16.注意脚。脚也是身体语言的一部分。比如，当你靠近一个人，对方只是把身体转向你，并没有把脚转向你，可能是他更愿意独处。当你跟一个人在对话的时候，当他的脚已经指向别处，可能他已经想走了。\\n\\n17.不要做中间的那个人。你去面试的时候，尽量让自己第一个或最后一个进去面试。因为人们对头和尾记得比较清楚，中间的很多人就会趋向模糊。\\n\\n18.如果你问了一个问题，希望对方能同意你，在说的时候，微微点头。这在心理学上被称为镜像效应，如果你发出积极的信号，人们更有可能赞同你。\\n\\n19.如果你想让某人平静下来。你可以对他们表达同理心，同时用降级的方式进行描述。我明白你为什么那么生气。你有理由抓狂。这个世界也让我很烦。他们会接受他们想要的这种认同，同时也会接受这种递减的情绪，从而变得更加平静\\n\\n**从人后遭黑到被人尊重：五个简单易学的社交常识**\\n\\n以下这些人际交往的技巧常识，都是我在吃亏后总结出来的。我因为不懂这些技巧常识而吃过的亏主要有：\\n\\n1. 被人在背后负面评价。\\n2. 被一些朋友疏远，再聊已觉尴尬。\\n\\n我希望看到这个答案的知友，能从中学到一些常识技巧，并应用在自己的社交生活中，避免遭遇我曾经历的社交窘境。哪怕只有一部分知友能记住并应用这些常识技巧，或只能记住并应用其中一部分，我写这个答案的目的也就达到了。\\n\\n1. **怎样才能让别人相信你是 “对事不对人” ？**\\n\\n国人较为热衷评价他人。这种热衷从知乎常见的提问方式 “如何评价...” 就可见一斑。当然，评价他人本身是无罪的，但评价他人有可能招致他人的厌烦。当我们在给予他人的作品差评或中评时，我们往往喜欢先说一句 “我这是对事不对人噢” 或 类似的话。仿佛我们说出这句话后，别人就会客观理性地对待我们之后要给出的差评或中评。可现实往往事与愿违。在大多数人看来，对于自己作品的评价就是对于自己的评价。否定我的作品，就是否定我这个人。也许这些人冷静下来后可能会意识到我们的评价的确是对事不对人，但在接到负面评价的那一刻，很少有人能心平气和。\\n\\n针对这个问题，我现在践行一个解决方案---不单独评价一个人作品，而是去对比眼前的作品和这个人曾经的作品。比如，当我觉得一个合作伙伴设计的一个按钮与整个页面不太搭时，我不会单单评价这个按钮的设计，我会说，我觉得你之前为C页面设计的C1按钮，和为D页面设计的D1按钮都和整个页面非常契合，我都非常喜欢，但是现在这个E页面的E1按钮我觉得跟整体不太搭，要不你再改一下？\\n\\n基于我的经验，这种评价方式对他人的情绪冲击往往比较小，也不容易让人觉得你是通过否定他的作品在否定他这个人。当然，有的人也许会问，如果第一次评价一个人的作品，没有参照物怎么办？我也经常遇到这种情况，我会拿这个作品与他的其相关他闪光点去进行对比。比如，选择字体的品味对比选择配图的品味，或对于挑颜色的品味等。\\n\\n1. **我们问了一个具体问题后，别人的回答的回答却很抽象，我们应该怎么办？**\\n\\n这种情况下最明智的做法就是不要再追问了，因为对方十有八九是不想告诉你。我来简单分析一下对方的内心想法。\\n\\n张同学问李同学：“你上学期平均分多少呀？”\\n\\n李同学心想，我上学期门门都上了90分，GPA 4.0，但是这个张同学好像没一门上80分。我如果直接说了，会不会让他感觉不痛快，觉得我在炫耀？就算真的要说，用什么语气说比较好？哎，好麻烦，不说算了。但他都问了我总不能不回答。看来，我只能泛泛而答了。\\n\\n于是，李同学就回答：“还不错吧。”\\n\\n这时，机智的张同学应该就此打住，而不是追问：“不错是多少呀？”\\n\\n1. **怎样向他人寻求帮助，他人才会更愿意帮我们？**\\n2. 让别人知道你已经做了许久的前期资料搜寻。实在是有个点搞不懂，解决不了，才寻求帮助。\\n3. 具体描述你的问题，以及你所需要的帮助。如果对方意识到他需要像剥洋葱一样，一次又一次和你对话，才能了解你的意图以及提供帮助的难度，那么他帮助你的意愿就会大大下降。\\n4. 如果对方没有时间或精力寻求帮助，可以询问对方是否有其他渠道可以获得帮助，比如某个论坛，某个网站，或者某个人。\\n\\n本条建议尤其适合应用在向不太熟的人寻求帮助时。\\n\\n1. **我们真的有很多朋友吗？**\\n\\n其实我们认识的许多人最终都不会成为我们的朋友。因为友谊的建立需要信任，而信任往往需要两个人互相不断解决对方的痛点才能逐渐建立。考虑到我们有限的精力和时间，我们能拥有的真正的朋友往往不会太多（所以一定要珍惜友谊）。茫茫人海，为什么范晨同学与李冰冰同学成为朋友，而不是和章新宇同学成为朋友？这就是缘分，也是友谊双方共同努力的结果。这个道理之于社交生活的意义就是，不要随便认为别人是你的朋友，更不要随便要求别人以朋友的标准来对待自己。如果我们在与一个萍水相逢的人相处时，始终生活在 “对方是我的朋友” 的幻觉中，很容易既让自己内心受挫，也让对方觉得自己 “想太多” 。\\n\\n1. **和别人说话时目光不时游移，别人会怎么想？**\\n\\n我就这个问题问过许多人，各个年龄层，各个职业的都有。大部分人都告诉我，如果别人说话时目光游移，他们会觉得对方心不在焉。所以，为了给对方起码的尊重，我们在说话时最好看着别人的眼睛（当然，要把握好度不要直勾勾地看）。\\n\\n我曾经对这点不是特别重视，总以为自己即便目光不时游移，对方也不太可能察觉。但后来我发现他人目光游移真的很容易察觉。大家可以专门训练几次。我大概注意过三次之后，就能做到看着别人的眼睛说话了。这也许是很小的一个社交细节，但如果我们做不好，很容易让对方觉得我们不真诚。\\n\\n我们怎样才能学会优雅地说“不”呢？下面就是一些基本的原则和具体的方案，教你怎样来优雅地说“不”。 1.把决定和关系分开来 当别人让我们去做什么的时候，我们往往会把这个要求和自己同他们的关系搅和在一起。有时候，它们看上去是如此的相互关联，以至于让我们忘记了拒绝这个要求并不等同于否定这个人。只有当人们把决定和关系分开来的时候，才能做出一个明确的决定，然后另行找到勇气和同情来传达它。 2.优雅地说“不”并不代表必须使用“No”\\n\\n精要主义者接受自己不能始终都让所有人喜欢的现实。诚然，怀着敬意，合情合理并且优雅地说“不”，会在短期内带来社交成本。但是，精要人生的题中之义，就是认识到从长远来看，受人尊重比被人喜欢重要得多。\\n\\n推荐一本书《10秒钟让自己不同反响》，作者莉儿·朗蒂是国际著名的人际沟通专家，她专门教授人际沟通技巧，替《财富》杂志前500强的大企业的高级主管及面队对客户的第一线职员授课，让他们成为更有效率的沟通专家。\\n\\n这是我读过最好最实用的人际交往书，上至与五百强高管打交道，下至泡妞把妹装绿茶，只要领会其中精髓无往而不利，比某些恋爱都没谈过几次就出来教人把妹的泡学大V强得多，不服来辩！\\n\\n这本小册子里面有很多trick，我列出几条供大家参考：\\n\\n一，排山倒海的笑容\\n\\n和人打招呼时**不要立刻微笑**，那会让人觉得，每个进入你视线的人都是你微笑的对象。\\n\\n你应**先注视对方一秒，停一下**，把他的脸输入脑子里，然后以**又大又温暖的笑容**，让笑扩散到整个脸庞，连眼里也充满笑容。\\n\\n这种笑容会将对方仿佛吸入温暖的水流中，如此不到一秒的延迟，会让对方感觉你的笑容十分真诚，且是他独享的特别待遇。\\n\\n二，眼神像太妃糖\\n\\n即使他说完话之后，你的眼神也**不要移开**，如必须移开，做得**慢一点**，百般不情愿似的。\\n\\n三，眼神要像超级强力胶\\n\\n即使别人在发言，你也要盯住你的目标。**不论讲话的人是谁**，一直**盯住你的目标对象**就是了。\\n\\n一个温和一点的方式：你的眼看着说话的人，不过每当说话人讲到一个段落，你的眼神要顺势溜转到目标对象身上。这样，目标对象仍感受到你对他的注意，不过感觉稍舒缓。\\n\\n四，天生的领袖会率先鼓掌。\\n\\n不论台上的名声多显赫，他的内心其实是一只畏缩的小猫咪，很担心观众的反应。台上的人看到你率领听众给他鼓励、为他喝彩时，他会把你也当做一号大人物。所以，请做第一个鼓掌的人，率先对你赞同的演讲人（或你想巴结的人）提出公开赞美。\\n\\n五，大孩子自我中心\\n\\n对初识者予以100%的注意力，给对方亲切的微笑，**整个身体转向他**，全部注意力都集中在他身上。那表示你在大声对他说：“我觉得你非常非常特别！”\\n\\n六，嗨！老朋友\\n\\n初见某人时，**想象他是你的老朋友**，只不过无情的命运将你们分开。但是，今天在这个场合，你和失散许多年的老朋友又重逢了！\\n\\n七，控制你的毛躁\\n\\n**毛躁的小动作**会让听众觉得你在胡诌，比如扯衣领、摸脖子、眼神飘忽。\\n\\n八，神骏汉斯的灵性\\n\\n表达自己的看法，同时更要注意对方的反应。根据他的反应，**谋定而后动**。别人会说你处处仔细体贴，凡事一点就通，非常上道。\\n\\n十，配合对方的情绪\\n\\n张嘴之前，先对谈话对象进行“声音采样”，以**试探他的情绪**状态。如果你希望别人听你的想法，你就必须先配合他们的情绪和语调，可能只要一下子就够了。聊天的重点不在于具体事实或措词，而在于其音乐性，在于它的旋律。\\n\\n十一，热情的平板乐章\\n\\n担心一开始不晓得说什么吗？别怕，80%的人对你的印象都和你说的话无关，开场白说什么，几乎都没关系。无论内容多么平板，只要你**怀着平常心**，正面的心态，热情地把话说出来，就很容易引人入胜。\\n\\n十二，随时穿戴“那是什么”\\n\\n如果你去一个都是陌生人的社交场合，记得穿戴一个**新奇甚至古怪**的饰品，如此一来，有人远远看到你，产生好印象，就借口上前攀谈：“不好意思，我注意到你的……**那是什么**？” 然后你就有话说了：“哦，那是我去xxx旅行时买的blabla……”或者“哦，这是当年我blablabla……” 然后话题就展开了。\\n\\n十三，那是谁？\\n\\n这个技巧是有史以来最少用（除了政客以外），却最有效的。\\n\\n只要**请主人介绍**你们认识，或问问足够的资讯，然你有话题上前打破僵局即可\\n\\n十四，偷听\\n\\n找不到别人身上的“那是什么”，找不到主人，使不出“那是谁”这一招，没问题！你只要悄悄贴近你想渗透的那群人，洗耳恭听就行了。听到稍值利用的借口就赶快插入，“不好意思，我刚好听到……”\\n\\n十五，高手不会喋喋不休\\n\\n在和名人聊天时，别去称赞他的作品，只要表达你从中得到许多喜悦和启发就足够了。如果真提起作品或成就这方面的话题，最好谈的是对方目前或近期的表现。不要翻出已经泛黄的陈旧记忆。如果女王蜂身边还生着一只雄蜂，记得要让它也能参与你们的对谈。\\n\\n十六，别急着说“我也是！”\\n\\n当你发现和某人有共同点，**你越慢说出来，对方的惊喜和震撼会越大**。如此一举，你的举止也像只耐心、优雅的大老虎，而不是毛躁、孤单、急着找伴的小野猫。附带声明：别等太久才说出你们的共同兴趣，否则会让人觉得你在卖弄。\\n\\n十七，你做哪一行？——千万别问\\n\\n正确方式是“你平常都做什么？”\\n\\n留给别人一点疑猜的空间，能够显示你的高手风范。当然，你会去求证这个问题的答案，只不过别说出那5个字。否则，别人马上把你定位成：无礼又爱套交情，一心晋升上流社交圈，钓金龟婿或吃软饭的淘金者，没有过过优质生活的小角色……\\n\\n十八，“精彩，让我们再听一次”。\\n\\n对你的谈话对象来说，从你口中发出的最美妙的声音，莫过于在一群人面前说：“跟他们说说你那次……”\\n\\n十九，有样学样\\n\\n仔细观察别人，看看他们的举手投足是有何特点。仔细观察它的肢体语言，然后**模仿其举止风格**。这样，对方下意识会觉得跟你在一起很自在\\n\\n二十，提前使用“我们”\\n\\n就算你刚认识这个人，也可以建立亲密的感觉。与人交谈时，跳过第一阶段（陈年老套）和第二阶段（描述事实），直接进入第三（感受及个人问题）和第四（“我们”表述法）阶段。巧妙地运用“我们”这两个字，两人的亲密关系一触即发。\\n\\n二十一，速成的共同回忆\\n\\n如果我想和一个陌生的朋友拉近关系，想想你们两个第一次见面时有什么特别值得回忆的共同片刻。找一些话题唤起当初两人之间的笑语或亲切感觉。现在，你们就像老朋友一样，有共同经验可以一起分享。\\n\\n啊呀，**我们真像：我们简直一模一样**。\\n\\n二十二，哇，是你啊！\\n\\n接电话时的态度不要给人造成一种印象：我随时随地都很快乐。态度要亲切，说话要简洁，给人专业的感觉。**等对方表明身份之后，**再露出温暖，灿烂的笑容，让声音里也充满笑意。更让对方感觉到，你的笑容是专为他绽放的。\\n\\n二十三，向配偶致敬\\n\\n每次打电话到别人家中，一定要报出自己的姓名，并向接电话的人致意。如果你是打到别人的办公室，次数不止一两次，最好跟秘书建立友谊。对方如能替大人物接电话，他也有能力左右大人物对你的看法。**致电家中时：向配偶致敬；致电办公室：向秘书致敬**。\\n\\n二十四，另一部电话响了\\n\\n讲电话时，如果听到对方另一部电话响了，就要停止你的发言——必要时，就算说到一半也得立刻停下来——告诉她：“我听到另一部电话响了（我听到你的狗在叫/宝宝在哭/你老公在叫你）”问对方需不需要先处理。不论对方的反应如何，从你体贴的举动中，她已了解你是个沟通高手。\\n\\n二十五，必杀赞美术\\n\\n如果和你谈话的陌生人可能在你个人或事业未来扮演举足轻重的角色，你要从他身上找出一个很明确、个人独有的特质。两人谈话即将结束时，**直至看着他的眼睛。先说出他的名字，再把你的必杀赞美词说出来**，对方必定满心欢喜。\\n\\n注意事项：\\n\\n1，必须在**私底下使用**。假如你和四五个人站在一起，只称赞其中一个女人身材苗条，其他的女人一定会觉得自己像一桶猪油。如果你对一个男人说他玉树临风，其他男人会觉得自己成了钟楼怪人。此外，你也会让被称赞的人窘迫不已；\\n\\n2，赞美词必须有**可信度**。要是有人笨到对自知是个音痴的人说喜欢他的歌声，他会把赞美当作猪吃的馊水；\\n\\n3，对同一个人重复使用这一招**至少间隔半年**。否则别人会觉得你不真诚，好阿谀奉承，必有所企图。那就不妙了。\\n\\n2.当你在见新的朋友或者客户时，在谈话当中，记得多叫提及他们的名字，即可以让他们觉得你很在乎他们，你也可以把他们的名字记牢。\\n\\n3.遇到有人气势汹汹，脱掉他们的上衣准备和你大干一场时，你可以默默地脱掉你的裤子，当他们问你在干嘛时，你说你喜欢一丝不挂地打架。（气氛有点怪异）\\n\\n4.下次当你担心自己在公共场合时自己的所做所为，别人会怎么议论你时，试着想想你平时是怎么看待别人的。你一定会克服所有的顾虑，因为who cares?没有人多么在意你的所作所为，以及你所谓的出洋相，所以，大胆地做吧！\\n\\n5.多观察别人说话当中的言行举止，例如他们有什么常说的词汇？他们在平时有什么经常做的动作？试着和他们一样，这样会拉近你们之间的距离。\\n\\n6.**在道歉的时候，用停顿来代替“但是”。**\\n\\n例如：\\n\\n对于这件事情我很抱歉，但是我是身不由己。\\n\\n下次试着这样子说：\\n\\n对于这件事情我很抱歉（停顿），我是身不由己。\\n\\n这是相关的链接https://www.youtube.com/v/J1wTgyWvQ38?start=0&end=20&autoplay=1\\n\\n7.如果你想引起一个受欢迎或者非常有魅力的人的注意，记得坐在Ta的旁边，和Ta周围的朋友谈笑风生，如同把Ta当做不存在，慢慢地Ta就会开始找你聊天，这叫欲擒故纵。\\n\\n8.血糖真的很重要。记得不要在别人饥肠辘辘的时候向他求助；当然，更要**记得千万不要在没有吃饭的情况下和女友逛街。**\\n\\n9.如果你知道你做了一些东西让某人感到不悦，可以选择在公共场合坐在Ta的身边，这样会降低Ta对你的怨气。\\n\\n10.在问别人问题的时候，可以一边问，一边轻轻的点点头，这样可以增加别人同意你的机会。例如：“你会支持我的是吧？”，一边问，一边点头。\\n\\n12.如果确实要和中意的人表白，选一个黄昏的时候吧，Ta接受你的机会会增大。\\n\\n13.承诺和一致性。想要人为了做一件事情，不要在私底下跟他说，要在公共场所和他提及这件事情，如果他答应了，那他完成的几率会大大增加，因为他为了保持承诺的一致性。\\n\\n14.如果你在面试之前紧张，可以到厕所做一个胜利姿势，没错就是刘翔刘飞人夺冠时的动作，这样可以增强你的自信心。\\n\\n15.**抛选择，而不是询问。**\\n\\n例如：\\n\\n我：爸爸，我们家要为了准备50岁的生日Party！\\n\\n爸爸：我们家没有要开Party！\\n\\n下次这样说\\n\\n我：爸爸，我们去哪里庆祝你的50岁生日Party？在家还是去XXX酒店？\\n\\n爸爸：去什么酒店，直接在家啊。\\n\\n1。把“男神/女神”当普通异性好友对待；把普通异性好友当男神女神对待。你的人际关系会好很多很多。\\n\\n2。和陌生人发短信一律用“您”，如果是自己主动发的末尾加上“打扰您了”\\n\\n3。面对网上的喷子，不回复不讨论；如果必须要回，回复“你开心就好了：）” 。 ：）\\n\\n4。面对他人的冷眼冷语甚至挑衅，微笑地看着他，不解释就好。\\n\\n5。帮别人的忙，就算很轻松，也最好答应的时候言语上给自己加点麻烦，别人的印象会更深些。比如你正好顺路，也要说“啊本来我要走这边的，这样吧，我绕一下，没事的。”\\n\\n（有知友说这一条有争议，建议不给人心理压力把这一条反过来，也有道理，大家自己选择啦：））\\n\\n（更多讨论见后）\\n\\n6。不要吝惜赞美。赞美别人正在努力的方面，比如今天特意穿了漂亮的裙子，比如告诉正在减肥的胖子自己看到效果了。 你的赞美会让别人很开心。\\n\\n7。做错了事情不要找借口理由。认错就是了。\\n\\n8。永远给自己准备planB，没有准备也要在面临困难时自己想出应对办法。你的上级只会在意你有没有完成。\\n\\n9。永远不要因为寂寞和你不喜欢的人做情侣。\\n\\n10。做一个充满正能量的人\\n\\n1. 出现问题（无论大小）时候先表示歉意，再解释。\\n2. 多察言观色，发现别人对话题失去兴趣或者想离开的时候，能自然的过渡话题、自己主动做终结话题的人。\\n\\n（sns同理 争取每次最后一句话都是自己说的）\\n\\n**1.切忌不要大嗓门，提高声调。**\\n\\n有理不在声高，大嗓门的yelling会让人下意识的产生抵触情绪，阻碍交流。交流的目的不是说服别人，而是为了让双方都能听到不同声音，看到不同的的观点。\\n\\n能说服人们的，永远只有他们自己。\\n\\n**辩论是最没有意义的事情，无论最后在形式上哪一方获胜，其实都是双输。**\\n\\n**2.当对方想和你辩论时，请选择不说话。**\\n\\n如果一个人还想试图说服一个人时，说明他还不是一个人情练达的人。能让人做出改变的，只有权衡利弊。\\n\\n既然他渴望一次对你而言毫无影响，属于他自己的的胜利，满足他。\\n\\n**3.当你意识到你的观点是不正确的，请立马承认，并指出自己的不对，为对方点赞。**\\n\\n承认不足并道歉，是一个人成熟的标志。\\n\\n**4.当你的观点正确时，请感谢不同观点的人，仅此而已。**\\n\\n对的就是对的，其他人不惜要虚情假意的安慰，因为这只是讨论问题。\\n\\n**5.讨论问题请不要带入任何个人情感。**\\n\\n让我们成为朋友的原因不是观点的相同，而是优秀的品质和人格魅力的相互吸引，即便观点相左，我们还是朋友。\\n\\n**夸人的时候直接用you，给人提负面意见的时候用物品，如(your ) paper is**\\n\\n作者：车长\\n\\n链接：https://www.zhihu.com/question/327635458/answer/819827259\\n\\n来源：知乎\\n\\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\\n\\n**1、关注对方感受**\\n\\n比如：\\n\\n把“嗯嗯”，换成“没有问题”\\n\\n把“随便”，换成“听你的”\\n\\n把“无所谓”，换成“我ok的”\\n\\n把“听懂了吗”，换成“我说明白了吗”\\n\\n**2、积极上进，阳光乐观**\\n\\n比如：\\n\\n把“我不会”，换成“我可以学”\\n\\n把“我不知道”，换成“我马上了解”\\n\\n把“你不行”，换成“你再努力下”\\n\\n**3、少传递负能量，不推脱责任**\\n\\n比如：\\n\\n把“我好烦”，换成“我需要冷静”\\n\\n把“我怎么知道”，换成“这个我也不太明白”\\n\\n**4、懂得示弱的人很会说话**\\n\\n我看过一个故事：\\n\\n英国维多利亚女王经常加班，经常夜深人静的时候才回寝室。这引起丈夫的强烈不满。一天晚上，女王还是加班到深夜才回寝室，和往常一样，她准备推门而入，却发现门被反锁了。\\n\\n于是尊贵的女王只好敲门。“谁？”里面的一个声音在问。“我，女王。”在她回答之后，门里面没有了反应，门依然锁着。\\n\\n于是女王只好继续敲门。“谁？”里面又问，女王回答：“是我，维多利亚”，但门还是没开。\\n\\n女王只好再次敲门。“你是谁？”里面已经很不耐烦。女王终于意识到问题所在，温柔的回答：“是你的妻子。”们开了。\\n\\n**5、会化解尴尬的人很会说话**\\n\\n林志林和梁朝伟拍《赤壁》后，记者采访时问了个尖锐问题：\\n\\n“你是否介意于梁朝伟的身高不相称？”\\n\\n林志玲是这么回答的：\\n\\n“我觉得身高我是没有办法克服的，但是我和梁朝伟演戏也没必要给他小板凳。在我心里，男人的风度远胜于高度”\\n\\n一句话，既化解了尴尬，又赞了梁朝伟有风度，一举两得\\n\\n1. **不评判（non-judgmental）对方。**\\n\\n人们都喜欢不会评判自己的人，在这样的人面前，我们会感到安全。从而愿意更多地向对方袒露自己、推进话题。\\n\\n这听上去很简单，你大概以为就是语言是不批判别人不就行了。但大多时候，**不评价的态度是很难做到的。**\\n\\n**首先，它要求一个人有着足够复杂的世界观、和对他人足够的尊重和愿意付出理解的努力。**否则当人们接收到超出自己世界观的信息时，是很容易因为不知所措，而快速做出（自己不自知）的批判性的结论的（例如恐同现象）。\\n\\n**不评判的态度还要求人有足够的谦卑，不以为自己能够知道他人的所有处境。**不认为自己比他人优越，一定能够做出更好的判断。自恋者是很难做到不评判的。\\n\\n**它还要求一个人有足够的情绪管理能力，能够在互动的过程中，牢牢掌握住自身的情绪。**否则，难以承受共情造成的情绪负担时，我们很容易为了让自己好受一点，而说出一些自以为好意安慰的话，比如“你想太多了”、“你太消极了”、“别胡思乱想了”，而实际上都暗含着评判**——会瞬间打消对方继续聊天的意愿。**\\n\\n1. **三步走的探索技术，给予ta真诚的关注。**\\n\\n在这里，我们结合咨询心理学教授Clara E. Hill在他的著作《Helping Skills》中总结的三个步骤，并结合了一些探索技术，帮助你和他人开展更真诚的对话。\\n\\n**Step 1：专注地倾听和观察ta**\\n\\n在第一步里，**你不用考虑说太多，但你需要让对方感到安全和舒适。** **想要做到这一点，很多时候，你需要使用肢体语言、表情等非语言沟通方式。**看着对方，不时微笑、点头、保持眼神接触、以一种更开放的坐姿面对ta……这一系列举动，都可以表达你的真诚和专注。\\n\\n- **记得给予对方认可（approval-reassurance）：**\\n\\n认可给对方提供了**支持和保证，让ta知道自己的感受是可以被接纳的。**比如，使用一些轻微表达认可的话语：“确实是这样”、“我也这么觉得”。或者**只是用认可的表情和肢体语言聆听对方、接纳对方的情绪。**\\n\\n要注意的是，**不要试图通过认可去淡化对方的情绪。** 例如，ta工作很累、感到压力很大，如果你说“大家都是这样的，每个人都不容易”、“过段时间就好啦”，这样的话很可能会起到反效果，让ta觉得自己好像不该产生这样的感受，感到自己被否定了。\\n\\n- **能够允许聊天中有沉默：**\\n\\n“此时无声胜有声”，沉默是你和ta都不说话的时候，发生的短暂停顿。**你不需要非得立刻做出反应，因为这是一个给你倾听ta、同时让ta继续思考的机会。**\\n\\n例如，当ta说：“我真的要气死了，我都不知道要怎么说了”、“我太难过了，什么话都说不出口”，**这时候，对方需要一些时间，来处理消化自己的情绪。你可以保持沉默等待ta继续补充，不要打断ta的思考。**\\n\\n**Step 2：和ta一起探索ta的想法**\\n\\n通过第一步，你可能发现了一些ta潜藏的想法和感受，循着这些线索，你要在这一步中和ta一起沉浸当下，进一步探索。\\n\\n- **适时地重述对方的话（restatement）：**\\n\\n**重述需要你对ta讲过的话、表达的意思加以复述，但往往你要讲得比ta简短清晰。** 重述可以是试探性地发问，例如：“你刚刚是说，你爸妈准备离婚？“也可以直接一点：”你爸妈分居了？“\\n\\n**重述的重点是聚焦在对方身上的。**即使ta一直在谈自己的同事、朋友，你不能只顾着好奇和八卦，而是要帮ta重新回到自己身上。**重述的常见开场白有这样几种，比如：“我听到你说……”、“听起来好像……”、“你刚刚说……”**\\n\\n**这个过程，是让对方感受到你的关注点真正地在ta身上，并且你的提问让ta有机会再一次思考自己。**\\n\\n- **多用开放式提问：**\\n\\n这个技术可以有效地避免你“把天聊死”。**通过开放式提问，你不会限制对方只做出“是/否”的回答，或一两个字的反应。**\\n\\n以下这样的提问就是开放式的：“你觉得怎么样？”“你上次说道……是怎么想的？”“可以举个例子吗？”\\n\\n**要注意语气尽可能温和、缓慢，不要像审问或是教育对方一样。让对方感受到你善意的、理解性的好奇，而绝对不是“质疑”，否则会引起对方的抵触情绪。**同时，**也不要一次性问好几个开放式的问题，这样对方会不知道要先回答哪一个，还有可能忽略你的问题。**\\n\\n**Step 3：鼓励ta体验和表达**\\n\\n- **在情感上给对方反映（reflection of feelings）**\\n\\n情感反映是指在重述的同时，明确地指出对方的情感。有时候，ta可能直接说出了情绪，但更多时候，ta可能是以情绪化的语气说出了一些信息。**你需要和ta一起识别、澄清这些情绪，并深入体验它们。**\\n\\n比如，你可以试探或肯定地这样说：**“你看上去对你朋友爽约感到很生气？”“你刚说你拒绝了领导加班的要求，你现在看起来很开心。”**\\n\\n- **适时表露自己的情感：**\\n\\n**聊天不是单方面的。你可以呈现自己和ta在相似情境下的感受。**通过情感表露，示范自己可能体验到的类似情绪，可以激发对方认识并表达自己的情绪。\\n\\n例如：“我找工作的时候，想到面试就害怕，我想知道你是不是也有这样的感觉？”**“在你说的那种状况下，我也会感到很紧张。”**但也需要注意，**分享自己过去在相似情境下的感受，不意味着你可以滔滔不绝讲自己的事，让对方无话可说。**\\n\\n四种倾听手段\\n\\n为鼓励对方道出行为动机，我们应当使用四种有效的倾听技巧营造安全感，让对方坦率说出心中想法。这四种技巧分别是：询问观点、确认感受、重新描述和主动引导，它们既可以解决沉默应对问题，也可以解决暴力应对问题。\\n\\n1.询问观点\\n\\n鼓励对方说出想法，最简单直接的方式就是请他们开口表达。在对话中要想打破僵局，你只需理解对方的观点即可。当我们表现出真正的兴趣时，对方就不会迫于压力而陷入沉默或暴力了。例如：温蒂问道：“你喜欢我这身装扮吗？还是准备骂我有伤风化？”\\n\\n“为什么这么说呢？我想听听你的看法。”你回答道。\\n\\n如果你能做到不强迫对方接受你的观点，而是鼓励对方说出他们的看法，这样就能走入正轨，破除对话反复陷入危机的恶性循环，找到问题的根源。\\n\\n常见的鼓励性话语包括： \\n\\n“发生什么事了？”\\n\\n“我想听听你对这件事的看法。”\\n\\n“如果你有不同的观点，可以直接告诉我。”\\n\\n“别担心你的想法和我不一致，我很愿意了解你的想法。”\\n\\n2.确认感受\\n\\n如果直接询问无法让对方开口，你可以通过确认感受的方式营造更多安全感。使用这种方式时，我们会客观描述在对方行为模式中观察到的细节，然后鼓励对方对此进行讨论。由于我们目前能够观察到的只有对方的行为表现以及情绪反应带来的暗示，因此我们必须从这里开始入手。\\n\\n在确认对方感受时，我们应当扮演“镜子”的角色，描述他们的外在表现或行为。虽然我们不了解对方的想法和观察到的事实，但我们可以观察并模拟他们的行为。\\n\\n当对方的语气或体态（可暗示出隐藏其中的情绪）和他们的表达内容不一致时，利用这种方法鼓励对方开口特别有效。例如：“没事，我很好。”（但说话人的表情和语气表明他其实在生气，因为他在皱眉头、焦虑地四处张望，脚不耐烦地踢来踢去。）\\n\\n“真的吗？从你说话的方式来看，好像感觉并不好。”\\n\\n也就是说，当对方所说的话和语气与体态的表现格格不入时，我们应当抓住这个细节了解其感受。这样做的好处是，我们可以对对方表现出尊重和关注。\\n\\n当回顾你观察到的表现时，注意控制你的语气和表达方式。实际上，承认对方的情绪就能营造出对话安全感，这个观点并非事实。安全感的营造是因为我们的语气和表达方式可以让对方感受到，我们毫不排斥对方产生的情绪。只要我们做好了这一点，对方就会认为压抑自己的情绪是多余的，从而产生和我们坦率交流的信心。\\n\\n因此，在描述观察到的情况时，我们必须做到冷静客观。如果我们紧张不安或是流露出厌恶对方观点的表现，安全感的营造就无从谈起了。恰恰相反，我们这样做只会坚定他们想要保持沉默的念头。\\n\\n确认感受的例子有：\\n\\n“你嘴上说没事，可听起来不像没事的样子。”\\n\\n“你好像对我很生气。”\\n\\n“看起来和他理论让你有点紧张，你确定要这样做吗？”\\n\\n3.重新描述\\n\\n询问观点和确认感受能帮助你部分了解对方的想法。如果意识到对方为什么会出现特定感受，你可以通过复述对方表达的方式营造更大的安全感。注意，复述指的不是一字不差地把对方的话重说一遍，而是用你自己的语言简略地说明自己了解的内容。例如：\\n\\n“好吧，你看看我的理解是否正确。你感到紧张是因为我对你的衣着方式表达了看法，这让你感到我太专制太守旧，是这样吗？”\\n\\n和确认感受一样，重新描述这个技巧的关键之处在于，你必须在陈述时保持冷静镇定。记住，我们的目的是要营造对话安全感，而不是表现得惊慌失措，暗示对话马上就要出问题。你应当努力思考的是，为什么一个理智而正常的人会做出这样的举动呢？这个问题可以帮助你远离愤怒感或抵触情绪，学会冷静面对问题。你只需用自己的话重新描述对方的表达，同时注意保持冷静即可。这样就会让对方感到你在努力理解他们的感受，支持他们坦率地说出内心想法。\\n\\n不要急于求成。现在我们来看看已经取得了哪些成绩。可以肯定的是，对方一定有很多想法没有分享过。他们准备以沉默或暴力方式面对问题，我们很想知道他们为什么会这样做，想从这种行为回溯到问题的源头（即事实），然后成功地解决问题。为鼓励对方分享观点，我们尝试了三种倾听手段，即询问观点、确认感受和重新描述，但对方还是紧张不安，不愿说出全部的想法或事实。\\n\\n现在该怎么办呢？这种情况很容易让我们泄气。在这种心理的影响下，我们用不了多久就会失去耐心，让对方感觉到我们先前的努力并不真诚。如果我们太急于求成，结果只会适得其反，破坏对话的目的和尊重感。对方会认为我们的目的是为了私利而了解其观点，根本不在乎其个人感受。因此，我们必须放慢节奏。正确的做法不是直奔对方的情绪源头，而是及时退出或是询问对方希望看到怎样的结果。询问对方的目的可以避免他们陷入攻击或逃避的简单思维模式，转而思考更重要的问题解决方案。同时，这样做也能帮助揭示他们出现问题的根本原因。\\n\\n4.主动引导\\n\\n不可否认的是，有些时候尽管对方愿意袒露心声，但你能感觉到他们还是缺乏安全感。或者，他们仍处在暴力状态中，未完全消除肾上腺素的影响，自己也说不清楚为什么会感觉愤怒。遇到这种情况下，你就该使用主动引导技巧了。如果意识到对方没有充分表达内心看法，还需要你做出努力时，你可以主动引导对方实现对话。\\n\\n主动引导这个表达源自“灌液泵”这个概念。如果你用过老式的手泵，肯定会明白其中的比喻含义。在使用这种泵时，你必须灌入水才能启动机器。然后，它才开始正常运转。在对话中倾听对方表达也是这样，有时候你必须提出一些关于对方想法和感受的猜测，然后才能顺利打开他们的话匣子。换句话说，要想让对方做出积极响应，你必须首先向共享观点库中提供一些信息。\\n\\n几年前，本书的一位作者曾为某公司提供过培训服务，该公司管理层决定在某个工作区增加中班，原因是公司设备产能未得到充分使用，如果不这样做无法承担高额的场地使用费。这就意味着，原来只上日班的员工必须两周一轮换，改上早班和中班。虽然很折磨人，但公司这样决定也是无可奈何之举。\\n\\n当管理层开会宣布这个不受欢迎的决定时，员工们都沉默不语。他们显然并不开心，但并没有人站出来表达意见。生产经理担心大家误解公司的决定，认为公司这样做只是想拼命多赚钱。实际上，这个工作区一直处于亏损状态，但这个决定肯定会对员工造成影响。如果不改成双班制，大家恐怕连工作都保不住。生产经理很清楚，轮班制会让大家在午后和晚上没有时间陪家人，给他们带来严重的心理负担。\\n\\n面对员工闷闷不乐的沉默，这位经理想尽办法让大家说出内心想法，以免他们带着愤怒情绪离开。他开始确认对方感受：“看得出，你们并不高兴，谁不是这样啊？我们有什么办法呢？”结果还是没人响应，最后，他开始主动引导大家。也就是说，他努力猜测员工此刻可能产生的想法，然后用一种鼓励的方式表明欢迎大家开门见山地讨论这个问题，最后在此基础上寻找解决办法。他向大家说道：“你们是不是觉得公司这么做就是为了多赚钱？觉得我们毫不关注你们的家庭生活？”\\n\\n这下有人开始发话了：“事情看起来就是这样嘛，你知不知道这样会给我们带来多大的麻烦？”很快，其他人开始插话，大家热烈地讨论了起来。\\n\\n需要提醒各位的是，在你尝试其他三种方式之前，不要轻易使用最后这种技巧。只有在非常想了解对方的看法，而且非常清楚他们的想法时你才可以这样做。主动引导是一种展示信念、承担风险、主动示弱和营造安全感的行为，目的是让对方彻底敞开心扉，说出内心想法。\\n\\n如果对方观点错误怎么办\\n\\n有时候，如果对方的看法和我们相距甚远，真诚探索他们观点的做法会让人感觉很危险。对方有可能根本是错误的，而我们还必须表现得镇定自若，这会让我们感到紧张不安。\\n\\n要想在探寻对方行为动机时消除自己的紧张感，无论他们的看法多么离谱或错误，我们都必须牢记这样一点，即我们的目的是要了解他们的看法，这样做并不表示同意或支持他们的看法。理解并不等于认同，但敏感却等同于默许。通过逐步了解对方的行为模式，我们答应会接受他们的观点，我们有足够的时间接下来和对方分享我们的看法。但是现在，我们只是试图了解他们的想法，以便弄清楚他们为什么会这样想和这样做。\\n\\n四种倾听手段\\n\\n为鼓励对方道出行为动机，我们应当使用四种有效的倾听技巧营造安全感，让对方坦率说出心中想法。这四种技巧分别是：询问观点、确认感受、重新描述和主动引导，它们既可以解决沉默应对问题，也可以解决暴力应对问题。\\n\\n1.询问观点\\n\\n鼓励对方说出想法，最简单直接的方式就是请他们开口表达。在对话中要想打破僵局，你只需理解对方的观点即可。当我们表现出真正的兴趣时，对方就不会迫于压力而陷入沉默或暴力了。例如：温蒂问道：“你喜欢我这身装扮吗？还是准备骂我有伤风化？”\\n\\n“为什么这么说呢？我想听听你的看法。”你回答道。\\n\\n如果你能做到不强迫对方接受你的观点，而是鼓励对方说出他们的看法，这样就能走入正轨，破除对话反复陷入危机的恶性循环，找到问题的根源。\\n\\n常见的鼓励性话语包括：\\n\\n“发生什么事了？”\\n\\n“我想听听你对这件事的看法。”\\n\\n“如果你有不同的观点，可以直接告诉我。”\\n\\n“别担心你的想法和我不一致，我很愿意了解你的想法。”\\n\\n2.确认感受\\n\\n如果直接询问无法让对方开口，你可以通过确认感受的方式营造更多安全感。使用这种方式时，我们会客观描述在对方行为模式中观察到的细节，然后鼓励对方对此进行讨论。由于我们目前能够观察到的只有对方的行为表现以及情绪反应带来的暗示，因此我们必须从这里开始入手。\\n\\n在确认对方感受时，我们应当扮演“镜子”的角色，描述他们的外在表现或行为。虽然我们不了解对方的想法和观察到的事实，但我们可以观察并模拟他们的行为。\\n\\n当对方的语气或体态（可暗示出隐藏其中的情绪）和他们的表达内容不一致时，利用这种方法鼓励对方开口特别有效。例如：“没事，我很好。”（但说话人的表情和语气表明他其实在生气，因为他在皱眉头、焦虑地四处张望，脚不耐烦地踢来踢去。）\\n\\n“真的吗？从你说话的方式来看，好像感觉并不好。”\\n\\n也就是说，当对方所说的话和语气与体态的表现格格不入时，我们应当抓住这个细节了解其感受。这样做的好处是，我们可以对对方表现出尊重和关注。\\n\\n当回顾你观察到的表现时，注意控制你的语气和表达方式。实际上，承认对方的情绪就能营造出对话安全感，这个观点并非事实。安全感的营造是因为我们的语气和表达方式可以让对方感受到，我们毫不排斥对方产生的情绪。只要我们做好了这一点，对方就会认为压抑自己的情绪是多余的，从而产生和我们坦率交流的信心。\\n\\n因此，在描述观察到的情况时，我们必须做到冷静客观。如果我们紧张不安或是流露出厌恶对方观点的表现，安全感的营造就无从谈起了。恰恰相反，我们这样做只会坚定他们想要保持沉默的念头。\\n\\n确认感受的例子有：\\n\\n“你嘴上说没事，可听起来不像没事的样子。”\\n\\n“你好像对我很生气。”\\n\\n“看起来和他理论让你有点紧张，你确定要这样做吗？”\\n\\n3.重新描述\\n\\n询问观点和确认感受能帮助你部分了解对方的想法。如果意识到对方为什么会出现特定感受，你可以通过复述对方表达的方式营造更大的安全感。注意，复述指的不是一字不差地把对方的话重说一遍，而是用你自己的语言简略地说明自己了解的内容。例如：\\n\\n“好吧，你看看我的理解是否正确。你感到紧张是因为我对你的衣着方式表达了看法，这让你感到我太专制太守旧，是这样吗？”\\n\\n和确认感受一样，重新描述这个技巧的关键之处在于，你必须在陈述时保持冷静镇定。记住，我们的目的是要营造对话安全感，而不是表现得惊慌失措，暗示对话马上就要出问题。你应当努力思考的是，为什么一个理智而正常的人会做出这样的举动呢？这个问题可以帮助你远离愤怒感或抵触情绪，学会冷静面对问题。你只需用自己的话重新描述对方的表达，同时注意保持冷静即可。这样就会让对方感到你在努力理解他们的感受，支持他们坦率地说出内心想法。\\n\\n不要急于求成。现在我们来看看已经取得了哪些成绩。可以肯定的是，对方一定有很多想法没有分享过。他们准备以沉默或暴力方式面对问题，我们很想知道他们为什么会这样做，想从这种行为回溯到问题的源头（即事实），然后成功地解决问题。为鼓励对方分享观点，我们尝试了三种倾听手段，即询问观点、确认感受和重新描述，但对方还是紧张不安，不愿说出全部的想法或事实。\\n\\n现在该怎么办呢？这种情况很容易让我们泄气。在这种心理的影响下，我们用不了多久就会失去耐心，让对方感觉到我们先前的努力并不真诚。如果我们太急于求成，结果只会适得其反，破坏对话的目的和尊重感。对方会认为我们的目的是为了私利而了解其观点，根本不在乎其个人感受。因此，我们必须放慢节奏。正确的做法不是直奔对方的情绪源头，而是及时退出或是询问对方希望看到怎样的结果。询问对方的目的可以避免他们陷入攻击或逃避的简单思维模式，转而思考更重要的问题解决方案。同时，这样做也能帮助揭示他们出现问题的根本原因。\\n\\n4.主动引导\\n\\n不可否认的是，有些时候尽管对方愿意袒露心声，但你能感觉到他们还是缺乏安全感。或者，他们仍处在暴力状态中，未完全消除肾上腺素的影响，自己也说不清楚为什么会感觉愤怒。遇到这种情况下，你就该使用主动引导技巧了。如果意识到对方没有充分表达内心看法，还需要你做出努力时，你可以主动引导对方实现对话。\\n\\n主动引导这个表达源自“灌液泵”这个概念。如果你用过老式的手泵，肯定会明白其中的比喻含义。在使用这种泵时，你必须灌入水才能启动机器。然后，它才开始正常运转。在对话中倾听对方表达也是这样，有时候你必须提出一些关于对方想法和感受的猜测，然后才能顺利打开他们的话匣子。换句话说，要想让对方做出积极响应，你必须首先向共享观点库中提供一些信息。\\n\\n几年前，本书的一位作者曾为某公司提供过培训服务，该公司管理层决定在某个工作区增加中班，原因是公司设备产能未得到充分使用，如果不这样做无法承担高额的场地使用费。这就意味着，原来只上日班的员工必须两周一轮换，改上早班和中班。虽然很折磨人，但公司这样决定也是无可奈何之举。\\n\\n当管理层开会宣布这个不受欢迎的决定时，员工们都沉默不语。他们显然并不开心，但并没有人站出来表达意见。生产经理担心大家误解公司的决定，认为公司这样做只是想拼命多赚钱。实际上，这个工作区一直处于亏损状态，但这个决定肯定会对员工造成影响。如果不改成双班制，大家恐怕连工作都保不住。生产经理很清楚，轮班制会让大家在午后和晚上没有时间陪家人，给他们带来严重的心理负担。\\n\\n面对员工闷闷不乐的沉默，这位经理想尽办法让大家说出内心想法，以免他们带着愤怒情绪离开。他开始确认对方感受：“看得出，你们并不高兴，谁不是这样啊？我们有什么办法呢？”结果还是没人响应，最后，他开始主动引导大家。也就是说，他努力猜测员工此刻可能产生的想法，然后用一种鼓励的方式表明欢迎大家开门见山地讨论这个问题，最后在此基础上寻找解决办法。他向大家说道：“你们是不是觉得公司这么做就是为了多赚钱？觉得我们毫不关注你们的家庭生活？”\\n\\n这下有人开始发话了：“事情看起来就是这样嘛，你知不知道这样会给我们带来多大的麻烦？”很快，其他人开始插话，大家热烈地讨论了起来。\\n\\n需要提醒各位的是，在你尝试其他三种方式之前，不要轻易使用最后这种技巧。只有在非常想了解对方的看法，而且非常清楚他们的想法时你才可以这样做。主动引导是一种展示信念、承担风险、主动示弱和营造安全感的行为，目的是让对方彻底敞开心扉，说出内心想法。\\n\\n如果对方观点错误怎么办\\n\\n有时候，如果对方的看法和我们相距甚远，真诚探索他们观点的做法会让人感觉很危险。对方有可能根本是错误的，而我们还必须表现得镇定自若，这会让我们感到紧张不安。\\n\\n要想在探寻对方行为动机时消除自己的紧张感，无论他们的看法多么离谱或错误，我们都必须牢记这样一点，即我们的目的是要了解他们的看法，这样做并不表示同意或支持他们的看法。理解并不等于认同，但敏感却等同于默许。通过逐步了解对方的行为模式，我们答应会接受他们的观点，我们有足够的时间接下来和对方分享我们的看法。但是现在，我们只是试图了解他们的想法，以便弄清楚他们为什么会这样想和这样做。\\n\\n小结——了解动机\\n\\n要想鼓励观点的自由交流，帮助对方摆脱沉默或暴力的错误应对方式，你应当了解他们的行为动机，应当在对话中表现出巨大的好奇心和耐心，只有这样才能恢复安全感。\\n\\n然后，你可以使用四种有效的倾听技巧，从对方的行为中寻找潜在的动机。\\n\\n·询问观点。表明你很有兴趣了解对方的看法。\\n\\n·确认感受。通过表示高度理解对方的感受增强安全感。\\n\\n·重新描述。当对方说出自己的看法时，你应当重述他们的表达，表明自己不但理解其观点，而且鼓励他们分享内心的想法。\\n\\n·主动引导。如果对方还是退缩迟疑，你应当“先发制人”，对他们的想法或感受做出最符合情况的猜测。\\n\\n在和对方分享观点时，应当注意以下几点：\\n\\n·赞同。在分享观点时对他人表示赞同。\\n\\n·补充。如果对方的观点有遗漏之处，赞同你们共享的部分，然后做出补充。\\n\\n·比较。当你们的观点相差甚远时，不要简单地认为对方是错误的，而应当把你们的看法进行比较。\\n\\n面对棘手问题时，关键对话高手是这样关注任务目标的：\\n\\n从我做起\\n\\n·记住，你唯一能直接控制的人只有你自己。\\n\\n关注你的真正目的\\n\\n·发现自己即将陷入沉默或暴力状态时，停止对话，冷静思考你的动机。\\n\\n·问自己这样一个问题：“我现在的行为显示出我的动机是什么？”\\n\\n·明确你的真正目的，问自己：“我想为自己、他人和人际关系做些什么？”\\n\\n·最后，问自己：“如果这是我的真正目的，我该怎么做？”\\n\\n拒绝“傻瓜式选择”\\n\\n·在分析行为目的时，留意在哪些情况下你会说服自己做出“傻瓜式选择”。\\n\\n·留意你是否总是告诉自己必须在说实话和顾面子之间、在成与败之间做出两极化选择。\\n\\n·利用对比说明的方式消除“傻瓜式选择”的影响。\\n\\n·说出你希望的目的之后，明确你不希望实现的目的，然后开动脑筋寻找可以实现对话的健康方式。\\n\\n他们当然关注对话内容，这是必然前提，此外还关注人们感到害怕的细微信号。当朋友、家人或同事退出健康对话模式（双方可自由交换观点）时，他们要么会强迫对方接受自己的观点，要么会故意隐瞒自己的真实想法，对话高手马上会注意到对方是否失去了安全感。\\n\\n只有在安全的对话气氛中，你才可以畅所欲言。这就是对话高手总是密切关注安全问题的原因。顾名思义，对话需要双方实现观点的自由交流。对对话机制来说，阻止观点交流的最大元凶莫过于恐惧感了。当你担心对方拒绝接受你的看法时，你便会表现得非常强势，迫使他们接受自己。当你担心说出真实看法会受到某种伤害时，你便会犹豫畏缩，隐瞒内心。显然，无论是对抗还是逃避，这些反应全都是由恐惧感催生的。与此相反，只要在对话中营造出足够安全的气氛，你可以随心所欲地讨论任何问题，对方会全神贯注地聆听你的看法。同样，如果不怕受到攻击或羞辱，你可以坦诚接受对方的任何观点而不会产生抵触情绪\\n\\n1.沉默\\n\\n沉默包括任何有意拒绝观点交流的行为，这种方式几乎从来都是逃避潜在问题的方式，总是会限制观点在对话中的交流。其表现方式有很多，从玩文字游戏到对对方不理不睬都在此列。最常见的三种沉默形式是掩饰、逃避和退缩。\\n\\n·掩饰是指对问题轻描淡写或有选择性地表达观点。冷嘲热讽、甜言蜜语和字斟句酌是掩饰做法的常见形式。例如：\\n\\n“呃……我觉得你的想法很棒，真的。我只是有些担心其他人注意不到这些细微变化。要知道，如果观点比较超前，结果反而会遭到大家抵制。”\\n\\n真正含义：你的想法太疯狂了，没人会接受！\\n\\n“是啊，这个办法太好了。为顾客打折，人们便会为买一盒肥皂省六分钱而穿过大半个城，亏你想得出来！”\\n\\n真正含义：这算哪门子白痴主意？\\n\\n·逃避是指完全避开敏感话题的行为。我们虽然表面上在对话，但总是避重就轻，从不涉及真正重要的问题。例如：\\n\\n“你的新西装怎么样？嗯，你知道蓝色是我最喜欢的颜色。”真正含义：有没有搞错！你是在马戏团买的衣服吗？\\n\\n“说到缩减成本问题，你觉得这样行不？把供应的咖啡冲淡些，复印纸双面使用。”\\n\\n真正含义：提出微不足道的建议或许可以转移目标，避免讨论员工效率低下的问题。\\n\\n·退缩是指彻底退出对话机制。我们不是退出对话就是离开房间。例如：\\n\\n“抱歉，我得接个电话。”\\n\\n真正含义：这种无聊透顶的会议，我一分钟也不想多待！\\n\\n“抱歉，我不想再讨论分摊电话费的问题，再这样下去恐怕连朋友都做不成了。”（转身而去）\\n\\n真正含义：哪怕是芝麻大点儿的琐事也要吵，我受够了！\\n\\n2.言语暴力\\n\\n暴力包括任何试图迫使、控制或强迫对方接受自己观点的言语行为。这种做法的特征是人们把自己的信息强行加入信息库中，因此也会破坏对话的安全气氛。表现形式从口出秽言、自顾自说到恫吓威胁，不一而足，其中控制、贴标签和攻击是最常见的三种形式。\\n\\n·控制是指胁迫对方按照你的思路考虑问题，表现方式有两种，要么强迫对方接受你的观点，要么在对话中搞一言堂作风。具体做法包括经常打断对方讲话，过度强调自己的观点，大量使用绝对性字眼，经常改变话题以及使用指令性问题控制对话过程。例如：\\n\\n在开始对话之前，一定要检查你的动机是否具有共同性。你可以询问自己“从心开始”的问题：\\n\\n·我希望为自己实现什么目标？\\n\\n·我希望为对方实现什么目标？\\n\\n·我希望为我们之间的关系实现什么目标？\\n\\n例如，你正在和一群监督员讨论复杂的产品质量问题，你非常想彻底解决问题，因为它直接关系到你能否保住自己的工作。但不幸的是，你总觉得这些监督员光拿钱不干事，产品质量问题频频出现漏洞。你坚定不移地认为，这帮人不但难以管理，而且总是把事情办砸，其中有几个人的做法甚至违反职业道德。\\n\\n在听取监督员的意见时，你不由得翻起了白眼。这个细微的动作一不小心透露出了你内心对他们的轻蔑看法。结果可想而知，失去了互相尊重，这场对话只能以失败告终。监督员们对你的提议大加抨击，你则把他们批评得一无是处。当双方的注意力转移到互相攻击时，最终大家都成了失败者。换句话说，虽然具有共同目的，但这场对话并没有顺利完成，原因是对话者之间缺少互相尊重。\\n\\n指示信号。要想准确识别什么情况下尊重感会出现危机，导致对话出现安全问题，你应当关注的信号是人们维护自尊的行为，其中情绪变化是一个关键线索。当人们感到不受尊重时，他们会变得非常情绪化，从恐惧变得愤怒异常。然后，他们会采用生闷气、骂骂咧咧、大声咆哮或言语威胁等应对方式。在判断互相尊重感是否出现危机时，你可以问问自己下面这个问题：\\n\\n·对方觉得我是否尊重他们？\\n\\n如果我们在开口之前必须和对方分享每一个目标，尊重对方性格中的每一个方面，这样的对话恐怕会以失败告终。如果真的是这样，那我们只能在对话中保持缄默了。其实，对话还是可以继续的，前提是我们必须想办法尊重对方最基本的人性。实际上，在对话中尊重感的丧失通常是因为我们认为对方和自己不同，是完全不一样的两种人。如果我们能把对方看成和自己差不多的人，这种感受便会自动消失。我们要做的不是为自己的行为找借口，而是努力理解对方，学会站在对方的角度看问题。\\n\\n发现互相尊重感或共同目的出现危机时，你必须高度关注。你应当找到一种既能确定共同目的又能营造互相尊重感的方式，哪怕对方和你的观点南辕北辙。\\n\\n具体应该怎么做呢？怎样才能实现这些目标呢？我们已经谈了一些看法（主要是错误做法），下面来介绍三种非常有效的应对技巧。它们分别是：\\n\\n·道歉\\n\\n·对比说明\\n\\n·创建共同目的\\n\\n当然，如果内心没有真正意识到这一点，口头上的道歉并不是真诚的道歉。要想做出真诚道歉，你必须改变自己的动机，放弃什么面子、争强好胜以及“只有我是正确的”之类的错误想法，学会关注自己的真正目的。换句话说，你应当牺牲一点自尊心，承认自己的错误。牺牲才能换来回报，当你放弃一些立场时，往往会得到更有价值的回报——健康的对话和理想的结果\\n\\n当对方误解你的目的或意图时，你应当暂停争执，然后利用对比法重建安全感。\\n\\n对比法是一种是非型陈述，其结构包括：\\n\\n·打消对方认为你不尊重他们或抱有不轨企图的误解（否定部分）。\\n\\n·确认你对他们的尊重，明确你的真实目的（肯定部分）。\\n\\n例如：\\n\\n（否定部分）“我不希望你们认为我不重视你们的工作，认为我不想向公司副总汇报。”\\n\\n（肯定部分）“正相反，我认为你们的工作表现非常突出。”\\n\\n这样一来，你就能消除安全威胁，可以继续讨论视察问题，寻找补救措施了：\\n\\n“很不幸，就在我准备安排视察工作时，公司副总提出了一个问题。这个问题必须当场解决，否则会给公司业务带来巨大影响。这样吧，我看明天能否让她过来视察你们的工作，她要来这里参加剪彩仪式，我看看到时候能否向她展示你们的工作成绩。”\\n\\n情景一：愤怒的室友。你对室友说，让她把冰箱里属于自己的东西从你那一格拿走，放到她那一格里。对你来说这只是件小事，只是想和对方平等共享使用空间而已，完全没有其他目的。实际上，你还挺喜欢对方的。没想到，她反驳道：“看你又来了，总是对我的生活方式指手画脚，就连换个垃圾袋你也总是絮叨个不停！”\\n\\n你该如何用对比法消除误解呢？\\n\\n我不希望_________________________________________________________________________\\n\\n而是希望_________________________________________________________________________\\n\\n声明一：情绪并不是笼罩在你周身的一层薄雾，它们不是别人强加给你的。不管你多么振振有词地指责别人，实际上其他人是无法让你陷入某种情绪的，是你制造了自己的情绪，是你让自己感到害怕、烦恼或气愤。让你产生情绪的只能而且永远是你自己。\\n\\n声明二：产生负面情绪后你只有两个选择，要么控制它，要么被它控制。也就是说，在出现强烈情绪时，如果你无法驾驭它，就会成为它的俘虏。\\n\\n小结——控制想法\\n\\n如果强烈情绪让你在对话中陷入沉默或暴力，试试下面的方法。\\n\\n行为方式回顾\\n\\n关注你的行为表现。如果发现自己正在远离对话，问问自己在做什么。\\n\\n·我是否陷入了沉默或暴力应对方式？\\n\\n确定行为背后的感受。学会准确识别行为背后的情绪。\\n\\n·导致这种行为的情绪感受是什么？\\n\\n分析感受背后的想法。学会质疑你的结论，寻找感受背后其他的可能解释。\\n\\n·造成这种情绪出现的想法是什么？\\n\\n寻找想法背后的事实。回到事实本身，放弃绝对表达，区别客观事实和主观想法的区别。\\n\\n·形成这种想法的事实依据是什么？\\n\\n注意似是而非的“小聪明”式想法。尤其是受害者想法、大反派想法和无助者想法。\\n\\n改变主观臆断\\n\\n你应当询问自己以下问题：\\n\\n·我是否故意忽略自己在这个问题中的责任？\\n\\n·一个理智而正常的人为什么会这样做？\\n\\n·我的真实目的是什么？\\n\\n·要想实现这些目的现在我该怎么做？\\n\\n当你彻底放弃自己的看法，或是表达方式让人感觉非常不自信时，这种做法对你的观点非常不利。记住，表现得谦逊和开放是一回事，但缺乏自信完全是另外一回事。你的表达方式应当给人坦率开放的印象，而不是让人感觉像个紧张不安的废物。\\n\\n正确表达方式测试\\n\\n要想准确陈述你的想法，你必须做到不卑不亢。我们来看看下面的例子：\\n\\n过于软弱：“这样说可能有点傻，不过……”\\n\\n过于强硬：“嘿！你怎么敢敲我们的竹杠？”\\n\\n不卑不亢：“看起来你准备把这个软件据为己用，是这样吗？”\\n\\n过于软弱：“我觉得很不好意思跟你提，可是……”\\n\\n过于强硬：“你什么时候又开始嗑药了？”\\n\\n不卑不亢：“我注意到你又开始碰那些毒品了，这一次你有什么要解释的吗？”\\n\\n过于软弱：“这件事也许是我的错，不过……”\\n\\n过于强硬：“好啊，你连你妈煮的鸡蛋都信不过！”\\n\\n不卑不亢：“我感觉到你好像不信任我，是这样吗？如果是，我想知道我做错了什么。”\\n\\n过于软弱：“或许是我在那方面要求得太多，可是……”\\n\\n过于强硬：“如果你没办法满足我，就早点说！”\\n\\n不卑不亢：“我想你也不是故意这样的，不过我真的觉得在夫妻问题上咱们互动得不够。”\\n\\n打破恶性循环。接下来会发生什么情况呢？当我们处于对方的惩罚、指责和攻击状态时，我们很少这样思考问题：“嗯，这个人身上肯定发生了很有趣的经历，到底会是什么情况呢？”正相反，我们常常会做出同样不健康的应对行为。隐藏在我们基因中的经过亿万年形成的自我保护意识开始作祟，让我们在失去理智的状态下仓促做出完全错误的举动。\\n\\n对话高手是这样打破这种恶性循环的，他们会暂时退出人际互动，营造安全感，让对方说出自己的行为动机。他们会鼓励对方远离负面情绪和条件反射式的反应，帮助他们寻找出现错误表现的根本原因。实际上，他们会回顾对方整个行为模式的发展过程。在他们的鼓励下，对方会渐渐退出激烈情绪，转而寻找自己的行为原因和观察到的事实情况。\\n\\n在帮助对方回顾其行为模式时，我们不但能抑制自己的反应，还可以回到情绪产生的根源去解决问题，即了解情绪背后的事实和想法。\\n\\n询问技巧\\n\\n时间。我们说道，当对方有想法和事实要分享时，我们的任务是鼓励他们积极做出行动。我们的任务线索很简单：对方即将陷入沉默或暴力状态，我们能够看出他们感到不安、害怕或愤怒。如果找不到这些负面情绪产生的根源，我们势必会受到这些情绪的影响。这些外部反应就是我们展开行动的时间线索，它们的出现意味着我们必须帮助对方回顾其行为模式了。\\n\\n方式。在鼓励对方开口说出行为动机时，我们必须注意方式要诚恳。虽然困难重重，但我们必须做到面对对方的敌意、恐惧或伤害时保持真诚的态度。\\n\\n内容。我们到底应该怎样做呢？怎样才能让对方积极分享自己的行为动机、说出内心的想法和观察到的事实呢？简而言之，实现这些目的都需要良好的倾听技巧。为了帮助对方不再压抑内心感受，坦率说出真实想法和所见所闻，我们的倾听方式必须保证足够的安全感，这样才能让他们毫无顾虑地侃侃而谈。我们必须让他们感觉到，分享内心观点既不会冒犯他人，也不会让他们因为直言不讳而受到惩罚\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b7eb1060-17f6-4578-96f7-8b48e30e2512', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nj2\\n\\nSo we shift from seeking the affirmation and attention of our parents or family, and we start seeking affirmation and self-worth from a romantic relationship. We just need to find \"the one\" and they will complete us. They will make us feel whole and happy and love us unconditionally for the rest of our lives!\\n\\nWhat an impossible standard to put on others! My friends, this is a dangerous game. We invest all our hopes and dreams and everything on somebody else. And it\\'s a dangerous game because they will always let us down. It\\'s impossible for them not to let us down. We want them to affirm that we are worthwhile. We want them to give us love where we can\\'t give it to ourselves.\\n\\nBut it\\'s not like that is their goal in life. It\\'s not like that is their mission. They didn\\'t grow up thinking \"I can\\'t wait to meet somebody that I have to reaffirm ALL the time so they don\\'t feel inadequate about themselves.\"\\n\\nThe definition of this is codependency. The need to have others affirm us. We do everything in our power to make this happen. And when those people let us down, we find ways to manipulate them to not let us down.\\n\\nBut of course, that\\'s just a fleeting pursuit. There\\'s only so long someone wants to be manipulated or controlled before they just can\\'t take it anymore. Or maybe we put up a barrier where we don\\'t want those people to get too close because we are afraid they are going to leave us. We believe they are the only things that can make us whole and worthwhile, so let\\'s build a barrier and find their flaws so it doesn\\'t hurt so much when the inevitable happens (we think it\\'s inevitable they will leave).\\n\\nThis begins a vicious cycle of control and manipulation, while at the same time feeling desperate for their attention and love. And eventually they do leave because who would want to live in that world?\\n\\nUnknowingly, we have self-sabotaged. And now we feel even worse than we did before. Because we still don\\'t have the self-esteem or self love, but now we also believe that others won\\'t love us. In fact we have PROOF (even if we were the ones who helped cause it).\\n\\nWe feel horrible. We feel terrible. How could they leave us? How could they not love us? We feel horrible. We feel sad and lonely and isolated. We just want all of this pain to go away.\\n\\nInsert your addiction.\\n\\nDoing an addictive behavior temporarily takes the pain away! When was the first time you realized this? Who knows. All we know is that when we are doing ________, we don\\'t feel bad about ourselves. In fact, it feels awesome.\\n\\nOur __________ just happens to be filled with \"masturbating to pornography.\" And We get the urge to do it frequently.\\n\\nSo you act out on that urge, but then something inside also tells you that you aren\\'t a good person for doing this. It\\'s not that your inner voice told you you did something wrong (feeling guilty), it\\'s that your inner voice told you that YOU are wrong, or something is bad within you (feeling shame).\\n\\nAddiction is about shame. When we do something, when we act on our urges to soothe this feeling, then we feel shame and that we are actually a bad person.\\n\\nIt\\'s a pretty horrific spiral when you think about it. I just don\\'t want to feel a certain way about myself or my situation, so I\\'m going to do something so I don\\'t have to think about it. Okay, I just did that something, and now I believe I\\'m a bad person for doing it. That makes me feel even worse about myself, so I want to go do something again where I don\\'t have to think about it.\\n\\nThen I feel even more shame.\\n\\nShouldn\\'t I be able to control my thoughts? Shouldn\\'t I be able to control my own actions?\\n\\nMasturbating to pornography feels so good, and the orgasm feels amazing!\\n\\nAnd if that\\'s the case, why do I feel like absolute shit the moment I start having an orgasm? I don\\'t feel like shit when I have an orgasm with an actual person?\\n\\nI must be a bad person for doing this.\\n\\nAnd that my friend, is the power of the addiction. That shame spiral is what\\'s behind everything.\\n\\nAnd it\\'s why your willpower alone cannot break an addiction. You are wishing away years of thinking a certain way, and acting out on that certain thing.\\n\\n1. How your brain actually rewires itself as an addict.\\n\\nWhen we feel something pleasurable, our brain releases dopamine. This dopamine hit leaves us feeling you euphoric. It leaves us feeling pleasure and joy and satisfaction.\\n\\nAnd every time you masturbated to pornography, your brain released dopamine. And the more you did it, the more dopamine was released.\\n\\nAnd now your brain has rewired itself to actually crave that hit of dopamine to even function every day. You may not even feel sexual arousal, but it\\'s time to go PMO because your brain is saying I need that dopamine.\\n\\nOver time, you actually desensitize yourself to this dopamine. Our brains like to adjust and try to reach a point of stasis with anything. Our brain does not like imbalance. It\\'s always trying to seek stability. But you have been flooding it with dopamine. Maybe you are PMO just a couple times a week to start out with. And then it becomes three to four. Your brain is adjusting to this new level of dopamine, and it becomes the new baseline. If you aren\\'t actually giving it that hit three to four times a week, it starts wondering why not.\\n\\nBut don\\'t forget, that is just to get back to where your brain feels \"normal.\" You moved where the normal is by repeatedly doing something. And now, the same behavior doesn\\'t make you feel nearly as euphoric. That three to four times a week doesn\\'t hold any Joy anymore, it\\'s just to get you back to normal.\\n\\nSo then you start doing it more. Maybe it\\'s once a day. Then once a day isn\\'t enough. So now it\\'s twice a day.\\n\\nUnfortunately, Something else is happening simultaneously. Our brain craves novelty. Our brain craves New sensations. We want that feeling of joy and euphoria. So the thing that used to turn us on... It\\'s still okay, but it isn\\'t like it was when we first watched it.\\n\\nAnd these two processes combined, seeking more and more dopamine and seeking more and more stimulating material, this is how you became an addict.\\n\\nAnd during all this, Your brain actually changes its neurological connections through the association of P with MO.\\n\\nYour brain re-wires itself to do this.\\n\\nBut the good news is, your brain is an ever-changing organ in your body. And after a certain amount of time, you can actually rewire it to not have those same neural pathways that were ingrained in yourself for so long.\\n\\nThis process of your brain rewiring yourself is called neuroplasticity. And the younger you are, the easier this will happen. That\\'s what\\'s great about all of you who are still pretty young figuring this out. It\\'s actually much easier for your brains to rewire themselves.\\n\\nThe typical time it takes to reset your brain is about a month. But the older you are, the more time it takes. And everybody\\'s different, so some people it will take a little bit longer even if they\\'re younger.\\n\\nThis is why I tell people that if I can do it, a 48-year-old man with a 30-year addiction, I believe anyone can do it. My brain is pretty damn hard wired a certain way at this point. I\\'ve been using my \"drug\" for almost 2/3 of my life.\\n\\n1. Addiction needs shame and secrecy\\n\\nAll addiction also needs secrecy. Can you imagine what it would be like if you just told people that you were going to go masturbate to porn?\\n\\nI mean it. Take a moment right now to think about what it would be like if you could just share that\\'s what you were going to do! Do you have a lover? Do You have a boyfriend/girlfriend? Do you have a spouse? Or maybe even your roommates or your siblings or your parents?\\n\\n\"Hey, I\\'m going to go masturbate. See you in a few minutes (or hours).\"\\n\\nI doubt that many of you have ever in their lives actually talked about masturbation in a serious way. Or just said you were going to go do it because you enjoy it and it feels really freaking good.\\n\\nSo we go do it in secret. We do it in covert ways. We hide our tracks and we hide the evidence. We do it behind locked doors, we do it while the rest of our family is sleeping, all the while fearing that we are going to be caught. Probably adds to The thrill of the experience, but it also means we have to hide everything that we do.\\n\\nI thought I would die in addict. I hoped I would die an addict. I prayed every single day that no one would ever find out.\\n\\nI used to have repeated visions - I was an old man on my deathbed, surrounded by friends and family. My death was imminent and everyone knew it, including me. And in those moments, my last on this Earth, I used to think about what a relief it was going to be that no one would have ever found out about my addiction.\\n\\nAnd I used to think about this repeatedly.\\n\\nI\\'m 48 years old. I\\'ve been an addict for 30 years. Let\\'s say I made it another 30. I\\'m on my deathbed at 78 years old. I\\'ve been an addict for 60 years. I\\'ve Been living through the pain and misery and secrecy and shame of addiction.\\n\\nFOR 60. FUCKING. YEARS.\\n\\nThis was what I hoped would happen! This is what I wanted to happen.\\n\\nThis is proof of how much addiction can fuck with your brain.\\n\\nOr secrecy and shame becomes even more damaging if somebody does catch us, especially somebody who has something to lose. If it\\'s a lover or a significant other or a spouse, they find our collection of porn. They compare themselves to the images that we so carefully saved and categorized... Sometimes terabytes of data collected for years.\\n\\nThey realize that they were deceived, lied to, and that this hidden thing became more important to you than they were. That may not be true, but it\\'s often what they perceive. So now we have something even more powerful at play - The shame of somebody else finding out.\\n\\nSo not only do we have our own issues to fix here, now we have somebody else who feels hurt and violated by our actions. We see the look of hurt on their face. It crushes us. And maybe in that moment we vow that we will never do it again. We vow to give it all up because we have hurt them so badly.\\n\\nThe problem is for most people, they don\\'t understand the origins of their addiction. And they don\\'t understand that their brain has hard wired it to make this very powerful connection. They just know they were caught, they know their significant other is hurting.\\n\\nAnd they feel terrible. The sense of immense shame is significant. And as we covered before, addicts are hardwired with this shame response in place).\\n\\nSo they may want to quit. They may want to quit more than anything else in the world. And now they want to quit for somebody else. They want to quit to see if spouse, or to keep their family intact.\\n\\nThey make promises. Promises to never do it again. Promises that are going to be nearly impossible to keep if they haven\\'t learned what\\'s behind the addiction.\\n\\nOne of the most powerful things you can do in your recovery is to tell others. I know it\\'s scary. I know it\\'s terrifying. I know it fills you with a sense of dread. But you need to rob the addiction of its secrecy. You need to rob the addiction of it shame.\\n\\nSecrecy and shame are the lifeblood of the addiction. When we start to share the story with others, we start starving the addiction of these two necessary things. It starts removing some of the power that the addiction has over us.\\n\\nStart by just telling one person. If you have the means, find a therapist and start by telling them. It\\'s probably the safest person you can tell because they are bound legally to never share what you share.\\n\\nI will never forget the feeling of telling my therapist. I\\'m not exaggerating when I say this, it was the greatest feeling I\\'ve ever had in the world. It was a feeling of immense relief. It was as though the weight of the entire world have been lifted from my body. If I could find a way to sell that feeling, even the tiniest fraction of what that felt like, I would be the richest man in the world. People would pay me hundreds of millions of dollars to get the euphoria of that feeling. I\\'m not kidding.\\n\\nNext, I told my brother. I knew I could trust him and I knew he had gone through similar experiences. Then I told my best friend. Someone who would always have my back no matter what.\\n\\nI also told my spouse. I was fortunate enough that she was not judgmental but actually quite understanding. But that wasn\\'t a simple thing, because the reason I decided to quit my addiction was that we chose to get separated. And I\\'m only responsible for 50% of the relationship, but of that 50%, the porn addiction contributed to probably 90 or 95% of the problems that I brought to the marriage.\\n\\nLater on, I ended up telling her parents. We were still separated, but I thought it was important that they understood what had happened and what had contributed to the separation. This was one of the most empowering experiences of my life. My in-laws have this perfect view of me. Their daughter married a phd, and I was able to hide this addiction from everyone. On the surface, I was a perfect spouse. Or at least as perfect as they could have hoped for.\\n\\nWhenever I used to think about someone finding out about the addiction, I used to always picture my mother-in-law. She thought so highly of me that she would take my side when we would have arguments. This was another person who was contributing to my sense of self-worth. So the idea of telling this person was terrifying. It was my biggest fear. I didn\\'t want to take away this vision of being a perfect son-in-law and a perfect spouse.\\n\\nTelling her scared the shit out of me. Which is why it was so empowering to do.\\n\\nGuess what? They told me they loved me. They Told me they accepted me just for who I am, and that I was family (even though my wife and I are still separated).\\n\\nYou Want to talk about a powerful experience. Facing your biggest fear, and coming out of it with more support because you were brave enough to share.\\n\\nThat is some powerful healing of the soul.\\n\\nAnd now your know my journey to healing. There you have it folks. Those are the three things that I learned that have impacted me most in my recovery. I learned about how I became an addict and what was behind it. I learned about my brain and how it functions during addiction. And I learned that all addiction needs secrecy and shame to survive.\\n\\nBecause I Now understand all that about myself, I have really robbed the addiction of most of its power. It\\'s not that I don\\'t have urges. It\\'s not that I can\\'t still think about those old images in my head. Some of that will never go away. I did it for too long for it to ever go away. But it just doesn\\'t have the same hold over me.\\n\\nKnowledge is power. And that knowledge was very powerful for me.\\n\\nSo I hope this all gives you a little hope. The sooner you understand all this stuff, the sooner you can make a difference in your life.\\n\\nHere is where I will tell you about the resources that I used to get this information. In terms of understanding my family of origin story, the book that helped most was called no More Mr Nice. This book isn\\'t even about pornography addiction, but he has some very effective exercises to do that help you understand where you came from and what impacted you as a young person, which led to why you feel or act a certain way as an adult. He finds that the majority of people with the nice guy syndrome are also porn addicts.\\n\\nThe information about how your brain works I learned in a book called Dopamine Nation. The entire book is about addiction and how our brain physiology works to support that addiction. But it\\'s also about what it takes to unravel that brain physiology.\\n\\nIt was the combination of these two things, along with a lot of other self-help resources like books and podcasts and therapy, that really unlocked the mystery behind my addiction.\\n\\nBecause once I understood how all of it worked, again it just didn\\'t hold the same power. The urges were not a mystery anymore. The reason that I practiced such self-destructive behavior were revealed, and I could forgive myself and love myself for just being an imperfect person who is carrying out their programming from when they were a child.\\n\\nI still had to take ownership of my actions. But it was a lot easier to take that ownership when I understood why.\\n\\nThank you for your reply. And Holy crap that\\'s such a great question. If I had to take a guess, based on what you were writing I would think you have some codependent tendencies. There are a lot of books out there about codependency, and the most well known are from an author named Melody Beattie. I was able to access her books for free from my library website, and I like to do a lot of audio books because I walk a lot. It has been the new habit that I took on to help replace the times when I used to pmo. So while I\\'m walking, I listen to audiobooks about self-help.\\n\\nI also think you would learn a tremendous amount from the no More Mr Nice Guy book I recommended. I was in your exact same shoes, always seeking validation from others. It\\'s one of the characteristics of being codependent, and it\\'s one of the main tenants that make up the nice guy syndrome. We do things for others because We crave their approval. But basically, we are trying to manipulate them into looking at us a certain way because we struggle with self-esteem. And we think the more thoughtful and considerate and overt thing that we do for somebody, the more that they should owe us. Except we never actually Express what we are looking for, we just expect other people to know it because we pay such attention to their needs and try to fix things and make their lives better. Then when they don\\'t actually behave the way we want to, it leaves us feeling abandoned, depressed, alone, and like nobody loves us. After all, if they did, they would treat us like we treat them.\\n\\nHe calls this behavior using covert contracts. We silently enter into a contract with somebody where they are supposed to do something for us because we do something for them. But of course, they don\\'t actually know they have entered into the contract because we didn\\'t tell them what we want from them. We do things with the expectation of getting something back from someone. This will ultimately fail because people can\\'t guess our needs. But we feel like they should be able to if they really loved us. After all, we guess other people\\'s needs all the time and try to fix their stuff constantly to get their approval.\\n\\nIf you are seeing yourself or able to identify your behaviors with the last few paragraphs I wrote, then you are probably a codependent and you could definitely benefit from reading the nice guy book. It helps explain how we got to a place where we make covert contracts with people. It helps explain what is missing in our lives that we feel like we need constant approval from others. And then it has you practice doing these specific things. For instance, one of the activities is to take an entire week and just do exactly what you want, never asking somebody else if it is okay with them. I\\'m not talking about forcing people to do things, we are talking about what you do for yourself.\\n\\nFor example, let\\'s say you\\'re meeting up with your family or group of friends. Everyone is talking about where they want to go, and you don\\'t really offer a suggestion because you want the rest of the group to be happy. Or, when you do offer a suggestion, you base it on somebody else\\'s needs. And in his activity, you would take an entire week or the choices that you make regarding yourself are based solely on your needs - no one else. It\\'s a pretty hard thing to do for people like you and I because we feel like we have to do things for others to get their love and approval.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='105f1b0d-cb46-41d1-a8a8-e28407393c23', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nray dalio 笔记\\n\\nOne read on this: The U.S. government kept the economy from collapsing with stimulus checks, sector bailouts, employee retention loans, rent mitigation and so forth. In doing so, it shifted private debt to the public books.\\n\\n**Throughout History Wealth Was Gained by Either Making It, Taking It from Others, or Finding It in the Ground**\\n\\nI wrestled with how much I should worry about the differences between countries, kingdoms, nations, states, tribes, empires, and dynasties. Nowadays we think mostly in terms of countries. However, countries as we know them didn’t come into existence until the 17th century, after Europe’s Thirty Years’ War.\\n\\neconomic destruction periods and war periods typically don’t last very long—they tend to last roughly two or three years.\\n\\nIn a nutshell, the ascent phase comes about when there is…\\n\\n- strong enough and capable enough leadership\\xa0to provide the essential ingredients for success, which include…\\n- strong education.\\xa0By strong education I don’t just mean teaching knowledge and skills; I also mean teaching…\\n- strong character, civility, and a strong work ethic, which are typically taught in the family as well as in school. These lead to improved civility that is reflected in factors such as…\\n- low corruption\\xa0and\\n    \\n    high respect for rules, such as rule of law.\\n    \\n- People being able to work well together, united behind a common view of how they should be together and a common purpose, is also important. When people have knowledge, skills, good character, and the civility to behave and work well together, and there is…\\n- a good system for allocating resources, which is significantly improved by…\\n- being open to the best global thinking, the country has the most important ingredients in order to succeed. That lead to them gaining…\\n- greater competitiveness in the global market, which brings in revenues that are greater than expenses, which leads them to have…\\n- strong income growth, which allows them to make…\\n- increased investments to improve their infrastructures, education systems, and research and development, which leads them to have…\\n- higher productivity\\xa0(more valuable output per hour worked). Increasing productivity is what increases wealth and productive capabilities. When they achieve higher productivity levels, they can become productive inventors of…\\n- new technologies.\\xa0These new technologies are valuable for both commerce and the military. As these countries become more competitive in these ways, naturally they gain…\\n- a significant share of world trade, which requires them to have…\\n- a strong military\\xa0to protect their trade routes and to influence those who are important to it outside its borders. In becoming economically pre-eminent they develop the world’s leading…\\n- financial centers\\xa0for attracting and distributing capital. (For example, Amsterdam was the world’s financial center when the Dutch empire was pre-eminent, London was it when the British empire was on top, and New York is now it because the US is on top, but China is beginning to develop its own financial center in Shanghai.) In expanding their trade globally, these growing empires bring their…\\n- strong equity, currency, and credit markets. Naturally those dominant in trade and capital flows have their currency used much more as the preferred global medium of exchange and the preferred storehold of wealth, which leads to their currency becoming a reserve currency. That is how the Dutch guilder became the world’s reserve currency when the Dutch Empire was pre-eminent, the British pound became the world’s reserve currency when the British Empire was pre-eminent, and the US dollar became the world’s reserve currency in 1944 when the US was about to win World War II and was clearly pre-eminent economically, financially, and militarily. Having one’s currency be a reserve currency naturally gives that country greater borrowing and purchasing power. As shown in the most recent chart, gaining and losing of reserve currency status happens with a significant lag to the other fundamentals.\\n\\nIn a nutshell, the top phase typically occurs because within the successes behind the ascent lie the seeds of decline. More specifically, as a rule:\\n\\n- Prosperous periods lead to people earning more, which naturally leads them to become more expensive, which naturally makes them less competitive relative to those in countries where people are willing to work for less.\\n- Those who are most successful typically have their ways of being more successful copied by emerging competitors, which also contributes to the leading power becoming less competitive. For example, British shipbuilders, who had less expensive workers than Dutch shipbuilders, hired Dutch shipbuilding architects to design ships that were built more cost-effectively than the Dutch ships. Because it takes less time and money to copy than invent, all else being equal, emerging empires tend to gain on mature empires through copying.\\n- Those who become richer naturally tend to work less hard, engage in more leisurely and less productive activities, and at the extreme, become decadent and unproductive. That is especially true as generations change from those who had to be strong and work hard to achieve success to those who inherited wealth—these younger generations tend to be less strong/battle-hardened, which makes them more vulnerable to challenges. Over time people in the prosperous society tend to want and need more luxuries and more leisure and tend to get weaker and more overextended in order to get them, which makes them more vulnerable.\\n- The currencies of countries that are richest and most powerful become the world’s reserve currencies, which gives them the “exorbitant privilege” of being able to borrow more money, which gets them deeper into debt. This boosts the leading empire’s spending power over the short term and weakens it over the longer run. In other words, when borrowing and spending are strong, the leading empire appears strong while its finances are in fact being weakened. That borrowing typically sustains its power beyond its fundamentals by financing both domestic over-consumption and the military and wars that are required to maintain its empire. This over-borrowing can go on for quite a while and even be self-reinforcing, because it strengthens the reserve currency, which raises the returns of foreign lenders who lend in it. When the richest get into debt by borrowing from the poorest, it is a very early sign of a relative wealth shift. For example, in the 1980s, when the US had a per capita income that was 40 times that of China’s, it started borrowing from Chinese who wanted to save in US dollars because the dollar was the world’s reserve currency. This was an early sign of that dynamic beginning. Similarly, the British borrowed a lot of money from its much poorer colonies, particularly during WWII, and the Dutch did the same before their top, which contributed to the reversals in their currencies and economies when the willingness to hold their currency and debt suddenly fell. The United States has certainly done a lot of borrowing and monetization of its debt, though this hasn’t yet caused a reduced demand for the US currency and debt.\\n- The leading country extends the empire to the point that the empire has become uneconomical to support and defend. As the costs of maintaining it become greater than the revenue it brings in, the unprofitability of the empire further weakens the leading country financially. That is certainly the case for the US.\\n- Economic success naturally leads to larger wealth gaps because those who produce a lot of wealth disproportionately benefit. Those with wealth and power (e.g., those who benefit commercially and those who run the government) naturally work in mutually supportive ways to maintain the existing system that benefits them while other segments of the population lag, until the split becomes so large that it is perceived as intolerably unfair. This is an issue in the US.\\n\\nThe decline phase typically happens as the excesses of the top phase are reversed in a mutually reinforcing set of declines, and because a competitive power gains relative strength in the previously described areas.\\n\\n- When debts become very large, when the central banks lose their ability to stimulate debt and economic growth, and when there is an economic downturn, that leads to debt and economic problems and to more printing of money, which eventually devalues it.\\n- When wealth and values gaps get large and there is a lot of economic stress (wherever that stress comes from), there are high probabilities of greater conflict between the rich and the poor, at first gradually and then increasingly intensely. That combination of circumstances typically leads to increased political extremism—i.e., populism of both the left (i.e., those who seek to redistribute the wealth, such as socialists and communists) and the right (i.e., those who seek to maintain the wealth in the hands of the rich, such as the capitalists). That happens in both democratically and autocratically run countries. For example, in the 1930s, increasingly extreme populists of the left became communist and those from the right became fascist. Populists tend to be more autocratic, more inclined to fight, and more inclined to respect power than law.\\n- When the rich fear that their money will be taken away and/or that they will be treated with hostility, that leads them to move their money and themselves to places, assets, and/or currencies that they feel are safer. If allowed to continue, these movements reduce the tax and spending revenue in the locations experiencing these conflicts, which leads in turn to a classic self-reinforcing hollowing-out process in the places that money is leaving. That’s because less tax money worsens conditions, which raises tensions and taxes, causing still more emigration of the rich and even worse conditions, and so on. For example, we are now seeing some of that happening via the rich leaving higher-tax states where there is financial stress and large wealth gaps. When it gets bad enough, governments no longer allow that to happen—i.e., they outlaw the flows of money out of the places that are losing them and to the places, assets, and/or currencies that are getting them, which causes further panic by those seeking to protect themselves.\\n- When these sorts of disruptive conditions exist, they undermine productivity; that shrinks the economic pie and causes more conflict about how to divide the shrinking resources well, which leads to even more internal conflict that increasingly leads to fighting between the populist leaders from both sides who want to take control to bring about order. That is when democracy is most challenged by autocracy. This is why in the 1920s and 1930s Germany, Japan, Italy, and Spain (and a number of smaller countries) all turned away from democracy to autocratic leadership, and the major democracies (the US, the UK, and France) became more autocratic. It is widely believed that, during periods of chaos, more centralized and autocratic decision making is preferable to less centralized and more democratic, debate-based decision making, so this movement is not without merit when there is unruly, violent crowd fighting.\\n- When a country gains enough economic, geopolitical, and military power that it can challenge the existing dominant power, there are many areas of potential conflict between these rival world powers. Since there is no system for peacefully adjudicating such disputes, these conflicts are typically resolved through tests of power.\\n- When a leading country’s costs of maintaining its empire abroad become greater than the revenue that the empire brings in, that economically weakens the country. When that happens at the same time that other countries are emerging as rival powers, the leading power feels compelled to defend its interests. This is especially threatening to the leading country both economically and militarily, because greater military spending is required to maintain the empire, which comes when worsening domestic economic conditions are making it more difficult for leaders to tax and more necessary for them to spend on domestic supports. Seeing this dilemma, enemy countries are more inclined to mount a challenge. Then the leading power is faced with the difficult economic and military choice of fighting or retreating.\\n- When other exogenous shocks, such as acts of nature (e.g., plagues, droughts, or floods), occur during times of vulnerabilities such as those mentioned above, they increase the risk of a self-reinforcing downward spiral.\\n- When the leadership of the country is too weak to provide what the country needs to be successful at its stage in the cycle, that is also a problem. Of course, because each leader is responsible for leading during only a tiny portion of the cycle, they have to deal with, and can’t change, the condition of the country that they inherit. This means that destiny, more than the leader, is in control.\\n\\n\\n\\nthose with large savings, low debts, and a strong reserve currency can withstand economic and credit collapses better than those that don’t have much savings, have a lot of debt, and don’t have a strong reserve currency. Likewise those with strong and capable leadership and civil populations can be managed better than those that don’t have these, and those that are more inventive will adapt better than those that are less inventive.\\n\\nMy point is that while these periods can be depressing and lead to a lot of human suffering, we should never, especially in the worst of times, lose sight of the fact that humanity’s power to adapt and quickly get to new, higher levels of well-being is much greater than all the bad stuff that can be thrown at us. For that reason, I believe that it is smart to believe and invest in humanity’s adaptability and inventiveness. So, while I am pretty sure that in the coming years both you and the world order will experience big challenges and changes, I believe that humanity will become smarter and stronger in very practical ways that will lead us to overcome these challenging times and go on to new and higher levels of prosperity.\\n\\nSince one person’s spending is another person’s income, that cutting of expenses will hurt not just the entity that is having to cut those expenses but it will hurt the ones who depend on that spending to earn income. Similarly, since one’s debts are another’s assets, that defaulting on debts reduces other entities’ assets, which requires them to cut their spending. This dynamic produces a self-reinforcing downward debt and economic contraction that becomes a political issue as people argue over how to divide the shrunken pie. As a principle, debt eats equity. What I mean by that is that for most systems, when the rules of the game are followed, debts have to be paid above all else so that when one has “equity” ownership—e.g., in one’s investment portfolio or in one’s house—and one can’t service the debt, the asset will be sold or taken away. In other words, the creditor will get paid ahead of the owner of the asset. As a result, when one’s income is less than one’s expenses and one’s assets are less than one’s liabilities (i.e., debts), one is on the way to having one’s assets sold and going broke.\\n\\nThat is why money, credit, debt, and economic activity are inherently cyclical. In the credit creation phase, demand for goods, services, and investment assets and the production of them is strong, and in the debt paying back phase it is weak.\\n\\nBut what if the debts never had to be paid back? Then there would be no debt squeeze and no painful paying back period. But that would be terrible for those that lent to them because they’d lose their money, right? Let’s think about that for a moment to see if we can find a way around that problem. Since government (i.e., the central government and the central bank combined) has the abilities to both make and borrow money, why couldn’t the central bank lend money at an interest rate of about 0% to the central government (to distribute as it likes) and also lend to others at low rates and allow those debtors to never pay it back.\\n\\nAll countries can create money and credit out of thin air to give to people to spend or to lend it out. By producing money and giving it to debtors in need, central banks can prevent the debt crisis dynamic that I just explained. For that reason I will modify the prior principle to say debt eats equity, money feeds the hunger of debt, and central banks can produce money. So, it should not be surprising that governments print money when there are debt crises that are causing debt to eat more equity and causing more economic pain that is politically acceptable.\\n\\nBut one cannot create more wealth simply by creating more money and credit. To create more wealth, one has to be more productive. The relationship between the creation of money and credit and the creation of wealth (actual goods and services) is often confused yet it is the biggest driver of economic cycles, so let’s look at this relationship more closely.\\n\\nThere is typically a mutually reinforcing relationship between a) the creation of money and credit and b) the amount of goods, services, and investment assets that are produced so it’s easy to get them confused.\\n\\nThink of it this way. There is both a real economy and a financial economy. Though they are related, they are different. Each has their own supply and demand factors that drive them.\\n\\n牛逼的对滞涨的解释\\n\\nAt such times high inflation can occur because the supply of money and credit has increased relative to the demand for it, which we call monetary inflation. That can happen at the same time as there is weak demand for goods and services and the selling of assets so that the real economy is experiencing deflation. That is how **inflationary depressions** come about. For these reasons to understand what is likely to happen financially and economically one has to watch movements in the supplies and demands of both the real economy and the financial economy.\\n\\nSimilarly confused is the relationship between the prices of things and the value of things.\\n\\no the extent that spending increases economic production and raises the prices of goods, services, and financial assets, it can be said to increase wealth, because the people who own those assets become “richer” when measured by the way we account for wealth. However, that increase in wealth is more an illusion than a reality for two reasons: 1) the increased credit that pushes prices and production up has to be paid back, which, all things being equal, will have the opposite effect when it has to be paid back and 2) the intrinsic value of things doesn’t increase just because their prices go up\\n\\nAs far as how the economic machine works, the big thing is that money and credit is stimulative when it’s given out and depressing when it has to be paid back. That’s what normally makes money, credit, and economic growth so cyclical.\\n\\nThe short-term cycles of ups and downs typically last about eight years, give or take a few. The timing is determined by the amount of time it takes the stimulant to raise demand to the point that it reaches the limits of the real economy’s capacity to produce. Most people have seen enough of these short-term debt cycles to know what they are like—so much so that they mistakenly think that they will go on working this way forever. They’re most popularly called “the business cycle,” though I call them “the short-term debt cycle” to distinguish them from “the long-term debt cycle.” Over long periods of time these short-term debt cycles add up to long-term debt cycles that typically last about 50 to 75 years\\n\\n**These long-term debt cycles start when debts are low after previously existing excess debts have been restructured in a way so that central banks have a lot of stimulant in the bottle, and they end when debts are high and central banks don’t have much stimulant left in the bottle. More specifically, the ability of central banks to be stimulative ends when the central bank loses its ability to produce money and credit growth that pass through the economic system to produce real economic growth. That lost ability of central bankers typically takes place when debt levels are high, interest rates can’t be adequately lowered, and the creation of money and credit increases financial asset prices more than it increases actual economic activity. At such times those who are holding the debt (which is someone else’s promise to give them currency) typically want to exchange the currency debt they are holding for other storeholds of wealth. When it is widely perceived that the money and the debt assets that are promises to receive money are not good storeholds of wealth, the long-term debt cycle is at its end, and a restructuring of the monetary system has to occur. In other words the long-term debt cycle runs from 1) low debt and debt burdens (which gives those who control money and credit growth plenty of capacity to create debt and with it to create buying power for borrowers and a high likelihood that the lender who is holding debt assets will get repaid with good real returns) to 2) high debt and debt burdens with little capacity to create buying power for borrowers and a low likelihood that the lender will be repaid with good returns. At the end of the long-term debt cycle there is essentially no more stimulant in the bottle (i.e., no more ability of central bankers to extend the debt cycle) so there needs to be a debt restructuring or debt devaluation to reduce the debt burdens and start this cycle over again.**\\n\\nTrouble approaches either when there isn’t enough income to survive one’s debts or when the amount of the claims (i.e., debt assets) that people are holding in the expectation that they can sell them to get money to buy goods and services increases faster than the amount of goods and services by an amount that makes the conversion from that debt asset (e.g., that bond) implausible. These two problems tend to come together.\\n\\nIt is important to understand the difference between money and debt. Money is what settles claims—i.e., one pays one’s bills and one is done. Debt is a promise to deliver money. In watching how the machine is working it is important to watch a) the amounts of both debt and money that exist relative to the amount of hard money (e.g., gold) in the bank and b) the amounts of goods and services that exist, which can vary, remembering that debt cycles happen because most people love to expand their buying power (generally through debt) while central banks tend to want to expand the amount of money in existence because people are happier when they do that. But this can’t go on forever. And it is important to remember that the “leveraging up” phase of the money and debt cycle ends when bankers—whether private bankers or central bankers—create a lot more certificates (paper money and debt) than there is hard money in the bank to give and the inevitable day comes when more certificates are turned in than there is money to give.\\n\\nIn other words, in virtually all cases the government contributes to the accumulation of debt in its actions and by becoming a large debtor and, when the debt bubble bursts, bails itself and others out by printing money and devaluing it.\\n\\nIn other words, in virtually all cases the government contributes to the accumulation of debt in its actions and by becoming a large debtor and, when the debt bubble bursts, bails itself and others out by printing money and devaluing it.\\n\\nThat typically does a good job of pushing up financial asset prices but is relatively inefficient in getting money and credit and buying power into the hands of those who need it most. That is what happened in 2008 and has happened for most of the time since until just recently. Then, when the printing of money and the central bank’s buying up of financial assets fails to get money and credit to where it needs to go, the central government—which can decide what to spend money on—borrows money from the central bank (which prints it) so it can spend it on what it needs to be spent on. In the US the Fed announced this plan on April 9, 2020. **This approach of printing money to buy debt (called debt monetization) is vastly more politically palatable as a way of getting money and shifting wealth from those who have it to those who need it than imposing taxes, which leads taxed people to get angry**.\\n\\nWhen taken too far, the over-printing of fiat currency leads to the selling of debt assets and the earlier-described bank “run” dynamic, which ultimately reduces the value of money and credit, which prompts people to flee out of both the currency **and the debt (e.g., bonds)**. 注意国债也会贬值\\n\\nNaturally those who are governing the countries that are suffering from this flight from their debt, their currency, and their country want to stop it. So, at such times, governments make it harder to invest in assets like gold (e.g., via outlawing gold transactions and ownership), foreign currencies (via eliminating the ability to transact in them), and foreign countries (via establishing foreign exchange controls to prevent the money from leaving the country).\\n\\n神奇的债务自动失效机制\\n\\nThese cycles of debt and writing off debts have existed for thousands of years and in some cases have been institutionalized. For example, the Old Testament provided for a year of Jubilee every 50 years, in which debts were forgiven (Leviticus 25:8-13). Knowing that the debt cycle would happen on that schedule allowed everyone to act in a rational way in preparation for it. Helping you understand this dynamic so that you are prepared for it rather than are surprised by it is the main objective behind my writing this.\\n\\n这不止是一个从hard money到fiat money的演变，也是一个循环\\n\\n\\n\\nmkt还没有习惯fiat money system的时期\\n\\nThis move to a fiat monetary system freed the Federal Reserve and other central banks to create a lot of dollar-denominated money and credit, which led to the inflationary 1970s, which was characterized by a flight from dollars and dollar-denominated debt to goods, services, and inflation-hedge assets such as gold. That panic out of dollar debt also led interest rates to rise and drove the gold price from the $35 that it was fixed at in 1944 and officially stayed at until 1971 to a then-peak of $670 in 1980.\\n\\n收割新兴国家的过程\\n\\nTo deal with that monetary inflation crisis and to break the back of inflation, Volcker tightened the supply of money, which drove interest rates to the highest level “since Jesus Christ,” according to German Chancellor Helmut Schmidt. Because the interest rate was far above the inflation rate debtors had to pay much more in debt service at the same time as their incomes and assets fell in value. That squeezed the debtors and required them to sell assets. Because of the great need for dollars, the dollar was strong. For these reasons, inflation rates fell, which allowed the Federal Reserve to lower interest rates and to ease money and credit for Americans. Of course many debtors and holders of these assets that were falling in value went broke. So in the 1980s these debtors, especially foreign debtors and more especially those in emerging countries, went through a decade-long depression and debt-restructuring period. **The Federal Reserve protected the American banks by providing them with the money they needed, and the American accounting system protected them from going broke by not requiring them to account for these bad debts as losses or value these debt assets at realistic prices.** This debt management and restructuring process lasted until 1991, when it was completed through the Brady Bond agreement, named after Nicholas Brady who was the US Secretary of Treasury at the time. This whole 1971-91 cycle, which affected just about everyone in the world, was the result of the US going off the gold standard. It led to the soaring of inflation and inflation-hedge assets in the 1970s, which led to the 1979-81 tightening and a lot of deflationary debt restructuring by non-American debtors, falling inflation rates, and excellent performance of bonds and other deflationary assets in the 1980s. The entire period was a forceful demonstration of the power of the US having the world’s reserve currency—and the implications for everyone around the world of how that currency was managed.\\n\\nAs I explained more comprehensively in my book Principles for Navigating Big Debt Crises than I can explain here,\\xa0**there are four levers that policy makers can pull to bring debt and debt-service levels down relative to the income and cash-flow levels that are required to service one’s debts:**\\n\\n- **Austerity (spending less)**\\n- **Debt defaults and restructurings**\\n- **Transfers of money and credit from those who have more than they need to those who have less than they need (e.g., raise taxes)**\\n- **Printing money and devaluing it**\\n\\n为何所有货币都在贬值，因为政府要解决债务问题；不是对商品贬值\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a89e5ca7-7238-4d8f-9009-a5a0cdbc89e1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n杂 58\\n\\n亚里士多德也把农业放在第一位,“ 因为它符合正义( 自然)”; 后来美洲的圣人本杰明· 富兰克林宣称, 耕作是惟一诚实的谋生之路。这种轻视工商业的偏见广为流布, 且根深蒂固。在中世纪的基督教里, 这种偏见表现得非常突出。重农学派认为, 只有农业才生产出剩余价值。商业不过是交换而已, 制造业所增添的仅仅是工匠劳动的价值; 但是, 土地能够生产出价值。这种“ 唯农”\\n\\n主义实际上与重农学派的自由贸易思想紧密相连。他们坚决主张取消一切税费, 只保留一种土地税, 理由是, 所有的税收都应该出自那一个来源, 因为只有它才创造价值。而且, 他们建议应该放手实行贸易竞争, 因为他们认为大富商是在通过垄断来获得不义之财\\n\\n在波特理论中最重要的就是三句话：**总成本领先，高差异化，专门聚焦。**中国人把它简化了，叫低成本、高差异、专门化。这其实解释错了，他没有讲低成本，而是总成本领先，他没有讲差异化，而是高差异，他也没有讲专门化，而是聚焦\\n\\n**增长一定是来源于产品和市场，一个是市场维度，一个产品维度;除了用产品和市场去做增长，还有一个核心要素就是团队，那么如何带领团队成长？**\\n\\n现代历史学与社会学一样, 起源于法国大革命后的保守主义思想流派, 而不是起源于启蒙运动。在历史观念方面, 休谟是一个例外, 而18 世纪哲学家基本上持极端的理性主义和乐观主义态度。他们相信有一种理想的社会秩序, 理性可以发现它, 政治工程师可以实现它。那种缓慢而无限的生长过程的观念不适合他们的口味。他们喜欢的是一蹴而就直奔乌托邦\\n\\n“理”怎样和“你”结合在一起？你唯有找到真实的你，才能觉察真正的道理（所谓“真理”）。\\n\\n《庄子·大宗师》说：“且有真人，而后有真知。”哲学有“存在”、“在场”，有“诗意地栖居”；佛学有“随顺觉性”、“明心见性”。你可曾明了自己人生的真心？\\n\\n试读一读李贽的《童心说》看：\\n\\n夫学者既以多读书识义理障其童心矣，圣人又何用多著书立言以障学人为耶？童心既障，于是发而为言语，则言语不由衷；见而为政事，则政事无根柢；著而为文辞，则文辞不能达。非内含于章美也，非笃实生辉光也，欲求一句有德之言，卒不可得，所以者何？以童心既障，而以从外入者闻见道理为之心也。”\\n\\n夫既以闻见道理为心矣，则所言者皆闻见道理之言，非童心自出之言也，言虽工，于我何与？岂非以假人言假言，而事假事、文假文乎！\\n\\n以上我们主要论述了法国的启蒙运动。在18 世纪, 法国是西方世界最重要的国家; 除了在艺术和科学领域里的巨大声望外, 它还有数量超过其他任何一个欧洲国家的庞大人口———是英国人口的4 到5 倍, 甚至比当时俄国的人口还多。此外, 它还拥有巴黎这个独一无二的伟大城市, 巴黎集中了它的全部辉煌。巴黎属于全世界\\n\\nEven in his first writings Camus reveals a spiritual attitude that was born of the sharp contradictions within him between the awareness of earthly life and the gripping consciousness of the reality of death. This is more than the typical Mediterranean fatalism whose origin is the certainty that the sunny splendour of the world is only a fugitive moment bound to be blotted out by the shades. Camus represents also the philosophical movement called Existentialism, which characterizes man’s situation in the universe by denying it all personal significance, seeing in it only absurdity. The term “absurd” occurs often in Camus’s writings, so that one may call it a leitmotif in his work, developed in all its logical moral consequences on the levels of freedom, responsibility, and the anguish that derives from it. The Greek myth of Sisyphus, who eternally rolls his rock to the mountain top from which it perpetually rolls down again, becomes, in one of Camus’s essays, a laconic symbol of human life. But Sisyphus, as Camus interprets him, is happy in the depth of his soul, for the attempt alone satisfies him. For Camus, the essential thing is no longer to know whether life is worth living but how one must live it, with the share of sufferings it entails.\\n\\nPersonally Camus has moved far beyond nihilism. His serious, austere meditations on the duty of restoring without respite that which has been ravaged, and of making justice possible in an unjust world, rather make him a humanist who has not forgotten the worship of Greek proportion and beauty as they were once revealed to him in the dazzling summer light on the Mediterranean shore at Tipasa.\\n\\nActive and highly creative, Camus is in the centre of interest in the literary world, even outside of France. Inspired by an authentic moral engagement, he devotes himself with all his being to the great fundamental questions of life, and certainly this aspiration corresponds to the idealistic end for which the Nobel Prize was established. Behind his incessant affirmation of the absurdity of the human condition is no sterile negativism. This view of things is supplemented in him by a powerful imperative, a nevertheless, an appeal to the will which incites to revolt against absurdity and which, for that reason, creates a value.\\n\\n“除了冥想，修行时你还做些什么？”\\n\\n“并没有修行时，生命的每时每刻都是修行。我要遵守六戒：不杀生、不偷窃、不撒谎、不饮酒、不吸烟、不邪淫。”\\n\\n“遵守这些戒训难吗？”\\n\\n“不难，更难的是从娱乐和消费中寻找快乐。”\\n\\n法国名牌“hermes爱玛仕”，一个手袋可以为卖200,000元，假设很多女仕都爱不释手，又假设你是女生，今天你生日，我一次過送了两个手袋送给你，一个是真正的“hermes爱玛仕”手袋。\\n\\n同时，我模仿制成了一个跟以上牌子从肉眼看来，无论“形状、质料、设计”也跟“hermes”牌子标签一模一样的式样手袋，肉眼、质感也是看分辨不出来，现在同时送给你，请先代入了解六天情景，再想想几个问题：\\n\\n**1）第一天，（你在不知真假）的情况下，先将“真”hermes揹上肩膊上街，好让其他人看见，结果你换来的是羡慕目光，你内心生起一种无法形容的愉悦感。**\\n\\n**2）第二天，（你在不知真假）的情况下，再将“假”hermes揹上肩膊上街，好让其他人看见，结果你换来的是羡慕目光，你内心生起一种无法形容的愉悦感。**\\n\\n**3）第三天，（你清楚知道）手袋是“真”，再揹上肩膊上街，好让其他人看见，结果你换来的是羡慕目光，你内心仍然生起无法形容的愉悦感。**\\n\\n**4）第四天，（你清楚知道）手袋是“假”，再揹上肩膊上街，好让其他人看见，结果你换来的是羡慕目光，你内心仍然生起无法形容的愉悦感。**\\n\\n**5）第五天，（你清楚知道）手袋是“假”，再揹上肩膊上街，好让其他人看见，但我（要你告诉别人是假的），结果别人便带着“轻视眼神”对待你，你感到似被悔辱。**\\n\\n**6）第六天，（你清楚知道）手袋是“真”，再揹上肩膊上街，好让其他人看见，但我要你告诉别人，是好朋友送给我的生日礼物，结果别人更带着轻视的“羡慕眼神”对待你，你感到似被赞叹。**\\n\\n**名车与虚荣**\\n\\n**问题一：究竟一般人喜欢那个名牌手袋什么？**\\n\\n**此刻跳出手袋领域.......**\\n\\n**问题二：一般男人在众目睽睽下，驾上一架“法拉利”跑车，那份愉悦感来自什么？**\\n\\n**问题三：如果他戴上面具，无人知道他是谁，但在众目睽睽下看着他驾上一架“法拉利”跑车，他内心又有愉悦感吗？**\\n\\n**问题四：如果他坐上一台价值三百万重型铲泥车在烈日当空去做铲泥工作，他的感觉跟驾上“法拉利”一样吗？**\\n\\n**问题五：如果他驾着上一台价值三百万“法拉利”進入一个没有名牌车子观念的非洲部落，他内心又有愉悦感吗？**\\n\\n說句笑話，这班非洲人可能是食人族，见到这个男人非常肥美.....hahahahaha.....說笑而已，不要理会我这句话。\\n\\n好了！以上五个问题你都不需要答，现在请你只需说出人的“苦乐”究竟来自什么？\\n\\n**喜恶源自主观的心～二元对立**\\n\\n在你未给自己一个具体答案前，勿急于看以下文字。\\n\\n**原来，人對世事的态度，全来自透过六种感知功能～“六根”聚合而生起的感受，亦即是透过视觉、听觉、嗅觉、味觉、触觉及思维活动，所引起一连串身体內外的生理及化学反应所带来的“感觉”，故此，全部只是内部主观发生，人所“追随、拒绝”的只是“内在感觉”，并非来自被感知的外界，亦即跟外界的“名牌手袋、法拉利跑车”无关，世间只是“中性活动”，批判、分别的是“心”作怪。**\\n\\n**这份感觉凝聚成“喜欢、嫌弃”的两种态度作为主轴，一部份你更能回忆，另一部份你却会完全忘记，但无论是否记得起，那份因六根感知聚合的“生化反应”复杂细节过程，都会不知也不觉「累积讯息」储存下来，形成了“感觉记忆体～（识）”，导致使人面对世事，也情不自禁、不受控，在不知不觉地自动提出记忆中的累积讯息去运作面对眼前的际遇。**\\n\\n**人总活在“荣辱、得失、对错、善恶”等等之间的对立世界里，所以矛盾、纠结、烦恼是必然的，这种“二分法”形成了一种“惯性思维（Inertial thinking）”。**\\n\\n**我们从小就被教导这样分化所有人事物的价值，彷佛以为这“二分法”就是处世理所当然之道，谁知这种“二分法思维”存在大量逻辑谬误，难道世界上颜色又岂会只得“非黑即白”这样简单？**\\n\\n**人总惯性看表面事相作基准，局限只得“两种对立”角度层面，常常只考虑一系列“两极端”的可能性，而排除其他可能，看不见“两极端”以外领域，佛法上叫“边见”，亦即永远执取“偏侧于一边见解，却看不见整体”，这就是痛苦根源，佛法叫“着相”。（注：“着”指滞留。“相”指由感知后而出的观点。）**\\n\\n**掉进二元沼泽～烦恼**\\n\\n**这种“喜欢～取、嫌弃～捨”的惯性思维，进一步地操控着我们的身所行、口所言、意所想，让人无知的向“外界表象～相”作出无止境的“攀附”，到头来做出很多不是伤害别人，就是伤害了自己的决定，就为满足那份内在的化学反应～“我、我的、我的感觉”。**\\n\\n**稱合我的感觉，想它不断延续～取。**\\n\\n**不称合我的感觉，想它马上消失～舍。**\\n\\n**修行佛法就是超越这两种令人上瘾“惯性思维”，目的就是“破除我的执迷”，人就可以用中庸的思路去平衡生活上种种际遇，再调炼超越“表象框架束缚～离相”，當平等心出现的时候，烦恼就会越来越少，内心就会感觉越来越自在，如果未能超越这种“落两边”的思路，人是永远没有办法自在，始终也是在苦乐中浮沉。**\\n\\n**顺带一提，如果你经常运用离相的心，别人眼中的你是充滿慈悲的，慈悲心并不是一种情绪，而是一种思路上的平等心流露。**\\n\\n如果无法令自己每一天也开心，而只有忧愁的话，人生存还有什么意义，如果死真能解決一切，我早就马上自我了断。\\n\\n人人每天都很多烦恼及阻力，你我他一模一样，每天自有每天愁，彷佛烦恼永远解决不完，挥之不去，日日新鲜。\\n\\n从修行中，我明白了世事也有阴阳造化，有黑暗必有光明，既然每天自有每天“愁”，也肯定每天自有每天“乐”。\\n\\n**人人一出生下来的最大使命，就是来“面对困难，解决困难”，万物平等，谁也不能独善其身，面对生活，解决人生上种种障碍是人的责任，没法推卸，人人所面对的其实大同小异，谁都不比谁“好命”，谁也不比谁“歹命”，重点是谁有“智慧、勇气、能力”去面对？上天非常公平，并不会因你的眼泪而放你一马。**\\n\\n**跳出二元幻影～自在**\\n\\n故此，我决定从那份苦恼的环境中，时刻再找出光明面去自得其乐，男儿女儿也当自强，为了自强就必须不能极端，一定要看出世事的真相，那个真相原来全部在我们的心，不能被相所迷，誓要火焰化红莲。\\n\\n**由此可见，我們就不难理解佛法为何叫“不二法门”？**\\n\\n**简言之，就是“没有二元对立”的法门，亦即是觉醒的方法。**\\n\\n佛者，觉也。**佛法是“觉醒”的方法，是指出万象的“实相”而已，根本并非谈对错，更并非另定基准去为世间“认定”或“否定”，“对、错”只是人心的基准，并非指事件本身意味了什么。**\\n\\n**佛法“不会认定、不会否定”世间任何表象，只是借止恶行善去维持人间活动、因果作为指引去面对人生，目的是离苦得乐，却是权宜之计。**\\n\\n**“世俗心”；这思维使人心不断在“二元对立”之中交战似的，执取一边观点却推开另一边观点，那份矛盾使内心产生冲突，使眼中看出的世界容易产生不顺眼。**\\n\\n**称我心意就认为是对。**\\n\\n**逆我心意就认为是错。**\\n\\n**这种“顺我者对、逆我者错”的思维属执着，这是烦恼根源**。\\n\\n**人心执着创造了概念世界，充满了二元对立，当中包括了“上下、左右、这个、那个、内外、对错、真假、正负、好坏、你我、应该、不应该”等等数之不尽的二元对立观念，佛法上叫“分别心”，亦是“着相”。**\\n\\n**着相的观念全部是一厢情愿创造出来，宇宙万物只有生灭表象活动，更以不断转化以存在，本质皆空，实相是不生不灭，并没有对立，种种对立及分别只是人心创造错觉，主观道德标准界定出来的幻影。**\\n\\n**驾驭一切观点与立场的般若波罗蜜多～离相apratisthita laksana**\\n\\n**“出离心”；是指“从感知中出离”，叫“apratisthita laksana~不住于相”。**\\n\\n**出离心，并非放弃或讨厌世间，而是指从感知出离。佛法，从来都没有“认定”或“否定”世间所出现的活动，而是超越二元对立。**\\n\\n《般若波罗蜜多心经》内每一个“不”字就是说实相，**“不要滞留”表象二元对立，万法唯心，森罗万象的千差万别，都只是自心创造出来的幻影。**\\n\\n佛法从来都并非为世间另定基准，佛陀四十九年言教只是指出“实相”，也完全跟阁下秉持什么宗教、理念及接受“实相”与否无关，任何人只要攀附表相，结果就只有苦。\\n\\n**佛法上的修行，其实就是修心，扼要地说就是“思维修”，佛陀所教的一切的「功法、心法」就是教人认识我们这个心，去了悟人生的“实相”，用“不取不舍”的思路去面对人生，离心说道，就叫「外道」，这样就已偏离了佛法本身。**\\n\\n说到这里，读者应该对“相”已有一定的体会，更加知道什么叫“着相、离相”。**人生際遇，無非只是內心的一份主观感觉，佛者；觉也，求觉者首先要明白“实相”，才愿意学习“离相”。学佛最基本就是要“明心见性”，除此以外，再没其他，这才算是依教奉行的真正佛弟子。**\\n\\n**人的苦乐跟外界无关，外界只是一连串“中性活动～缘”，苦乐是内在的心所创造出来的错觉，不要再受“相”欺骗，一切唯心造。**\\n\\n**着相pratisthita laksana**\\n\\n最后我再分享一下什么叫“名、相”，作为这篇文章的总结。\\n\\n**相；指森罗万象呈现，其形相状，也各尽不同。**\\n\\n**名；指依据各种形相，定出种种名称，以作表诠。**\\n\\n简单说，不论有形无形、有色、无色，有想、无想、非有想、非无想，动态静态，**只要由心所感知，再去执取的名或相而建立的内心种种观点，就叫“相”**，简单说，相就是指由感知后而出的观点，“际遇”也。\\n\\n**着：指滞留。**\\n\\n**相：指由感知后而出的观点。**\\n\\n**全句内涵，指被感知后所生起的观点而牵制滞留。**\\n\\n**P.S**\\n\\n**梵音：pratisthita，指“住”。**\\n\\n**梵音： a-prati-sthita，指“不住”。**\\n\\n**梵音： laksana，指“相”。**\\n\\n**古代语叫“住相”。**\\n\\n**近代语叫“着相”。**\\n\\n最近在看《沉思录》，摘一段：\\n\\n如果你在一生中做每一件事都像是做最后一件事一般，（便可）避免一切粗心大意，避免一切违反理性的感情激动，避免一切虚伪、自私，以及对自己那一份命运的抱怨\\n\\n如果做更狭义的界定, 启蒙运动就相当于启蒙哲学家的时代, 大体上相当于伏尔泰长寿的一生, 其中叠加着卢梭和百科全书派的鼎盛时期, 或许还可以叠加上重农学派的活跃时期。这就意味着大约从1720 年到1778 年的时期。因此, 人们倾向于把结束时间定在18 世纪末\\n\\n然而, 启蒙运动还在延续。也就是说, 它在19 世纪和20 世纪依然具有重要影响; 与所有伟大的思想运动一样, 它在其后人类的思想上留下了永久的印记; 在它发生之后, 世界完全改变了。关于它的种种影响, 这里不可能全面具体地论述, 只能举几个例子。从学术角度看, 在大部分英美世界,哲学依然倾向于休谟给它修正的方向。经济学理论虽然已经远非亚当·斯密时代的面目, 但至今还带有他的独特视角所打下的印记, 将来或许还会如此。从更一般的角度看, 甚至在20 世纪种种社会悲剧发生之后, 启蒙运动特有的乐观的理性主义依然焕发着活力, 在美国尤其如此。\\n\\n我们认为18 世纪贡献了这样的观念: 人类现在和将来都会“进步”, 科学技术对推动人类进步起了最大作用, 人类的目的就是享受世俗的幸福。虽然有越来越多的知识分子对这些说法表示怀疑, 但大多数平民百姓可能还是信奉它们。与许多社会科学一样, 现代自由主义和社会主义都是在18 世纪孕育出来的。今天的公共政策的目标也是由启蒙运动确定的: 物质福利、幸福。\\n\\n人们还会想到宗教宽容、人道主义、法律面前人人平等, 言论自由以及民主和社会平等。所有这些都主要源于这个世纪。\\n\\n既然启蒙运动给我们带来了如此之多美好事物, 我们不免要问, 为什么浪漫主义时代的人们要否定它, 至少是对它的某些部分深恶痛绝? 简单地说, 为什么我们没有停留在启蒙时代, 而是跨越它? 浪漫派认为, 启蒙运动具有他们所说的那种庸人之气, 因为它蔑视诗歌, 贬低天才的想像, 不尊重崇高或“热情”。在18 世纪,“ 热情”是一个贬义词。热烈的情感和奔放的想像都受到怀疑, 被视为产生谬误、贻害社会的根源。它们甚至会引导人们回归宗教! 洛克建议父母扼杀孩子的诗兴, 因为那是胡思乱想的领域。牛顿也认为诗歌是“ 精心编造的胡言乱语”。当然, 这种“ 冰冷哲学的触及”不可能消灭一切诗歌, 但是它会使大部分诗歌变得平淡乏味生硬造作, 成为一种说教。诗歌可以用来点缀科学的真理, 但是它不能创造这些真理“: 真理用诗歌包装,放射出更耀眼的光芒。”〔1 〕伏尔泰和达朗贝尔坦率地承认“, 靠想像创作的现代作品普遍不如前人的同类作品”,因为“追求眼见为实而不愿推测臆想的哲学精神已经广泛传播, 深入到纯文学中”(达朗贝尔)。\\n\\n大多数启蒙运动代表人物都排斥高昂的宗教热情和艺术热情, 而仅仅让人们追求幸福\\n\\n追求幸福, 这个著名的口头语体现了那个时代人们的旨趣。保罗·哈泽德〔1 〕在《18 世纪欧洲思想》一书中专辟一章来论述“ 幸福”。他指出, 当时有许多著作、小册子、小说和诗歌都涉及这一主题。“ 什么是幸福?”可以与“为什么有邪恶?”一起被视为18 世纪的最大热门话题。人们已经强烈地感受到, 时代已经开始让人们在人间而不是在天堂享受幸福, 那些导致人们相互残杀的狂热理想都是欺骗, 应予以揭穿。因此, 人们所能做的就是通过实在的、符合常识的方式来享受当下的幸福。但是当人们仔细探究这一问题时, 根本不清楚到底什么是幸福。如果幸福仅仅意味着物质和感官享受, 那么人们可以谴责这个时代缺乏高贵精神, 把人降低为畜生。我们会由此想起约翰·穆勒在批评功利主义伦理学时讲的那个故事: 一个绅士总是把糖果藏在房子周围, 因为他只有在意外地发现糖果时才能享用它! 如果你也这样孜孜不倦地寻找“幸福”, 你可能根本找不到它; 因为幸福是追求其他事物的副产品。\\n\\n18 世纪最后30 年, 情感崇拜在欧洲文学界已经深深地扎下根了, 有时与“理性主义”平分秋色, 有时则取而代之。( 卢梭竟然能够几乎同时创作《新爱洛绮思》和《社会契约论》。)“ 情感人”可以落泪, 可以尽情宣泄。一种沉思和伤感情绪风靡社会\\n\\n怀特海说的好, 这是一个“基于信仰的理性时代”。它始于一种坚定的信念: 科学的经验主义能够使人们得出某些普遍法则。但是经验主义导致了怀疑主义, 或者说没有能够找到人们所期待的简单而放之四海皆准的法则。对理性的信仰是对澄明的信仰, 即相信一切终究水落石出。这也是对不含有形而上学成分的科学的信仰。还有一种如影相随的信念:“ 自然”和科学能够给道德和宗教提供明确的指导。但是, 这些信念都过于简单化了, 或者说是严重的误解。科学并不具有规范作用, 不能提供价值; 科学的“法则”(定律) 只是尝试性的、操作性的, 除此之外, 我们不能赋予它们其他的意义。卡尔·贝克尔所提出的著名概念“18 世纪哲学家的天城”说的是:这些哲学家实际上保持了中世纪基督教的信念, 即这个世界具有上天安排的秩序, 同时把这种说法用于现世而不是来世的天堂, 用科学理性而不是用神学来揭示这个天堂。( 戈登· 赖特指出, 启蒙哲学家嘲笑那些宗教奇迹, 却相信人类能够尽善尽美的奇迹。)从18 世纪人们对“ 自然”这个词的不同定义中, 我们可以发现造成上述基本问题上的思想混乱的某种根源。毋庸赘言, 这个词与那个时代的主题紧密相关。新古典主义作家所追求的是“ 自然”, 自然神论者用以取代天启宗教的是“自然”, 社会科学家在寻求人类事务的严格规范时所求助的是“自然”, 道德家探求人类行为准则时所依据的是“ 自然”。阿瑟·洛夫乔伊在《作为美学标准的自然》一文中列举了这个词的8 种定义。他指出,“追随自然”的原则对于新古典主义是饮鸩止渴, 因为“ 几乎所有反对新古典主义标准的人都祭起这面旗帜”。约瑟夫·巴特勒早就向英国的自然神论者指出“, 自然”是完全不确定的。18 世纪的人在“自然”里看到的是秩序、简明和规律; 也有人像某些浪漫主义者那样看到的是无规则、不讲规则、自由精神、追随“ 自然”而不墨守成规。启蒙运动衰落的根本原因或许就在于,它诉诸“自然”时满怀信心地期待着“自然”有一个明确的而毫不含糊的权\\n\\n威标准, 但是结果却恰恰相反。〔1 〕最后, 法国大革命这场大灾变当然也促成了启蒙运动的结束。大革命从18 世纪哲学家那里获得了启示和理念。但是, 首先, 它很快就横扫了他们的文雅规则, 进而探索前所未闻的政治领域; 其次, 它规模如此宏大, 使人不由地认为它开辟了一个新时代———不言而喻, 也结束了一个旧时代。\\n\\n它的暴烈和过激引起了反感, 导致人们反对所有那些被认为激发了大革命的理念。欧洲知识界大多追随伯克和梅斯特尔, 反对法国大革命以及作为其背景的思想运动(指启蒙运动———译注) 。1789 年开始的长达25 年的苍黄反复使得整个文明发生了彻底的变化———无论是社会领域, 还是政治领域, 还是思想领域。\\n\\n启蒙哲学家的思想究竟在多大程度上引发了或促成了法国大革命, 影响了大革命的方向? 这已经是思想史研究中长期争论的一个问题。几乎没有人否认伏尔泰、孟德斯鸠、卢梭和狄德罗起了重大作用: 破坏了人们对旧制度的尊崇, 鼓励了人们对美好社会的向往。卢梭还向人们提示, 在现存社会秩序的废墟上将会矗立起一个更美好的新社会。很显然, 成为大革命导火索的那些哀怨、不满和愚蠢举措, 都不是他们制造的; 贵族的贪婪、国王的软弱、商人的野心, 也不是他们引发的。但是, 他们提供了点燃这场大火的火柴, 而且, 正是在他们的指引下, 大革命才成为一场伟大的、影响深远的运动, 而没有成为多少世纪以来所发生的那类无目的的暴动和抗议\\n\\n当然, 我们承认, 启蒙哲学家和百科全书派既没有期盼革命, 也不曾欢迎民主。革命者歪曲了他们, 用自己的愿望和目标来解释他们的学说。在大革命期间, 卢梭、伏尔泰和孟德斯鸠的语录挂在每一个人的嘴上; 这些语录大多被断章取义, 所附会的意思会令原作者目瞪口呆。我们现在能够看得很清楚了。例如, 卢梭的政治思想很微妙也很复杂, 甚至自相矛盾, 但是, 按照我们的理解, 他的思想并不像圣茹斯特和罗伯斯比尔所想像的那样支持革命的全民民主。这给我们的启示是, 历史的运动把思想家和实践家卷入同一潮流, 但彼此并不完全理解\\n\\n无论是大革命的支持者, 还是反对者, 大概都会同意, 启蒙思想家的思想主要起了一种否定的作用, 其破坏性大于建设性。总体而言, 他们希望扫除他们称之为迷信、腐败、矫揉造作的东西和过时的法律。至于用什么取代旧社会, 他们则含含糊糊地诉诸理性、自然或自然法。伏尔泰认为, 如果人们摆脱了僧侣控制的宗教, 就会变得非常有理性, 这就够了。卢梭则提出, 如果人们把非自然的污垢清除掉, 就会变得尽善尽美, 这就够了。重农学派和亚当·斯密补充说, 如果给社会松绑, 消除国家干预和社会不平等这些累赘的机制, 那么社会就能自行运转。这些人其实并没有搞明白任何社会问题, 只是要求消灭现有的一切。有人批评启蒙哲学家犯了“ 虚无主义”, 是破坏者, 不是建设者。他们在主观上并非如此, 但是我们也不难理解为什么会出现这种指责。〔1 〕最早的革命者并不认为这是虚无主义, 恰恰相反, 他们天真地相信, 只要把旧制度消灭掉, 一切都会好起来。但是,在现实中, 他们不得不接受沉重的教训: 人就其“自然本性”而言不是那么有理性, 也不是那么有道德, 因此美好社会不可能自动产生。\\n\\n法国大革命的支持者认为, 正是18 世纪的哲学家促成了人们摧毁不公正、不合理的社会政治秩序, 高扬起理性和人道的旗帜, 给人类指出了前进的基本方向(尽管不能准确地知道具体的路线)。对于他们来说, 有这样的荣誉就足够了。而且, 仅仅是一代人就享有这么大的荣誉。法国大革命的反对者总是宣称, 伏尔泰、卢梭等一伙人歪曲人性、歪曲历史, 从而使得欧洲人陷入万劫不复的痛苦之中。也许我们还涉世不深, 没有对现代世界的足够体验, 难以客观地对这两种对立的观点做出判断。但是没有人能够否认, 法国大革命开辟了新的现代历史阶段; 它在很大程度上体现了18 世纪哲学家的伟大思想和情感, 同时也结束了他们那个时代', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='80f2f5a0-0c04-4df6-96e6-65ef890b54ef', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n**Use notions**\\n\\n**要利用确定的概念或某个想法**\\n\\n- 使用有事实做依托的大想法\\n- 明白事情的真正意义\\n- 简化\\n- 利用规则并且学会筛选\\n- 知道我要达成的目标\\n- 寻找并且考量替代方案\\n- 了解各种后续情况以及整体结果\\n- 定量\\n- 搜寻各种根据，并在此基础上行事\\n- 能够反向思考\\n- 记住，要有大的成效，就需要诸多因素的总和\\n- 评估如果做错了会有什么后果\\n\\n**What is the issue?**\\n\\n**问题的解决**\\n\\n- 问题是什么？事情的实质是什么？\\n- 事情的核心或重点在哪里？相应的主要问题是什么？\\n- 什么是相关的？什么是可解决的？什么是重要的？什么是可知的？什么是可以适应的（Utility-applicability）？\\n- 我了解事情的全貌吗？为了使自己对事情有想法，我需要一些相关数据和基本知识储备，否则我就得承认“我不知道”。\\n- 我的判断比其他人好吗？\\n- 我必须对什么做出预测？它是可预测的吗？\\n- 需要做出决定吗？如果我不当机立断，会发生什么？为解决此事，我能做什么？“我”应该去做吗？\\n- 我对此问题的思考花了多少时间？此刻我思考到了哪一步？处于谁的立场上？\\n- 简化问题，先解决“不需大脑思索”的大问题，然后从自己的境地开始思考。\\n\\n**Understand what it means**\\n\\n**理解背后的含义**\\n\\n- 把语言和各种想法翻译成我明白的方式。我能理解所用的语言和结论的真正含义和暗示吗？它意味着什么？它是否有助于我对未来做出有用的预测吗？\\n- 我是否了解事情发生和运转的方式和原因？它正在产生什么影响？为什么会有这样的影响？现在的情况如何？怎样及为何会发生如此情况？其结果会是什么（会有什么观察、发现、事件、体验）？\\n\\n**Filters and Rules**\\n\\n**过滤和规则**\\n\\n- 利用规则和缺省规则来筛选——我可以如何检测？\\n- 改变规则，以适应我的心理特征、心理承受能力、心理优势和心理局限。\\n- 考虑我的价值观和偏好，由此判断事物的轻重缓急，以及希望规避的事情。\\n\\n**What do I specifically and measurable want to achieve and avoid and when and why?**\\n\\n**我具体想达成和避免的是什么，什么时候达成，原因为何？**\\n\\n- 就数字而言，我想获得什么样的价值？目标数字是什么？目标效果是什么？所设定的时间范围是什么？\\n- 假设我已经达到了目标，那么这个目标如何反映在数字和效果上？接下来又需要达到什么？这个目标是否合理？如果从目标逆向回溯到现在，是否可行？\\n- 我是否有办法衡量目标的完成程度？这个标准的关键指标是什么？\\n- 如果我达到了目标，随之而来的会是什么？那是我希望得到的吗？\\n- 我可否把大的目标分成若干有期限的短期目标？\\n- 我做此事的真正原因何在？是因为我想这样还是因为我不得不这样？我在阐述目标时，是基于内外两方面的现实呢，还是受到了某些心理力量的影响？\\n- 我能否简短地阐述我的目标，以使人更容易明白我达到此目标的方式？\\n- 这是不是我真正希望达到的结果？\\n\\n**What is the cause of that?**\\n\\n**成因何在？**\\n\\n- 为了达到目标，我必须知道能让我达成目标的各种成因。\\n- 通过哪些要素的组合可以达成的目标？我如何佐证此公式？\\n- 什么是我不希望获得的结果？可能促使“非目标”出现的原因是什么？我能如何规避？我必须不做什么，或者必须避免什么？\\n- 整个系统要运转起来，会受哪些变量的影响？这些关键性变量是什么？主要的未知因素何在？有哪些确定的因素可以帮助我评估和优化这些变量？\\n- 哪些变量有赖于其他变量（或情形、环境、背景、时机、行为）？哪些变量是独立于其他变量的？\\n- 什么外力才能促使某一变量出现？这些外力来源于何处？是短期还是长期外力？其相对优势是什么？这些外力之间如何组合、互动，效果会如何？我如何才能让诸多外力作用于共同的方向？缺乏了哪个外力会毁掉整个系统？这个外力来源于何处？可预测度有多高？这些外力如果出现，会促发什么样的合理结果？哪些外力是暂时性的，哪些外力是永久的？作用于变量的这些外力若发生变化，整个系统会发生怎样变化？\\n- 在这些变量和外力发生变化时，系统抵制这些变化的惯性有多强？变量和外力产生变化（上升或者下降）后，可能导致哪些希望的和不希望的短期和长期结果（数字或效果），如规模、体积、强度、密度、长度、时间维度、环境、参与者等？一组较小的外力如果长期发生作用，会怎样？如果作用于变量的某一外力长期发生作用，结果会如何？什么外力能加以改变？需要什么才能达到临界质量？添加了哪些外力后能达到临界质量？如何发生作用？如果我改变了一个变量或者外力，会导致其他事情发生吗？什么情况会让一个外力发生改变？这个改变会产生其他结果吗（请注意我感兴趣的是整个系统的效果和最终结果）？一个变量发生了变化，会对整体结果带来戏剧性的改变吗？属性是否也会随之改变？如果变量之间的关系发生变化，结果会如何？促发改变的那个点是什么？障碍是什么？催化剂是什么？引爆点是什么？拐点在哪里？暂停点在哪里？局限是什么？有多久时滞才能等到效果发生？反馈是什么？什么能令此成因加速？效果若要发生逆转，临界点在哪里？对这个公式，我能做什么改变，其他人能做什么改变？如何做？谁来做？什么时候做？我需要改变哪些变量才能达成目标？我如何度量变化的程度？Degree of sensitivity if I change the assumptions？对目标和路径会产生什么影响？如果某一变量保持不变，会如何？我在提高某一变量的同时降低另一变量呢？会有怎样的网络效应？如果一次只改变一个变量或者外力呢？外部环境中有什么会改变我的处境？如果对其中一个变量进行优化，会产生其他什么优势和劣势？什么才能导致最终结果发生变化？如果我改变了条件，变量还会成立吗？\\n- 这一公式会否出现例外，为什么？需要哪些条件才能达成目标？Has my goal different cause short-term and long-term？这个成因是否依赖于时间条件？通过观察效果，我能否追溯其成因？我有否采用不同的角度和立场来审视整个系统？对主题的考量依赖于什么东西？\\n- 限制我达成目标的主要力量是什么？\\n\\n**What available alternatives do I have to achieve my goal?**\\n\\n**有没有现成的替代方案可以帮助我达成目标？**\\n\\n- 通过目标、主体问题、规则、因果、行为、佐证、反证、资金的机会成本、时间、其他资源、精力、理解力、风险和精神压力等因素，来对其他替代方案进行判断。\\n- 我有什么依据（包括模式）来判断这些替代方案很有可能帮我达成目标？\\n- 这些因素有赖于某些特定的时间点或者事件吗？\\n- 每一个行动可能产生的后果是什么？可能产生什么效用？可能性有多高？你对每个结果的期待值有多高？\\n- 如果现在采取某些行动，我是否会放弃未来的一些机会？\\n\\n**What are the consequences?**\\n\\n**结果是什么？**\\n\\n- 通过对结果的预估，寻找最可能帮助我达成目标的替代方案。\\n- 如果我做出了一个选择，什么将随之发生，什么又不会发生？\\n- 每个替代方案（逻辑上）可能产生的想要的结果或者不想要（或不希望）的结果是什么（定性并定量）？结果之后的次生结果（短期内或者长期内产生的）会是什么？\\n- 会产生什么样不同的情形和结果？在这样的依据下，会产生什么长期或短期的效应？\\n- 什么可以帮助我预测事情的结果或其真伪？\\n- 为了达到目标，什么事情必须发生？必要事件发生以及发生在我身上的可能性分别有多高？如果我逆向而行，会有什么后果？\\n- 什么不确定因素会极大影响到结果？重复出现的效应或者复杂因素会产生什么不希望的结果？\\n- 错误的选择和正确的选择分别会产生什么后果？\\n- 我是否从不同的角度全面考虑了整个系统？我有否考虑过其社会、财务、生理和情感上的结果？别人可能会怎么做？依照我的经验和以前的行为，我会怎么应对？如果别人也照我这样做，会产生什么样的结果？\\n\\n**Bias**\\n\\n**偏见**\\n\\n- 个人兴趣或者心理原因带来的会导致我产生误判的偏见，有何原因可以解释吗？\\n- 我所做出的结论或者选择的事实中，是否存在偏见？事实判断和价值判断分别是什么？ — 他有多可靠？他有足够的能力做出判断吗？如何加以证明？他做此判断的目的是什么？他有没有撒谎的动机？他如何判断正误？\\n\\n**The hypothesis**\\n\\n**假设**\\n\\n- 假设需要基于我想要达成的目标，并用此假设去检验我关于结果的预估。\\n- 对每个替代方案都要问：这个替代方案可能达成我的目标吗？对每个观点都要问：这个想法是对的吗？\\n- 我如何检验某个表述的真伪？在证实之前，我能否先证伪？\\n- 要检验这一表述，我需要知道些什么？首先，我必须知道什么公式能够让我的表述成立，然后我才能知道，对于判断未来的真正结果，什么才是最重要的。其次，我要知道能够支撑和推翻这一表述成立的证据。哪些表述是需要论证的？\\n- 最简单的假设是什么？\\n\\n**Look for evidence and judge the evidence寻找依据，判断依据**\\n\\n- （促发目标、非目标和意见的）主要成因出现的可能性有多高？\\n- 对判断做出真伪评判时，去寻找其意义、动力、成因、后果和正反依据\\n- 如果判断是正确的，那么后果将意味着什么？后果会不会超乎逻辑或者不可思议？其中有任何可预测之处吗？\\n- 我怎样并且从哪里才能找到支持某一判断的代表性证据？已知的有哪些？哪些东西是毋庸置疑的？若反复加以检验或用其他方式的考量，会出现相同的结果吗？我能否对结果进行检验？这些依据都是基于已知因素吗？我是否正确理解了各项数据？依据在哪里？反面依据呢？我认可依据的理由是什么？这一依据的权重是多少？What is the quality of the evidence？可信度有多高？是否紧密依赖于外部环境？样本是否太少？结论跟手中的依据相符吗？有没有违反科学法则或自然法则？\\n- 我有什么代表性的信息？对其加以观察会出现发生什么？我能够通过实验来证实我的猜测吗？\\n- 对于将会发生什么（可行还是不可行），有没有相关的过往纪录（案例评估、变异性、平均率、随机程度、自身经历、环境、伙伴与对手，以及其他相关的因素）？有什么理由相信这些纪录对未来会发生的事情并不具有代表性？什么能让未来与以往大不相同？什么是恒在的，什么不是？\\n- 这能持续多久？现在的主要成因是什么？什么外力能让其持续、能带来改变、或者造成阻碍，为什么？可能性有多高？\\n- 如果我拿到了能够推翻我之前信念的依据，我必须自问：为什么会这样？现在是什么情况？我拿到的是什么样的依据？我接受这一依据的理由是什么？\\n\\n**Disprove my (or others) conclusion by thinking like a prosecutor**\\n\\n**像检察官一样对我（或其他人）的结论提出反驳**\\n\\n- 思考会导致误判的原因\\n- 如何检验和证明我的结论是错误的？我可能犯错的理由是什么？从哪里可以找到证明我错误的依据？这个依据可信吗？有没有什么事实和依据与我的结论/观点不符？\\n- 我做的分析基于哪些主要假设？是基于真实情况吗？假设的结果符合逻辑吗？有人证明过我的假设是正确的吗？如果我的想法和假设是错误的，结果会是什么？\\n- 我有否忽视了什么？有没有更好的选择？我是否忽视了某些依据？当有人力介入时，我是否考虑到了其局限性？什么因素是不确定的，为什么？我是否只考虑到了目前的趋势？我有否误解了什么？我使用了正确的定义吗？我是否综合考虑了所有相关的因素？我采用了合适的衡量标准吗？我有否混淆了成因和相关性？如果我的目标是基于某个我认为正确但其实是错误的理念，会如何？其中会有随机性的或者系统性的错误吗？对于我所得到的结果，有没有其他原因可以解释？我有否考虑过，整个系统或者某些互动环节的可能会出现我不希望发生的变化？\\n- 我的想法是否存在偏见？在做出一个极具智慧的决定时，我的自我是否过于膨胀？我真的会创造历史纪录吗？我有否看到可能产生的反作用？\\n- 我没看到的是什么？其重要之处是什么？如果逆转我的假设，会否得到极度不合逻辑的结果？这个可能性是不是更高？有没有反例？什么依据可以证明我是错的（或者证明我无法达成目标）？实验（或者经验、观察）得出的证据中，有哪些是错误的？有更多支持性的证据吗？这些错误是如何导致的？\\n- 意义何在？我能否向人们证明正确假设所得到的结果是不可能出现的？如果我用数学的方式准确描述出来，其隐含的影响是什么？相反的方向是不是更有可能？如果是，那么现在的想法就是错误的。\\n- 负面影响表现在什么地方？\\n- 我会因何受到伤害？什么可能向错误的方向发展？什么会让事情走偏？如果这样，结果如何？\\n- 事情出错的频率如何？会否有意料之外的因素？什么事情发生后会极大改变整体结果？\\n- 可能发生的最坏的境况是什么？发生的可能性有多大？如果不幸发生了，我该怎么做？如果事情继续恶化，后果会是什么？这个后果的后果又会是什么？\\n- 如果我受到多种外力的阻碍，结果会如何？哪种有效效应是危害最低的？\\n- 执行中会面临什么风险？\\n- 我最不希望出现的是什么？我最不确定的是什么？\\n- 一个看上去是优势的因素有没有可能让我得到不希望的结果？我会怎样失去某个优势？\\n- 怎样构建系统才能将负面影响降到最低？有修正办法吗？发生了意想不到的事情，我有没有备选方案？我能加以修正吗？设定什么样的规则可以帮助我达成目标而规避不希望的结果？有没有内在的安全隐患？\\n\\n**What are the consequences if I am wrong?**\\n\\n**如果我错了，后果会怎样？**\\n\\n- 我把赌注押在哪些关键要素上？我是否拿对我重要的东西去冒险，换取的有可能是对我效用相对较低的东西？\\n- 与现有的次优机会相比，我的正确决定所带来的益处和价值是什么，错误决定的成本（金钱、时间、精神压力等）是多少？\\n- 我这样做是因为我坚信其结果能最好地实现我的利益；或者我相信能符合我利益，但后来证明我错了；或者它根本就不符合我的利益。上述三个可能性给我的目标带来的短期和长期的后果会是什么（实际损失和机会成本）？我能否加以应对和/或还原？\\n- 我不这样做是因为我坚信其结果不能最好地实现我的利益；或者我相信不符合我利益，但后来证明我错了；或者它根本就符合我的利益。上述三个可能性给我的目标带来的短期和长期的后果会是什么（实际损失和机会成本）？我能否加以应对和/或还原？\\n- 如果我因为认为不必要而此刻不采取任何行动，但时候证明我错了，这给我的目标带来的短期和长期的后果是什么？我能否加以应对和/或还原？\\n\\n**What is the value?**\\n\\n**价值是什么？**\\n\\n- 对我来说，每一个替代方案的实际效用和优势是什么？哪个方案最有利于我达到目标？它是否真的比其他选择更有吸引力？\\n- 我用什么标准来判断替代方案之间的优劣？\\n- 通过对每个替代方案的特点进行打分，我最看好的是哪一个？\\n- 这个选择能不能让我脱颖而出？能不能造成一定的影响？我是否愿意接受某种特定的结果？\\n\\n**What yardstick can be used to measure progress or to measure things against?**\\n\\n**采用什么标杆来衡量事情的进展？**\\n\\n- 我采用了哪些标杆？用作决策依据的标杆是哪些？\\n- 我怎样才能容易地评估我向目标推进的程度？有哪些指标可供我对照？\\n- 我所构建的系统能否激励人们按照最有利于达成目标的方式去行动？或者，这个系统是否会阻碍目标的完成？\\n\\n**How act now?**\\n\\n**现在如何行动？**\\n\\n- 我可以执行吗？我现在必须开始采取的特定行动是什么？首先需要做的是什么？\\n- 谁做什么，什么时候做，在哪里做，为什么做，以及如何做？\\n- 我知道决定性的点（时间和效果）在哪里吗？\\n- 我是否设置了一定的控制体系和规则？为什么这些规则是合适的？如果我不设置这些规则（或者不改变我做事的方式），结果会如何？这个规则要求我必须采取哪些管理和实践的举措？要遵循这个规则，会花费多少时间？我能否决定自己如何遵循这些规则？我可以设置一个有时间限定的规则吗？这些规则在哪些地方会失效？\\n\\n**Have I made an active decision?**\\n\\n**我所做的是一个灵活的决定吗？**\\n\\n- 我是否准备好了改变决定，以适应新的信息和新的判断？\\n- 如某一特定事件发生，是否需要做出新的决定？如此问题今天就存在，我有否对其进行过评估？支持此决定的理性思考现在是否存在？有什么新的证据证明这个可能性能可以得到改变？我衡量进展的标准，是否能让我判断之后将要发生的事情？哪些事件是相关的，哪些是不相关的？我的目标会否因此发生改变（若不考虑时间长度）？\\n\\n**Post mortem or learning from mistakes**\\n\\n**死后反思还是边学习边成长？**\\n\\n- 事情进展的情况有多好或者有多不好？我有否采取什么行动？我说到做到了吗？当时我是怎么考虑的？初衷和现实的出入在哪里？\\n- 为什么我会犯错？犯错的过程是什么？在哪里犯错了？机会成本有多大？\\n- 我如何判断现状是否会照此继续下去？我对错误有没有采取行动？如何才能不重演错误？我该做却未做的是什么？我应该把精力集中在哪里？我必须提高和学习的地方在哪？\\n\\n**What exactly is the problem?**\\n\\n**问题的本质在哪？**\\n\\n- 我想达到的是什么？为什么我没有达成目标？发生了什么？怎么发生的？在哪里发生的？什么时候发生的？谁被影响了？\\n- 我的目标因何而达成？能够促使我达到目标的因素会受到什么干扰？这些因素是标还是本？对我达成目标构成限制的因素中最重要的是哪一个？我把目标建立在什么原则和假设上？如果这些原则和假设有误的话，结果会如何？假若不存在任何限制，最好的行为链是什么？其他可能的结果是什么？\\n\\n**WHAT ARE THE LIKELY CONSEQUENCES CONSIDERING HUMAN BEHAVIOR?**\\n\\n**人类行为纳入考量后的结果**\\n\\n**What is causing me to do this?**\\n\\n**什么促使我这样去做？**\\n\\n目前我所处的环境和我的心理状态如何？如果避免了痛苦，我能获得什么好处？我如何判定什么是结果？它们让我难受还是愉悦？哪种心理趋向会影响我？这些因素会导致我做出误判吗？\\n\\n**What is the context?**\\n\\n**做决策的语境是什么？**\\n\\n环境和参与者（包括其规模）是什么状况？谁是决策者，他做决策的标准是什么？谁获益，谁买单？谁为结果负责？参与者对现实结果的看法会受什么影响？\\n\\n**Can I judge him?**\\n\\n**我能对他做出正确的判断吗？**\\n\\n我能判断他的角色是什么吗?他的经历如何？哪些临时性或者永久性的特征在影响着他（如年龄、文化背景、健康情况或者心情）？什么环境（内部或者外部的）或者处境会影响他？他是否意图向我出售什么？\\n\\n**What is in his\\xa0self-interest\\xa0to do?**\\n\\n**他的个人利益在哪里？**\\n\\n什么会符合他的逻辑？他如果避免痛苦，可以获得什么益处？他将什么视为痛苦？他害怕什么，为什么？他想多得到一些什么，不想失去什么？什么“资源”会带给他动力？是他的健康、工作、家庭、职衔、声名、地位还是权力？什么会激发他，什么会打击他？什么会（被他视作）是对他的惩罚？他可以如何被评估？他怎样看待“非目标”的后果？对某事的相信（或者不相信）是否能给他带来优势/利益？\\n\\n**What are the psychological tendencies and shortcuts that influence him and can cause misjudgment?**\\n\\n**什么心理取向或缺陷会导致他做出误判？**\\n\\n什么偏见会影响他得出的结论？有什么外在原因可能会影响他？哪些诱惑会符合他的个人利益？什么会激发他去行动？\\n\\n**What are the consequences?**\\n\\n**结果是什么？**\\n\\n我最终的结果是什么？能达到我的目标吗？有利于他的东西是否也有利于我？我们建立的系统是否能让相关参与者的利益与我的目标相一致？他的错误决定是否会由这个系统来买单？他是否知道他的行为的结果？对他来说，短期或长期的结果是什么？责任链是什么？他对结果是否负有责任？如果换一个人也做同样的事情，会如何？\\n\\n**What system would I like to have if the roles were reversed?**\\n\\n**如果交换角色，我希望构建什么样的体系？**\\n\\n如果将我的角色调换，我希望怎样被对待？什么会促使我去做我希望他做的事情？我能通过什么行为取向来影响他的行为？如果我执意要达成“非目标”，我需要怎么做？把角色转换回来后，我能够避免以上事情发生吗？\\n\\n**Is this the right system?**\\n\\n**系统正确否？**\\n\\n我可以满足他的个人利益吗？我能否消除他对失去声名、金钱、地位，以及家庭的恐惧？我可以改变他对痛苦的看法吗？怎样架构体系才能使某些影响最小化？我有否告诉过他我的期望是什么？我有否检查过已经完成的事情？对于成功完成的事情，我是否给出了鼓励与支持？他掌握必需的技能、知识和相关的信息吗？他知道自己肩负的期望吗？他是否明白无误地知道目标是什么，如何达到，以及为何这是最优途径？他会评估自己的进度吗？这与他的日常行为有关吗？他是否负有责任并获得了授权？他所能得到的奖励是否和目标一致？我可以设定什么样的规则来应对人类共有的局限性？设置一个相反的规定会怎样？哪些改变是必须发生的？谁对此负有责任？发生改变的可能性会有多大？他的价值观是什么？他的目标？他会将什么视为结果？如果他如我们所希望的那样去做，他会如何看待结果？如果他不照我们希望的那样去做呢？\\n\\n**BUSINESS EVALUATION**\\n\\n**业务评估**\\n\\n**Filter 1 - Can I understand the business - predictability?**\\n\\n**过滤器1：我能理解业务吗——可预测性**\\n\\n- 需求的原因——我有多确定（并且能解释为何如此确定）人们将来仍会继续购买这类产品或服务？过去的情况是什么，未来可能发生什么？需求是否呈周期性？生产能力与需求的对比是什么样子？\\n- 回报能力——产业和公司的回报能力，以及其过去10年的发展状况是什么？\\n- 产业结构——竞争者的数字和规模？谁在该产业中拥有发言权？要在该产业中获利，什么因素是必须的？公司在产业中的地位如何？我是否知道谁会在这个市场上获利，为什么？\\n- 真正的消费者——谁对购买行为有决定权？其决定的标准是什么？\\n\\n**Filter 2 - Does it look like the business has some kind of sustainable competitive advantage?**\\n\\n**过滤器2：此业务是否有足够的竞争优势？**\\n\\n- 竞争优势——我有多确定（并且能解释为何如此确定）别人会购买我公司的产品或服务而不是其他人的？其中的原因是否10年来几无变化？在下一个10年会不会改变？\\n- 价值——我们的优势能有多强大和可持续？这些优势在若干年后会更强大更具持久性吗？什么会破坏或减少这些优势？市场进入壁垒？品牌忠诚度？受需求或价格变化的影响程度？是否容易复制？产品生命周期是否很长？客户改变供货商的成本和动机为何？每年能够抵御机竞争的价格差异（Annual cost differential against competition）？需要多大的资本投入？议价能力如何？产品过时的风险？客户新的替代选择是什么？购买习惯或购买力会有何改变？若成本结构相同，竞争对手会有多大的降价空间？需要做什么才能保持稳固的竞争优势？还有成长空间吗？市场对此产品的需求有上升的可能吗？定价能力如何？\\n- 盈利能力。竞争优势能否转化成利润，并且如何才能做到？公司怎样盈利？要实现收入增长，需要多少资本？财务特征：资本回报率（营业利润率和投入产出比）、毛利率、销售额增长、成本/资本结构及其使用效率？正常情况下的现金流是多少？有无规模优势？有无决定性的因素？\\n- 财政特征。资本回报（操作成本的富余和资本转化），毛利润，销售增长，成本和资本结构的效用率？正常的现金流？规模优势？\\n\\n**Filter 3 - Able and honest management?**\\n\\n**过滤器3：能干且诚信的管理层？**\\n\\n组成管理团队的，是能力出众、诚实可信，并且理解和全力去创造价值的人吗？\\n\\n**Filter 4 - Is the price right?**\\n\\n**过滤器4：价格正确否？**\\n\\n我能够以比其他选择有更好回报的价格买下这个产品么？需要有事实和数据为依据。\\n\\n**Filter 5 – Disprove**\\n\\n**过滤器5：反证**\\n\\n生意会怎样被毁掉？如果公司要彻底将一个竞争对手置于死地，这个对手会是谁？为什么？如果公司继续运营下去，5年之后谁会是竞争对手，为什么？公司业务抵抗不利因素的能力如何？如果公司花光了所有的股权投资，它还会不会有价值？会否出现某人获得大量资金和人才，在竞争中胜过公司？如果竞争对手并不在乎回报，他可以对公司产生多大的破坏？公司对经济衰退的敏感程度如何？执行时所面临的风险有多大？新技术会有益还是有害？\\n\\n**Filter 6 - What are the consequences if I'm wrong?**\\n\\n**过滤器6：如果我错了，结果会如何？**\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2463bcfd-6f05-4f12-9753-060af31411e0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n杂 59\\n\\n**Porn Addiction is Being Severely Underestimated**\\n\\nHow do I know this?\\n\\nBecause most people in the community believe that in order to get rid of this addiction, all they have to do is keep trying over and over again, until eventually things will just \\'click\\' and their brains will finally become rebooted.\\n\\nVery few are treating this as a\\xa0**true**\\xa0addiction. They just see it as a habit they want to break.\\n\\nThis is evidenced by the stubborness of many, relying purely on willpower for months, only to constantly reset their counters and beat themselves up for not making any progress.\\n\\nMost people don\\'t realize how\\xa0**incredibly difficult**\\xa0it is to completely remove artificial stimulation (of any kind) for the rest of their lives. We\\'re talking about years and years of brain conditioning here.\\n\\nMany of us here have been in this community since 2010 and we\\'re still struggling in one way or another. That is almost 4 years of trying to quit for good. 4 years of trying to get to 100 days or whatever. 4 years of wanting to be the next GABE.\\n\\nWe\\'re dealing with some powerful stuff here, but it is not treated seriously enough, probably because it\\'s widely accepted by society and is not a substance like heroine or cocaine.\\n\\nI cringe when people relapse, reset their counters, and proclaim \"*This is it, I\\'ve had enough, I\\'m going to do it this time*\"...\\n\\nStop kidding yourself.\\n\\nThis is an addiction that has to be attacked from\\xa0**many**\\xa0different angles. You need a full arsenal of tools and strategies, as well as a proper mindset.\\n\\nWillpower alone won\\'t do shit.\\n\\n**Abstinence is\\xa0NOT\\xa0Recovery**\\n\\nWhat people usually try to do is go as many days clean as they can.\\n\\nThat\\'s all they do.\\n\\nThat\\'s all their goal.\\n\\nThey achieve a certain amount of days, then for whatever reason they relapse, so they start over and repeat.\\n\\nThat is abstaining. That is not recovering.\\n\\nIt is extremely common for people to achieve a certain milestone, such as 30, 90, or 100 days, relapse a few days later, and then find themselves unable to get momentum again. They go back to the beginning and they feel like they lost all their progress from their run.\\n\\nThere is a constant frustration for lack of progress. People are feeling overwhelmed and discouraged, trying the same thing over and over again without success.\\n\\nThis is because very few are addressing the real roots of their problems. Very few.\\n\\nEveryone is focused on how many days they have managed and if their symptoms are either present or gone. They judge their progress by measuring dick hardness, spontaneous erections and morning woods.\\n\\nThey are \"trying to quit porn\" so that they can \"get rid of their ED\".\\n\\nSo they abstain for as long as they can, hoping that this can cure their symptoms.\\n\\n**Completely wrong approach.**\\n\\nIf they don\\'t see ED improvements, they get discouraged.\\n\\nIf they see ED improvements, then maybe a porn session or two won\\'t hurt, right?\\n\\nIf there is no woman around, they justify watching a couple of times. After all, they are not having sex anytime soon, so whats the point?\\n\\nThey delay dating until their ED is cured or they have managed to go 100 days. But they never achieve this in the first place precisely because of this incorrect mentality.\\n\\nThe same applies to other symptoms such as social anxiety, energy levels, motivation, etc.\\n\\nThey try to quit porn, so that the symptoms can go away, and so they can finally live life.\\n\\nPeople are focusing on the wrong things.\\n\\nThey are not changing the way they think.\\n\\nThey are not changing the way they live.\\n\\nThey are not changing the way they view sex and women.\\n\\nThey are just trying not to masturbate, while everything else\\xa0**remains the same**.\\n\\nThat, my friends, is abstinence, not recovery.\\n\\n**The Foundation of a Proper Reboot**\\n\\nPorn addiction is not the cause of your shitty life.\\n\\nRead that again.\\n\\nOf course, it\\'s difficult to improve your life when you\\'re having intense porn sessions every single day that drain your energy and make you a zombie. But porn is not the reason your life sucks.\\n\\nPlease, this is very important to understand, you have to stop blaming porn for your problems.\\n\\nThis mentality of \"life awaits me after recovery\" is\\xa0**destructive**.\\n\\nPorn is not the reason you\\'re a procrastinator. Porn is not the reason you\\'re depressed. Porn is not the reason you\\'re lonely. Porn is not the reason you haven\\'t been able to lose weight or gain muscle.\\n\\n**Porn is the symptom.**\\n\\nYou watch porn to escape reality. You watch porn to manage your emotions. You watch porn because you\\'re bored, lonely, stressed, depressed, angry, isolated. You watch porn to feel good for a moment, to replace uncomfortable emotions and situations in your life.\\n\\nHere\\'s how you get rid of this addiction:\\n\\n**You don\\'t focus on quitting porn so you can finally get to live life after you\\'re recovered.**\\n\\n**You focus on learning how to live, how to manage your emotions, how to change the way you think and view the world.**\\n\\n**You put all your energy into building the life you want.**\\n\\n**This will naturally lead your mind away from porn.**\\n\\nSuccess is not measured by how many clean days you\\'ve managed.\\n\\nIt\\'s measured by how much your life has improved since you started rebooting.\\n\\nThis is what you need to do (credit to\\xa0RecoveryNation):\\n\\n**Step #1**: Write a life vision for yourself\\n\\nHow do you envision your life a few weeks, months, or years from now?\\n\\nSpend a whole day (or week) thinking about this.\\n\\nDon\\'t say \"*I don\\'t know what to do with my life*\".\\n\\nAre you telling me you have\\xa0**no clue**\\xa0what you want in any of the following areas: study, work, family, friends, hobbies, health, etc?\\n\\nEven if you\\'re not sure, you need to give your life some\\xa0direction.\\n\\nThis is by far the most important part of recovering from pornography addiction.\\n\\nWrite like crazy. Write many pages if you want. Make the biggest post you\\'ve ever done in your journal talking about how you envision your future life.\\n\\n**This life vision will be the foundation of your reboot.**\\n\\n**This is what you will focus on 100% from now on.**\\n\\nClose your eyes. Visualize it. Write it down.\\n\\n**If you don\\'t know what you want in life, then this is actually a more serious issue than porn addiction itself.**\\n\\nLike I said, spend a whole week if you need to.\\n\\nBrainstorm.\\n\\nAsk for advice.\\n\\nTake a notebook and go to a park.\\n\\nInspire yourself.\\n\\nThis is the beginning of your recovery.\\n\\nTake it seriously.\\n\\n**Step #2**: Give urgency to your life vision\\n\\nOk, now you know what you want in life. Even if you\\'re still unsure in some areas, such as not knowing what to study, that\\'s ok. At least you can give your life some direction for the moment. This is very important. You need to give your life direction. You need to move towards something.\\n\\nHere\\'s the problem. Many of us know what we want, but we keep delaying it. We\\'re experts at delaying goals. We wait until New Years, or the beginning of a month, or until circumstances get better.\\n\\nSo this is what you\\'re going to do now:\\n\\nYou\\'re going to give\\xa0**urgency**\\xa0to your life vision.\\n\\nWrite down why you ABSOLUTELY MUST start working on it right now.\\n\\nMake another huge post or journal entry about it.\\n\\nLet\\'s suppose you\\'re 27 and you have no job, no car, still live with your parents, and spend most of the day playing video games. Why in the world would you wait more time before starting to do something about it? This is urgent bro. You\\'re fucking 27!\\n\\nOr maybe you\\'ve never had a girlfriend in your life before. Well, what are you waiting for? Go buy some nice clothes, start going out more frequently, make mistakes, get rejected, ask women on dates. Start getting some experience NOW.\\n\\nYou have back pain? Start working on it. Don\\'t wait. The more you wait the worse it gets. Start doing yoga or swimming. Move your hips and back constantly every day.\\n\\nWrite down reasons why you must start pursuing your life vision right now.\\n\\nYou have to stop living like this.\\n\\n**This is urgent.**\\n\\n**This is high priority.**\\n\\nWe must convince ourselves that change is imminent.\\n\\nIt\\'s very important.\\n\\n**A life vision is no good if you have no urgency**.\\n\\nYou\\'ll just keep delaying it. Waiting for circumstances to improve. Waiting for motivation to arrive. Waiting for the beginning of new year.\\n\\nCreate urgency.\\n\\n**Step #3**: Develop an indestructible belief in yourself\\n\\nOne of the main reasons we quit goals is because deep inside we don\\'t believe we\\'re actually able to do it.\\n\\nWhen successful people like Arnold Schwarzenegger decide they want to achieve something, they become completely\\xa0*obsessed*\\xa0about it. They have an indestructible belief that they will achieve it.\\n\\nThey are not affected by circumstances. They create results in their head before they even get them.\\n\\nThis is what you have to do if you want to accomplish anything.\\n\\nFor example, let\\'s say you want to learn how to play guitar. And you have the urgency to do it, because you know it takes time, so the sooner you start the better. You have to start now.\\n\\nHowever, after a few days of learning the basics, you start losing motivation and becoming discouraged. You realize that playing guitar is not easy at all. You feel overwhelmed by how much practice you need to put into it. You start doubting yourself and thinking \"*There\\'s no way I\\'ll ever become a great guitar player and form my own band*\". Friends tell you things like \"*Dude, you should\\'ve started years ago. All great guitarists started when they were young*\".\\n\\nSo you quit.\\n\\nThis is a result of a weak belief in yourself. You don\\'t believe you have the potential to become a good guitarist. Which is obviously completely false. We as humans have unlimited potential.\\n\\nArnold Schwarzenegger doesn\\'t think like this.\\n\\nLook at what he said:\\n\\n***How many times have you heard \\'You can\\'t do this\\', \\'You can\\'t do that\\', \\'It\\'s never been done before\\'. I love it when someone says \\' No one has ever done this before\\', because when I do it, that means I\\'m the first person that\\'s ever done it!***\\n\\nThis is how we should think when we set up to do anything in life.\\n\\nUncertainty is what kills people. Not knowing if they\\'re able to achieve it.\\n\\nWe need to brainwash ourselves every day into believing that we\\xa0**WILL**\\xa0do it\\xa0**NO MATTER WHAT**.\\n\\n**Step #4**: Read \"The Slight Edge\"\\n\\nThis is by far one of the most helpful self-improvement books I\\'ve ever read.\\n\\nAnd trust me, I\\'ve read quite a few.\\n\\nHuge paradigm shift, huge.\\n\\nJust get it.\\n\\nHere\\'s the link:\\n\\nThe Slight Edge: Turning Simple Disciplines into Massive Success and Happiness\\n\\nJUST GET IT!!!!!\\n\\nThe main idea from that book can be applied to quitting porn addiction as well. Basically it can be applied to anything in life.\\n\\nI highly recommend it.\\n\\nHIGHLY.\\n\\nAll of these steps are equally important.\\n\\nDo not skip them.\\n\\n**They are the foundation of your reboot.**\\n\\nThey make rebooting so much easier. Your mind will be completely focused on what you want in life. You will be fixing the root of all your problems.\\n\\n**The secret of change is to focus all your energy not fighting the old, but on building the new.**\\n\\nStop making posts complaining about your shitty life. Stop making posts saying how you\\'re sick of being addicted to porn. Stop talking about porn altogether.\\n\\nInstead, transform your journal into a self-improvement journal, focused 100% on moving towards the life you want.\\n\\n\"Forget\" about porn.\\n\\nThis is basic rebooting stuff, yet many people are constantly breaking this rule. They write about porn cravings, morning woods, spontaneous erections, what day they\\'re on, how much they struggled to abstain, how they can\\'t wait to reach 90 days, etc.\\n\\n**When you consistently focus 100% on building the life you want, your mind will naturally move away from porn. You will also lessen the void left by quitting porn, which is very real.**\\n\\nMany people quit porn only to find themselves in this life emptiness that is very hard to handle. Then they go back to porn precisely because this void is too much for them.\\n\\nFocusing on your life vision is a superior rebooting approach.\\n\\nRelapses aren\\'t that discouraging if you\\'re actually improving your life. Ironically, you will notice that the more you focus on what you want, the less frequently you will relapse.\\n\\nIt\\'s important that you think in terms of life vision and pursuing your dreams, not in terms of \"*I have to get busy and fill my life with activities so that I don\\'t watch porn*\". This is something you\\'re doing for yourself.\\n\\nStop ranting about porn.\\n\\nThis journey is about your\\xa0**LIFE**.\\n\\nFocus on that and the porn will go away.\\n\\n**Managing Your Emotional Life**\\n\\nOk, let\\'s keep going.\\n\\nThis is\\xa0**mistake #1**\\xa0from my post\\xa0The TOP 3 Fatal Mistakes Rebooters Make.\\n\\nIf you haven\\'t read it yet, I highly suggest you do it after you finish reading this thread.\\n\\nI want to talk about it again because it really is important.\\n\\nPorn addiction is much more than just getting cravings and relapsing.\\n\\nOne of the reasons we become addicted is because of our inability to manage our emotional lives.\\n\\nYou have to remind yourself that quitting porn is about growing up and becoming a much more mature person.\\n\\nIt\\'s much more than \"*I want to quit porn so I can cure ED and have plenty of sex with women!*\".\\n\\n**Much more than that.**\\n\\nWe\\'ve been using porn for years as a method of handling our emotions.\\n\\nWe need to stop hiding away from uncomfortable life situations. We need to stop using porn in order to escape from reality.\\n\\nWe must learn how to handle life and emotions without the need of porn.\\n\\nI\\'m going to quote\\xa0Recovery Nation\\xa0here:\\n\\n\"*The second common trap that people fall into when transitioning from compulsions to recovery (or from any emotionally intense behavior to another) is their perception involving the emptiness phase of a healthy transition. To understand this, let\\'s take a brief look at the broader addictive process in a person\\'s life. In most addictions, the person has come to depend on their addictive behavior to manage their emotional state. The longer this person relies on such patterns, the more intense and ingrained this pattern becomes. Now, this is an extremely brief synopsis, with many additional issues to be discussed later in the workshop, but the point is: without the ingrained addiction, they are left with an emotional void that is very real. And very uncomfortable. The trap is in seeing this void as proof that their addiction was a natural, necessary entity in their life. They begin to feel an emotional emptiness...no urges...no pleasure...no anything. And they assume that something is wrong. That they need their addiction in order to feel normal. And here comes the porn, or the masturbation, or the affairs. And then, right on cue...here comes the excitement and pleasure and passion. Along with the guilt and shame and depression. But it doesn\\'t matter. They would rather feel all of the emotions, than to feel nothing at all. And so, relapse occurs.*\\n\\n*I remember thinking many times throughout my own struggles that I would rather experience the highest of highs and the lowest of lows than to ever take a medication that would dim my emotions. I never feared feeling bad. I never feared the chaos that was my life. Not the misery, nor the pain. I cherished my emotional extremes as I believed that it was my ability to experience such extremes that made me who I was. My only fear was to feel nothing at all. This is common with many people who struggle with addictive behavior. Even those who state that they drink or use drugs or otherwise act out in an effort to \"numb the pain\" of past abuse, overwhelming stress, etc., are not completely accurate. They drink, use or otherwise act out to shift the emotions that they are experiencing — not to dull them.*\\n\\n*The point to this is simple. To someone used to experiencing the extremes of the emotional experience — and suffering from true compulsive behavior is to experience emotions to their extreme — the emptiness that comes with a transitional ending can be overwhelming. The blandness, the void that is created when eliminating the behavioral patterns that managed the majority of your emotions is like removing your soul. You no longer feel \"normal\". You feel as if there is something wrong inside of you; like you are broken somehow. You might even feel that, without these compulsive behaviors, life isn\\'t even worth living. That it is these behaviors that made you special. So, inevitably, you go back to acting out because even the potential negative emotional consequences of your behavior (guilt, shame, failure, loneliness, etc.) are better than to have no emotions at all.*\"\\n\\nand\\n\\n\"*A second common motivator in recovery is the hitting of \\'rock bottom\\' or, in realistic terms, hitting the point where the pain of the addiction can no longer be numbed by the addiction itself. When the emotional pain of the addiction\\'s consequences have grown too great, the motivation to end the addiction kicks in as the addiction is no longer capable of serving its purpose. Although this is a significantly more powerful motivator than the first, it too, is ultimately doomed for failure in long-term recovery. Or, more accurately, it is doomed for a long-term recovery/relapse cycle.*\\n\\n*What happens is this: when the emotional pain becomes too great to temporarily manage with compulsive behaviors, the decision to recover provides an intense emotional boost that helps to manage that pain. The person feels good. That feeling may last for weeks, it may last for months. But eventually, inevitably, the emotional intensity that came with the commitment to recover wanes, and the person finds themself, once again, lacking the ability to manage their emotional life. A return to the addiction (or another addiction) is the only emotional management strategy that they have. This, followed by a re-commitment to recovery...followed by another relapse...followed by, well, you get the idea. The cycle will not end until the motivation for ending it has changed.*\\n\\n*Those who are motivated by a desire to end the pain of their addiction fare much better than those who are recovering for the sake of others. Such individuals can generate sustained, long-term recovery efforts. However, in order to make a true transition to health, the key will be found in their ability to move past the initial stages of recovery and begin to adopt healthy life management skills that will allow them to achieve emotional maturity.*\\n\\n*So, in preparing your road to recovery, you will need to prepare yourself for a time when you might feel empty inside. It will come after the euphoria of beginning your recovery, and it will come after you have put an end to your desire to continue your life the way that it is. This period may last a few days, it may last a few weeks. Rarely, will it ever last longer than that. And in those few weeks, your goal will be to recognize this emptiness, and begin to fill it with the values and the dreams that you believe in.*\"\\n\\nWe need to understand that one of the reasons we relapse is because\\xa0**we\\'re pussies that can\\'t handle negative emotions**.\\n\\nWe use porn as medication. We use porn to hide away from life. We use porn to temporarily relief anxiety, stress, loneliness, boredom, anger, etc.\\n\\nOnce again, if you haven\\'t read\\xa0The TOP 3 Fatal Mistakes Rebooters Make, you should read it later on.\\n\\nLearn how to manage your emotions without using porn and you will be achieving\\xa0**long term**\\xa0success.\\n\\nEmbrace all your emotions, negative or positive.\\n\\nBy the way,\\xa0Recovery Nation\\xa0is fucking awesome.\\n\\nI highly recommend you check it out.\\n\\n**Readjusting Your Sexual Expectations**\\n\\nThis is by far one of the most difficult things to do.\\n\\nWhen you quit porn, you\\'re not just saying good bye to artificial stimulation.\\n\\nYou\\'re leaving behind the world of \"never ending flow of hot chicks with big tits and round asses\".\\n\\nReal life is\\xa0**nothing**\\xa0like that.\\n\\nWe\\'ve been spoiled by porn. We believe that we should be out there having lots of sex with different women. We believe that this is the key to happiness and fulfillment.\\n\\nThe problem is that it is\\xa0**incredibly difficult and unrealistic**\\xa0to \"fuck hot chicks on a regular basis\".\\n\\nThere\\'s nothing particularly wrong with having that goal, but you have to be willing to take some MASSIVE action. You have to go through hundreds of rejections. You need to have a lot of balls. You must do what 99.9% of men are incredibly afraid to do.\\n\\nHow many people here are doing what it takes to have the kind of sexual life we all dream about?\\n\\nVery few, if any.\\n\\nThe only guy in the forum who was brave enough to do it is\\xa0**ssk08**.\\n\\nThe rest of us are living in a dream world.\\n\\nWe see these super hot chicks on tight dresses and we hope that maybe\\xa0*someday*\\xa0we might be able to have sex with them. We read books about seduction. We visit PUA forums. We watch videos on YouTube of guys approaching girls. We make theories and discuss them online.\\n\\n**But we aren\\'t doing shit about it.**\\n\\nIt\\'s all just a dream. An idea we have in our minds. Something we hope\\xa0*someday*\\xa0we will do.\\n\\nHere\\'s how most people get laid in real life:\\n\\nA guy meets a girl he finds attractive, so he asks her out. They get to know each other. Then they continue to go out and form some sort of relationship. Then after some months the relationship either becomes serious or falls apart.\\n\\nThat\\'s the real world.\\n\\nYou have to accept that, unless you\\'re willing to take massive action, you won\\'t fuck anywhere near as many women as you expect to.\\n\\nAccepting this is very difficult, but it is necessary.\\n\\nWe need to learn how to live without this world of endless hot chicks, otherwise we\\'ll become extremely disappointed and dissatisfied with real life, which is nothing like porn.\\n\\nAnyone here is more than capable of getting a girlfriend. But our girlfriends most likely won\\'t look like pornstars, nor they will act like them.\\n\\nThere\\'s a very high probability the sex won\\'t be pornographic in nature. There\\'s going to be a lot of sensuality, caressing, and also clumsiness. Some days your girl will look sexy, other days not so much. Some days she\\'ll be in the mood, other days she won\\'t. Some days you\\'ll struggle to keep it hard, other days you\\'ll cum too fast. She might be able to achieve orgasms, or she might not. You might do it every other day, or maybe only 3-4 times per month.\\n\\nRemember, pornstars are paid thousands of dollars to do what they\\'re told and fulfill all your fantasies.\\n\\nYou must stop living in dream land.\\n\\nI know this is very difficult to accept,\\xa0**but we have to give meaning to our lives outside fucking hot chicks**.\\n\\nOur happiness cannot depend on that. Otherwise you\\'ll keep coming back to porn every time you fail to get laid in real life. You will remain attached to \"sex with hot chicks\" for the rest of your life.\\n\\nOne of the reasons GABE is such an incredible successful rebooter is because he had a\\xa0**deep change in heart**. By this I mean that his approach is completely based on\\xa0**love**\\xa0and not lust. He views sex in terms of intimacy and connecting with another person. Watching porn doesn\\'t even cross his mind anymore.\\n\\nNow, I\\'m not saying you have to think exactly like him. But you should definitely change the way you view sex and women, because I can assure you\\xa0**it has been completely distorted by porn**.\\n\\nBy the way, I don\\'t believe there\\'s anything wrong about pursuing casual sex instead of a relationship, just make sure you keep both feet on the ground.\\n\\nI would also like to add that relationships are about sharing your life with another person. I know many people here want a girlfriend so that they can finally start getting laid, but relationships go much more deeper than that.\\n\\nIf you\\'ve never had a girlfriend before you\\'ll know once you get one.\\n\\n**Thinking About Sex is\\xa0USELESS**\\n\\nWhat\\'s the point of fantasizing?\\n\\nIt accomplishes nothing.\\n\\n**It slows down the reboot, increases the urge to masturbate, and reinforces neurological pathways related to porn.**\\n\\nIt\\'s a meaningless activity that should be eliminated.\\n\\n**It keeps your mind focused on sex, tits, asses, fucking, when it should be shifted towards other activities in life.**\\n\\nIf you find yourself thinking about sex, you should mindfully and calmly redirect your attention to something else.\\n\\nYou want sex?\\n\\nGreat.\\n\\n**Then do something to actually make it happen.**\\n\\nFantasizing by itself serves no purpose at all.\\n\\nYou need to understand that if you want to abstain from orgasm and masturbation, you cannot be thinking about sex and women, because this will inevitably cause you to relapse. Trying to abstain while at the same time fantasizing or peeking at pictures of chicks will only lead to frustration.\\n\\nStay away from\\xa0**any**\\xa0kind of artificial stimulation. Don\\'t take peeks. Don\\'t browse pictures of girls online. Don\\'t type pornstar names on Google image search. Don\\'t read escort forums.\\n\\nDo not arouse yourself.\\n\\n**Basically you have to adopt a philosophy of \"I\\'m either trying to get laid (approaching, texting girls, going out on dates, flirting with women, hanging out with friends, getting rejected) or doing something completely unrelated to sex (work, studying, exercise, fun, reading, playing an instrument, chores, housework, watching movies)\".**\\n\\n**There is no grey area where you are alone thinking about sex or checking out girls online. This accomplishes nothing. It serves no real purpose. It will only increase urges, lead to relapse, and make you frustrated.**\\n\\nAs soon as erotic thoughts pop up in your mind, you should calmly ignore them and refocus your attention to something else. You keep practicing this forever until you master it.\\n\\nYou have to attack this addiction right from the root. Trying to abstain from hardcore porn accomplishes nothing if you\\'re still constantly fantasizing and peeking.\\n\\nIf you keep strengthening the mindset I talked about above, you will be making meaningful progress.\\n\\nThis used to be called \"Monk Mode\", but I don\\'t like that name because it implies that you\\'re going to become celibate.\\n\\n**This isn\\'t about becoming celibate. This is about doing what it takes if you want to get laid, instead of wasting mental energy on sexual thoughts that will only improve the chances of relapsing.**\\n\\nIf you ever want to achieve a long streak, you can\\'t be checking out girls online, even if it\\'s just some bikini pictures. You can\\'t be fantasizing when you wake up in the morning. You can\\'t be taking 5 second peeks at porn.\\n\\nAs soon as you do any of those things, this huge beast called porn addiction will take control over your prefrontal cortex and it\\'s just a matter of time before you relapse.\\n\\nYou have to be extreme.\\n\\nBut don\\'t worry, it\\'s much easier than it sounds.\\n\\n**It\\'s actually harder to stop yourself from relapsing once you\\'re already thinking about sex, than it is to not think about sex in the first place.**\\n\\nHow do you not think about sex?\\n\\nSimple.\\n\\nFocus your mind 100% on your life vision.\\n\\n**Every....single....day.**\\n\\nThere\\'s an interesting thread related to this created by bigbookofpenis (lol, nice username) here:\\n\\nhttp://www.yourbrainrebalanced.com/index.php?topic=14525.0\\n\\nTake a look.\\n\\nI\\xa0completely support\\xa0his \"No Arousal Method\".\\n\\nDismissing erotic thoughts as soon as they arrive in your mind is the cornerstone for preventing relapses.\\n\\nThis is basic stuff guys.\\n\\n**It\\'s Not Orgasm What You Crave**\\n\\nMany people here believe that abstaining from orgasm is the most difficult part of rebooting.\\n\\nWRONG.\\n\\nWhen you get porn cravings, your brain is not asking for orgasm. As an addict, it is begging you for your hit. It misses the high, the tits, the asses, the novelty, the rush, the unrealistic sexual scenarios, the fantasies, the super hot chicks, the perfect camera shots, the feeling of letting go and indulging in pleasure, the fucking, the cumshots, the doggystyles, the boobs bouncing around, etc.\\n\\n**If lack of orgasm was the problem, then everyone would just fap without porn (or any other artificial stimulation). There would be no relapses and everyone would have 500+ day counters.**\\n\\nThe urge to ejaculate only becomes a real problem once you start peeking, edging or fantasizing constantly. When you find yourself in a state of arousal then obviously you will want to cum.\\n\\nBut the initial urges are \"addiction urges\". They are mental. They are not a physical need for ejaculation.\\n\\nIf you feed these urges by peeking, even if it\\'s just pictures of hot babes in bikini, then they will invade your mind and rob you of your ability to concentrate or remain calm. Eventually \"autopilot\" mode will be engaged and we all know what happens next.\\n\\n**You\\'re not having urges to cum.**\\n\\n**You\\'re having urges for a \"high\" and a \"rush\".**\\n\\nRemember that.\\n\\nThis is why porn urges don\\'t go away when you get a girlfriend.\\n\\nIt is a\\xa0drug, and you need to learn how to live without it,\\xa0**regardless of whether you have a girlfriend or not**.\\n\\nWhen you abstain for several days or weeks, your sensitized pathways are anxiously waiting for any sexual cue, no matter how short or brief. This is why people mistakenly confuse real libido with porn cravings. They take a peek at porn after 15 days and they feel this intense rush and urge to cum, so they conclude that it is libido and that they must relieve pressure.\\n\\n**The problem was taking a peek in the first place. Had they just focused on more important things instead, they would\\'ve been able to finish the day clean without problem.**\\n\\nIf you manage to completely abstain from fantasizig and checking out chicks online (in any form), then going a long time without orgasm won\\'t be a problem.\\n\\nAnd don\\'t even think about testing or caressing your dick.\\n\\nFocus 100% on your life vision.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='85314c15-98f1-4caf-a4ab-e2dd2b91382e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n杂 66\\n\\n《不要在不断的优秀里走向平庸》——冲着各种Goal埋头苦干的人应该读读此文；抛开约定俗成走自己的路的人也应该读读\\n\\n万伟，没想到在和你聊完我的困惑之后，竟然马上读到这样一篇文章！此文是威廉•德莱塞维茨（William Deresiewicz）在斯坦福大学的演讲。作者真知灼见，也道出了我心中的观点。这些不太主流的观点，我一直持有，但不敢确信，因为我毕竟只是个在不断尝试和探索的年轻人，没有那种自信去执信什么。在我对于自己的道路产生困惑的时候读到这篇文章，我感觉很欣慰，受到了这位William Deresiewicz的鼓励。\\n\\n我知道有不少人，他们不迷信各种社会所谓的success，他们在顶着压力，探索自己的人生道路，寻求比金钱地位更纯净的东西，开发自己的潜力，创造自己的独一无二的人生。可是现实并不像那句“走自己的路，让别人说去吧”那么简单，这样的独行者往往承受着很大的压力。他们或多或少放弃了追求那些社会认可的东西，无论是名牌学校，高薪工作还是其他。其实，要放弃一样东西，有时比要争取一样东西更难，特别是在你有能力去争取那样东西的时候。社会把那样东西奉为至宝，但你清楚，在你心中，那样东西一文不值。在这种矛盾下，要坚持自己的价值观，是很难的。这些走自己的路的人，读读这篇文章，或许会受到鼓舞，更坚定自己的道路。\\n\\n我也知道很多人，应该说是大多数人。像蜜蜂和蚂蚁一样，忙忙碌碌，追求着很多东西。他们不断的specialized，不断的follow the flow，不断的getting into那些社会认可的东西。这些东西，看起来不同，其实是exactly the same，用William的话说，他们追求的things无外乎可用这句来形容：It\\'s prestigious, it\\'s hard to get into, it\\'s something that you and your parents can brag about, it looks good on your résumé, and most important, it represents a clearly marked path.他们至始至终很少做出自己的选择，很少真正的计划自己的人生，他们更多的是在copy别人的人生轨迹，别人的“成功的”人生轨迹。然而，一个独一无二的生命的存在的意义，难道就是copy吗？这样的copy，即使copy成功了，又算什么喃？当然，你会得到reward，金钱地位，更重要的，你会得到社会的认可，这些都极大的满足了你的自尊心和虚荣心。但是这些不能印证你的存在！极端的说，你的墓志铭上刻的不会是你的银行存款数目。希望忙碌的永远“没有时间”的奋斗者们可以静下来读读这篇很长的演讲。或许此文不会让你放弃你紧紧握在掌心的（已经握出臭汗的）那些成功学信念，但是至少，希望此文是一个提醒，或者是一种alternative的观点，一种反对的意见，可以使你对人生的意义有更深刻的理解。\\n\\n- -------建议读英文原文，有勾出启发人的句子-------\\n\\nWhat Are You Going to Do With That?\\n\\nBy William Deresiewicz\\n\\n*The essay below is adapted from a talk delivered to a freshman class at Stanford University in May.*\\n\\nThe question my title poses, of course, is the one that is classically aimed at humanities majors. What practical value could there possibly be in studying literature or art or philosophy? So you must be wondering why I\\'m bothering to raise it here, at Stanford, this renowned citadel of science and technology. What doubt can there be that the world will offer you many opportunities to use your degree?\\n\\nWe should start by talking about how you did, in fact, get here. You got here by getting very good at a certain set of skills. Your parents pushed you to excel from the time you were very young. They sent you to good schools, where the encouragement of your teachers and the example of your peers helped push you even harder. Your natural aptitudes were nurtured so that, in addition to excelling in all your subjects, you developed a number of specific interests that you cultivated with particular vigor. You did extracurricular activities, went to afterschool programs, took private lessons. You spent summers doing advanced courses at a local college or attending skill-specific camps and workshops. You worked hard, you paid attention, and you tried your very best. And so you got very good at math, or piano, or lacrosse, or, indeed, several things at once.\\n\\nNow there\\'s nothing wrong with mastering skills, with wanting to do your best and to be the best. What\\'s wrong is what the system leaves out: which is to say, everything else.\\xa0I don\\'t mean that by choosing to excel in math, say, you are failing to develop your verbal abilities to their fullest extent, or that in addition to focusing on geology, you should also focus on political science, or that while you\\'re learning the piano, you should also be working on the flute.\\xa0It is the nature of specialization, after all, to be specialized. No,\\xa0the problem with specialization is that it narrows your attention to the point where all you know about and all you want to know about, and, indeed, all you can know about, is your specialty.\\n\\nThe problem with specialization is that it makes you into a specialist.\\xa0It cuts you off, not only from everything else in the world, but also from everything else in yourself.\\xa0And of course, as college freshmen, your specialization is only just beginning. In the journey toward the success that you all hope to achieve, you have completed, by getting into Stanford, only the first of many legs. Three more years of college, three or four or five years of law school or medical school or a Ph.D. program, then residencies or postdocs or years as a junior associate. In short, an ever-narrowing funnel of specialization.\\xa0You go from being a political-science major to being a lawyer to being a corporate attorney to being a corporate attorney focusing on taxation issues in the consumer-products industry. You go from being a biochemistry major to being a doctor to being a cardiologist to being a cardiac surgeon who performs heart-valve replacements.\\n\\nAgain, there\\'s nothing wrong with being those things. It\\'s just that, as you get deeper and deeper into the funnel, into the tunnel, it becomes increasingly difficult to remember who you once were. You start to wonder what happened to that person who played piano and lacrosse and sat around with her friends having intense conversations about life and politics and all the things she was learning in her classes.\\xa0The 19-year-old who could do so many things, and was interested in so many things, has become a 40-year-old who thinks about only one thing.\\xa0That\\'s why older people are so boring. \"Hey, my dad\\'s a smart guy, but all he talks about is money and livers.\"\\n\\nAnd there\\'s another problem. Maybe you never really wanted to be a cardiac surgeon in the first place. It just kind of happened.\\xa0It\\'s easy, the way the system works, to simply go with the flow. I don\\'t mean the work is easy, but the choices are easy. Or rather, the choices sort of make themselves.\\xa0You go to a place like Stanford because that\\'s what smart kids do. You go to medical school because it\\'s prestigious. You specialize in cardiology because it\\'s lucrative. You do the things that reap the rewards, that make your parents proud, and your teachers pleased, and your friends impressed. From the time you started high school and maybe even junior high, your whole goal was to get into the best college you could, and\\xa0so now you naturally think about your life in terms of \"getting into\" whatever\\'s next. \"Getting into\" is validation; \"getting into\" is victory.\\xa0Stanford, then Johns Hopkins medical school, then a residency at the University of San Francisco, and so forth. Or Michigan Law School, or Goldman Sachs, or McKinsey, or whatever. You take it one step at a time, and the next step always seems to be inevitable.\\n\\nOr maybe you did always want to be a cardiac surgeon. You dreamed about it from the time you were 10 years old, even though you had no idea what it really meant, and you stayed on course for the entire time you were in school. You refused to be enticed from your path by that great experience you had in AP history, or that trip you took to Costa Rica the summer after your junior year in college, or that terrific feeling you got taking care of kids when you did your rotation in pediatrics during your fourth year in medical school.\\n\\nBut either way, either because you went with the flow or because you set your course very early, you wake up one day, maybe 20 years later, and you wonder what happened: how you got there, what it all means.Not what it means in the \"big picture,\" whatever that is, but what it means to you. Why you\\'re doing it, what it\\'s all for. It sounds like a cliché, this \"waking up one day,\" but it\\'s called having a midlife crisis, and it happens to people all the time.\\n\\nThere is an alternative, however, and it may be one that hasn\\'t occurred to you.\\xa0Let me try to explain it by telling you a story about one of your peers, and the alternative that hadn\\'t occurred to\\xa0*her*. A couple of years ago, I participated in a panel discussion at Harvard that dealt with some of these same matters, and afterward I was contacted by one of the students who had come to the event, a young woman who was writing her senior thesis about Harvard itself, how it instills in its students what she called\\xa0*self-efficacy*, the sense that you can do anything you want. Self-efficacy, or, in more familiar terms, self-esteem. There are some kids, she said, who get an A on a test and say, \"I got it because it was easy.\" And there are other kids, the kind with self-efficacy or self-esteem, who get an A on a test and say, \"I got it because I\\'m smart.\"\\n\\nAgain, there\\'s nothing wrong with thinking that you got an A because you\\'re smart. But what that Harvard student didn\\'t realize—and it was really quite a shock to her when I suggested it—is that there is a third alternative. True self-esteem, I proposed, means not caring whether you get an A in the first place.\\xa0True self-esteem means recognizing, despite everything that your upbringing has trained you to believe about yourself, that the grades you get—and the awards, and the test scores, and the trophies, and the acceptance letters—are not what defines who you are.\\n\\nShe also claimed, this young woman, that Harvard students take their sense of self-efficacy out into the world and become, as she put it, \"innovative.\" But when I asked her what she meant by innovative, the only example she could come up with was \"being CEO of a Fortune 500.\"\\xa0That\\'s not innovative, I told her, that\\'s just successful, and successful according to a very narrow definition of success.\\xa0True innovation means using your imagination, exercising the capacity to envision new possibilities.\\n\\nBut I\\'m not here to talk about technological innovation, I\\'m here to talk about a different kind. It\\'s not about inventing a new machine or a new drug. It\\'s about inventing your own life. Not following a path, but making your own path. The kind of imagination I\\'m talking about is moral imagination. \"Moral\" meaning not right or wrong, but having to do with making choices.\\xa0Moral imagination means the capacity to envision new ways to live your life.\\n\\nIt means not just going with the flow. It means not just \"getting into\" whatever school or program comes next. It means figuring out what you want for yourself, not what your parents want, or your peers want, or your school wants, or your society wants.\\xa0Originating your own values.\\xa0Thinking your way toward your own definition of success. Not simply accepting the life that you\\'ve been handed. Not simply accepting the choices you\\'ve been handed.\\xa0When you walk into Starbucks, you\\'re offered a choice among a latte and a macchiato and an espresso and a few other things, but you can also make another choice. You can turn around and walk out. When you walk into college, you are offered a choice among law and medicine and investment banking and consulting and a few other things, but again, you can also do something else, something that no one has thought of before.\\n\\nLet me give you another counterexample. I wrote an essay a couple of years ago that touched on some of these same points. I said, among other things, that kids at places like Yale or Stanford tend to play it safe and go for the conventional rewards. And one of the most common criticisms I got went like this: What about Teach for America? Lots of kids from elite colleges go and do TFA after they graduate, so therefore I was wrong. TFA, TFA—I heard that over and over again. And Teach for America is undoubtedly a very good thing. But to cite TFA in response to my argument is precisely to miss the point, and to miss it in a way that actually confirms what I\\'m saying. The problem with TFA—or rather, the problem with the way that TFA has become incorporated into the system—is that it\\'s just become another thing to get into.\\n\\nIn terms of its content, Teach for America is completely different from Goldman Sachs or McKinsey or Harvard Medical School or Berkeley Law, but in terms of its place within the structure of elite expectations, of elite choices, it is exactly the same.\\xa0It\\'s prestigious, it\\'s hard to get into, it\\'s something that you and your parents can brag about, it looks good on your résumé, and most important, it represents a clearly marked path.\\xa0You don\\'t have to make it up yourself, you don\\'t have to do anything but apply and do the work—just like college or law school or McKinsey or whatever. It\\'s the Stanford or Harvard of social engagement. It\\'s another hurdle, another badge. It requires aptitude and diligence, but it does not require a single ounce of moral imagination.\\n\\nMoral imagination is hard, and it\\'s hard in a completely different way than the hard things you\\'re used to doing. And not only that, it\\'s not enough.\\xa0If you\\'re going to invent your own life, if you\\'re going to be truly autonomous, you also need courage: moral courage.\\xa0The courage to act on your values in the face of what everyone\\'s going to say and do to try to make you change your mind.\\xa0Because they\\'re not going to like it.\\xa0Morally courageous individuals tend to make the people around them very uncomfortable.\\xa0They don\\'t fit in with everybody else\\'s ideas about the way the world is supposed to work, and still worse, they make them feel insecure about the choices that they themselves have made—or failed to make. People don\\'t mind being in prison as long as no one else is free. But stage a jailbreak, and everybody else freaks out.\\n\\nIn\\xa0*A Portrait of the Artist as a Young Man*, James Joyce has Stephen Dedalus famously say, about growing up in Ireland in the late 19th century, \"When the soul of a man is born in this country there are nets flung at it to hold it back from flight. You talk to me of nationality, language, religion. I shall try to fly by those nets.\"\\n\\nToday there are other nets. One of those nets is a term that I\\'ve heard again and again as I\\'ve talked with students about these things.\\xa0That term is\\xa0\"self-indulgent.\"\\xa0\"Isn\\'t it self-indulgent to try to live the life of the mind when there are so many other things I could be doing with my degree?\" \"Wouldn\\'t it be self-indulgent to pursue painting after I graduate instead of getting a real job?\"\\n\\nThese are the kinds of questions that young people find themselves being asked today if they even think about doing something a little bit different. Even worse, the kinds of questions they are made to feel compelled to ask themselves.\\xa0Many students have spoken to me, as they navigated their senior years, about the pressure they felt from their peers—from their\\xa0*peers*—to justify a creative or intellectual life. You\\'re made to feel like you\\'re crazy: crazy to forsake the sure thing, crazy to think it could work, crazy to imagine that you even have a right to try.\\n\\nThink of what we\\'ve come to. It is one of the great testaments to the intellectual—and moral, and spiritual—poverty of American society that it makes its most intelligent young people feel like they\\'re being self-indulgent if they pursue their curiosity. You are all told that you\\'re supposed to go to college, but you\\'re also told that you\\'re being \"self-indulgent\" if you actually want to get an education. Or even worse, give yourself one. As opposed to what? Going into consulting isn\\'t self-indulgent? Going into finance isn\\'t self-indulgent? Going into law, like most of the people who do, in order to make yourself rich, isn\\'t self-indulgent? It\\'s not OK to play music, or write essays, because what good does that really do anyone, but it is OK to work for a hedge fund. It\\'s selfish to pursue your passion, unless it\\'s also going to make you a lot of money, in which case it\\'s not selfish at all.\\n\\nDo you see how absurd this is?\\xa0But these are the nets that are flung at you, and this is what I mean by the need for courage. And it\\'s a never-ending process. At that Harvard event two years ago, one person said, about my assertion that college students needed to keep rethinking the decisions they\\'ve made about their lives, \"We already made our decisions, back in middle school, when we decided to be the kind of high achievers who get into Harvard.\"\\xa0And I thought, who wants to live with the decisions that they made when they were 12? Let me put that another way. Who wants to let a 12-year-old decide what they\\'re going to do for the rest of their lives? Or a 19-year-old, for that matter?\\n\\nAll you can decide is what you think now, and you need to be prepared to keep making revisions. Because let me be clear. I\\'m not trying to persuade you all to become writers or musicians. Being a doctor or a lawyer, a scientist or an engineer or an economist—these are all valid and admirable choices.\\xa0All I\\'m saying is that you need to think about it, and think about it hard. All I\\'m asking is that you make your choices for the right reasons. All I\\'m urging is that you recognize and embrace your moral freedom.\\n\\nAnd most of all, don\\'t play it safe.\\xa0Resist the seductions of the cowardly values our society has come to prize so highly: comfort, convenience, security, predictability, control. These, too, are nets.\\xa0Above all, resist the fear of failure. Yes, you will make mistakes. But they will be your mistakes, not someone else\\'s. And you will survive them, and you will know yourself better for having made them, and you will be a fuller and a stronger person.\\n\\nIt\\'s been said—and I\\'m not sure I agree with this, but it\\'s an idea that\\'s worth taking seriously—that you guys belong to a \"postemotional\" generation.\\xa0That you prefer to avoid messy and turbulent and powerful feelings. But I say, don\\'t shy away from the challenging parts of yourself. Don\\'t deny the desires and curiosities, the doubts and dissatisfactions, the joy and the darkness, that might knock you off the path that you have set for yourself.\\xa0College is just beginning for you, adulthood is just beginning. Open yourself to the possibilities they represent.\\xa0The world is much larger than you can imagine right now. Which means, you are much larger than you can imagine.\\n\\n我的题目提出的问题，当然，是一个经典的面向人文科学的专业所提出的问题：学习文学、艺术或哲学能有什么实效价值？你肯定纳闷，我为什么在以科技闻名的斯坦福提出这个问题呢？大学学位当然是给人们带来众多的机会，这还有什么需要质疑的吗？\\n\\n但那不是我提出的问题。这里的“做”并不是指工作，“那”也不是指你的专业。我们的价值不仅仅是我们的工作，教育的意义也不仅仅是让你学会你的专业。教育的意义大于是上大学的意义，甚至大于你从幼儿园到研究生院的所接受的所有正规学校教育的意义。我说的“你要做什么”的意思是你要过什么样的生活？我所说的“那”指的是你得到的正规或非正规的任何训练，那些把你送到这里来的东西，你在学校的剩余时间里将要做的任何事。\\n\\n我们不妨先来讨论你是如何考入斯坦福的吧。你能进入这所大学说明你在某些技能上非常出色。你的父母在你很小的时候就鼓励你追求卓越。他们送你到好学校，老师的鼓励和同伴的榜样作用激励你更努力地学习。除了在所有课程上都出类拔萃之外，你还注重修养的提高，充满热情地培养了一些特殊兴趣。你参加了许多课外活动，参加私人课程。你用几个暑假在本地大学里预习大学课程，或参加专门技能的夏令营或训练营。你学习刻苦、精力集中、全力以赴。所以，你可能在数学、钢琴、曲棍球等方面都很出色，甚至是个全能选手。\\n\\n掌握这些技能当然没有错，全力以赴成为最优秀的人也没有错。错误之处在于这个体系遗漏的地方：即任何别的东西。我并不是说因为选择钻研数学，你在充分发展话语表达能力的潜力方面就失败了；也不是说除了集中精力学习地质学之外，你还应该研究政治学；也不是说你在学习钢琴时还应该学吹笛子。毕竟，专业化的本质就是要专业性。可是，专业化的问题在于它把你的注意力限制在一个点上，你所已知的和你想探知的东西都限界于此。真的，你知道的一切就只是你的专业了。\\n\\n专业化的问题是它只能让你成为专家，切断你与世界上其他任何东西的联系，不仅如此，还切断你与自身其他潜能的联系。当然，作为大一新生，你的专业才刚刚开始。在你走向所渴望的成功之路的过程中，进入斯坦福是你踏上的众多阶梯中的一个。再读三年大学，三五年法学院或医学院或研究型博士，然后再干若干年住院实习生或博士后或者助理教授。总而言之，进入越来越狭窄的专业化轨道。你可能从政治学专业的学生变成了律师或者公司代理人，再变成专门研究消费品领域的税收问题的公司代理人。你从生物化学专业的学生变成了博士，再变成心脏病学家，再变成专门做心脏瓣膜移植的心脏病医生。\\n\\n我再强调一下，你这么做当然没有什么错。只不过，在你越来越深入地进入这个轨道后，再想回忆你最初的样子就越发困难了。你开始怀念那个曾经谈钢琴和打曲棍球的人，思考那个曾经和朋友热烈讨论人生和政治以及在课堂内容的人在做什么。那个活泼能干的19岁年轻人已经变成了只想一件事的40岁中年人。难怪年长的人总是显得那么乏味无趣。“哎，我爸爸曾经是非常聪明的人，但他现在除了谈论钱和肝脏外再无其他。”\\n\\n还有另外一个问题，就是或许你从来就没有想过当心脏病医生，只是碰巧发生了而已。随大流最容易，这就是体制的力量。我不是说这个工作容易，而是说做出这种选择很容易。或者，这些根本就不是自己做出的选择。你来到斯坦福这样的名牌大学是因为聪明的孩子都这样。你考入医学院是因为它的地位高，人人都羡慕。你选择心脏病学是因为当心脏病医生的待遇很好。你做那些事能给你带来好处，让你的父母感到骄傲，令你的老师感到高兴，也让朋友们羡慕。从你上高中开始，甚至初中开始，你的唯一目标就是进入最好的大学，所以现在你会很自然地从“如何进入下个阶段”的角度看待人生。“进入”就是能力的证明，“进入”就是胜利。先进入斯坦福，然后是约翰霍普金斯医学院，再进入旧金山大学做实习医生等。或者进入密歇根法学院，或高盛集团或麦肯锡公司或别的什么地方。你迈出了这一步，似乎就必然会迈出下一步。\\n\\n也许你可能确实想当心脏病学家。十岁时就梦想成为医生，即使你根本不知道医生意味着什么。你在上学期间全身心都在朝着这个目标前进。你拒绝了上大学预修历史课的美妙体验的诱惑，也无视你在医学院第四年儿科病床轮流值班时照看孩子的可怕感受。\\n\\n但不管是那种情况，要么因为你使随大流，要么因为你早就选定了道路，20年后某天你醒来，你可能会纳闷到底发生了什么：你是怎么变成了现在这个样子，这一切意味着什么。不是说在宽泛意义的事情，而是它对你意味着什么。 你为什么做它，到底为了什么呢。这听起来像老生常谈，但这个被称为中年危机的“有一天醒来”的情况一直就发生在每个人身上。\\n\\n不过，还有另外一种情况，或许中年危机并不会发生在你身上。让我告诉你们一个你们的同龄人的故事来解释我的意思吧，即她是没有遇到中年危机的。几年前，我在哈佛参加了一次小组讨论会，谈到这些问题。后来参加这次讨论的一个学生给我联系，这个哈佛学生正在写有关哈佛的毕业论文，讨论哈佛是如何给学生灌输她所说的“自我效能”，一种相信自己能做一切的意识。自我效能或更熟悉的说法“自我尊重”。她说在考试中得了优秀的学生中，有些会说“我得优秀是因为试题很简单。”但另外一些学生，那种具有自我效能感或自我尊重的学生，会说“我得优秀是因为我聪明。”\\n\\n我得再次强调，认为得了优秀是因为自己聪明的想法并没有任何错。不过，哈佛学生没有认识到的是他们没有第三种选择。当我指出这一点时，她十分震惊。我指出，真正的自尊意味着最初根本就不在乎成绩是否优秀。真正的自尊意味着，尽管你在成长过程中的一切都在教导你要相信自己，但你所达到的成绩，还有那些奖励、成绩、奖品、录取通知书等所有这一切，都不能来定义你是谁。\\n\\n她还说，哈佛学生把他们的这种自我效能带到了社会上，并将自我效能重新命名为“创新”。但当我问她“创新”意味着什么时，她能够想到的唯一例子不过是“当上世界大公司五百强的首席执行官”。我告诉她这不是创新，这只是成功，而且是根据非常狭隘的成功定义而认定的成功而已。真正的创新意味着运用你的想象力，发挥你的潜力，创造新的可能性。\\n\\n但在这里我并不是想谈论技术创新，不是发明新机器或者制造一种新药，我谈论的是另外一种创新，是创造你自己的生活。不是走现成的道路而是创造一条属于自己的道路。我谈论的想象力是道德想象力。“道德”在这里与对错无关，而与选择有关。道德想象力是那种能创造新的活法的能力。\\n\\n它意味着不随波逐流，不是下一步要“进入”什么名牌大学或研究生院。而是要弄清楚自己到底想要什么，而不是父母、同伴、学校、或社会想要什么。即确认你自己的价值观，思考迈向自己所定义的成功的道路，而不仅仅是接受别人给你的生活，不仅仅是接受别人给你的选择。当今走进星巴克咖啡馆，服务员可能让你在牛奶咖啡、加糖咖啡、浓缩咖啡等几样东西之间做出选择。但你可以做出另外的选择，你可以转身而去。当你进入大学，人家给你众多选择，或法律或医学或投资银行和咨询以及其他，但你同样也可以做其他事，做从前根本没有人想过的事。\\n\\n让我再举一个不随波逐流的例子。几年前我写过一篇涉及同类问题的文章。我说，那些在耶鲁和斯坦福这类名校的孩子往往比较随大溜，去追求一些传统职业。（译者：比如去投行，高级律师事务所等等）我得到的最常见的批评是：那些名校的孩子不都去参加“为美国而教”这个教育项目了吗？从名校出来的很多学生毕业后很多参与这个教育项目，因此我的观点是错误的。TFA，TFA我一再听到这个术语。“为美国而教”当然是好东西，但引用这个项目来反驳我的观点恰恰是不对的，而且实际上正好证明了我想说的东西。“为美国而教”的问题或者“为美国而教”已经成为体系一部分的问题，是它已经成为另外一个需要“进入”的门槛。\\n\\n从其内容来看，“为美国而教”完全不同于高盛或者麦肯锡公司或哈佛医学院或者伯克利法学院，但从它在精英期待的体系中的地位来说，完全是一样的。它享有盛名，很难进入，是值得你和父母夸耀的东西，如果写在简历上会很好看，最重要的是，它代表了清晰标记的道路。你根本不用自己创造，什么都不用做，只需申请然后按要求做就行了，就像上大学或法学院或麦肯锡公司或别的什么。它是社会参与方面的斯坦福或哈佛，是另一个门槛，另一枚奖章。该项目需要能力和勤奋，但不需要一丁点儿的道德想象力。\\n\\n道德想象力是困难的，这种困难与你已经习惯的困难完全不同。不仅如此，光有道德想象力还不够。如果你要创造自己的生活，如果你想成为真正的独立思想者，你还需要勇气：道德勇气。不管别人说什么，有按自己的价值观行动的勇气，不会因为别人不喜欢而试图改变自己的想法。具有道德勇气的个人往往让周围的人感到不舒服。他们和其他人对世界的看法格格不入，更糟糕的是，让别人对自己已经做出的选择感到不安全或无法做出选择。只要别人也不享受自由，人们就不在乎自己被关进监狱。可一旦有人越狱，其他人都会跟着跑出去。\\n\\n在《青年艺术家的肖像》一书中，作者詹姆斯•乔伊斯让主人公斯蒂芬•迪达勒斯就19世纪末期的爱尔兰的成长环境说出了如下的名言“当一个人的灵魂诞生在这个国家时，有一张大网把它罩住，防止它飞翔。你们给我谈论民族性、语言和宗教。但是我想冲出这些牢笼。”\\n\\n今天，我们面临的是其他的网。其中之一是我在就这些问题与学生交流时经常听到的一个词“自我放任”。“在攻读学位过程中有这么多事要做的时候，试图按照自己的感觉生活难道不是自我放任吗？”“毕业后不去找个真正的工作而去画画难道不是自我放任吗？”\\n\\n这些是年轻人只要思考一下稍稍出格的事就不由自主地质问自己的问题。更糟糕的是，他们觉得提出这些问题是理所应当的。许多学生在高年级的时候跟我谈论，他们感受到的来自同伴那里的压力，他们想为为创造性的生活或独特的生活正名。你生来就是为了体验你自己的疯狂的：疯狂地打破常规，疯狂地认为事事皆有可能，疯狂地想到你有天赋之权去尝试。\\n\\n想象我们现在面临的局面吧。这是对我们个体，对道德，对灵魂的一个重要见证：美国社会思想的贫乏竟然让美国最聪明的年轻人认为听从自己的好奇心的行动就是自我放任。你们得到的教导是应该上大学去学习，但你们同时也被告知如果你想学的东西不是大众认可的，那就是你的“自我放任”。如果你是自己学习自己感兴趣的东西的话，更是“自我放任”。这是那个门子的道理？进入证券咨询业是不是自我放任？进入金融业是不是自我放任？像许多人那样进入律师界发财是不是自我放任？搞音乐，写文章就不行，因为它不能给人带来利益。但为风险投资公司工作就可以。追求自己的理想和激情是自私的，除非它能让你赚很多钱。那样的话，就一点儿也不自私了。\\n\\n你看到这些观点是多么荒谬了吗？这就是罩在你们身上的网，就是我说的需要勇气的意思。而且这是永不停息的抗争过程。在两年前的哈佛事件中，有个学生谈到我说的大学生需要重新思考人生决定的观点，他说“我们已经做出了决定，我们早在中学时就已经决定成为能够进入哈佛的高材生。”我在想，谁会打算按照他在12岁时做出的决定生活呢？ 让我换一种说法，谁愿意让一个12岁的孩子决定他们未来一辈子要做什么呢？或者一个19岁的小毛孩儿？\\n\\n唯一你能做出的决定是你现在在想什么，你需要准备好不断修改自己的决定。让我说得更明白一些。我不是在试图说服你们都成为音乐家或者作家。成为医生、律师、科学家、工程师或者经济学家没有什么不好，这些都是可靠的、可敬的选择。我想说的是你需要思考它，认真地思考。我请求你们做的，是根据正确的理由做出你的选择。我在敦促你们的，是认识到你的道德自由并热情拥抱它。\\n\\n最重要的是，不要过分谨慎。去抵抗我们社会给予了过高奖赏的那些卑怯的价值观的诱惑：舒服、方便、安全、可预测的、可控制的。这些，同样是罗网。最重要的是，去提抗失败的恐惧感。是的，你会犯错误。可那是你的错误，不是别人的。你将从错误中缓过来，而且，正是因为这些错误，你更好地认识你自己。由此，你成为更完整和强大的人。\\n\\n人们常说你们年轻人属于“后情感”一代，我想我未必赞同这个说法，但这个说法值得严肃对待。你们更愿意规避混乱、动荡和强烈的感情，但我想说，不要回避挑战自我（，不要否认欲望和好奇心、怀疑和不满、快乐和阴郁，它们可能改变你预设的人生轨迹。大学刚开始，成年时代也才刚开始。打开自己，直面各种可能性吧。这个世界的深广远超你现在想象的边际。这意味着，你自身的深广也将远超你现在的想象。', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='25cae90d-703e-469c-8326-c838082fe8af', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n杂 86 nice guy\\n\\n**Being integrated means being able to accept all aspects of one\\'s self.** An integrated man is able to\\nembrace everything that makes him uniquely male: his power, his assertiveness, his courage, and his\\npassion as well as his imperfections, his mistakes, and his dark side.\\n\\nAn integrated male possesses many of the following attributes:\\n\\n- ●\\xa0**He has a strong sense of self. He likes himself just as he is.**\\n- ●\\xa0**He takes responsibility for getting his own needs met.**\\n- ●\\xa0**He is comfortable with his masculinity and his sexuality.**\\n- ●\\xa0**He has integrity. He does what is right, not what is expedient.**\\n- ●\\xa0**He is a leader. He is willing to provide for and protect those he cares about.**\\n- ●\\xa0**He is clear, direct, and expressive of his feelings.**\\n- ●\\xa0**He can be nurturing and giving without caretaking or problem-solving.**\\n- ●\\xa0**He knows how to set boundaries and is not afraid to work through conflict.**\\n\\n**Breaking Free Activity #3**\\n\\nIt is impossible to cover every factor that might cause a young boy to try to hide his perceived\\nflaws and seek approval from others. I don\\'t believe it is essential for Nice Guys to uncover *every*\\nexperience that ever made them feel unsafe or bad. But I have found that some understanding of\\nwhere a life script originated is helpful in changing that script.\\n\\nReread the stories of Alan, Jason, and Jose. Think about how these stories are similar to your own\\nchildhood experiences. On a separate piece of paper or journal, write down or illustrate the\\nmessages you received in your family that seemed to imply that it wasn\\'t OK for you to be who\\nyou were, just as you were. Share these experiences with a safe person. As you do, make note of your feelings. Do you feel sad, angry, lonely, numb? Share this information as well.\\n\\nThe purpose of this assignment is to name, rather than blame. Blaming will keep you stuck.\\nNaming the childhood experiences that led you to believe that it was not a safe or acceptable thing for you to be just who you were will allow you replace these messages with more accurate ones and help you change your Nice Guy script.\\n\\n**Breaking Free Activity #6**\\n\\nLook over the lists above. Write down examples of situations in which you have tried to hide or\\ndistract attention from any of these perceived flaws. How effective do you think you are in keeping\\nthese things hidden from the people you love?\\n****\\n\\n**Breaking Free Activity #9**\\n\\nBegin with the list above and add good things that you can do for yourself. Put the list up where\\nyou will see it and choose at least one thing per day and do it for yourself.\\n\\n**Making It Difficult For Others To Give To Them Prevents Nice Guys From Getting Their\\nNeeds Met**\\n\\nIn addition to using ineffective strategies to get their needs met, **Nice Guys are terrible receivers.** Since\\ngetting their needs met contradicts their childhood paradigms, Nice Guys are extremely uncomfortable when they actually do get what they want. Though most Nice Guys have a difficult time grasping this concept, they are terrified of getting what they really want and will go to extreme measures to make sure they don\\'t. Nice Guys carry out this unconscious agenda by connecting with needy or unavailable people, operating from an unspoken agenda, being unclear and indirect, pushing people away, and sabotaging.\\n\\n**Caretaking**\\n\\n**1) Gives to others what the giver needs to give.**\\n\\n**2) Comes from a place of within emptiness within\\nthe giver.**\\n\\n**3) Always has unconscious strings attached.**\\n\\n**Caring**\\n\\n**1) Gives to others what the receiver needs.**\\n\\n**2) Comes from a place of abundance the giver.**\\n\\n**3) Has no strings attached.**\\n\\n**victim triangle.** The victim triangle\\nconsists of three predictable sequences:\\n\\n1) The Nice Guy gives to others hoping to get something in return.\\n\\n2) When it doesn\\'t seem that he is getting as much as he gives or he isn\\'t getting what he expected, he feels frustrated and resentful. Remember, the Nice Guy is the one keeping score and he isn\\'t totally objective.\\n\\n3) When this frustration and resentment builds up long enough, it spills out in the form of rage attacks, passive-aggressive behavior, pouting, tantrums, withdrawing, shaming, criticizing, blaming, even physical abuse. Once the cycle has been completed, it usually just begins all over again.\\n\\n**Since Nice Guys learned to sacrifice themselves in order to survive, recovery must center on learning to put themselves first and making their needs a priority.**\\n\\t\\n\\n**Taking Responsibility For Their Own Needs Helps Nice Guys Get Their Needs Met**\\n\\nIn order for Nice Guys to get their needs met, they must begin to shift their core paradigms. This shift includes coming to believe:\\t\\t\\n\\t\\t\\t\\n    ◦ ● \\xa0Having needs is part of being human.\\n\\n    ◦ ● \\xa0Mature people make meeting their own needs a priority.\\n\\n    ◦ ● \\xa0They can ask for help in meeting their needs in clear and direct ways.\\n\\n    ◦ ● \\xa0Other people really do want to help them meet their needs.\\n\\n    ◦ ● \\xa0This world is a place of abundance.\\n\\n**Breaking Free Activity #16**\\n\\nMake a decision to put yourself first for a weekend or even a whole week. Tell the people around you what you are doing. Ask a friend to support you and encourage you in this process. Pay attention to your initial anxiety. Pay attention to your tendency to revert to old patterns. At the end of the time period, ask the people around you what it was like for them when you put yourself first.\\n\\nRemember, you don\\'t have to do it perfectly. Just do it.\\n\\na common denominator for Nice Guys is that they did not get their needs\\nmet in a timely, healthy fashion in childhood. These little boys were helpless to prevent people from\\nabandoning them, neglecting them, abusing them, using them, or smothering them. They *were* victims to\\nthe people who failed to love them, pay attention to them, meet their needs, and protect them\\n\\nThe more frightened they are, the more they use their childhood survival mechanisms. The more they use these ineffective mechanisms, the less successful they are at negotiating the complexities, challenges and ambiguities of life. The less successful they are, the more fearful they become . . . you get the picture\\n\\nI define **personal power** as a state of mind in which a person is confident he can handle whatever may come. This kind of power not only successfully deals with problems, challenges and adversity, it\\nactually welcomes them, meets them head on, and is thankful for them. Personal power isn\\'t the absence\\nof fear. Even the most powerful people have fear. Personal power is the result of feeling fear, but not giving in to the fear.\\n\\nSurrender allows recovering Nice Guys to let go and respond to life\\'s complex beauty, rather than trying\\nto control it. Surrender allows these men to see life as a laboratory for learning, growth, and creativity.\\n**Surrender allows recovering Nice Guys to see each life experience as a \"gift\" from the universe to stimulate growth, healing and learning.** Instead of asking, \"Why is this happening to me?\" the\\nrecovering Nice Guy can respond to life\\'s challenges by pondering, \"What do I need to learn from this\\nsituation?\"\\n\\nI frequently hear Nice Guys rationalize the withholding of their feelings by claiming they don\\'t want to hurt anyone. The truth is they are covering their own butts. What they are really saying is that they don\\'t want to do anything that might recreate their childhood experiences. They\\'re really not trying to protect anyone from harm, they\\'re just trying to keep their world smooth and under control.\\n\\t\\t\\t\\t\\t\\nThe goal of teaching Nice Guys to embrace their feelings is not to make them soft and \"touchy-feely.\" Men who are in touch with their feelings are powerful, assertive, and energized. Contrary to what many Nice Guys believe, they don\\'t have to become more like women in order to have their feelings. This is why I support men in learning about their feelings from other men.\\t\\t\\t\\t\\n\\n**Breaking Free Activity #20**\\n\\n**Some guidelines about expressing feelings.**\\n\\n- **Don\\'t focus on the other person, \"You are making me mad.\"**\\n\\n**Instead, take responsibility for what you are feeling: \"I am feeling angry.\"**\\n\\n- **Don\\'t use feeling words to describe what you are thinking, as in \"I feel like Joe was trying to take advantage of me.\"**\\n\\n**Instead, pay attention to what you are experiencing in your body: \"I\\'m feeling helpless and frightened.\"**\\n\\n- **In general, try to begin feeling statements with \"I\", rather than \"you.\"\\nTry to avoid the crutch of saying \"I feel *like*.\" As in \"I feel *like* you are being mean to me.\"**\\n\\n**Facing present day fears is the only way to overcome memory fear.** Every time the Nice Guy confronts a fear, he unconsciously creates a belief that he can handle whatever it is he is afraid of. This challenges his memory fear. Challenging this memory fear makes the things outside of him seem less threatening. As these things seem less frightening, he feels more confident in confronting them. The more this confidence grows, the less threatening life seems.\\n\\nTelling the truth is not a magic formula for having a smooth life. But living a life of integrity is actually easier than living one built around deceit and distortion.\\nDeveloping integrity is an essential part of recovery from the Nice Guy Syndrome. My definition of integrity is \"deciding what feels right and doing it.\"\\n\\nThe alternative is using the \"committee approach.\" This method of decision-making and acting is based on trying to guess what everyone else would think is right. Following this committee approach is the quickest path to confusion, fear, powerlessness, and dishonesty.\\n\\n**Breaking Free Activity #22**\\n\\nChoose one area in which you have been out of integrity. Identify your fear that keeps you from telling the truth or doing the right thing. Reveal this situation to a safe person. Then go and tell the truth or do what you have to do to make the situation right. Tell yourself you can handle it. Since telling the truth may create a crisis for you or others, have faith that everyone involved will survive this crisis.\\n\\n**Setting Boundaries Helps Nice Guys Reclaim Their Personal Power**\\n\\nBoundaries are essential for survival. Learning to set boundaries allows Nice Guys to stop feeling like\\nhelpless victims and reclaim their personal power.\\n\\n**Do you agree to do something to avoid conflict? Do you avoid doing something because someone might get upset at you? Do you tolerate an\\nintolerable situation, hoping that it will just go away? Write these observations down and share them with a safe person.**\\n\\n**Look over the list above. Note the ways you have consciously or unconsciously tried to be different\\nfrom your father and/or other men. How does the belief that you are different keep you\\ndisconnected from other men?**\\n\\n\\t\\t\\t\\t\\t\\n\\nAs long as Nice Guys are disconnected from men or believe they are different from other men, they cut\\nthemselves off from the many positive benefits of male companionship and the power of a masculine\\ncommunity.\\n\\n\\t\\t\\t\\t\\t\\n**Nice Guys Tend To Be Disconnected From Their Masculinity**\\n\\n**I define masculinity as that part of a man that equips him to survive as an individual, clan, and\\nspecies**. Without this masculine energy we would have all become extinct eons ago. Masculinity\\nempowers a man to create and produce. It also empowers him provide for and protect those who are\\nimportant to him.\\n\\nMost Nice Guys believe that by repressing the darker side of their masculine energy they will win the approval of women. As Nice Guys try to avoid the dark side of their masculinity, they also repress many other aspects of this male energy force. As a result, they often lose their sexual assertiveness, competitiveness, creativity, ego, thirst for experience, boisterousness, exhibitionism, and power.\\n\\nThe process involves coming to believe that it really is a good\\nthing to be a man and embracing all of their masculine traits. Reclaiming one\\'s masculinity involves:\\n\\n- ●\\xa0Connecting with other men.\\n- ●\\xa0Getting strong.\\n- ●\\xa0Finding healthy male role models.\\n- ●\\xa0Reexamining one\\'s relationship with one\\'s father.\\n\\nThe best thing you can do for your relationship with your girlfriend or wife is to have male friends\\n\\n**They realize that if their lives are a reaction to Dad, then Dad is still in control.**\\n\\nIn relationships, a life-and-death struggle is played out to balance their fear of vulnerability with their fear of isolation. Vulnerability means someone may get too close to them and see how bad they are. Nice Guys are convinced that when others make this discovery, these people will hurt them, shame them, or leave them\\n\\nThe first is through\\nbecoming overly involved in an intimate relationship at the expense of one\\'s self and other outside\\ninterests. The second is through being emotionally unavailable to a primary partner while playing the\\nNice Guy role outside of the relationship. I call the first type of Nice Guy an **enmesher** and the second\\ntype an **avoider**.\\n\\nThe enmeshing Nice Guy makes his partner his emotional center. His world revolves around her. She is\\nmore important than his work, his buddies, his hobbies. He will do whatever it takes to make her happy.\\nHe will give her gifts, try to fix her problems, and arrange his schedule to be with her. He will gladly\\nsacrifice his wants and needs to win her love. He will even tolerate her bad moods, rage attacks,\\naddictions, and emotional or sexual unavailability — all because he \"loves her so much.\"\\n\\nI sometimes refer to enmeshing Nice Guys as **table dogs**. They are like little dogs who hover beneath\\nthe table just in case a scrap happens to fall their way. Enmeshing Nice Guys do this same hovering\\nroutine around their partner just in case she happens to drop him a scrap of sexual interest, a scrap of her\\ntime, a scrap of a good mood, or a scrap of her attention.\\n\\nOn the surface it may appear that the enmeshing Nice Guy desires, and is available for an intimate relationship, but this is an illusion. The Nice Guy\\'s pursuing and enmeshing behavior is an attempt to hook up an emotional hose to his partner. This hose is used to suck the life out of her and fill an empty place inside of him. The Nice Guy\\'s partner unconsciously picks up on this agenda and works like hell to make sure the Nice Guy can\\'t get close enough to hook up the hose. Consequently, the Nice Guy\\'s partner is often seen as the one preventing the closeness the Nice Guy desires.\\n\\nThe people who like them just as they are will hang around. The people who don\\'t, won\\'t. This is the only way to have a healthy relationship\\n\\nWhen trying to decide how to deal with a behavior they have deemed unacceptable, I encourage Nice\\nGuys to apply the **Healthy Male Rule.** Following this rule of thumb, they simply ask themselves, \"How\\nwould a healthy male handle this situation?\" For some reason, just asking this question connects them\\nwith their intuitive wisdom and helps them access the power they need to respond appropriately.\\n\\nOnce the Nice Guy knows he can set a boundary any time he needs to, he can let people move toward him, get close, have feelings, be sexual, and so on. He can let these things happen because he is confident that at any point, if he begins to feel uncomfortable, he can say \"stop,\" \"no,\" or \"slow down,\" or can remove himself. He can do whatever he needs to do to take care of himself.\\n\\n**Breaking Free Activity #34**\\n\\n**Are there any areas in your personal relationships in which you avoid setting appropriate\\nboundaries? Do you:**\\n\\n- ●\\xa0**Tolerate intolerable behavior.**\\n- ●\\xa0**Avoid dealing with a situation because it might cause conflict.**\\n- ●\\xa0**Not ask for what you want.**\\n- ●\\xa0**Sacrifice yourself to keep the peace.**\\n    \\n    **If you applied the Second Date rule or the Healthy Male rule to these situations, how might you\\n    change your behavior?**\\n    \\n\\nBy focusing on their relationship instead of their partner, recovering Nice Guys are able to use their partner to get in touch with their childhood experiences of abandonment, neglect, abuse, and smothering. They can use this information to better understand why they have created the kind of relationship system they have. This process enables them to make changes that allow them to get what they want in their intimate relationships.\\n\\nDoing something different also means refraining from being sexual in new relationships. Nice Guys\\nmust give themselves a chance to accurately evaluate the traits listed above by staying out of bed with a\\nperson until they really get to know her. Once the sex begins in relationships, the learning stops. Sex\\ncreates such a powerful bond that it is difficult to accurately evaluate the appropriateness of a new relationship', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7d5fbb76-beb8-4434-8996-d4050982f88e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n**自上而下表达，结论先行**\\n\\n由于受众的知识背景和理解力千差万别，他们很难对你所表达的思想组作出与你完全一样的解读。事实上，如果你不预先告诉读者某一组思想之间的逻辑关系，他们很有可能会认为某一组中的思想之间根本没有任何关系。退一步说，即使受众能够作出与你完全一样的解读，你也增加了他们阅读的难度，因为他们必须自己找出这种你没有提前说明的逻辑关系\\n\\n“上个星期我去了趟苏黎世。你知道，苏黎世是一个比较保守的城市。我们到一家露天餐馆吃饭，你知道吗？在15分钟里我至少见到了15个留长胡子的人。”\\n\\n我说这番话是向你传递了一个信息，但是我并没有意识到，你会主动地推测我向你传递这个信息的原因。也就是说，你会将我说的这番话看做是一组还未表达出的思想的一部分，你会假设某种可能的原因，并据此调整你的思路，准备接着听后面的话。这种预期性的准备能够减轻大脑分析信息的负担，因为你没必要分析随后接收的每一个信息的所有特征，而只需寻找与前面信息相同的特征即可\\n\\n文章中的思想必须符合以下规则：\\n\\n1. 纵向：文章中任一层次上的思想必须是其下一层次思想的概括。\\n2. 横向：每组中的思想必须属于同一逻辑范畴。\\n3. 横向：每组中的思想必须按逻辑顺序组织。\\n\\n卢梭是直接民主的第一拥趸，他便对代议制民主极其不满，章首引言以外，还有名言：“**自以为自由的英国人民是大错特错的。只有在选举国会议员的期间，他们才是自由的，一旦议员被选出，他们就又是奴隶了。**”\\n\\n汉娜.阿伦特所指的代议制民主的逻辑缺陷亦十分精到。如果民主的目的只是为了“选擢精英然后独掌权力”，那专制体制也不是为着选拔笨蛋的，两者的区分又在哪里？如果不能信任人民在行政及立法方面的专业能力，那为何可以相信他们却独有能力投票选拔出精英，而非和普罗群众一样的蠢才？而如果整个民主制度的目标即在于令精英得以显露，那以今日朝韩局面，朝鲜金氏的个人才能显然强出隔壁远甚，我们是否可以认定朝鲜的制度就更胜于韩国？\\n\\n组织思想基本上只可能有4种逻辑顺序：\\n\\n- 演绎顺序：大前提、小前提、结论\\n- 时间（步骤）顺序：第一、第二、第三\\n- 结构（空间）顺序：波士顿、纽约、华盛顿\\n- 程度（重要性）顺序：最重要、次重要，等等\\n\\n金字塔结构的巨大价值就在于它迫使你在理清思路时，从视觉上使纵向的疑问／回答式对话关系清晰化。你的每一个表述都应当引发读者的疑问，而你也必须在这一表述下的横向结构层次上逐个回答读者的疑问\\n\\n!Untitled\\n\\n自上而下法构建金字塔步骤如下：\\n\\n**1.画出主题方框**\\n\\n这个方框就是你文章的金字塔结构最顶部的方框。在方框中填入你要讨论的主题，当然前提是你知道要讨论什么主题，否则请跳到步骤2。\\n\\n**2.设想主要疑问**\\n\\n确定文章的读者。你的文章将面对哪些对象？你希望文章能回答读者头脑中关于该主题的哪些疑问？如果你能确定读者的主要疑问，请写出来，否则跳到步骤4。\\n\\n**3．写出对该疑问的回答**\\n\\n如果你还不清楚答案，请注明你有能力回答该疑问。\\n\\n**4．说明“背景”**\\n\\n你需要证明，现阶段你能够清晰论述该主要的疑问和答案。具体做法是：把要讨论的主题与“背景”相结合，作出关于该主题的第一个不会引起争议的表述。首先，关于该主题的哪些表述肯定不会引起读者的疑问呢？（因为读者知道这一表述，或者根据以往经验很容易判断该表述的正确性。）\\n\\n**5．指出“冲突”**\\n\\n现在你已经开始与读者进行疑问／回答式对话了。想象一下，读者表示同意，点着头说：“对，我知道这个情况，有什么问题吗？”此时，你就应当考虑“背景”中发生了哪些能使读者产生疑问的“冲突”，例如发生了某种意外，出现了某个问题，或出现了明显的不应当出现的变化。“背景”中发生了哪些“冲突”，以致引发了读者的“疑问”呢？\\n\\n**6．检查“主要疑问”和“答案”**\\n\\n对“背景”中“冲突”的介绍，应当直接导致读者提出主要疑问（已在步骤2中列出）。否则，应重新介绍“背景”中的“冲突”，使之可以直接导致读者提出主要疑问。可能有时“背景”中的“冲突”与主要疑问对不上号，这就需要你重新构思。\\n\\n进行以上步骤的目的，是确保你了解自己将要回答哪些疑问。一旦确定了主要“疑问”，其他要素都很容易在金字塔结构中各就各位\\n\\n!Untitled\\n\\n**1.一定先搭结构，先尝试自上而下法**\\n\\n一旦你着手将思想变成文字，它就似乎戴上了最美丽的光环，令你感觉“字字珠玑”，甚至不愿意进行必要的修改。因此，不要试图一下子就把整篇文章都写出来，因为你稍后就可能很容易地想出文章的结构。一旦你的思想变成了文字，你就可能会觉得写得不错，而根本不管你的思路实际上是不连贯的。\\n\\n**2.序言先写背景，将背景作为序言的起点**\\n\\n一旦你知道自己想在序言的主体部分说什么——背景、冲突、疑问和回答，你就可以根据你希望产生的效果，按任何顺序写出这些内容。选择不同的顺序会影响文章的文风，当然你肯定想运用不同的文风写不同的文章。但是，一定要从“背景”开始构思，因为按照这个顺序，你更容易准确地找到“冲突”和“疑问”。\\n\\n**3.先多花时间思考序言，不要省略**\\n\\n先想好序言，避免开始论证时还在想背景或冲突。当你坐下来开始写作时，经常头脑中已经有了一个完整的主要思想。这样，触发主要思想的“疑问”也很明显。这时，你就很容易直接跳到关键句层次，开始回答由主要思想引起的新的疑问。我劝你不要这样做。因为多数情况下，你会发现自己还在思考和组织属于“背景”或“冲突”的信息，这样会使你自己陷入复杂、混乱的论证中。你应当先整理出序言的信息，然后把注意力完全集中在金字塔结构中较低层次的思想上。\\n\\n**4.将历史背景放在序言中**\\n\\n你不应该在文章的正文部分才告诉读者过去发生的事情。正文部分应当只包括思想（即：因能够为读者提供新思维而引起读者疑问的表述），思想只能以逻辑方式互相联系。也就是说，只有在描述一些通过分析发现的因果关系时，你才可以在正文部分列举读者已知的信息。简单的历史事件不是逻辑思考的结果，因而也不能算作思想。\\n\\n**5.序言仅涉及读者不会对其真实性提出质疑的内容**\\n\\n序言的目的只是告诉读者一些他们已经知道的信息。当然，有时你并不知道读者是否确实知道某些信息；有时你则可以肯定读者不知道某些信息。如果所表达的信息能够很容易地由客观的第三方进行检验和证实，那就可以假定你的读者“知道”该信息，因为读者不会对其真实性提出质疑。\\n\\n同时，注意不要在序言中涉及任何读者不知道的信息，因为这样的信息可能导致读者提出非你所愿的“疑问”。反之亦然，不要在金字塔结构中涉及任何读者已经知道的信息。如果你利用读者已经知道的信息，回答金字塔结构中较低层次上的问题，就说明你在序言中遗漏了重要的信息。如果在序言中提供该信息的话，也许读者会提出不同的疑问。\\n\\n**6.在关键句层次上，更宜选择归纳推理法而非演绎论证法**\\n\\n这一点在第5章“演绎推理与归纳推理”中有更详尽的说明。在关键句层次上使用归纳推理比使用演绎推理更容易使读者接受，因为归纳法更容易被人理解。人们的倾向是按照思维发展的顺序表达自己的思想，而思维发展的顺序通常都是演绎的顺序。但是，以演绎的顺序发展的思想并不一定要以演绎的顺序表达出来。在大多数情况下，你都可以将以演绎法发展的思想用归纳法的形式表达出来。\\n\\n假设你建议某人购买一座库房，你根据以下的演绎推论支持对方购买库房\\n\\n文章序言中的“冲突”虽然经常指的是某种不利的变化，但并不总是具有“不利的变化”的含义。这里的“冲突”类似于讲故事时推动情节发展的因素，能够促使读者提出“疑问”。\\n\\n用描述文章主题的公认事实开始讲“故事”，“冲突”就是推动“故事”情节发展的因素，并且必须引发读者的“疑问”。读者的“疑问”可能有多种形式，但通常相当于询问“接下来怎么样”\\n\\n序言必须采取“背景—冲突—疑问—解决方案”的结构，但是，各部分的顺序可以有所变化，以创造不同的文章风格。以下是一个序言的基本结构和分别用4种顺序写出来的例子。注意体会用每一种顺序后文章风格的变化。\\n\\n!Untitled\\n\\n**基本结构**\\n\\n背景（S）＝ 业务多元化研究服务在过去5年中增长了40％。\\n\\n冲突（C）＝ 无法证明我们的工作对客户有明显益处。\\n\\n疑问（Q）＝ 如何确保多元化研究确实对客户有明显益处？\\n\\n回答（A） ＝ 实施“公司发展项目”，以研究该问题。\\n\\n**标准式：背景—冲突—答案**\\n\\n近几年来，本公司已经因为提供多元化研究服务而向许多客户收取了大量费用。但是至今伦敦办事处也没有一个员工能够证明，某客户的某项收购案或并购案与本公司的工作密不可分。该办事处为庆贺第一个能够作出此项证明的员工而准备的香槟酒也一直无人开启。然而，本公司的多元化研究服务在过去5年中增长了40％。因此，现在应该实施一项“公司发展项目”，以研究我们如何确保多元化研究确实能够为客户带来显著的利益。\\n\\n本备忘录列举了该项目实施过程中应当解决和进行试验的主要问题及假设。\\n\\n**开门见山式：答案—背景—冲突**\\n\\n我们实施“公司发展项目”的首要目标，是提高我们帮助客户实施业务多元化的能力。仅在伦敦办事处，我们帮助客户寻找收购或并购对象的服务在5年中增长了 40％，但是我们不能证明任何一项收购案或并购案是与我们的工作密不可分的。\\n\\n**突出忧虑式：冲突—背景—答案**\\n\\n据我了解，目前伦敦办事处还没有一个员工可以说，他为客户所做的多元化研究，已经为客户带来了客户自身无法达到的明显效益。这种情况令人吃惊，因为我们在多元化研究领域的服务在过去5年中已经增长了 40％。从良心上讲，我们不能继续为无法取得显著收益的工作收取客户的报酬，这样做也难以维持我们的良好声誉。因此，我建议实施一项“公司发展项目”，研究如何使我们的业务多元化研究服务真正为客户带来显著的利益。\\n\\n**突出信心式：疑问—背景—冲突—答案**\\n\\n我们如何确保业务多元化研究继续成为我们的重要服务项目？此项服务目前已占我们公司总营业额的40％，但是我们难以举出几个我们为客户提供了必不可少帮助的例子。如果不采取措施提高我们工作的价值，我们就会面临失去在该领域发展优势的实际风险。\\n\\n为此，我建议立即实施一项“公司发展项目”，以研究如何改善我们在该领域的服务能力，使该项服务能够不断为客户带来显著利益\\n\\n想写出好的序言，必须遵循以下原则：\\n\\n**1.序言的目的是“提示”读者而不是“告诉”读者某些信息**\\n\\n序言中不应含有读者需要证明后才能接受的信息，例如，不应当含有图表。\\n\\n**2.序言必须包含讲故事的3个要素，即“背景”、“冲突”和“答案”**\\n\\n在较长的文章中，你还应当对接下来要讨论的内容给予解释。“背景”、“冲突”和“答案”这3个要素不一定要按标准的叙述顺序排列，但是这3个要素必须齐全，缺一不可，还应当组织成故事结构。\\n\\n**3.序言的长度取决于读者和主题的需要**\\n\\n序言可包括读者充分理解所必需的任何信息，如：问题的背景与渊源、你与该问题的关联、你或他人之前对该问题所做的研究及发现、术语的定义等，但所有的信息都可以也应当用讲故事形式。\\n\\n通过以上例子可以看出，全篇文章的中心都依赖于读者提出的第一个疑问，或称初始疑问。这种疑问全篇只能有一个。如果存在两个初始疑问，这两个疑问肯定是互相关联的，如：“我们是否应当进入市场？如果是，应当如何进入？”这两个疑问实际上只相当于一个疑问，即“我们应当如何进入市场”。因为如果对第一问的答案是否定的，就不存在第二问；而如果对第一问的答案是肯定的，那么这个回答就成为金字塔结构的顶端思想，而由之引起的疑问“如何进入市场”将引出关键句要点对其回答。\\n\\n有时，你难以仅凭对序言部分的思考确定初始疑问。这时你可以看看你打算放在正文中的素材。任何时候你打算说明一些思想，是因为你认为读者应该知道这些思想。那么为什么读者应该知道这些思想呢？肯定是因为这些思想能够回答某个问题。这样，通过倒推法，你也能够想出一个合理的序言，并引出你要回答的疑问\\n\\n演绎是一种线性的推理方式，最终是为了得出一个由逻辑词“因此”引出的结论。在金字塔结构中，位于演绎论证过程上一层次的思想是对演绎过程的概括，重点是在演绎推理过程的最后一步，即由逻辑词“因此”引出的结论。归纳推理是将一组具有共同点的事实、思想或观点归类分组，并概括其共同性（或论点）。在演绎过程中，每个思想均由前一个思想导出；而在归纳过程中则不存在这种关系\\n\\n在金字塔横向结构中，同一组中的思想之间存在着逻辑顺序，具体的顺序取决于该组思想之间的逻辑关系是演绎推理关系，还是归纳推理关系。\\n\\n位于演绎推理过程上一层次的思想是对演绎过程的概括，重点是在演绎推理过程的最后一步，即由“因此”引出的结论。归纳推理是将具有共同点的事实、思想或观点归类分组，并概括其共同性（或论点）\\n\\n!Untitled\\n\\n!Untitled\\n\\n我将演绎推理过程看做需要完成以下3个步骤：\\n\\n- 阐述世界上已存在的某种情况。\\n- 阐述世界上同时存在的相关情况。如果第二个表述是针对第一个表述的主语或谓语的，则说明这两个表述相关。\\n- 说明这两种情况同时存在时隐含的意义。\\n\\n演绎推理也可以是以下3个步骤：\\n\\n- 出现的问题或存在的现象。\\n- 产生问题的根源、原因。\\n- 解决问题的方案。\\n\\n!Untitled\\n\\n用归纳法时，必须具有以下两项主要技能：\\n\\n- 正确定义该组思想。找到一个能够表示该组所有思想共同点的名词。\\n- 识别并剔除该组思想中与其他思想不相称（不属同类、不具有共同点）的思想\\n\\n下面我举例说明读者在阅读用演绎法组织的文章时，被迫进行的思考过程。假设你想告诉某人必须以某种方式进行改革，你的论述的基本推理过程如图 5 -4所示\\n\\n!Untitled\\n\\n!Untitled\\n\\n为了理解你的思维过程，读者必须首先理解和接受“目前存在的问题”（问题一、问题二、问题三）。做到这一点并不难。但是，随后你要求读者将第一个存在的问题“问题一”带到“产生问题的原因”这一组思想中，与产生第一个问题的原因“原因一”联系起来，然后将这种联系保存在大脑中，再依此类推，将“问题二”与“原因二”，以及“问题三”与“原因三”联系起来。然后，读者还必须再次重复以上过程：将“存在的问题”中的“问题一”，与“产生问题的原因”中的“原因一”相联系，共同与“应采取的措施”中的“措施一”相联系。问题二、原因二、措施二以及问题三、原因三、措施三也需要同样处理。\\n\\n这种方法，不仅使读者必须费尽周折才能知道要采取什么措施，而且使读者在获得“回报”（了解要采取的措施）前，必须重复作者解决问题的思维过程。这就相当于对读者说：“我费了很大的劲儿才找出解决方案，我必须让你知道我付出的辛苦劳动。”但是，用归纳法表达同样的思想，可以使作者和读者都省去不少工夫\\n\\n你写完全篇文章时，还必须仔细检查一下全文的结构，因为你可能发现自己犯了以下两种常见错误之一：\\n\\n- 仅仅因为可以用同一个名词概括，而将关联性很小的思想排列在一起（如“10个步骤”或“5个问题”等），实际上这些思想之间不存在逻辑关系。\\n- 金字塔结构顶端的中心思想，使用的是“缺乏思想”的句子（如“该公司存在5个问题”），而非具有揭示性的观点\\n\\n通常有3种划分组织活动的方式：根据活动本身（如：研发、生产、市场营销）；根据活动发生的地点（如：国家东部、国家中西部、国家西部）；根据针对特定产品、市场或客户活动的集合（如：各事业部、各业务单元，轮胎部、硬件部、体育设备部）。\\n\\n- 如果划分时强调活动本身，那么各部分展现的是一个逻辑过程（流程），因此应采用时间顺序。\\n- 如果划分时强调地点，那么各部分呈现的是地理状况，应采用结构顺序。\\n- 如果划分时强调与某一产品或市场有关的活动，那么划分就是一种归类。各部分思想应采用重要性顺序，判断重要性的标准可以是任何排序标准（如：销量、投资额等）\\n\\n!Untitled\\n\\n商界人士在谈论成本、销售额、价格、投资和投资回报率时，都假定对方了解这些因素之间的关系，如图6 -7所示。如果你按照此树形图将相关因素一一对号入座，你就能够比较容易了解作者的思想：此项目将对投资回报率造成负面影响\\n\\n那么这3类问题应当按什么顺序排序呢？这取决于你讨论的是编制报告的过程，阅读报告的过程，还是解决问题的过程。换句话说，逻辑顺序呈现的是一个过程，而过程取决于需要回答的问题\\n\\n!Untitled\\n\\n检查逻辑顺序是检查某一分组是否恰当的重要手段。当你遇到任何一组归纳性思想，需要找出其真实意义时，一定要先快速浏览一遍该组中的所有思想。你能否发现某种逻辑顺序\\n\\n（时间顺序、结构顺序、重要性顺序）？如果不能，你能否发现这种分组的基础（过程、流程、结构、类别），并采用某种逻辑顺序？如果某组中所列的思想很多，你能否发现它们具有某些共同特点，并根据这些共同特点将思想归纳、分组，然后使用逻辑顺序？\\n\\n“缺乏思想”的句子对读者而言是枯燥无味的，因为这种句子难以吸引和保持读者的注意力，不能激发读者继续往下读，还可能使读者根本无法了解你表达的思想。举个例子，以下是我曾从收音机中听到的一段对话：\\n\\n甲：约翰·韦恩说他是最适合写塞缪尔·约翰逊传记的人，他有3个理由：\\n\\n他们都来自贫困的斯塔福德县；\\n\\n他们都在牛津大学学习过；\\n\\n他们都有类似的文学爱好。\\n\\n乙：我不同意。斯塔福德县根本没有人说真话。\\n\\n听到这里，听众都笑起来，然后甲和乙又开始说别的事情。我当时就想：“我好像没听懂。”现在我们来分析一下。你坐在那儿，等待别人向你表达某种思想，但是你听到的只是一个“缺乏思想”的句子（“有3个理由”），而没有从对方获得任何思想。然后你听到对方说：“都来自贫困的斯塔福德县……”你就会假定这就是对方的主要思想，于是对随后的两个要点都听不进去。如果你需要回答的话，你就只能回答你听到的第一点。\\n\\n但是，如果甲当时这样说：\\n\\n“约翰·韦恩说他是最适合写塞缪尔·约翰逊传记的人，因为他和约翰逊属于同一类人。”\\n\\n虽然你还必须再听甲说一些支持此观点的论据，但是你的回答肯定更及时，更有针对性。但是在前一个例子中，甲和乙的对话完全没有重点，没有提炼。\\n\\n这个例子说明了总结性思想的重要性。当你听到“他这样做是因为他们属于同一类人”时，你的思维肯定比听到“他这样做是有3个原因”时更有准备，更容易理解随后听到的信息。“有3个原因”这样的话听起来索然无味，实际上也确实没有什么内容\\n\\n避免使用“缺乏思想”的句子还有更重要的理由。这种句子会掩盖思考不完整的事实，使你错失了一个进行有逻辑性和创造性思考的绝好机会。对一组思想进行严谨的提炼、概括、总结，必然能够推动思维的发展。如果你已经得出了一个概括性的思想，你就可以在该思想的基础上用以下两种方式延续你的思路：\\n\\n- 对其作进一步评论（演绎法）。\\n- 找出与之类似的思想（归纳法）。\\n\\n但是在用这一过程产生新的概括性思想之前，你必须保证，原有的概括性思想，是根据一个适当的思想组，合理概括出来的，如图7-1所示。\\n\\n举个例子，我的一位同事曾这样写道：“该公司存在两个组织问题。”然后列举这两个问题。这个句子就是一个“缺乏思想”的句子，所以他必须重写这句话。如果这句话下面列举的两个问题：1.都是组织问题；2.存在某种逻辑顺序，那么重写这句话是很容易的。但是，他似乎没有发现这两个问题之间存在任何逻辑顺序\\n\\n因此，多花一些精力从思想组提炼出正确的概括性思想非常重要。那么，应该怎样进行正确的概括呢？首先，如上一章所述，你必须检查该组思想的分组基础，保证其相互独立，完全穷尽（即：其逻辑顺序呈现的是一个有效的过程、结构或分类）。然后，你需要确定准备得出的概括性思想的语句类型。\\n\\n思想的表达方式与其分组基础无关，可以是行动性语句，即告诉读者要做什么事；也可以是描述性语句，即告诉读者关于某些事的情况。\\n\\n- 概括行动性思想（介绍采取的行动、行为、动作、步骤、流程）时，应说明采取行动后取得的结果（效果、达到的目标）。\\n- 概括描述性思想（介绍背景、信息）时，应说明这些思想具有的共同点的含义（共同点的意义）\\n\\n!Untitled\\n\\n这句话想让我们做什么？我们怎样才能知道自己已经达到了这一目标？你能将一个已经“培养了全球意识”的人，与一个还没有做到这一点的人区分开吗？如果不能，就说明你根本无法了解作者的实际意图。更糟的是，你\\n\\n也没有办法想出为实现这一目标所必须采取的措施，即你无法回答“如何做到”这个问题，无法填写图7-5中的方框。从这个意义上说，上面这句话没有任何意义。当然，也可能有人认为这句话还是有情感价值的\\n\\n如果你用模糊的措辞表达一系列行动步骤，问题就更严重了，因为人们不可能理解你究竟想让他们做什么。例如：\\n\\n为了就事论事，进行健康有益的辩论，尽可能降低将矛盾演化为冲突，特别工作组必须做到以下几点：\\n\\n- 处理好各种个人态度问题。\\n- 与公司员工建立良好关系。\\n- 培养良好的谈话技巧。\\n- 有效安排和进行谈话。\\n- 学会求同存异。\\n\\n特别工作组究竟要做到什么，才能保证进行健康有益的辩论呢？如果他们做到了这5件事情，将会达到什么结果呢？我们无法想象最终的结果\\n\\n使用明确的语言表达最终结果的必要性，无论怎样强调也不过分。除非你使用的语言达到了这一要求，否则你根本无法客观判断你列出的行动、步骤，是否包括了所有应当包括的步骤。\\n\\n有人认为，他们可以通过使用疑问句绕过必须使用明确的语言这个要求。他们认为，对问题的回答将引出明确的结果。但是，这种方法实际上只能使你的思路更复杂，因为你避免不了要想象最终的结果，并确定这一结果是否正是你想要的。\\n\\n请看下面的例子：\\n\\n为了使内部和外部股东都能够看到战略联盟带来的广泛利益，并提供支持（股东同意），必须回答以下问题：\\n\\n1．相关的股东是否相信此结盟行动符合其利益？\\n\\n2．此举将对公司的声誉产生哪些影响？市场将作出什么反应？\\n\\n3．主要高层管理人员是否愿意结成联盟（意识到此举不会对其权力和位置构成威胁）？\\n\\n4．如果联盟可能对公司的任何个人或群体构成威胁，如何说服他们继续努力工作，以便在形成联盟后取得成功？\\n\\n5．客户、供应商、现有的合作伙伴、金融机构和竞争者将如何反应？\\n\\n判断这种提问方式是否有意义的最简单的方法，就是想象你派出5名亲信为你收集有关信息。他们每一个人回来后都会向你提供一份答案。你会得到5类不同的信息，这些信息之间也未必存在关联\\n\\n你也可以想象，只有一名不很精干的同事为你提供帮助，时间非常有限，并且没有任何预算。什么方法能够最有效地指导该同事合理利用时间，最终使你得到一份让使股东们看到战略联盟的好处的计划？你可以这样做\\n\\n!Untitled\\n\\n这样，所有的人都能够理解这一过程，而且这一过程的第一步（第一列）已经完成了。你现在只需派出一名同事完成第二步（第二列），完成第二步之后你进行第三步（第三列）\\n\\n际上，这样做根本没有意义。我们知道，行动、步骤、流程等是无法分类的；某些行动被归于一组，完全是因为它们能够共同产生某一特定结果。对行动、步骤等分类必定会造成重复，因为“任务”、“目标”和“利益”之间并没有本质的区分。组织各种行动、步骤等唯一合理的方法，就是根据其产生的结果归类分组。\\n\\n举个例子，一家咨询公司受聘为一家企业培训其员工的战略规划能力。这家咨询公司把建议采取的行动分了类。该咨询公司建议执行6项任务，这6项任务又要求公司制定5个目标，实现这5个目标以后，可以取得三大利益。各行动步骤简化如下\\n\\n一个好方法可以理顺这样的一系列思想，即尽量砍去其修饰成分，只留下最本质的核心内容，然后找出其中的重复部分。如果我们对以上思想采用这个方法，会得到什么呢\\n\\n现在，重复的部分和不同的抽象层次都比较容易看出来。你可以根据行动产生的结果重新建立一个金字塔结构\\n\\n!Untitled\\n\\n然后，只要稍加思考，你就可以得出下面的思路：\\n\\n我们将迅速使贵公司掌握战略规划知识。（2、7、13）\\n\\n1．对两个产品咨询小组，进行有关战略规划方法和概念的培训。（1、12）\\n\\n2．根据贵公司现有的规划机制调整教学内容。（4、5、8、9）\\n\\n3．在下一个规划周期中，与贵公司员工一起应用所学概念。（3、6、10、11、14）\\n\\n将某一系列行动的所有步骤都理顺后，你将遇到处理行动、步骤等最艰难的一关——总结概括其结果（效果）。对于这一关，我没有简便易行的好办法，只能说：\\n\\n- 该组行动、步骤之间必须相互独立不重叠，完全穷尽无遗漏。\\n- 总结概括性语句，必须说明完成各行动步骤后导致的直接结果，且措辞必须明确、具体。\\n\\n!Untitled\\n\\n!Untitled\\n\\n你可以用这两条规则衡量你的每一步思路。在前一个例子中，如果该公司拥有训练有素的人员、合适的规划机制和战略规划知识手册，显然就具有了制定正确战略的能力，但这并不等同于该公司能制定出正确的战略。同样，我提出的这两条规则也不能保证你肯定能完成正确的概括。\\n\\n我只能介绍一些我修改过的例子，并告诉你我的想法。下面这个例子使用的语言比较模糊。\\n\\n为了提高在伦敦市场上的股票销售量，我们应当：\\n\\n- 按地区、按客户的收入潜力，将客户排序。\\n- 确定希望在各地区达到的市场占有率。\\n- 相应调整销售力量的分配。\\n\\n一看到这一组行动步骤，我就想：“做这些事情的目标不明确，因为即使我只多卖了一份股票，也算是提高了销售量。”然后，我又提出疑问：“如果我对客户收入排了序，确定了市场占有率，也重新分配了销售力量，将会达到什么结果呢？或者说，如果我不这样做，会有什么结果呢？”然后我就想到这样修改其总结概括性语句：\\n\\n为了提高伦敦市场上的股票销售量，我们需要将资源重点集中在潜力大的客户身上。（怎样做到这一点呢？）\\n\\n这个概括性的句子读起来显然比原来的句子更有意义，因为这个句子提出了一个观点，而原来的句子只是一个“缺乏思想”的句子。这个句子迫使读者提出“怎样做到”的疑问，因而使读者更容易理解作者随后提出的观点。作为作者，你自己也可以检查所列的步骤是否能够达到这一结果\\n\\n再举一个语言模糊的例子：\\n\\n为了改善蓝领阶层的培训状况：\\n\\n- 应向企业决策层表明，政府非常重视对劳动者的培训。\\n- 应设计一个培训课程的总体框架，让培训讲师准备。\\n- 应通过劳动者向上施压。\\n\\n在本例中，因为每一个句子都比较复杂，可以先找出每一个句子的核心内容，然后继续下一步。首先，找出每一个句子的真正主题：\\n\\n- 企业决策层\\n- 培训者\\n- 劳动者\\n\\n然后，问问自己：为什么我们只讨论这3个主题，而不包括其他的？这些主题之间有什么共同之处？答案是：这3个主题似乎都是培训体制中的参与主体。\\n\\n接下来看看每个句子是怎样对各自的参与主体采取行动的：\\n\\n- 向其表明重要性\\n- 为其设计总体框架\\n- 通过其施加压力\\n\\n这3种行动有什么共同之处？答案是：它们都是某种推动因素。现在，我们可以比较肯定地将上面的例子概括如下：\\n\\n为了改善蓝领阶层的培训状况，我们必须提供一些推动因素，促使培训体制中各参与主体支持培训。（我们应当怎样做？）\\n\\n同样，这种概括也比原来的概括更有意义，既能引导读者按你的思路去理解，也能使你检查行动步骤是否完整、全面\\n\\n产品开发面临的问题包括：\\n\\n1．如何将公司和市场希望该产品应具有的功能，融入产品开发过程？\\n\\n2．如何针对不同项目安排和分配公司的资源？\\n\\n3．如何缩短产品开发时间，同时考虑到营销人员的需求？\\n\\n4．如何组织和管理公司的研发资源，以满足投产时间的要求？\\n\\n5．如何使公司内、外了解有关信息，以尽量提高产品上市的力度？\\n\\n6．如何促进研发人员和管理人员在产品开发方面的合作？\\n\\n按照我们一贯的步骤，首先应将各个句子尽量简化，以便于思考：\\n\\n1．开发适当的产品\\n\\n2．合理分配资源\\n\\n3．高效完成\\n\\n4．及时完成\\n\\n5．有效营销\\n\\n6．研发／管理人员合作\\n\\n然后，我们再将其划分为一些小组：\\n\\n1．确定满足市场需求的产品\\n\\n－包含所需的功能（1）\\n\\n－满足营销人员的需要（3b）\\n\\n2．尽量在最短时间内完成开发（3a）\\n\\n－合理分配资源（2）\\n\\n－使研发时间符合投产要求（4）\\n\\n－加强研发人员及管理人员的合作（6）\\n\\n3．以最有力有效的方式将产品推向市场（5）\\n\\n我们可以合理假定，该作者希望表达的思想应当如下：\\n\\n产品开发面临的主要问题，是我们能否比竞争对手更有效地应对市场需求。（我们如何迅速有效地应对市场需求？）\\n\\n1．我们能否发现市场需要的适当的产品？\\n\\n2．我们能否缩短产品上市前不必要的时间延误？\\n\\n3．我们能否进行有效的营销，以最大限度地提高销售量？\\n\\n**提示**\\n\\n直接总结行动产生的结果：\\n\\n- 该组行动、步骤之间必须相互独立不重叠，完全穷尽无遗漏。\\n- 总结概括的语句，必须说明完成各行动步骤后导致的直接结果，且措辞必须明确、具体。\\n- 先用明确的语句表述各行动、步骤、流程等，然后区分不同的抽象层次，再直接从各行动、步骤、流程等总结概括出行动的结果。', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6da654fb-5639-44dd-bd0c-adfcd2c3ee2d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n杂 拖延相关\\n\\n100天行动中的三个要求，其中一项是10点半前完成三件最重要的事，实际这种方式还有另一个时间管理术语-吃青蛙，这个术语来源于博恩·崔西的《吃掉那只青蛙》。\\n\\xa0\\n这本很古怪的书名是根据这条谚语得来的：“如果你每天早上做的第一件事就是吃掉一只活青蛙的话，那么你就会欣喜地发现，这一天里再没有什么 比这个更糟糕的事情了。”\\n\\n1.确立一个可操作的目标（可观察、具体而实在的），而不是那种模糊而抽象的目标。\\n\\n不是：“我要停止拖延。”\\n\\n而是：“我要在九月一日之前打扫和整理我的车库。”\\n\\n2.设定一个务实的目标。不要异想天开，而要从小事做起。不要过于理想化，而要选择一个能接受的程度最低的目标。\\n\\n不是：“我绝不再拖延！”\\n\\n而是：“我会每天花一个小时时间学习数学。”\\n\\n3.将你的目标分解成短小具体的迷你目标。每一个迷你目标都要比大目标容易达成，小目标可以累积成大目标。\\n\\n不是：“我打算要写那份报吿。”\\n\\n而是：“今晚我将花半个小时设计表格。[…]”\\n\\n“明天我将花另外半个小时把数据填进去，再接下来一天，我将根据那些数据花一个小时将报告写出来”\\n\\n4.现实地（而不是按照自己的愿望）对待时间。问自己：这个任务事实上将花去我多少时间？我真正能抽出多少时间投入其中？\\n\\n不是：“明天我有充足的时间去做这件事。”\\n\\n而是：“我最好看一下我的日程表，看看我什么时候可以开始做。上次那件事所花的时间超出了我的预期。”\\n\\n5.只管开始做！不要想一下子做完整件事情，每次只要迈出一小步。\\n\\n记住：“千里之行始于足下。”\\n\\n不是：“我一坐下来就要把事情做完。”\\n\\n而是：“我可以采取的第一个行动是什么？”\\n\\n“6.利用接下来的15分钟。任何事情你都可以忍受15分钟。你只能通过一次又一次的15分钟才能完成一件事情。因此，你在15分钟时间内所做的事情是相当有意义的。\\n\\n不是：“我只有15分钟时间了，何必费力去做呢？”\\n\\n而是：“在接下来的15分钟时间内，这件事的哪个部分我可以上手去做呢？”\\n\\n7.为困难和挫折做好心理准备。当你遭遇到第一个（或者第二、第三个）困难时，不要放弃。困难只不过是一个需要你去解决的问题，它不是你个人价值或能力的反映。\\n\\n不是：“教授不在办公室，所以我没办法写论文了。我想去看场电影。”\\n\\n而是：“虽然教授不在，但是我可以在他回来之前先列出论文提纲”\\n\\n8.可能的话，将任务分派出去（甚至扔掉不管！）。你真的是能够做这件事的唯一人选吗？这件事情真的有必要去做吗？\\n\\n“记住：没有人可以什么事情都做——你也是。\\n\\n不是：“我是唯一一个可以做好这件事的人。”\\n\\n而是：“我会给这件事找个合适的人来做，这样我就可以去做更重要的事了。”\\n\\n9.保护你的时间。学会怎样说不，不要去做额外的或者不必要的事情。\\n\\n为了从事重要的事务，你可以决定对“急迫”的事情置之不理。\\n\\n不是：“我必须对任何需要我的人有求必应。”\\n\\n而是：“在工作的时候，我没必要接听电话。我会收看留言，然后在我做完事情后再回电。”\\n\\n10.留意你的借口。不要习惯性地利用借口来拖延，而要将它看做是再做15分钟的一个信号。或者利用你的借口作为完成一个步骤之后的奖赏。\\n\\n不是：“我累了（抑郁/饿了/很忙/很烦，等等），我以后再做。\\n\\n而是：“我累了，所以我将只花15分钟写报告，接下来我会小睡片刻。\\n\\n11.奖赏你一路上的进步。将奖赏聚焦于你的努力，而不是结果。小心非此即彼的思维方式：你可以说杯子是半空的，也可以说它是半满的。\\n\\n记住：即便是迈出一小步也是进步。\\n\\n不是：“除非我全部完成，否则我就会感觉哪里不对。”\\n\\n而是：“我已经走出了几步，而且我做事非常努力”\\n\\n12.将拖延看成是一个信号。停下来问自己：“拖延传递给我的是什么信息？”\\n\\n不是：“我又在拖延，我恨我自己。”\\n\\n而是：“我又在拖延，我的感受是怎样的？它意味着什么？我可以从中学到什么？”\\n\\n“记住：你能够做出自己的选择。你可以拖延，你也可以行动。\\n\\n即便在你心里不舒服的吋候，你还是可以行动。\\n\\n以往的历史无法决定你当下要怎样做。\\n\\n你可以从学习、成长和挑战自己中获得快乐。\\n\\n你不必等到完美之后才觉得自己具有价值。”\\n\\n反效率信息包括：\\n\\n不得不做：传递出抵触和受害的消极情绪；用我选择，我决定做，这样的信息，可以把能量导向单一目标。有时候在两个决策中间自我推拉是不必要的。这种对话产生了理想状况和现实的负面对比。说出『我选择做XXX』的时候会产生一种自我感召的力量\\n\\n用『我什么时候开始』取代『我需要完成』\\n\\n用『我可以慢慢试错』取代『我必须完美』\\n\\n克服拖延，难的地方是从娱乐状态切换回工作状态。一旦已经处于正常的工作状态中，你就会发现，自己抵御诱惑的能力增强了。你劝说已经处于工作状态中的自己继续工作，要远比劝说自己从娱乐状态回到工作状态容易。如果觉得坐半小时太难，你也可以说服自己先工作十分钟。重要的是，要启动工作状态。让娱乐中的自己克服畏难情绪\\n\\n**对未来的真正慷慨，是把一切献给现在。**\\n\\n正如加缪说的，不要活在过去的阴影里，不要活在虚无缥缈的未来中，踏踏实实过好现在的生活。但现实中有太多人，嗟叹于过去，惧怕于将来，把大把大把的青春，白白浪费掉。时间无情，别等热血冷却\\n\\nMy dad always said to me (and this is a rough recitation) as long as you know you\\'ve pushed yourself as hard as you can and have left nothing on the table you cant be disappointed in yourself. He would see me studying and working as hard as I could and sometimes the results that came back were not so good but I always knew that he wasn\\'t disappointed in me and I wasn\\'t disappointed with myself either.\\n\\n麦格尼格尔在《自控力》中讲到，和我们的常识相悖，当人们屈服于诱惑时，不让他们感觉到内疚，相反，让他们感觉到快乐，居然能够增加人们抵御诱惑的能力。因为相比于内疚，自我谅解反而更能增强责任感。一旦摆脱了内疚和自责，我们反而能够思考为什么会失败，而不是简单地把原因归于自己的无能，我们也不用消耗大量的心理资源去安抚内心的挫败感了。这样，我们反而有更多的心理资源增强自控力，在和诱惑的战争中重整旗鼓。\\n\\n通常我们认为意志力是戒律、规条，是理性的东西。可是谈着谈着，我们却谈到爱了。事实上，要增加自控能力，同样离不开爱和自我怜悯这些更感性、更柔软的东西。\\n\\n如果你觉得自己可以改变，便会不断寻找支持自己的例证；如果你觉得无法改变，那么这篇文章在你看来无非是万千鸡汤中又新鲜煲出的一碗而已\\n\\n\"When conditions are good, you feel great, motivated and everything is going your way, it is easy to keep going. Everyone can do it. But...what do you do when things turn around and you are having a really terrible day? That is when it matters most, your decisions during those moment separates winners from the losers. Learn to endure during difficult times. When you feel like giving up but you keep going, this is when you are separating yourself from your old self. This is when you are growing. If you want to be extraordinary, don\\'t do what everyone else does during difficult times.\"\\n\\nloser往往在failure和survive直接挣扎，一旦被deadline逼迫努力了一下达到survive的阶段，就迅速堕落下去，下一次failure就是必然的；winner相反，会在survive和success之间波动。他们的succuss过去之后，会继续努力，不会满足停留在survival level的生活\\n\\n本质上，是学会将对时间的敏感度从24小时，精炼为25分钟。对于这25分钟发生的行为及其结果，尝试次数多了，预测能力极高，刚开始动笔，就清楚大约在第20分钟会写完。此时此刻，大脑自然收到正反馈了。这就是在时间维度上利用大脑的既有偏好。通过对时间的重组与倒计时，新鲜刺激也比以往更多。\\n\\n现在已经养成了做事情设闹钟的习惯。\\n\\n我基本上知道到单位的平均时间，洗漱时间，吃饭时间等等。\\n\\n所以，我每次回复和写计划的时间也会被我了解到。\\n\\n这样的好处便是：\\n\\n我的计划基本上不太会出现完不成，或者太忙的情况。【除非是突发事件，查找类的项目或者探索性的项目】\\n\\nPS：就是想说，其实会养成一种对自己的深刻了解的直觉\\n\\n**1、重新认识未来的自己：现在的我 = 未来的我2.0**\\n\\n高估未来的自己，是一种很正常的现象。凯利·麦格尼格尔教授在《自控力》这本书中提到过，脑成像研究发现我们在考虑现在和未来的自己时，运用的是大脑中不同的区域。当人们想象未来时，想在考虑别人的特征一样，大脑中想象自己的区域毫无反应。\\n\\n心理学家们用“未来自我的连续性”这个概念，表示“你在多大程度上认为，未来的自己在本质上和现在的自己是一样的”。较高的“未来自我的连续性”会让人现在就做到最好。**“未来的自我连续性”较弱的人，会倾向于把不想做的事情推给未来的自己，而让现在的自己享受即时的满足。**\\n\\n**我们要意识到，明天和今天的自己毫无区别，一个小时前的自己和现在的自己也毫无区别。今天不想做的事，到明天并不会突然变得乐于去做。现在的我 即 未来的我2.0，现在我会怎么做，未来也会一样。**\\n\\n所以当电脑里还躺着未完成的报告，却忍不住想打游戏时，不要再问自己“我现在想不想打游戏？”，而是问自己“我想不想接下来整天整夜都在打游戏？”。\\n\\n当拖延着不去学习或工作时，不要问自己“我是想今天做还是明天做？”，而是要问自己“我是不是打算永远拖延下去？是不是打算让『现在这个自己』承担后果？”。\\n\\n不过，要承认这一点现实，好像打破了一些对未来美好的期望，还真是有点忧伤啊！\\n\\n**2、重新看待低刺激枯燥的学习和工作：它们真的那么令人讨厌、让人痛苦吗？**\\n\\n**很多时候，令人讨厌的不一定是学习和工作本身，而是不知道这件事到底有没有意义/价值的不确定感。**\\n\\n不确定背了这么多单词考试到底会不会用到，\\n\\n不确定加班做出的方案会不会被老板认同，\\n\\n不确定每天的练习究竟会不会让自己有所成长，\\n\\n不确定付出这么多代价去学习和工作是否会得到应有的回报。\\n\\n**这种不确定感，成为我们心里的噪音，让我们很难静下心来。**\\n\\n**面对眼前的学习和工作任务，当我们不去想结果时，往往会发现事情没有想象中那么难。**\\n\\n这样想着，迈出第一步后，你会发现：\\n\\n这本总是看不下去的书，慢慢读倒也还好，没有那么晦涩嘛；\\n\\n这个拖了很久的报告，拆解下来好像也没那么复杂嘛；\\n\\n那个难搞的方案，好像有点初步的思路了哦。\\n\\n**一旦摆脱令人不安的不确定感，开始专注于事情本身，我们反而能够更加放松和平静。**\\n\\n慢慢的，完成了这个一直抗拒的任务的一小部分，也有了少许的成就感；\\n\\n慢慢的，沉浸在当下这一件事情中，感到一种和高刺激娱乐不同的快乐，这种快乐中带着自我认同，和发自内心的充实和满足；\\n\\n慢慢的，也从这些学习和工作中找到了乐趣，是不是不知不觉中也变成了高刺激的事情呢？\\n\\n不过，摆脱不确定感的噪音，完全做到并不容易。我们可以从这个角度想：决定事情结果的很多因素（如客户会不会满意、考试会不会考到），并不是自己能控制的。**与其在不可控的事情上焦虑和担忧，不如把精力放在可控的事情上，而当下唯一可控的就是自己要不要开始做这件事。**\\n\\n最后总结一下，\\n\\n**从现在起，不再逃避低刺激枯燥的学习和工作，而是选择面对，它们其实没那么讨厌；**\\n\\n**不再对未来的自己抱有不切实际的幻想，而是承认现在和未来的我是一样的这个现实；**\\n\\n**不再纠结于任务完成后的结果和回报，而是关注任务本身。**\\n\\n这几句话看上去简单，但做到却不容易，需要我们改变以往的思维模式和惯性。也许不能马上做到，但可以经常提醒自己，慢慢尝试。\\n\\n奇克森特米哈伊（1997）提出，具有“自主能动”人格的人更可能在完成任务或实现目标时，获得心流的体验。他所谓的具有自主能动人格的人，实际上就是**那些更少自我意识、更具有成就取向，及责任心更强的人**。这一看法，也得到后续许多研究的支持。\\n\\n1. **低自我（Low self-conscious）**\\n\\n自我意识（self-conscious），指的是一个人对于自身外在及行为表现的在意程度。高自我意识的人，通常因为过度在意自己在他人眼中的表现，而无法集中精力在所做的事情上。\\n\\nLee（2005）对韩国大学生的研究发现，那些过度自我意识的人往往因为太在意别人的看法，而做事犹豫不决、拖拖拉拉。相比这些人，低自我意识的人则更少拖延，也有更多可能进入“心流”的状态。\\n\\n1. **成就取向**\\n\\nEisenberger等人（2005）的研究发现，成就取向的人通常能够更好地协调工作中所遇到的挑战及自身所拥有的技巧。他们一方面愿意迎接新的挑战，另一方面也愿意为了挑战付出努力学习新的技能。\\n\\n不仅如此，成就取向的他们，还会从自己的努力与成就中获得满足感，而这又会使得他们更愿意参与到那些更具挑战的、需要不断学习的、也更可能产生心流的事情中去。\\n\\n1. **责任心强**\\n\\nUllen与Csikszentmihalyi等人（2012）对心流与大五人格的相关性研究发现，具有某些人格特质的人，更容易获得心流。Csikszentmihaly等人（2012）认为这主要是因为通常责任心更强的人，喜欢解决问题，也更愿意花时间做有挑战性的事，因而有更多机会进入心流。Pychyl（2008）则认为，责任心更强的人，更能够自我控制、专注做事，也可能是这类人更容易获得心流的原因。\\n\\n另外，他们还发现，情绪稳定性高的人，也更多地认为自己曾获得过心流。他们认为可能的原因是，不稳定的情绪干扰了人们注意力的集中，也使人无法进入心流中狂喜的、忘我的状态。\\n\\n1. **成长环境**\\n\\nCsikszentmihaly（2008）还指出，\\n\\n当家庭中父母能够与孩子明确地沟通他们的期待；\\n\\n孩子能够感受到父母在乎自己、对自己所做的事情感兴趣；\\n\\n孩子总能有机会对各种各样的可能性进行探索和选择；\\n\\n父母能给予孩子足够的信任，允许他们尝试，并在孩子面临挑战时，给予他们肯定和鼓励；\\n\\n这些家庭教养方式培养了人们的自主能动人格。\\n\\n有些心理自助书籍的作者只会照搬一切，他们把“激发动力”塑造成促进行动、改善生活的唯一途径，而很少会有人提出质疑，因为这是标准程序。人们纷纷登录励志网站寻找他们的“励志救星”，导致这些网站也跟着发展起来。我承认，“激发动力”是个比“什么都不做”更好的策略，但想做些什么并不难。\\n\\n举个例子，锻炼的预期效果可能有三种来源：动力、意志力或习惯。虽然每个行为都是动力和意志力结合后的产物，但我们往往会更依赖其中某一个。那么，一边努力激励自我，一边对自己说你无论如何都得做这件事（我们失败后总会这样），你不觉得这自相矛盾吗？\\n\\n坚信必须有动力才能行动是一种毁灭性的习惯。如果只是“我想激发动力”，那它就不成问题；可是如果不激发动力就什么都做不了，那它就是个问题。它是坠入“懒惰旋涡”的完美路线。懒惰会让你感到什么都不想干，如果你总是在什么都不想干的同时奉行动力法则，那你就会一直懒下去，没有出路。\\n\\n“动力高于行动”的观念是深入人心的，可没有谁规定你的感受和行为必须永远匹配。它创造了一种让你处处受限、时刻受挫的生活方式。\\n\\n“热情递减法则”不是一条真正的法则，而是我创造的术语，因为它比对应的经济法则“边际效用递减法则”（The Law of Diminishing Marginal Utility）更好理解。这条经济法则认为，吃第五块比萨时的愉悦感会略低于吃第四块时的，吃第四块时的又略低于吃第三块时的。我们在进行重复行为时也存在同样的现象。\\n\\n当一种行为成为一个习惯，你会变得没有刚开始时那么情绪高涨，这种行为甚至开始有些乏味无聊。事情当然会变成这样了。杰里米·迪安（Jeremy Dean）在《习惯:改变命运的关键力量》（*Making Habits，Breaking Habits*\\xa0）中写道：“习惯不仅无法被认知捕捉到，而且也不会掺杂情绪。……很奇怪，人们进行习惯行为时是不带有情绪的。”温迪·伍德博士及其同事们在德克萨斯农工大学进行的一项研究中记录了这样的情况：当参与者做出惯性行为时，毫无疑问，他们比平时更缺乏情绪变化。 这就是“需要动力才能行动”的想法在培养习惯时反而会帮倒忙的原因。重复不会让我们兴奋地行动起来，反而会让我们趋于平静。习惯带来的好处在于抵触情绪少了，自发性多了。迪安表示：“惯性行为不会引发强烈情绪，这是它的一大优势。”的确是这样，因为任何依赖人类情感的东西都是完全不可靠的。\\n\\n微习惯的好处在于无压力\\n\\n现场直播最新实例：今天打了 3 小时篮球以后，我特别累。大脑和身体都在告诉我今天不可能写作了，我快要睡着了，意志力为零。可我的目标是写 50 字，小到无法抗拒。所以，现在我又一次超越了原定目标（而且很清醒）。很多时候你的确很累，但给大脑或者身体找点事情做，身体就会被唤醒.\\n\\n绝大多数的人都会遇到拖延问题，在某本书上看过一项调查研究，70%的大学生都存在拖延的情况，在正常的成年人中也有大约20%的人每天出现拖延情况。\\n\\n拖延症与智力并没有什么关系，据说相对论提出者爱因斯坦也是拖延症患者；研究进化论的英国生物学家查尔斯.达尔文也是拖延症患者；诺贝尔经济学奖得主阿克洛夫也是拖延症患者，他准备把朋友的行李从印度邮寄到美国，可以这一拖竟是8个月，以至于他在1991年的论文《拖延和顺从》中写道“八个月里每早醒来我都决定第二天早上去把箱子寄给斯蒂格利茨。”\\n\\n其实拖延就像是一种心理上的逃避，一件事情，哪怕只有一点点的困难，也有可能让我们利用其它事情为借口，进行逃避。 而结果是，越拖下去压力越大。\\n\\n而目前常用的战拖方法，大体可以划分为两类：一类注重内心成长和价值观梳理，另一类注重任务解决和时间管理。\\n\\n我对任务解决和时间管理这类有些研究，目前比较管用的方法分为三种：规律的生活习惯、小系统和五分钟策略。这种应对方法无法解决所有的拖延问题，偶尔还会失控，但对我而言，努力把生活中重要事项做好，控制好精力，通过习惯来达成自律，能大大减少拖延对我的影响。\\n\\n**用规律的生活习惯来对抗拖延**\\n\\n**We need to develop and use the structure and routines in our lives to help, not hinder, our goal pursuit. Cultivating healthy practices and habits on a daily basis will pay dividends throughout our lives, particularly on those days when, for whatever reason, “we don’t feel like it.”**\\n\\n当人们接纳（这是一种通过正念发展出来的态度）自己的情绪时，他们也许将有能力把这些瞬时情绪视作一种采取合适应对举措的信号，而非依赖回避等惯性应对机制\\n\\n这方面的表率是村上春树，他在《当我谈跑步时我谈些什么》书写到：当他成为职业小说家，所做的第一件事就是早睡早起，彻底的改变生活状态，清晨五点起床，晚上10点前睡觉。\\n\\n通常的时间安排如下：\\n\\n在清晨的几个小时内用来写作和处理重要事情，随后的时间用来运动和处理杂事，打理那些不需要高度集中的工作。 日暮时分不再工作，用来读书、听音乐，放松精神，早点睡觉。\\n\\n从他开始这样做到现在，二十多年中一直工作顺利，效率甚高。\\n\\n欧内斯特·海明威说过类似的话：持之以恒，不乱节奏，对于长期作业实在至为重要。\\n\\n一旦节奏得以设定，其余的问题便可以迎刃而解。 然而要让惯性的轮子以一定的速度准确无误地旋转起来，对待持之以恒，何等小心翼翼亦不为过。\\n\\n当你形成良好的生活节奏，做事时会非常容易集中注意力，减少拖延。对来我说一天最容易拖延的事情大都是坚持习惯和一些重要事务，我会把尽量在精力最充足的时间段优先完成习惯清单和当天最主要的六件事务，努力完成主要的事情。对不重要的事，进行拒绝和拖延。\\n\\n**习惯清单**\\n\\n里面大都是一直在坚持的习惯列表，比如阅读、写作、跑步、腹部运动、外语、早起、微信之类的事项，这个清单是第一优先项，从早上起床一有时间就安排时间去做，每做完一项心情都会变得很好，通过这个清单来完成这些习惯，减少自己的拖延问题。\\n\\n我正在微信（read01）上发起100天行动，通过坚持100天来培养一个新的习惯，喜欢一起参加。\\n\\n**要事清单**\\n\\n通常我会在上班前，列出当天的6件主要事务，尽量在10点半之前把这六件主要事情完成，而10点半这前正是我一天中效率最高的几个时间段之一，会尽量把当天最重要最困难的事务放在刚上班的来完成，这个时间段基本上没人打扰，非常容易集中注意力，一旦搞定当天的几件重要事务，余下的事务就容易一点，也不会造成太大的压力。\\n\\n当我把这二个清单中的事务全部完成之后，会解决一天中最主要的拖延压力。\\n\\n**利用小系统来减少拖延**\\n\\n拖延的其中一个因素来源于缺乏足够的技能或流程不熟悉，对一件做起来很麻烦的事情，都会下意识开始逃避，这时利用小系统可以非常有效的解决这个问题。\\n\\n小系统：针对某一个具体的问题，形成详细或固定的流程化处理方案。\\n\\n分享几个例子：\\n\\n我以前的日常工作中，需要经常参加项目招投标，写项目投标书。在刚开始的时候，经常要花上二、三周或更长时间才能完成一份标书，每次都非常的痛苦和花费很大的时间和精力，后来我总结了一个标书完成清单，里面一共有几十项内容。里面记录了非常详细的步骤。通过这个清单把写标书这项任务的完成时间压缩到一周以内，最快一回只花了三天时间，同时还可以把整个标书的任务进行划分，由几个人同时进行。\\n\\n另一个是例子是打电话。这是我的打电话事务流程清单，除了紧急的电话之外会把当天的工作电话在统一时间集中打电话，进行批量处理。\\n\\n1、打电话前，先准备好要说明的问题，在纸上列出要点\\n\\n2、准备电话联系清单，完成一项划掉一项。\\n\\n3、早上上班前，和晚上下班前领导会在\\n\\n4、告诉对方我是谁及来电的目的\\n\\n5、别啰嗦，简明扼要\\n\\n6、以祝福对方的友好语言结束电话“非常高兴能与你通话” “谢谢你”\\n\\n还有邮件处理，在很长一段时间内每天收到邮件都在100-200封左右，一直在执行清空收件箱的策略，目前处理一封邮件一般在10-20秒之内，一天有二、三次的集中回复时间，处理邮件的时间不超过1小时。\\n\\n类似的小系统，总结了几十项，通过这些小系统把一件事行云流水般的完成，能减少很多事情的拖延问题。\\n\\n**五分钟策略**\\n\\n很多拖延者通常面临过这样的场景：\\n\\nn 工作必须找到状态，工作的环境必须近乎于理想\\n\\nn 我只有在自己想工作的时候才能工作\\n\\nn 要么不做，要做就必须做到最好\\n\\nn 我必须领先于他人，把一切事情都纳入掌控中，不能有瑕疵，否则我就会失去所有机会\\n\\nn 我要等一个合适的时间或机会才能开始\\n\\n针对上面这些情况有个很好的办法来解决这个问题，如果一项事务你已经拖延了一段时间还没有开始，那你可以在当天的任务清单加几个5分钟时间专门用来进行这项事情，只要坚持5分钟，就可以停止，然后去做别的。\\n\\n比如说你可以开始进行5分钟，然后去玩游戏、看小说、聊天、看美剧，具体的次数不限，可以是2个也可以是10个。只要定下时间就一定要去执行，这个时间越早越好，第一个最好是当天早上一起床或刚到公司。\\n\\n因为只需要坚持5分钟就可以做其它事，然后不会给自己找什么借口。只要你开始做下去，就会发现坚持5分钟之后，状态就来了，然后一口气工作下去。一天中多做几次，就能把一直拖延的事情进行下去。\\n\\n5分钟这个时间段最适合我自己。这个时间段可以是3分钟、5分钟、10分钟、15分钟，关键要看你的情况。\\n\\n**5分钟策略这种方法需要反复练习，一直练到形成条件反射。当你发现自己处于拖延场景时，马上停下手中的一切事情，开始一个5分钟。或者在闹钟上定上一个5分钟计划，时间到了立刻开始，拒绝所有的借口，不管你找什么借口，一定要做完这个5分钟再做其它事情。**\\n\\n“1）我做的事情直接反映了我的能力；\\n（2）我的能力水平决定了我作为一个人所具有的价值——也就是说，我的能力越强，我的自我价值感越高；\\n（3）我做的事情反映了我的个人价值。”\\n\\xa0\\n“如果我付出所有努力，而那份小结还是不够好的话，我认为我无法承受这样的结局。\\n就像比瑞博士指出的那样，拖延打断了能力与表现之间的等号。\\n自我价值感＝能力≠表现”\\n“表现不再等同于能力，因为其间缺少完整的努力。这意味着不管最终表现如何，自我价值感与能力之间的关系还能得以维系”\\n\\xa0\\n\\xa0\\n“一个适应型的完美主义者对自己要求很高，并且相信自己的表现能够与之相符。这样一种能够如愿以偿的完美主义，感觉上去就像一个人本性的一部分，也是一个人自尊的基石。然而，一个适应不良的完美主义者对自己要求也很高，但是却对自己不抱希望。在适应不良型的完美主义中，你对自己的要求跟你对自己表现的期待之间存在着一种矛盾，所以你更容易去自责，也更容易感到消沉，你的自尊因而也处于较低的水平\\n\\xa0\\n\\xa0\\n“那些喜欢拖延的完美主义者，有一些他们所钟爱的信念。这些信念甚至在你都没有意识到的情况下操纵着你，而且它们看上去似乎颇为冠冕堂皇。但是它们可以让你的生活极度失望，不但没有对你的进步起到作用，反而为你的拖延鸣锣开道。”\\n\\xa0\\n“事实真相是，很多完美主义者憎恨在竞争中失败，所以他们尽量避免参与到跟别人直接竞争的活动中。”\\n\\xa0\\n\\xa0\\n“不论什么时候，只要我们在做自己想做的事情，就会感到自己是成功的。“及时追随目标”会将你渐渐引向其他定义的成功，同时，它也可以让你在付出努力的时候能感受到成功。然而，一个拖延者却不能及时地追随自己的目标，所以每当他们再次让自己失望的时候，他们都感到失败”\\n\\xa0\\n\\xa0\\n\\xa0\\n“拖延者即便到达成功，也不能充分享受其喜悦。因为成功的喜悦早已被心急火燎、通宵达旦的最后冲刺冲得无影无踪。虽然没有其他人知道他们的成功只不过是赶出来的，但是他们自己知道——对他们来说，这意味着他们仍然没有真正成功。他们责备自己没有成功，并希望从拖延的锁链中解脱，但是在自责中经常会错过一个重要的问题：他们实际上很可能是在恐惧成功，并利用拖延来避免成功的到来”\\n\\xa0\\n\\xa0\\n“那些害怕成功的人，担忧人们会加大对他的期待，这是他们的焦虑所在”\\n\\xa0\\n\\xa0\\n\\xa0\\n用拖延代替选择\\n\\xa0\\n\\xa0\\n“拖延常常是一个人的独立宣言，一个人试图通过拖延来告诉人们：“我是一个拥有自主权的人。我根据自己的选择来行动。我没有必要按照你的规定或者要求来做事。”利用拖延来反抗束缚的人可能是想保有他们独立的个体感，他们必须确信他们是按照自己的方式在生活”\\n\\xa0\\n\\xa0\\n\\xa0\\n“不如说我们谈论的是一种来自于焦虑的内心需求，因为你觉得自己没有安全感，也无法独自在这个社会生存下去。这样的心态不仅仅是出于有人陪伴的需要，更是出于缺乏自我完整感，除非你成为对方的一部分或者对方成为你的一部分，你才会感到心满意足。当一个人感到他们无法完全依靠自己做事的时候，就会在需要独立运作的事情上退缩彷徨”\\n\\xa0\\n\\xa0\\n“便你对一项事情的焦虑让你不堪承受，我们还是鼓励你给予它密切的关注，而不是希望它消失。感受你的情感，以一种友善的态度观察它们，尝试去理解它们，尝试开始行动，这样焦虑和拖延才不会再次主导你的生活。心理学家乔治·艾非尔特（George Eifert）和约翰·福赛斯（John Forsyth）观察到他们的顾客“会花费很多精力想要去维护他们的焦虑，维护焦虑似乎成了他们的职业”。我们不得不说，他们所说的话也适用于很多拖延者。\\n如果你真的是在不断维护你的拖延，我们诚挚地建议你：到了采取行动离开拖延的时候了，不要再让拖延成为你生活的中心。\\n\\xa0\\n当你允许、接受、探究并尝试去管理你情绪的时候，这些情绪就不会再阻碍你采取行动。当你面对一个令人恐慌的任务\\n“的并不全然是一场灾难和浩劫；这样你就可以重新获得一种内心的平衡，好让自己可以真正地重新思考。记住：从大脑的恐惧中心发出的信号可以很快地到达大脑的思维中心，而从思维中心到恐惧中心的通路却要缓慢得多了，所以在你焦头烂额的时候，不要指望你可以迅速或者完全地从中恢复过来”\\n\\xa0\\n\\xa0\\n“如果你在某方面的拖延是出于你在另一方面的价值观，那么这其中一定传递了某些你应该去理解的重要信息。但是假如拖延妨碍了你按照自己的准则来生活，那么，你这样做无疑剥夺了自己跟最深层自我和谐相处的那份踏实感”\\n\\xa0\\n\\xa0\\n“个协调的自我系统可以给你带来活力、和谐与平衡。在这样一个系统下，你就可以发展出一种弹性应变能力。在生活的道路上不管发生什么，你相信自己都可以应对，这让你更有勇气去试验和探索。对自己弹性应变能力的确信可以让你变得更加自信，而这种自信就会成为你整个自我形象的基调。当你把自己看成是一个具有弹性应变能力的人，而不是一个必须去表现自己的人，你就打破了我们在前面章节中讲到的自我价值感等式，表现和能力都不可以再决定一个人的价值。\\n\\xa0\\n\\xa0\\n\\xa0\\n“检察官：你这个愚蠢的白痴！你又把事情搞砸了。想要在这家公司工作，你还不够聪明，迟早他们会发现原来你是个笨蛋。\\n辩护律师：早一点开始做的话，我会做得更好。\\n检：你应该很清楚必须早点开始，而不是一直坐等这么久。\\n辩：我还有很多其他事情要处理。\\n检：你总是找借口。不是这件事情，就是那件事情，没完没了。\\n辩：请你消停消停，这样我就可以更轻松地回到工作中。\\n检：我为你好才对你严格要求，没有我，你就没有动力了。\\n辩：这不是事实。如果你没有不断地攻击我，我可以做得更多更好。\\n检：但是那样你就一文不值了。\\n“辩：不要为我预测未来。我已经学到很多东西，每一个经验都会让我变得更好，我只能脚踏实地，从眼前做起。\\n困难在于，你要学会为自己辩护，容许你的价值感和言行，以不加评判的方式，尽可能多地呈现出来。对于像“我自己想要的是什么”这样的问题，没有一个现成的答案可以决定你是对还是错。这意味着，你不仅必须把作为一个个体的自我与他人对你的影响区分开来，而且还必须跟那个充当内心检察官的你区分开来，你必须从自我阻碍中走出来。自我辩护的练习做得越多，它在大脑中的运转就会越发自动，对维护大脑平衡起到的作用就越大，你也就会越来越自信。”\\n\\xa0\\n\\xa0\\n\\xa0\\n一些适合时常复习的原则\\n万事无完美\\n努力尝试是一件好事，而不是愚蠢或软弱的表现\\n失败并不危险。失败是正常的，这是生活的组成部分\\n真正的失败是不敢去经历。每个人都有局限，包括我自己\\n如果那是一件值得去做的事情，那么为它犯错误也是值得的\\n挑战有助于我的成长\\n我有成功的权利，我也能应对别人对此的反应\\n如果这一次没有做好，下一次我还有机会\\n遵守别人的规矩去做事并不意味着我一点权力也没有\\n当我展现出真实的自我，真正喜欢我的人就会跟我坦诚相对\\n答案很多，但我需要找到那个最符合我感觉的答案\\n\\xa0\\n\\xa0\\n\\xa0\\n四个拖延主要因素\\n\\xa0\\n1.对成功所需的能力缺乏自信。\\n为了建立起你成功的信心，我们建议你为自己设立一个比较现实的、能够达成的、同时又容易衡量的目标。然后你把这个目标细分成可以容易操作的几个小单元，每个小单元你都可以在一个较短的时间内完成它，这样你就可以顺风顺水、一顺百顺了。对于患有注意力缺失紊乱症和执行功能障碍的人，以及那些正在经历文化转型的人，我们将给出一些相应的处理技巧。\\n\\xa0\\n2.对要去完成某个任务有反感心理：认定做事的过程中会遭遇很多困难，结局也会很惨。\\n一件事情是否困难和是否令人厌烦跟这件事情本身没有什么关系。某件事情令你不舒服是因为你将它跟你的内心恐惧或焦虑联系在了一起，正是这样的不快使你对一件事情产生反感，并想要躲避它。当你对自己的恐惧有了了解并逐渐接受真实的自己之后，你或许会很惊讶地发现，这些事情实际上并不是那么令人讨厌。有些事情或许会变成中性的（是的，即便是交税这样的事情），还有些事情甚至让你觉得兴趣十足\\n\\xa0\\n3.目标和回报太遥远了，感受不到对我有什么意义。\\n为了使一个遥远的目标或回报看起来更为实在和突出，我们鼓励你将任务切分成小段，并经常地奖赏你自己。我们也会给出一些改善你跟时间关系的技巧，从而让未来不再那么模糊，使之与当下的联系更为紧密。同时我们也鼓励你去发掘你的价值观，这样你就随时提醒自己：一个长远的目标对界定你是谁以及你想要什么都是非常重要的。\\n\\xa0\\n4.无法自我约束，例如容易冲动和分心。\\n在自我约束方面，我们会给出一些帮助你的身体和大脑放松的技巧，这样你就能够管理自己的情绪，切断让自己分心的方向\\n\\xa0\\n\\xa0\\n\\xa0\\n看你的目标是否具有可操作性是非常关键的一步，当你想达成一个目标的时候，专注于即将投入的行动，因为这会帮助你看清自己的方向。一个具有可操作性的目标具有以下特征：\\n◎可观察性；\\n◎具体性和特殊性；\\n◎它可以被分解成几个小的步骤；\\n◎第一步可以在五分钟内被完成。”\\n\\xa0\\n\\xa0\\n\\xa0\\n注于必须采取的每一个步骤对你也是一个提醒，让你知道：为了到达终点，你必须一路走去。大部分拖延者脑子里只有“终点”，至于“怎样到达”，他们往往不愿意去多想。很多人后来才惊讶地发现，原来那个完成每一步的过程本身也是非常富有挑战性的，在这个过程中，我们已经得到了应有的报偿。\\n\\xa0\\n\\xa0\\n“将你的可操作目标和涉及的步骤写下来，同时也要写下你计划在下周的什么时候实施每一个步骤，别忘了将下周要做的其他一些事务和职责也考虑进去。\\n目标：周六、周日各花两个小时（共四个小时）将我的家庭办公间做一次整理和清洁。\\n步骤：\\nA.整理文件和打印物\\n1）清理桌面和地板上的堆物，将无关紧要的东西都扔掉。\\n2）将衣服和鞋子放进壁橱，杯子放进厨房，书本放进书柜，杂志放到回收箱。\\n3）将所有的账单放进一个盒子。（我有这样的一个盒子吗？）\\n4）将收据放进另一个盒子。（盒子够不够用？）\\n5）购买文件夹。\\n6）整理小物件、照片和剪报。\\n7）将需要保存的文件归档\\n\\xa0\\n\\xa0\\n不要等到自己有意愿和感觉的时候才开始。如果你想等到你有意愿和感觉的时候开始做事，那么你可能永远也不会开始。即便你不在理想的状态，或者心情不对，你依然可以动手做事。\\n\\xa0\\n\\xa0\\n\\xa0\\n修正目标并不一定意味着失败。事实上，它意味着你足够灵活，而不是顽固不化，一个稳健而又成熟的人做事方能变通。\\n\\xa0\\n\\xa0\\n\\xa0\\n“学会利用零碎时间。阿兰·卡凯因对拖延者有一个很好的建议，他在《如何掌控你的时间与生活》一书中描述了一个叫做“瑞士奶酪（上面有很多小孔的一种白色奶酪）”法的时间管理方法。他建议在一个比较大的任务中使用“见缝插针”的方法，就是利用零碎时间，而不是消极等待整块的时间段出现。这个方法对你启动一个项目或者启动之后使之保持连续性都有非常大的好处。\\n瑞士奶酪法的意义在于它看重任何一段时间的价值，无论这段时间是多么微小。你的目标需要10个小时来完成并不意味着你要等到一整块时间的出现才能开始做事。你只需要15分钟、10分钟甚至5分钟就可以完成很多重要的步骤。如果你觉得无法承受，你可以花一分钟时间做一个列表。如果你是在回避那个有很多未完成的事情等着你去做的办公室，你只要在里面站15分钟，继续保持你的呼吸，就可以习惯这个环境。如果你有很多整理工作要做，花上几分钟时间将你柜子里的文件夹梳理一下。任何一个步骤都比你回避的行为要更为有效，至少你又往前迈进了一步。\\n\\xa0\\n\\xa0\\n如果你决心自己来设定时间上的限制，决定做10分钟或者5分钟的事情，那么你就重新赢得了你所需要的控制感，你就会继续往前迈进\\n\\xa0\\n\\xa0\\n与依靠拖延相反，我们觉得我们应该有意识地接受那些提高我们生活品质的事情，而拒绝那些对此无益的事情，并且应该直接地表达出来，而不应该通过拖延的方式来拒绝”', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9e840dda-1724-4e9f-835d-680082a7e7a9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n杂  习惯养成\\n\\n上大学以后，我开始有了拖延的毛病。立下目标无数，但时常却动力奇缺，常常在网上浏览着各色的小说和帖子，或是玩很无聊的弱智在线小游戏，却不愿碰专业书本或文献一下，甚至哪怕deadline就在几天之后，只有在deadline之前一点点时间才会因紧迫感而开始着手学习任务。这样下来，学业上总体来说算是马马虎虎，但却离自己的理想越来越远。总之，就是无法完全地上进，又不愿彻底地堕落。\\n\\xa0\\n\\xa0拖延的基础，实际上是对自身很高甚至不切实际的期望。如果说完成任务是走过一块一人宽、十米长的厚木板，那么当它放在地面上时，几乎人人都可以轻松地走过。但对结果的高期望则像是将这块木板架到了两座高楼间十层楼高的地方，于是我们会害怕掉下去，即害怕失败或害怕成功（比如我有时偷偷希望实验不要成功，这样我就可以不用面对之后更大强度的后续实验，其实是害怕失败的一种变体），于是我们甚至不敢向前迈上一步。而deadline则是身后的一团火，当它离我们足够近时，害怕被烧着的恐惧感战胜了对掉下去的恐惧感，于是我们一下子冲了过去，在deadline前赶完了任务，尽管质量很难说。\\n\\xa0更可怕的是，很多拖延的人（包括我自己）甚至很享受那种deadline过后突然一下放松的感觉，而且拖延的结果有时反而挺好（比如我本科时写实验报告，如果我拖到最后，我往往可能因为能够和其他人讨论并参考其他人的观点而比我先完成要写得更全面更好）。这种时候，我会在心里表扬自己很有“效率”。同时，长期这样下来，尽管我不愿承认，但我潜意识里确实觉得自己如果花了很多时间成绩却平平，会是一件非常丢脸的事情。于是这一切的一切，都再次强化的拖延—— 即使结果不好，我也可以说，那是因为我没尽全力，如果我真正努力，肯定会结果很好的（心理学上的高自尊人格）。\\n\\xa0\\n但我们不能永远靠放火来逼自己走过木板，那样的话，总会有烧着自己的一天；而且，那种压抑的焦虑感和对自己不满意的感觉也并不令人愉快。因此，最好的办法是将木板的高度降低——不要对自己的结果（比如分数）太高的要求，认真完成就好。由于我们的天资和其它能力的限制，也许即使我们竭尽全力也无法像某些出众人物一样做得那么好，但不管怎么样，绝大多数情况下，尽力的结果都会比我们不去努力要来得好得多，不是吗？\\n\\xa0对此在豆瓣的那个帖子里有一个非常有趣的方法。那位作者将一篇论文拖了几个月之久都不愿开写，她的心理师让她不要管好坏，以她可以写出的最差的论文为目标写一篇出来作为试验。作者照做了，然后惊讶地发现她写出来的“最差”的文章竟然挺让自己满意的，感觉稍做修改就可以交给导师过目了。当然，有时写出来的初稿和论文要求还是有差距的，比如要求12页但初稿只有5页。这种情况下作者就用了心理师教她的另一个方法：每次工作一个小时，目标是让论文多一页，同样以“最差”为目标。这个方法实际就是大大降低了拖延者做事的心理成本和负担，相当于把“木板”放到了“地面”上。不妨一试。\\n\\xa0\\n人的完美倾向，严重点就是完美主义，在拖延中也起了很大作用。当我想起幼时的远大理想并希望为之努力时，我只要想起我的同学中无处不在的“牛人”，便会觉得自己已经浪费了太多时间，而且别人现在不仅比我强得多，更因为他自身的优势而占有了比我更好的资源，所以我现在即使努力估计也赶不上他了，更别提什么远大的理想了，于是就又开始了拖延。事实上，这就像是一场马拉松，你在开始因为种种原因落后于别人不少，于是你就开始纠结自己到底有没有跑下去的必要，却不努力去跑，于是被拉得更远。这时，你望着遥远的终点，感觉很绝望，都不想跑下去了。这时，如果你不去看不去想重点，也不去想别人，只看着自己的脚下跑——“管它呢，我先跑过这个小土丘再说。”就这样一个小目标一个小目标地跑，最终你一定会跑到终点线，那时你也许会发现，你并不是最慢的，甚至是很不错的。所以，专注于你努力的过程而非最后的结果，为你认真学习了一个下午或认真做了一个实验而不是最后考试的那个A或一个漂亮的实验结果而表扬自己，你会发现完成工作其实不怎么难。\\n\\xa0\\n\\xa0另外，专注于当下对拖延者来说也是很需要注意的一个地方。这里说的专注于当下，不是指专注于你现在脑子里的想法和情绪，而是专注于你现在在做的或选择要做的事情。其实，很多拖延的人恰恰就是太过关注自己一时的情绪，比如觉得自己不开心了，得放松一下，上上网……然后就开始了拖延。其实从心理学角度来说，过于关注自己一时的情绪是不懂得推迟满足感的一种表现，就像小孩子想要一个玩具就非要马上得到不可一样，这样的做法会大大削弱一个人的自制力。而且，心理学实验表明，满足自己一时的情绪需求并非最佳策略，从长期角度上来讲，它会降低一个人的自我满足感和幸福感而非增加，想想因为玩乐休闲而拖延了工作后自己的负罪感和焦虑感就知道了。\\n\\xa0在这个方面，我个人认为森田疗法的理念是很适用的。具体而言，就是不去理会那些打搅你的情绪波动（比如对自己说：“你要郁闷就郁闷吧，不管你了。”然后就不理会了），顺其自然，专心做你要做的事情。就像一颗小石子投入湖中，会泛起一圈圈涟漪，你若不理，湖面最终会自己平静下来；倘若你过于注意那颗小石子，试图把它捞出来，反而会激起更大的波澜，使湖面不得平静。豆瓣那个帖子的作者提供了一个方法，我觉得也是很有用的：把你当时因为一时情绪想要做的事情（比如上网、玩游戏、看电影、看小说等等）记下来，告诉自己等你做完工作就去做那些事情，然后就专心工作，等到工作结束再去做记下的事情\\n\\n\\xa0此外，不要为未来过度操心 也是专注当下的一个方面。这点很容易理解，我就不在此赘言了。\\n\\xa0同时，对于已经发生的不愉快的事情，或是对自己过去行为的不满，面对和接受好了，不必逃避。但面对和接受不是放任自流，而是不再沉浸于自责、痛苦等负面情绪中，客观地更好地理解当下的状况，进而为以后做打算。这一点可以和前文提到的“马拉松”的例子联系起来，只有真正面对和接受了你落后于别人的事实，你才不会为“我想得第一可是却落后别人那么多”之类的想法而纠结，进而才能以现实为基础，踏踏实实地努力去跑\\n\\n\\xa0另外，在看书的时候（当然也有其它时候），很多人都常有“刚才看了好几页却不知道讲了些什么”的“不自主飞翔”的神游体验。对此，养成发现新事物的习惯，像初生的婴儿一样去看世界，是很有好处的。无论在什么环境中，时刻注意那些熟悉的东西有什么变化，比如看书，就可以注意想想这书里讲的和我以前知道的有什么不同又有什么联系、有什么很有意思的地方等等，慢慢地，就能做到专注于当下而不至于迷失。\\n\\xa0\\n\\xa0战胜拖延，追根究底，还是要改变自己的思维方式。这并不容易，但不是不可能。改变思维方式，尤其是改变潜意识，最重要的是要改变自我对话的方式。下面是一些自我对话的tips（括号里是要丢弃的自我对话方式）：\\n\\xa01.我选择/我想要……\\xa0（vs.我必须/我一定得……）\\n\\xa02.这个任务我可以每次做一小步\\xa0（vs.这个任务太大了）\\n\\xa03.我今天要开始做……\\xa0（vs.我今天必须完成……）\\n\\xa04.我也可以是平凡人\\xa0（vs.我必须完美/出类拔萃）\\n\\xa05.我一定要休息娱乐/休息娱乐是正常生活的一部分\\xa0（vs.我没空休息娱乐/休息娱乐就是偷懒）\\n\\xa0\\n\\xa0我想大多数有拖延情况的人也许都还没到严重成“症”的地步，但也或多或少地被它影响了生活，影响了对心中理想的追寻。克服一个问题并不简单，需要自己给自己很多的支持与鼓励，期间也许会有许多的反复，但只要我们的大方向是好的，我们就该肯定自己。\\n\\xa0“认识到眼前残酷 （这个词程度太深，但我想不出换什么好）的现实，同时又看到未来的光明与希望。”这是我一直很欣赏的对生活的态度。谨以此文共勉。^_^\\n\\n昨天的文章中有个链接，今天把这篇文章补充一下\\n\\xa0\\n我在另一个Blog中“每天一本书”的最后有一句话，“影响结果的不是斗志，而是科学”，这句话出自《干劲的开关》，好象很多人都没有看明白，今天来解释一下。\\n在培养习惯过程中，很多人把意志力的因素看得高于一切，而这正是很多人失败的主要原因。\\n新年的时候，定下一下目标，比如决定每天阅读3小时、每天学习四小时外语，每天进行二小时体育动作，练习2小时音乐之类的事情，在刚开始的时候坚持一段时间，突然遇到事情，放弃了，最后得出的原因是意志力很弱，不能坚持去做事。\\n而“影响结果的不是斗志，而是科学”正好可以做个很好的回答，培养一个习惯时，意志力并不是最重要的因素，而是科学的分析和计划，并且在不断的尝试。\\n\\xa0\\n下面的是后来在知乎上回答的，跟原来在Blog中的文章有些不同，看完这些点，建议再点击下面的“原文链接”，查看一下原始的文章：\\n\\xa0\\n1、刚开始的时候，每次只培养一个习惯。\\n\\xa0\\n2、既然决定了就做个计划，就做好生活中的一切为这个习惯让路的准备。\\n\\xa0\\n3、提前分析一下，哪些习惯和事件有可能打断你的习惯，做好准备和备用方案。特别是征求你家人和朋友的意见，让他监督和配合你，记住人和人都是互相影响的，要么你影响他，要么他影响力你，尽量通过好的习惯去影响周围。\\n\\xa0\\n4、习惯坚持二个月，66天左右才能稳定\\n\\xa0\\n5、可以在新浪或豆瓣之类的地方，参加小组，每天打卡，利用群体的力量，如果能在日常生活找到一个跟你一起坚持的人，成功会大大增加。\\n\\xa0\\n6、还可以写一个承诺书，每天在微博上打卡，让大家一起监督你。或者对你自己狠一点，用钱或名誉来做赌著。比如不能完成请人吃大餐、裸奔、把钱存给某人，完不成就归他了，声明一下我可以来当这个角色，欢迎大家选择我。\\n\\xa0\\n7、用心愿来激励自己。比如完后之后就可以买一个礼物给自己，或者出去旅游。去年我的二个平板、N件衣服、大餐、旅游都是这样来的。\\n\\xa0\\n8、失败时要有平常心，别自暴自弃。就是有真的有特殊情况导致无法完成，明天再继续。总结一下原因，能避免的就避免，避免不了就平静的去接受它吧。\\n\\xa0\\n9、改变你周围的环境，让环境去帮助你完成这个习惯，而不是变成阻力\\n\\xa0\\n10、有时无法坚持，是因为缺少技巧无法流畅的完成，多向其它人请教，多尝试，多总结。\\n\\xa0\\n11、通过记录来分析，一个习惯在什么时间用哪种方式来完成最合适，对我自己来说，每天早起后的时间是一个非常不错的选择。\\n\\xa0\\n12、把你的习惯写下来，放在钱包上、手机和电脑桌面上，每天都能看到好几次，每一次都是一种提醒。\\n\\xa0\\n14、失败之后不要完全放弃，多尝试几次，成功率会提前很多，连续尝试五次之后，成功率可以提前60%以上。我的很多习惯都不是一次完成，有的习惯反复尝试了几十次才完成的，目前还有很多习惯还没有完成。\\n\\n最先开始坚持习惯的人已经快满一周，需要开始总结了，看看这一周进行的如何，遇到了哪些困难，效率怎么样，对你有什么了改变。\\n\\n不久以前，我的生活一塌糊涂。在凌晨3点到6点之间我不定时的睡觉，而在最糟糕的日子里我根本无法入睡。因为睡得晚，所以我起的也晚，然后导致的就是我的一天开始的也晚，这就意味着我的一天都是在赶工作，约会也总是迟到。不仅如此，我吃的东西也让人想象不到——为了熬夜我要吃大量的垃圾食品和零食。这种情况不停的形成恶性循环，我也终于不愿再这么继续下去了，我要彻底改变我的生活方式！\\n\\n我挑选了9个接下来的21天我想培养的习惯，比如说：（1）最晚12点一定要睡觉（2）5点钟准时起床（3）至少拿出一天看看书、听听广播（4）沉思（5）约会时提前一点到（6）吃生食。这六条对于有些人来说可能觉得有些受不了，但是又想到只是一个21天的计划，所以我觉得不妨尝试一些更与众不同的。\\n\\n当看到我的新习惯一个个都迸发出来后我打心里开心。我的生活瞬间变得井井有条。我每天都起得很早，约会也都准时或者提前到，每天要做的工作都能按时做完，还可以停下来好好反省一下自己，而且我开始吃生食了，并且晚上也能按时睡觉。跟我之前的生活方式比起来，现在可以说是焕然新生，来了个180度大转变。1\\n\\n有些人可能认为是因为我比其他人有更坚定的决心、意志力并能严格遵守去克服重重困难才做的到这些。我不想否定这种说法，但是事实却真的不是这个样子。其实真实的情况是，我觉得我自己平时是一个混乱无纪律的人。但是我还是能成功是因为我有6条特别的方法帮助我彻底改变了我的生活，也是它们让我的新习惯牢牢扎根在我的生活里。\\n\\n如果你也在尝试培养新习惯，但是结果却总是不尽如人意的话，那么着6点对你来说应该会有所帮助。这些方法并不是什么复杂的事，它们都很容易理解、做起来也很方便，不仅如此，在我身上它们发挥了奇效。\\n\\n让我们来看看吧：\\n\\n**1. 找出你以前的习惯为什么不能持之以恒的真正原因**\\n\\n要找出坚持不下来的根本原因，而不是只看到它所表现出来的现象。比如早上5点30你每天跟自己做激烈的思想斗争要起床，这只是一种由原因所产生的表面现象。而了解你到底为什么总是没办法5点30起来才是根本原因。\\n\\n打个比方，曾经我也一度有很长的时间没办法早起，而我在做的就是继续每天努力着早起然后再失败。这种情况持续了好几个月，直到最终我终于觉醒了，知道再这样下去不会有任何进展。于是我开始分析我的情况，通过自我反省找出到底为什么我早上没办法早起的原因。我好好的研究了我的情况，然后问自己起不来这个事情“为什么”会发生，由此更深一步去找出根源。\\n\\n以下就是深入探究、寻找本因的方法的例子：\\n\\n- 为什么我早上没办法早起？\\n\\n-因为我很累\\n\\n- 为什么我会累？\\n\\n-因为睡觉时间还不够。\\n\\n- 为什么会没睡够？\\n\\n-因为我睡得太晚。\\n\\n- 为什么睡那么晚？\\n\\n-因为有太多事情要做。\\n\\n- 为什么有那么多事情要做？\\n\\n-因为我做不完。\\n\\n- 为什么会做不完？\\n\\n-因为我给自己制定的计划任务超过了我可以在今天完成的。\\n\\n在处理这个根本原因时我明白了两件事，（1）所有的习惯都是环环相扣的（比如，晚上睡觉的时间，早上起床的时间，时间不够）（2）我低估了我要完成一项工作所需的时间（也就是高估了我做完这些工作的时间）。很多时候我的计划都是在一天内做很多项目，但是这根本不可能。\\n\\n这也就意味着，我要坚持早起的这个习惯的话，我就要做到：（1）我要改变跟早起有关的习惯（2）制定的计划要更符合自己的情况。与其每天要做的事情堆得满满的做也做不完，现在我能按照一份有挑战性的却可以做得完的计划来完成我的工作。\\n\\n一直为自己“为什么”找出根本原因。一旦你知道了真正的原因是什么，问题马上就迎刃而解了。\\n\\n**2. 坚持那些有密切关联的习惯**\\n\\n我们的习惯并不是孤立的，它们互相之间是有联系的。有些习惯相互之间的联系比与另外一些习惯密切得多。比如说，早睡合早起这两者的联系显而易见，但是早睡和每天读一本书之间可能就没那么密切的关联了。如果你想坚持一个习惯，那么就要格外关注跟它紧密相连的另一个习惯，这样才会有一个全局的改善。这些习惯会互相增强对方的作用，让你的改变也一环扣一环，天衣无缝。\\n\\n举个例子，我的新习惯是：（a）早上5点钟起床（b）晚上12点之前睡觉（c）准时做完该做的事（d）要自我反思（e）吃生食，这些习惯都是有密切联系的。1\\n\\n早上能起得早就意味着我有更多的时间昨晚自己的工作，这样一来，晚上我就可以早点睡觉，而这又可以让我第二天早上早点起来。\\n\\n准时做完该做的事可以让我暗示做完工作，而这样就能让我完成我指定的每日计划，这就意味着我的晚上睡觉时间和早上起床时间都不会受影响。\\n\\n沉思可以将脑子里的杂乱思想都清理干净，也可以帮助我减少睡眠时间。通常我都睡6-10小时，但是在沉思的那天，我可以只睡5-6小时。\\n\\n坚持简单的生食素食让我拥有一个清晰的头脑，这也就是说我不用像平时一样睡那么多觉。我的意思不是你为了养成早睡早起的习惯就一定要吃素，只是我发现我在这么做的时候受益良多。\\n\\n**3. 为你的习惯定一个计划（具体什么时间到什么时间做什么）**\\n\\n定一个计划表可以让你清楚的知道你什么时候该干什么。我新生活开始的第一天就制定了一个全天计划，而之后的日子里也都一直奉行。\\n\\n我是这样做的：\\n\\n在前一天晚上，把第二天要做完的所有事情列一张清单，然后把它们都记录在日历上（我用的是google日历的日程表）\\n\\n把所有工作分成（a）重要工作（b）一般重要的事（c）其他琐碎杂事\\n\\n然后就可以开始把它们放进我的时间表里去了。要给重要工作留出最多的时间。我经常使用的分配比例是a：b：c分别是60：30：10（%所花的时间）\\n\\n要注意做每件事情需要的时间。这很重要，因为大部分情况下我们都会低估我们真正需要的时间。要保持效率就要让计划变得更实用，通常在做完一件事情转到下一件之前，我会加入一个5-10分钟的缓冲时间。\\n\\n准确的制定一件事情开始和结束的时间点。比如说，9：00-10：00做A这件事，12:30-13:30吃午饭，18:30-19:30是回家的路上时间。\\n\\n如果要做的事情比我时间表上的要多，那我就会把不是很重要的事情放到另外一天时间充裕再做。\\n\\n制定好了这份计划之后，第二天我要做的就是严格的遵守。我会时刻关注时间，到时见之前5分钟，我就会准备一下做下面一件事情。\\n\\n严格遵守时间的好处就是不让我超时，从而让我更有效率的做事。有些时间不能少，就像睡觉时间和约会时间，因此，留给我完成工作的时间就有限，所以我必须提高效率。\\n\\n听上去制定时间表这可能很难，但实际上一点也不。我每天只要花10分钟就够了，而对于制定时间表的时间当然也是会写在时间表上的（23:00-23:10）。你所要做的只是做一个模板出来，以后你就可以一直用了。有些事情日以及日的重复，就像起床，吃早饭，上班路上时间，工作时间，吃晚饭，睡觉时间，所以其实做起来很简单。\\n\\n如果你不明确的说明什么时候你要把这个事情做完，而只是随意的说今天一定会做完，那么很有可能你今天是不可能完成的。这也就是为什么那么多人有新习惯却总是没办法持之以恒的原因。而每天有似乎总是有意想不到的事情冒出来，让你不去顾及你的时间表，这样你的新习惯就被这些事件有打压了下去。\\n\\n**4. 领先你的计划一步**\\n\\n能始终保持积极性的办法就是始终走在计划的前面。早上5点起床就让我走在了世界上大部分人的前面（当然也包括以前一塌糊涂的我自己），而这给了我一种快点工作、始终走在前面的动力。什么能持续给我这种那动力呢？就是我能比计划表上的时间更快做完一件事情，然后开始做下一件。知道了自己始终走在计划表的前面，我就自然而然的对要做的所有事都斗志满满，包括我的新习惯。这样对开始一个新习惯也就不会有任何抵触和难处。\\n\\n如果一项任务需要的时间比我预计要多，那我会做一个选择，或者我会：\\n\\n加油，抓紧！快点把它做完！\\n\\n把不重要的事情先不做了，或者\\n\\n向我之后要做的事情借时间来把手头上这件做完，而这也意味着之后我依然需要快马加鞭。\\n\\n做决定的过程很重要，因为一旦选择错误，你这一天在剩下来的时间里又要不停地追赶计划表了，这会对你的习惯和要做的事都造成影响。随即而来的是，这也会动摇你坚持你习惯的意志。比你的计划提前一步，你会时刻活力四射。\\n\\n**5. 做习惯的反馈**\\n\\n反馈是对你的习惯负责的一种表现。我房间里有一块白板，专门是反省我的习惯所用。我在白板上画了一个表格，然后横列和竖列分别是天数（养成新习惯的21天）和习惯。我遵守习惯的时候就会在相应的地方打一个勾，没有遵守就会打叉。每当按照习惯做事、看到一个个勾挂在白板上就会心满意足。同样，你也可以在纸上或者电脑上记录你的习惯坚持成果。\\n\\n**6.和周围让周围的人也加入进来**\\n\\n参与有两种表现——（1）积极参与，把你的计划告诉其他人，跟和你有同样兴趣的朋友一起培养你们的习惯（2）被动参与，你告诉他人你的计划，仅仅得到他们的支持。\\n\\n在我改变习惯的这段时间里，这两种状态我都有所体验。在我开始改变的2天前，我在博客上写了一篇有关21天我要革新我的生活状态的文章，其中我详细的阐述了这个计划的基本原理、益处、我要坚持的习惯以及具体要怎样做。同样，我邀请博客上的好友跟我一起来培养新习惯。出乎我意料的是，很多读者都表示很有兴趣，他们也愿意跟我一起开始一段21天的改变。\\n\\n关于我吃生吃素的问题，我告诉了我妈妈接下来的2个星期我只吃水果和沙拉，于是她就开始疯狂的买水果，像香蕉、葡萄、草莓什么的。事实上，在写这篇文章的时候我只吃了一盒草莓。昨天我跟朋友一起去看了一部电影，然后说服他跟我一起吃生食。然后他恋恋不舍的看了看本来那晚我们要去吃饭的餐厅。后来我美美的吃了一顿菠菜沙拉。这是我第一次吃，不能说有多喜欢它，但是这就是我的改变。\\n\\n不要觉得在你的坚持习惯之路上你形单影只，因为有很多人都跟你一起在走。你身边的许多人都会加入你而不只是支持你。\\n\\n结束语\\n\\n我的新习惯现如今已经跟我的生活渐渐融为一体了。一切都变得顺其自然，就像这件事长久以来我一直在做一样。以上的小贴士让我的生活发生了巨大的改变，尽管这六条建议很简单也很直白，但是千万不要小看它们。自己来试试看，有机会也告诉我你的新习惯进展如何吧！\\n\\n**首先，如果这个精神专注的意思是，一点都不分神，哪怕一秒钟，我认为是不可能的。**\\n\\n所以，我理解的是：走神或分心在可以忽略不计的范围内（如果你一定要说可以走神两秒钟，两秒不行三秒钟，再多就不行了，那么你还是自己去寻找神技吧。）\\n\\n=============================\\n\\n**以下是质疑各种回答：**\\n\\n在这样的前提下，我觉得很多的答案都是不成立的。比如，注意力只能集中一段时间，比如30、40分钟。很多人都有一打游戏就打一个下午，打一个晚上的经历，没有人会说，打四十分钟我就必须要休息一段时间，否则接下来就集中不了精力。当然，你会说打一局中间有休息一下啊。恩，事实会证明你打完一下午或者一天你可能对游戏过程或者经历记忆犹新，中间那休息停滞的一下你根本不记得干了什么说了什么话。我觉得这样的分神就是忽略不计的。大家看电影打游戏的时候，有说我看半小时玩半小时就要休息五分钟么？（除了保护眼睛的需求）。\\n\\n再举个例子，看武侠小说的时候，我猜绝大多数人都能长时间集中注意力。\\n\\n所以，我听闻番茄工作法已久。但是我自己从来没用过。我很好奇的是，无数的人赞同他的时候自己是否用过。对这个问题我有很多疑问，要是打断的时候正好是我精力最集中的时候，怎么办？正是我突破某个难点的时候，怎么办？不停地这样中断，每一次中断我就要保存一次现在看到哪儿了，然后下一次开始的时候我再去进入状态，接上思维，这个开销可以忽略不计吗？还有不停的设置时间提醒自己，不嫌麻烦吗？\\n\\n至于明天是deadline，如果明天是deadline，当然就能够长时间集中注意力了。如果明天不是deadline怎么办呢？等到了deadline再做？恩，这确实是一种方法，如果各位都喜欢玩这种高挑战动作而且保证自己不失手的话，我觉得这就是很好的方法了。或者我出于好心，再帮各位补充一种方法，没有deadline就创造deadline。这个，你要能够做到自己创造的deadline都和外界施加的deadline产生同样心理作用的话，也是很好的方法。可是，各位，有几个人能够愿意并且做到对自己产生强大的心理暗示把自己逼向死角呢？\\n\\n=============================\\n\\n**我的回答：**\\n\\n如何做到长时间（4个小时以上）精神专注？\\n\\n让你的注意力聚焦在你所做的事情上，你的处理速度趋近于你的接收速度，你的思维跟上你所做的事情的发展。然后保持，就OK了。\\n\\n这好像是句废话，你问我如何长时间集中注意力，我第一句就说你的注意力要集中才能长时间集中注意力。但是这句废话就是问题的实质，我们下面各种例证来揭示这个实质。\\n\\n当你看一部武侠小说的时候，你为什么能够两个多小时精神专注于这本武侠小说上呢？首先因为你的注意力很容易就放在这部武侠小说上，其次你的思维完全同步于武侠小说的发展，最后你的处理速度是趋近于你的接收速度的，（虽然你的大脑处理速度比正常阅读速度快很多，但是看武侠小说你可以跳过一些描写，脑补一些场景，所谓一目十行，再加上你的专注会放慢你的大脑速度。不信可以试想，同样是武侠小说，如果你不熟悉繁体字，突然看繁体竖排版的武侠小说，那么你能长时间保持专注吗？字都要认半天你怎么专注？）\\n\\n同样，如果这是一本教科书，你就很难两小时精神专注于教科书上，首先，你就不一定感兴趣或者不想看教科书的内容，注意力就集中不了，其次就算你集中注意力开始看，你的处理速度是跟不上你的接受速度的，因为教科书的知识密度大，一页书包含了可能很多的信息，你大脑处理不过来。（这一点大家联想下数学教科书就能更深刻的体会），最后，由于处理不过来，你跟不上教科书的思维，可能因为你不知道他说这些是在干嘛，也可能因为你不知道他要干嘛，看了一会儿你自然就很容易地丢失了注意力。\\n\\n大家试想，如果你去看电影的时候，心里面挂着的旁边的美眉或者帅哥究竟对你有没有好感，什么时候出手搞定她（他），你会像自己独自看电影时那么投入吗？可能看完电影你都不知道电影说了什么，因为你的注意力根本不在电影上。再试想，上课的时候，如果老师讲得简单，低于你的大脑处理速度，你是不是很容易就掏出手机来刷刷微博看看知乎了？如果老师讲得太难，你思维跟不上，都不知道他讲的是怎么一回事，你是不是也很容易就放弃了听课了？\\n\\n所以为什么这种问题一旦提出，就有关于deadline的答案。因为deadline到来的时候，你根本无暇关注其他的事情，必须要去关注你手上的事情，必须要去处理你手上的事情，必须要跟上你手上的事情的发展。\\n\\n而一旦你进入了专注的状态，你自己都会主动排斥掉外界的打扰。比如，你打游戏的时候，到了饭点也经常顾不上吃饭；看电视剧正到精彩部分，有个人挡在电视机面前，你肯定怒喊他走开。\\n\\n所以你真正要干的事情就是去想办法让去让你的注意力聚焦在你所做的事情上，让你的处理速度趋近于你的接收速度，让你的思维跟上你所做的事情的发展。\\n\\n**我们来讲一些具体的方法或者建议：**\\n\\n**（1）对于本可以不需要手参与的事情，增加手参与一下，是特别好的一个方法。**\\n\\n比如，看书的时候，同时手上做笔记。哪怕你一开始只是抄书。写字能够让你聚焦在你写的那一部分，由于写字速度的有限，对于特别难的内容，你可以理解一点写一点，写一点理解一点（特别指出，光抄但是不去理解，也就是说大脑不去处理也是白搭），对于简单的内容，写字也避免了浮躁地一目十行（不是一目十行不好，而是在不够专注的时候去增加接受速度很容易就让接受速度超过了你的处理速度，从而丢失掉专注）。而且同时在记笔记的过程中，你对书本的知识脉络有了掌握，跟上了书本讲述问题的节奏。这样很容易进入保持状态，具有一定的抗干扰能力，直到遇到无法抵抗的干扰或者中断。\\n\\n对于你的手已经参与了进去了，比如我现在在写答案，那你的大脑必须要积极地去思考你手上的事情，现在在哪一步，下一步是什么，联系是什么。这个也不是废话，很多时候我们的大脑是在被动工作，而并非主动工作。举个阅读的例子。阅读长篇英文的时候，大家是不是会有经常看着看着就不知道看到哪儿去了的经历？这时你的大脑就不是在积极主动工作，而是被动工作的。事实是，如果你一边读一边想这段文字的他在表达什么，和上一段是什么联系，和文章主题是什么联系，说完这一段，下一段你预期他应该讲什么，到了下一段是否符合你的预期，如果符合的话你可以加快处理速度，如果不符合你就去修正你的预期。这样去阅读的话，那么你会发现你就不容易走神了。（当然这个activereading经常需要记笔记去辅助完成）\\n\\n**（2）用一件你能够快速集中注意力的方法作为开头。**\\n\\n我有一段时间学习之前都先看一会儿闲书。比如带上一本王小波的《沉默的大多数》。先看四五篇之后再开始学习我的专业内容。这就是让你自己先完成那三个步骤之后，进入保持状态后，再切换任务做你该做的事情。\\n\\n**（3）在醒来之后没有多久开始。**\\n\\n人刚醒来没多久的时候大脑中没有什么事情，这时候去开始一段长时间的专注任务是比较好的时机。如果你已经脑子里面充满杂事儿，那么你很难去单纯的聚焦在某一件事情上。\\n\\n**（4）进行自我训练，每天都有一个长时间集中注意力的训练内容，保持自己在一个好的状态。**\\n\\n如果我们把一次长时间集中注意力当做一场比赛，而你是一个运动员，你以为你可以到了比赛日直接上场吗？如果是，那么你就是一个业余体育爱好者，如果是一个专业的运动员，是需要赛前训练去达到能够比赛的状态以及通过训练去保持身体状态。同理，你的大脑也像你的肌肉一样，它需要对你全神贯注的挑战作出更为显著的反应，这会拓展你的大脑，使你的大脑在更高层次运作。所以，你可以采取训练的方法让自己适应长时间的集中注意力。训练内容是什么？首先，请你从一两小时的专注开始做起，而不是直接就要专注五六个小时。其次，淡化刷微博水人人聊微信这种类似活动在你生活中意义，让你浮躁的心安定下来。还有，你可以选择一个每天的例行时间，保持一个规律的习惯。\\n\\n**（5）杜绝干扰（断网，断手机）。**\\n\\n我是这个方法的使用者。这就是让你去没时间顾及或者说没法顾及别的东西。即当你一开始还没有专注于某事物的时候，或者当你处理速度大于接受速度的时候，你想去处理点别的，没有可处理的。当然，要是你能够顺利到达保持专注的阶段，这些不太大的干扰都不是问题。\\n\\n**（6）用运动去唤醒你的身体，让你适当运动之后再开始，休息的时候也是让你的身体适当运动，而不是抱着冰淇淋去电视机面前坐着。**\\n\\n这个意见的来源是那本所谓豆瓣拖延小组宝典的书《拖延心理学》。他说运动后会加强血液循环，让你的大脑得到更多的氧气，内啡肽，脑源神经营养因子。书上的大意是运动一个小时之后再开始。但是由于题目要集中四个小时以上的注意力，我认为这个运动量太大以至于你可能两三个小时之后就会感到困倦。所以适当的运动唤醒一下身体就好了。(UPDATE：我觉得这个方法是非常有效的，比如我最近就是如果人不太清醒就去游泳400m之后去洗个澡出去学习，能够让你的身体活跃起来后适应大脑需要高速运转的需求。当然运动完有时会有些许倦意，要挺住，可以配合一杯咖啡)\\n\\n**（7）拒绝舒适。**\\n\\n当你想要在床上躺着看四个小时的教科书，我觉得很难成功。同理在沙发上躺着，甚至在家里面呆着，都不利于你长时间保持注意力。我以前高中假期想要学习的时候都会去图书馆的自修室，现在要学习也不会在寝室学习。\\n\\n**（8）对自己强烈地进行消极心理暗示。**\\n\\n创造deadline是属于此法，你暗示自己今天不做就不行了。同理，你可以暗示自己这事情不做好就完蛋了，暗示自己现在不好好复习，GPA低了很多机会就没有了，人生从此暗淡了。诸如此类，但是一定要足够地强烈，把自己逼向死角，让自己的精神无处可逃，必须要聚焦在你手上的事情。就是注意别把自己逼疯了。同时还要说一句，之所以不建议用积极暗示，是因为怕你自己积极过度，想入非非，YY得无法聚焦了。\\n\\n具体问题有具体手段，需要各位在实践中去发挥自己的想象力了。\\n\\n比如，等到deadline再做是吧?\\n\\n=============================\\n\\n**关于几个问题的实质：**\\n\\n**（1）做你热爱的事情。**\\n\\n兴趣，是促使你在第一下能够很容易地聚焦在你遇到的事情。人不可能永远都做自己热爱的事情，也不可能把所有的事情变成你热爱的事情。你只能是去充分理解你正在做的事情，至于理解前后你爱不爱它，那全看你自己了。我想，当你去试图理解你正在做的事情，你已经足够专注了。\\n\\n**（2）保持运动、充足睡眠。**\\n\\n保持运动是为了睡眠质量高，身体健康，醒着的时候精神状态好。精神状态好，你的机体都能发挥正常的功能，大脑能够正常处理它接受到的信息，眼睛能够正常地阅读。在极端情况下，比如明天deadline，即便你睡眠不足，我觉得你都会因为压力而刺激机体透支运转。所以保持运动是为了让你能够规律地长期地每天玩一次长时间集中精力。\\n\\n**（3）番茄工作法。**\\n\\n大问题划分成小问题，分而治之。集中精力二十五分钟当然比四个小时要容易许多。但是要是你二十五分钟都集中不了，那么番茄工作法不是白搭吗？难道变成工作十分钟休息五分钟？\\n\\n=============================\\n\\n**最最后：**\\n\\n这个答案我编辑了三个小时了，没有使用番茄工作法，没有出现不可忽略的走神。当然，也没有坚持到四小时以上，因为三小时我已经做完了。（什么？三小时没做完，又没法继续高度专注了怎么办？人啊，不要执念于超出自己能力范围太多的事情，休息休息，到你想继续做了为止。', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='df263148-ac2a-481a-999c-275b6c59b70e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n杂 16\\n\\n柯维的成功学讲：想要去做与决心去做，其结果往往截然不同。柯维强调“由衷的决定”，也就是说你下的决心是深思熟虑后全心认同\\n\\n关于知行合一，先生在《传习录》中有阐释：\\n\\n“未有知而不行者。知而不行，只是未知。 有如知痛，必已自痛了，方知痛。知寒，必已自寒了。知饥，必已自饥了。知行如何分得开？” “今人却就将知行分作两件去做，以为必先知了，然后能行。故遂终身不行，亦遂终身不知。”当时读到这段如醍醐灌耳。今人做事，绝大多先立志而后行，我亦属于那范畴，以致说多做少，最终落空。知行合一是为良剂，**知不弃行，行不离思，慎思之，笃行之。**后读黑塞也觉他与先生的思想有异曲同工之妙，这位曾深受东方禅思影响的德国人在Demian里写:\"only the thoughts that we live out have any value\" （唯有付诸实行的思想方能谈价值）\\n\\n另外《传习录》中相关的片段有：\\n\\n“立志用功，如树使然。方其根芽，犹未有干；及其有干，尚未有枝。枝而后叶，叶而后花、实。初种根时，只管栽培灌溉，勿作枝想，勿作叶想，勿作花想，勿作实想，悬想何益？但不忘栽培之功，怕没有枝叶花实？”\\n\\n与知行合一同理，先生此处指回到事物的本原，不前瞻后顾，心无旁骛地、纯粹地去做一件事。 我想不会有比这更“活在当下“的人生态度\\n\\n六经注我，而非我注六经（用经书里的内容来表达我的意思）\\n\\n儒家的观点：成己达人（类似慈心禅由己及人）\\n\\n尽己性，就能达到尽天地的境界\\n\\n『思无邪』 思无不可对人言。思，心中的愿望。心中所愿所想敢于示人，没有什么不好的。\\n\\n朱熹把这一句放在为政第二篇， 是引用孔子对诗经的评价告诉为政者，要心怀坦荡，光明磊落。\\n\\n撒切尔夫人传里说，我不能让所有人喜欢我，但要让所有人尊重我。罗振宇的解读是，尊重的意思是，即便人家反对，也要有理有据地站起来反驳\\n\\n人生若有知己相伴固然妙不可言，但那可遇不可求，真的，也许既不可遇也不可求，可求的只有你自己，你要俯下身去，朝着幽暗深处的自己伸出手去\\n\\n不以自我为尊，自我放逐，是构成圣贤人格的必备条件之一\\n\\n知识的传播者讲不了创见，只能涉足深远思想的一点皮毛\\n\\n一切内容，本质上都是社交；我想和张艺谋聊一晚上，但没条件所以只能看他的电影；我想和康德聊也聊不到；互联网把求知读书还原成了最本质的社交需求\\n\\n知识传播者应该根据那个时代的标准和功利，对知识和思想进行传播和延伸\\n\\n认知是人类互相理解和沟通的唯一壁垒了。过去的壁垒很多，身份，地理等\\n\\n有一些东西是不能改变的，这是（英美）保守主义的根源；罗振宇认为这些是人类的瑰宝\\n\\n你向自己发誓：“ 我只做带给我内心安宁的事情 ” 。开始这一过程可能对你来说很困难，因为我向你建议的正是你拒绝多年的方法。但是，这个过程最终将会让你认出你的真我。\\n\\n认知和行为出现巨大差异是很正常的事，尤其对于能人\\n\\n创业和做生意（开餐馆）的区别是什么？创业是开拓未知商业的新边疆，做生意是玩转已知商业规则来赚钱。商学院干什么？是找到规则，让普通人的交易成为可能。毕竟不管在哪个时代，创业的人都是少数\\n\\n**很多对人性有深刻洞察的人，都没有太快乐的童年，所以不要去羡慕那些对人性有深刻洞察的人，这不是什么好事**\\n\\n中国生意人道德不自信的原因：古代历史原因；文革浩劫；315树立了顾客就是上帝的理念\\n\\n别去想“他是怎样的人”，而是去想“他在嗨什么”；当 你get到思想的嗨点，就会觉得他不值得评价，但却值得理解\\n\\nYou don\\'t tell me who I am.\\xa0 I\\'m going to tell you who I am.\\n\\n1918年，19岁的海明威在米兰说：“与其在年老体衰、万念俱灰时死去，还不如在这无不充满幻想的幸福的青年时代死去，让生命在灿烂的光明中消逝。”\\n\\nHow much better to die in all the happy period of undisillusioned youth, to go out in a blaze of light, than to have your body worn out and old and illusions shattered\\n\\n他的父母在颠簸的车上让他降临到世间，而接着成为一个年轻的囚徒，他必须凭借不断地奔波才能让自己平静下来。\\n\\n在此，我顺便谈及“浪荡”一词的最初含义。在西班牙语中，浪荡子(libertino)最初的意思为“一个自由人的儿子”；而在法语里，17世纪时，这个词的意义与“慷慨的(liberal)”和“大方的(liberality)”相近，即慷慨和利他主义。卡萨诺瓦无疑体现了这种浪荡精神。茨威格说，卡萨诺瓦与唐璜的区别在于，前者永远将女人的快乐视作自己最大的快乐。不管对方美丑与否，卡萨诺瓦都毫无怨言地投入，让其获得快乐。而为了获得这种暂时的快乐，卡萨诺瓦可以藐视一切可能的风险。其回忆录中记述了太多这样的故事，他常常为了一声没有露面的女人的哀怨叹息，就放弃了去往伦敦的船票，然后毫不犹豫地进入一场冒险之中。而据茨威格推测，所有进入卡萨诺瓦世界的女人，都极度感激这位情人，因为他慷慨地付出了其获取欢乐的能力。\\n\\n改变世界。不要妄谈改变世界。有些人以为把冰箱打开能制冷，以为把小鱼扔回海里能放生。**他们行了善，而善恶充盈的世界永不动摇，只是加快了流转的速度。他们阻击了恶，却让应当遭受报应和反思的群体得以因循苟且，不得超生。**要改变世界，只能成为其中一员，身体力行地改变自己。太多的同路人投入了疯狂的公益，却只是加快了这世界的流动性。噪声依旧，流动的管道依旧。新人替换了旧人，宿命依旧。这样不叫改变世界，也不叫日行一善。只有智慧才是最大的善。否则他们所行的善，和去青海湖放生鲤鱼没有区别。\\n\\n让世界真正变好的方式，既是让穷人幸福，也是让富人幸福。而掌握智慧和良知的人必须拥有更大的话语权。也就是说，革命并不是江山易帜，而是改变一个阶级的社会角色。让富人成其为富人，而非简单地换人；如同让家庭成其为家庭，而非简单地逃离。这是我关注的两个话题有意思的共同之处。\\n\\n全民屌丝化有点类似于嬉皮文化。但时代的精英会一直在。他们恪守不变的良知和智慧，和三百年前的精英也区别不大。他们的自我成长就是对社会的担当。只有他们才会改变世界，不是通过揭竿一挥，而是建造起高耸的机器，传递更快的信号，用同理心和信念波及更多的土壤，长出更盛大的花瓣。\\n\\n卢梭在《爱弥儿》中说：好的社会制度是这样的制度，它指导如何能够最好的使人改变他的天性（自私，暴力，多变等），如何才能够剥夺他的绝对的存在，而给他相对的存在，并且把“我”转移到共同体中去，以便各个人不在把自己看作独立的人，而看作共同体的一部分。我对卢梭这段话的理解就是，人在社会中生活，就应该分一大部分自己的意识，使它变成社会契约。共同体，指人们在共同的条件下结成的集体。（社会契约论）\\n\\n亨利·梭罗的名言：我们首先是人，然后才是公民\\n\\n**心无挂碍的行动：预期，节奏，实力，长远眼界**\\n\\n**身无扭曲的行动：集中，专注，全力，活在当下**\\n\\n宽容不是道德，而是认识，唯有深刻地认识事物，才能对人和世界的复杂性有了解和体谅，才有不轻易责难和赞美的思维习惯。\\n\\n稻盛和夫说人必须每天都要辛勤的工作，然后通过工作每天让自己的智慧增加一点，进而达到某种灵魂上的精进。我想这是他定义的幸福之路。\\n\\n1. 人生始于今天，过去只存于脑中\\n\\n2、过去的包袱是否沉重，由自己决定\\n\\n3、你只能改变自己，不能改变别人或世界\\n\\n4、你的世界在你脑中，无需改变外面的世界\\n\\n5、你本人已具备所有能力，无需外求\\n\\n6、所有的改善，由信念开始\\n\\n7、每个人照顾自己的人生，不能假手于人\\n\\n8、值得做的都值得做好；值得做好的，都值得做得开心\\n\\n9、每个人都想有更好的明天\\n\\n10、事情本无意义，所有的意义都是人加上去的\\n\\n11、事情从不给人压力，压力来自一个人对事情的反应；事情从不带给人情绪，情绪来之于人的信念系统\\n\\n12、没有人能够伤害你，除非你容许他这么做；也没有人能控制你，除非你容许他这么做\\n\\n自信不代表自大。自信意味着你会承认自己的错误，并且有勇气改正它。\\n\\n人渴望被承认，也就是别人的目光，但是同时，当别人的目光围拢过来的时候，他又感到窒息，感到不自由。获得承认和追求自由之间，有一个多么辩证的关系。\\n\\n疯子并不是政治正确的词汇，政治正确的说法应该是，那些在另一个层次实现均衡的人。\\n\\n在兰德的观念里，成功与功成名就没有什么关系，成功就是一个人捍卫自己的完整性。道德只能建立在个体理性的基础上，而不是任何宗教、情感、社会、国家、阶级以及任何形式的集体\\n\\n如果天总也不亮，那就摸黑过生活；如果发出声音是危险的，那就保持沉默；如果自觉无力发光，那就别去照亮别人。但是——但是：不要习惯了黑暗就为黑暗辩护；不要为自己的苟且而得意洋洋；不要嘲讽那些比自己更勇敢、更有热量的人们。可以卑微如尘土，不可扭曲如蛆虫。\\n\\n—— 季业\\n\\n当然是鲁迅先生的这段话：\\n\\n愿中国青年都摆脱冷气，只是向上走，不必听自暴自弃者流的话。\\n\\n能做事的做事，能发声的发声。\\n\\n有一分热，发一分光。\\n\\n就令萤火一般，也可以在黑暗里发一点光。\\n\\n——不必等候炬火\\n\\n此后如竟没有炬火，我便是唯一的光。\\n\\n恶习结业善习结果，为善为恶逐境而生\\n\\n在生活的过程中，会有很多次机会，让你不得不面对自己身上的缺陷和弱点，事实会一次又一次提醒你，你是一个不完美的人，需要改变。可人的本性是懒惰又脆弱的，大部分人选择麻痹自己，转向短期的即时刺激（上网购物打游戏，暴食社交一夜情），少数人选择改变自己，于是就会有痛苦，有反复，有成功和放弃。”“没有人生来完美，每一个趋近于优秀的人格，都是经过了多次自我改造的结果。没有试图去改变的人，继续重复着自己日复一日的生活，看着那些早已看过的烂熟于心的风景，而对于正在改变的人来说，每一天都是新的。\\n\\nThe decision to self-handicap is seen in part as an effort to create a positive public image that is motivated by self-presentation concerns. That is, when faced with probable failure and the blow to one\\'s public image that comes with it, one strategy for avoiding this situation is to deliberately reduce one\\'s control over the outcome and thereby lower public accountability\\n\\n**只要你做了，无论多少，你都是在行动，都是改变。再微小的改变，都值得褒奖**\\n\\n习惯的养成，依赖于四个部分：**触机（cue）、惯性行为（routine）、奖励（reward）和信念（belief）：**\\n\\n**触机：**即触发习惯的原因，你可以想象成手枪的扳机，按下扳机，子弹就打出去了。习惯的触机有很多，可能是时间、地点或场景。你早上刷牙的触机是起床这个动作；去吃午饭是因为时间到了（额，好吧，还有肚子饿了）；有人习惯睡前刷微博，那么触机可能是你躺下来盖上被子。触机本身没有好坏之分，决定习惯好坏的，是它引发的惯性行为。\\n\\n**惯性行为：**之所以叫惯性，是因为它是无意识的，比如一打开电脑就先上网看看娱乐新闻；比如睡前一定要刷一下朋友圈。在建立新习惯的过程中，**我们的自制力，就用于修正那些引起拖延的旧行为，将其替换为新的惯性**。在更正坏习惯时，你需要格外留意引发它的触机，同时关注自己的行为，不断提醒自己不要重蹈覆辙。这一步是最消耗时间和精力的过程，可能要与旧习惯反复拉锯，因为良好惯性行为的建立不仅需要有自制力去克服旧的行为，还需要在行为结束时获得正向的反馈，也就是下面要说的“奖励”。\\n\\n**奖励：这是习惯养成中至关重要的一环，它往往被人们忽略**。为什么坏习惯容易养成且难以改变？因为它们的奖励往往即时而明显：打游戏、刷网页、吃零食这些哪个不是这样？好习惯难以形成，也恰恰因为短期的奖励不够明显。背单词、健身、练书法这些行为往往需要较长的时间才能看到效果，有些人天生能从过程中获得精神激励，但很多人不行，那我们需要人为的赋予自己奖励：比如记录自己的成长和进步、时不时发个微博鼓励下自己、达成一些小目标时吃顿好的庆祝下等等（啊喂也不要暴饮暴食啊！）\\n\\n关于奖励，稍稍多说几句。诸如学习、健身这种事本身是有点反人性的，有没有什么办法能快速建立正向奖励机制呢？这个展开能写两本书...知乎上有很多学习、健身相关的优质答案了，我这里不赘述。我想提供一种思维：**积极的、开放性的、成长性的思维。即多去看看那些享受学习、享受健身的人是怎么做到这些事情的，尝试去学习他们的方法，把目光放在积极面上，而不是怀疑自己能力不行，觉得自己做不好。**\\n\\n此外，强化你的“信念”有助于你获得精神上的正反馈。\\n\\n**信念：** 这是支撑你建立习惯的内在动力：你想要每天背单词，是为了干掉英语考试；你想要学会弹吉他，是为了能在各种晚会上一显身手；你想要规律饮食、早睡早起，是为了身体健康；你想要健身减肥，可能是想俘获女神/男神的芳心。总之，你是想成为一个更好的人。**信念能让你你在养成好的习惯时获得精神上的正向反馈，同时，你的信念越强烈，就越能忍受改变过程中的痛苦与反复。**\\n\\n世上的努力可以分为两种。\\n\\n一种是「清醒」的努力。一种是「不清醒」的努力。\\n\\n其区别就在于“努力者”的主观意识层面是否**明确且清晰地知道自己在做什么、自己想要达到的是一个怎样的方向或目的。**\\n\\n理论上讲：一个人一旦能够**清醒**的知道自己想要的是什么并付出行动去追求的话，那么他就会自然而然的进入到一种**在别人看来**可以称之为努力的**状态**。而对于“努力者”自己而言，他并不会在意自己是不是努力的。\\n\\n但我们也必须意识到两个前提条件的存在：\\n\\n第一，我们是否能够付出行动去追求自己的目标往往会还会受到**思维观念**（对自己为什么要努力的主观认知）、**意志力**（能否坚持不断地付出行动）、**行为习惯**（打破固有的行为习惯总是非常困难的）、**反馈时长**（我们的付出需要多长时间才会得到回报）等方方面面因素的影响。\\n\\n第二，人是一种群体性生物，所以总会不可避免的受到**普世价值观**（给人类行为贴上的好坏善恶等标签）**、集体无意识**、**别人的看法和评价**（大多数人都倾向于从别人那里获得认同）、**社会的奖惩体系**（什么样的努力能得到回报，什么样的努力得不到回报）外界诱惑（网络游戏，香烟毒品，色情）等各个方面的影响。\\n\\n这两个前提条件的存在看起来似乎令我们想要“努力”看起来会是一件非常困难的事，因为无论是主观还是客观层面都有着这么多的限制和影响会令我们懒惰，消极，找不到方向。\\n\\n但是当我逐渐接触到越来越多形形色色的人之后，通过对不同类型的人观察、分析、交谈，和我自己的内化与尝试之后，我发现想要“努力”其实是一件很简单的事。\\n\\n**Ⅱ、**\\n\\n**我们对于努力的错误认知：**\\n\\n当我们谈论到「努力」这个话题的时候，往往会因为这个词的词性而和励志、超越常人、忍受痛苦坚持付出等概念联系在一起。\\n\\n诚然，「努力」的确是和这些概念有着相关性，也的确存在许多的「努力」的事迹令人很感动。\\n\\n但事实上就我所接触到的那些正在努力的人们中，只有很少一部分会以为自己的「努力」是值得吹嘘的资本，他们也很少会觉得自己的努力很励志，觉得自己比普通人强上许多，甚至他们也很少会**意识到**自己正在努力。\\n\\n他们基本上都不会去思考自己要**怎样才能**努力，他们也很少会纠结于自己**要不要**努力，他们本身就已经是处于努力的状态之中，他们只会去思考如何将问题解决，怎样提升效率，他们更多的是着眼于如何把事情做好。\\n\\n我们可以将努力视为一道门槛，对于没有跨越这道门槛的人而言，他们看到的只是「努力者」**外在的状态和表现**，他们对于努力的概念完全是来自于他们对于努力的「想象」，并将「努力者」完全理想化。\\n\\n由此便产生了两个最大的错误理解，这两个错误理解是令大多数人无法努力的最根本的原因。\\n\\n**Ⅲ、**\\n\\n第一：**将努力视为一种牺牲。**\\n\\n在大众的潜意识里，努力意味着舍弃快乐，放弃享受，要减少社交，要承受痛苦，要忍耐枯燥，要坚持不断地付出等。\\n\\n但实际上，对于真正清醒的努力者而言，「努力」并不是牺牲，也不意味着要放弃什么，努力是本身正是一种享受，是一种自我认同感的不断延续，是在向着自我实现的目标不断地前进。\\n\\n我反复强调过很多次，**人无法坚持做他不「想」做的事**，如果你不喜欢背单词，你不喜欢跑步的话，那么你几乎不可能仅凭着一种“要努力”的信念坚持下去。\\n\\n如果对你而言努力意味着一种牺牲的话，那么当你试图去努力的时候，在这个过程中你根本无法真正的沉下心去付出努力。\\n\\n你只会不断地被痛苦折磨，你的心中会有一种来自惰性和习惯的声音不断的干扰着你要你放弃，你会因为“理性”上要求自己付出努力和“感性”上提醒你这并不是你想做的而产生的冲突变得越来越焦虑和烦躁，你会找出无数个理由来拖延说服自己明天再做也没什么。\\n\\n你至多只能够在外在的行为上做出一副很努力的样子，你可能也的确会看很多书，背很多单词，花很长时间用来写作，但你真正记住的、真正写出来的根本就没有多少。\\n\\n最可怕的是，有相当大一部分的人很喜欢用这种「只是看起来很努力」的努力来自欺欺人和自我感动，他们只谈论自己「付出」了多少，却刻意的忽略了是否是**真正、有效**的付出。\\n\\n所以你现在应当意识到，努力并不是意味着一种牺牲，当你努力的时候也并非是要和痛苦抗争，你不是在通过「强迫自己做自己不想做的事」「不断地和自己的负面情绪与惰性做抗争」等这种痛苦的方式来进步。\\n\\n努力其实是一种「愉悦」的付出，努力这个过程本身对于你而言就是一种享受，因为你清楚地知道你的「努力」是在令你不断的向你想到达的目标前进，你会在「努力」的时候体会到一种精神贯注的状态，努力是基于你整体的认知模式的一种“正向”的提升，你根本不需要去纠结和焦虑，你也没有必要在努力的时候有一种「被剥夺感」。\\n\\n你用来努力的时间的确原本也可以用在打游戏、躺着玩、吃零食等这些“享受”的地方，但这绝对不意味着牺牲，这并不是说你放弃了享受。\\n\\n你选择努力这是你自己的一种**「主动」**的**选择**，努力在你的认知体系里是一种比打游戏、吃零食等更为高级且「收益」更大的享受。\\n\\n但正是因为我们对于努力的错误理解，我们将努力视为一种牺牲，所以我们付出努力的行动的时候内心总会感觉到有些不情愿，我们感觉自己的努力要付出“放弃享受”的代价，我们想要努力的意愿会不断的和想要「止损」的意愿抗争，这就导致我们在努力的时候无法全身心的投入，所以努力的「效果」必然会大打折扣。\\n\\n久而久之，我们就会不再「信任」努力，因为在我们潜意识的认知里，努力是一种既要牺牲和付出、又得不到多大回报的「负收益」投资。所以我们就不愿再努力。\\n\\n如果你能将这种思维观念转变过来，将努力视为一种主动选择的愉悦享受的话，那么你不仅能够更好的努力，而且努力的效果也会有很好的提升，因为你不会再在努力的时候做无意义的自我消耗。\\n\\n但也并不是说我们要将所有的努力都视为一种主动选择的愉悦享受。\\n\\n因为努力真的可以成为一种享受的前提的是，这种努力你是主观意志上真正愿意去做的选择。\\n\\n这就涉及到我们接下来要谈论的第二点。\\n\\n**Ⅳ、**\\n\\n**第二：我们努力所想要达到的目标多数并非我们主观意志上真正愿意去做的选择。**\\n\\n大多数人对于他们想要通过努力到达怎样的层次、达成怎样的目的并没有一个清晰的概念。\\n\\n我们从小就被灌输要努力的理念，社会、家长、环境告诉我们，要努力读书，将来才会有个好出路，要努力工作，才能获得体面的生活。\\n\\n媒体总在宣传和美化一些励志的故事，如：JJ三厘米的老王身残志坚努力三十年终成亚洲花式撸管大赛的冠军，郭东林努力在动物园假扮狗熊终于迎娶到他的女神野生范冰冰，一个狗在保定努力卖驴肉火烧终于月入2300达成每天都能吃烤串的小康生活。\\n\\n我们从小所受到的教育和环境给我们造成的最大的一个限制就是：我们都是在追逐着别人告诉我们「应该」去追逐的目标，我们的努力是为了达到一种在别人看来很好、别人都推崇的目的。\\n\\n我们被社会和他人捆绑着向前，我们总是为了别人而活。\\n\\n但是实际上你的内心深处并不一定想要个大房子，你也不一定想结婚，你也不在乎必须要有一份体面的工作，可现实的压力和思维的惯性一直在强迫着我们去追寻我们本身并不想要的东西，强迫我们为了一个我们并不想达成的目的而努力。\\n\\n像那些通过自身的努力而成为万众瞩目的明星，成为身价百万的富翁，成为指点江山的风云人物的确在普世价值观中是很厉害的人，许多人都对这些人崇拜有加，甚至示其为偶像。\\n\\n但是无论这些人多么厉害，如果他所达到的成就并不是你所想要达成的，那么你根本没有任何必要去羡慕这些人。因为他们和你并不在一个频率上，他们的成就或光环对于你而言也并没有什么值得参考和借鉴的价值。\\n\\n比如：马云的确是通过努力才做成了阿里巴巴商业帝国，许多人都在分析他的思维方式，生活习惯，将他说的一些听起来很有道理的话奉为金句，去了解他过去的生活和人生节点，仿佛如果他们能从马云身上学到些什么，他们能像马云一样努力的话也一定能成功了。\\n\\n但马云之所以会努力去做阿里巴巴那是因为他热爱自己的事业，他所做的是他真正喜欢，真正想做的事。你让马云去唱歌，去参政，他做着这些他并不喜欢做的事你觉得他有可能成功么？\\n\\n许多人正是因为社会洗脑和思维惯性的影响令他们总是试图向着在社会价值观内那些光鲜亮丽的目标努力，但他们的内心深处却又在不断地提醒着他们：你并不想当个律师，你根本不喜欢学英语，你讨厌做产品经理。\\n\\n在这种情况下，你根本不可能做出真正有效的努力。\\n\\n所以努力的前提是**你要先确定你努力的方向的确是出于你的本心，是你真正想做的事。当你朝着一个你内心并不认同，你自己并不喜欢的方向努力的时候，在这个过程中的努力就的确是一种牺牲了，并且，这也是对你人生的消耗。**\\n\\n如果你在做着你自己想做的事，那么你根本就不需要任何激励，不需要任何励志、自我感动的手段来给予自己力量，不需要任何促使你变得努力的方法，因为你本身就已经处于努力的状态中了。\\n\\n但是你也要意识到现实的诸多限制因素。\\n\\n我们不可能总是都可以做自己喜欢做的事，我们很多时候也总是不可避免的必须要去做一些我们不喜欢做的事。甚至有许多我们不喜欢做的事也需要我们“努力”去做。\\n\\n但是如果我们生活的整体的大方向是向着我们所喜欢做的事、向着我们真正想达成的目标前进的话， 那么那些你不喜欢的、你讨厌做的事也是处于你整体宏大的使命感之下，你也因此会清醒的为之付出努力。\\n\\n**Ⅶ、**\\n\\n**「清醒」**比努力更重要。\\n\\n努力在本质上只是你达成目的的手段之一而已，它和投机取巧，依靠父辈的资源等并无优劣之分，只不过是在社会的价值观中我们给努力赋予了更多正面的、积极的意义罢了。\\n\\n但是你自己必须要清楚，你没有必要用「努力」来自我感动，踏踏实实做事的人比投机取巧的人并不高贵，其区别只在于你就你目前的意识层面和认知水平在对事物做了衡量之后作出了不同的选择而已。\\n\\n这也是为什么我说**「清醒」**比努力更重要的原因，一旦你知道自己的目的何在，你就能够很清楚的去选择对你而言最有利最便捷的达成目的的方式，在这个时候你并不是一定要去努力，努力对你而言只不过是在众多的可选项中你在这一次的抉择中所选择的一种。\\n\\n大多数人虽然看起来是在活着，行动着，思考着，但他们其实和在睡梦中并无区别。他们很少会主动地去思考问题，他们也并不知道自己每天的所作所为意义何在，他们从没想过自己究竟要去向何方。\\n\\n所以你看到许多人浑浑噩噩的上学，浑浑噩噩的毕业，浑浑噩噩的工作，浑浑噩噩的结婚。他们也许的确也曾努力追求过什么，也的确沉下心坚持不断的付出过，比如上学的时候努力的学习，谈恋爱的时候努力的为对方付出，但他们却从没想过自己为什么要努力学习？自己是想通过努力学习达到什么样的一个目的？为这个人付出合不合适？他是否是真的想和对方在一起？\\n\\n他们只会被生活推着走，毕业了就随便找个工作，到了该结婚的年龄就去相亲，到了周末就躺在家里一整天。\\n\\n**如果我们不能为了自己而活，如果我们这一生都只是匆匆忙忙的体验些什么却从没有深入而沉静的融入，如果我们囿于社会的道德标准，别人的评价，现实的种种限制而不得不去做一些自己并不喜欢的工作，如果我们还没有遇到自己真正喜欢的人却因为害怕孤单而随便找了个人恋爱，那么我们来这世上走了一遭又有什么意义呢？**\\n\\n随着年龄的增长，我们会越来越在生活中体验到一种失控感和茫然，我们会发现有些事情做也可不做也行，有些人见也罢不见也无妨，有些行为既非对也非错，所以我们会逐渐的感觉生活好像变得越来越不真实，不知不觉间时间就那么过去了，而我们却什么都没有留下。\\n\\n这种失控感和茫然其实就是来自于我们从来都没有做过真正的自己，我们也没有明确且果断的去追寻过自己的目标，所以未来会变得越来越模糊，越来越不确定，我们的内心也就越来越空虚，越来越浮躁。\\n\\n在我没有下定决心要做我真正想做的事情之前，我所过的每一天都觉得无比的煎熬和痛苦，因为我很清楚我只是在虚度光阴，我只是在不停地变老，我只是在消耗自己的生命。\\n\\n我那时很清楚，我其实是想成为一个作家，当我的智慧和能力积累到了足够的层次后我还要办培训班授课，将智慧传递给别人，让更多的人「**觉醒**」。\\n\\n然而一直有两年的时间，我基于现实层面的考虑为了生存而一直在做着和我的梦想毫不相干的工作，在这两年间我内心的那个声音不断地在提醒着我：凤红邪，你现在是为了一时的金钱和对未来愚蠢地估量，而在消耗和浪费着你宝贵的生命。\\n\\n直到三个多月前我终于开始为了我这个目标去做一些尝试，我开始关注于自己的内心世界，我开始主动地结交有趣的人，我开始走出去见识更多的风景，这段时间我的成长和进步比我过去二十多年那空虚的生命加起来都要多得多。\\n\\n有很多人开始说我很有智慧，说我坚持精心打磨写很长的文章很努力，说我很有行动力，whatever，这些对我而言都无所谓，随着我学到的越多，便越觉得自己的无知与渺小，我也并没有觉得自己有多么努力。\\n\\n但回过头来想想才发现，当我完全沉浸在我所感兴趣的书中，当我费尽心思花一周的时间写一篇文章的时候，我的这些行为就已经是很「努力」的了啊。\\n\\n我知道你看到这里或许会觉得略有些励志，但我必须在最后这一部分提醒你，我的确是在鼓励你不要畏惧各种各样的限制，要勇敢的去追求你想做的事。但我一直也在强调**「清醒」。**\\n\\n清醒的意思是，你可以选择去做你想做的事，但同时，**你也完全可以不去做你想做的事**。\\n\\n你对此拥有百分之一百的绝对选择权，我们不可能将现实层面的所有限制全部忽略不计，假如说你父母身患重病，家里八个小妹都还在上学，你们还欠了一百万的外债，这个时候你说你要追求梦想去造火箭，很明显，做出这样选择的你并不清醒。\\n\\n**Ⅷ、**\\n\\n但无论你是否会选择努力去追求你的梦想，那都不重要。\\n\\n重要的是，你得很清楚自己在做什么。\\n\\n如果基于现实层面的各种因素令你的确不能去做自己想做的事，那么就没必要再为此纠结。因为做自己想做的事并不是你必须要做的选择，这也不是一件多么高贵的事，所有的选择在本质上都是平等的。\\n\\n最根本的、也是我这篇文章真正想传达给你的是：\\n\\n你应当从浑浑噩噩的生活中清醒过来，你应该学会为自己负责，从现在开始你要意识到你所做的每一件事都应当是你「主动的选择」。\\n\\n人生的旅程就是不断地做出一个又一个的选择，如果每一个选择都是你**清醒**且**主动**的选择，那么，\\n\\n在你的这段生命中，你就是你自己完全的主宰者。\\n\\n而这样的生命，无论怎样对你而言都会是值得骄傲、且精彩纷呈的。\\n\\n而当一个人个性越明显，自我定位（self-identification）越明确，那同别人就越不同，隔离也就越明显，孤独感就越强烈。\\n\\n所以，孤独源于理性。因为人必然有理性，而理性必然导致隔离，隔离必然导致孤独，所以，孤独有本体论意义，或者说有客观性或普遍性，是存在于每一个人（有理性的存在着）身上的。\\n\\n自由和孤独是一体两面的，因为都来自于理性。\\n\\n所谓的自由，就是努斯，是理性的主观能动性，是意识到自己的选择、作出选择并付诸行动。《逃避自由》里讲了三种现代人逃避自由来逃避孤独的情况，其中有一样是机械协同，或者说盲目从众。比如，上了大学，独处时很孤独，所以干脆找人多的地方，同他们干一样的事情，比如沉溺于游戏，或者天天纵情于ktv，或者跟一群人陷入各种八卦聊天的泥沼谈话中，这样，ta就感觉没那么难受了，ta感觉大家都做一样的事，就有了归属感和安全感，就没孤独那种痛苦了。可其实，ta是放弃了理性，放弃去探索自我，通过消解自我去融入别人的圈子，这也就是ta放弃了为自己未来使命做选择的自由。他逃避自由，通过消解自我来逃避孤独。\\n\\n这时候，我们看到，理性、自我、自由、孤独其实都是共生的。我很多时候选择孤独，只不过我选择保持个性和自我，选择自由和理性，我舍不得放下这些东西，即便孤独往往是痛苦的。当然，孤独也是可以享受和超越的。若你读到这了，建议去读读《逃避自由》吧，看看作者如何论述用\"积极自由\"去真正超越孤独。这是影响我此生最大的一本书之一', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4dbaf8ae-b541-4f15-9341-37434a00a5cb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n杂 90\\n\\nSkinner believed that discussions of an animal's (or a human's) internal state were unnecessary. If you knew an animal's reinforcement history, you could determine what “stimuli” were likely to have positive or negative valence.\\n\\nThe fundamental problem with this argument is one of parsimony. It is impossible to know an animal's “reinforcement history”—particularly if that animal is as complex and long-lived as a human being.\\n\\n当限制了自己的选项，让自己focus在几件事后，虽然失去了选择的freedom，但得到了inner peace的freedom，因为你不再需要give a f about other things \\n\\ndeath is the ultimate form that helps you define what matters for u \\n\\nHope is having a tangible vision of the future that is worth living for.\\n\\nHOW TO START YOUR OWN RELIGION Step Five: Promise Heaven, Deliver Hell\\n\\nThe beauty of a religion is that the more you promise your followers salvation, enlightenment, world peace, perfect happiness, or whatever, the more they will fail to live up to that promise. And the more they fail to live up to that promise, the more they’ll blame themselves and feel guilty. And the more they blame themselves and feel guilty, the more they’ll do whatever you tell them to do to make up for it.\\n\\nExperiences generate emotions. Emotions generate values. Values generate narratives of meaning. And people who share similar narratives of meaning come together to generate religions. The more effective (or affective) a religion, the more industrious and disciplined the adherents. And the more industrious and disciplined the adherents, the more likely the religion is to spread to other people, to give them a sense of self-control and a feeling of hope. These religions grow and expand and eventually define in-groups versus out-groups, create rituals and taboos, and spur conflict between groups with opposing values.\\xa0*These conflicts must exist because they maintain the meaning and purpose for people within the group.*\\n\\nTherefore, it is the conflict that maintains the hope. So, we’ve got it backward: everything being fucked doesn’t require hope; hope requires everything being fucked.\\n\\nThe sources of hope that give our lives a sense of meaning are the same sources of division and hate. The hope that brings the most joy to our lives is the same hope that brings the greatest danger. The hope that brings people closest together is often the same hope that tears them apart.\\n\\nHope is, therefore, destructive. Hope depends on the rejection of\\xa0*what currently is*.\\n\\nBecause hope requires that something be broken. Hope requires that we renounce a part of ourselves and/or a part of the world. It requires us to be anti-*something*.\\n\\nThis paints an unbelievably bleak picture of the human condition. It means that our psychological makeup is such that our only choices in life are either perpetual conflict or nihilism—tribalism or isolation, religious war or the Uncomfortable Truth.\\n\\nNietzsche instead believed that we must look beyond hope. We must look beyond values. We must evolve into something “beyond good and evil.” For him, this morality of the future had to begin with something he called\\xa0*amor fati*, or “love of one’s fate”: “My formula for greatness in a human being,” he wrote, “is\\xa0*amor fati*: that one wants nothing to be different, not forward, not backward, not in all eternity. Not merely bear what is necessary, still less conceal it—all idealism is mendacity in the face of what is necessary—but love it.”20\\n\\n*Amor fati*, for Nietzsche, meant the unconditional acceptance of all life and experience: the highs and the lows, the meaning and the meaninglessness. It meant loving one’s pain, embracing one’s suffering. It meant closing the separation between one’s desires and reality not by striving for more desires, but by simply desiring reality.\\n\\nIt basically meant: hope for nothing. Hope for what\\xa0*already*\\xa0is—because hope is ultimately empty. Anything your mind can conceptualize is fundamentally flawed and limited and therefore damaging if worshipped unconditionally. Don’t hope for more happiness. Don’t hope for less suffering. Don’t hope to improve your character. Don’t hope to eliminate your flaws.\\n\\nHope for\\xa0*this*. Hope for the infinite opportunity and oppression present in every single moment. Hope for the suffering that\\xa0comes with freedom. For the pain that comes from happiness. For the wisdom that comes from ignorance. For the power that comes from surrender.\\n\\nAnd then act\\xa0*despite*\\xa0it.\\n\\nThis is our challenge, our calling: To act without hope. To not hope for better. To\\xa0*be*\\xa0better. In this moment and the next. And the next. And the next.\\n\\nEverything is fucked. And hope is both the cause and the effect of that fuckedness.\\n\\nBut the only thing that frees us\\xa0*is*\\n\\xa0that truth: You and I and everyone we know will die, and little to nothing that we do will ever matter on a cosmic scale. And while some people fear that this truth will liberate them from all responsibility, that they’ll go snort an eight ball of cocaine and play in traffic, the reality is that this truth scares them because it liberates them\\xa0*to responsibility*\\n. It means that there’s no reason to\\xa0*not*\\n\\xa0love ourselves and one another. That there’s no reason to\\xa0*not*\\n\\xa0treat ourselves and our planet with respect. That there’s no reason to\\xa0*not*\\n\\xa0live every moment of our lives as though it were to be lived in eternal recurrence\\n\\nEventually, though, we realize that the most important things in life cannot be gained through bargaining. You don’t want to bargain with your father for love, or your friends for companionship, or your boss for respect. Bargaining with people into loving or respecting you feels shitty. It undermines the whole project. If you have to convince someone to love you, then they don’t love you. If you have to cajole someone into respecting you, then they will never respect you. If you have to convince someone to trust you, then they won’t actually trust you.\\n\\nThe most precious and important things in life are, by definition, nontransactional. And to try to bargain for them is to immediately destroy them. You cannot conspire for happiness; it is impossible. But this is often what people try to do, especially when they seek out self-help and other personal development advice—they are essentially saying, “Show me the rules of the game I have to play, and I’ll play it,” not realizing that it’s the very fact that they think there are rules to happiness that is preventing them from being happy.\\n\\nWhile people who navigate life through bargaining and rules can get far in the\\xa0*material*\\xa0world, they remain crippled and alone in their\\xa0*emotional*\\xa0world. This is because transactional values create relationships that are built upon manipulation.\\n\\nHonesty is more important than getting what you want or achieving a goal. Honesty is inherently good and valuable, in and of itself. Honesty is therefore an\\xa0*end*, not a means to some other end.\\n\\nAn adolescent will say he loves you, but his conception of love is that he is getting something in return, that love is merely an emotional swap meet, where you each bring everything you have to offer and haggle with each other for the best deal. An adult will love freely without expecting anything in return because an adult understands that that is the only thing that can make love real. An adult will give without seeking anything in return, because to do so defeats the purpose of a gift in the first place.\\n\\nThe principled values of adulthood are unconditional—that is, they cannot be reached through any other means. They are ends in and of themselves\\n\\n!Untitled\\n\\nThere are plenty of grown-ass children in the world. And there are a lot of aging adolescents. Hell, there are even some young adults out there. That’s because, past a certain point, maturity has nothing to do with age.\\xa0What matters are a person’s\\xa0*intentions*. The difference between a child, an adolescent, and an adult is not how old they are or what they do, but\\xa0*why*\\xa0they do something. The child steals the ice cream because it feels good, and he is oblivious or indifferent to the consequences. The adolescent doesn’t steal because he knows it will create worse consequences in the future, but his decision is ultimately a bargain with his future self: I’ll forgo some pleasure now to prevent greater future pain.\\n\\nBut it’s only the adult who doesn’t steal for the simple principle that stealing is wrong. And to steal, even if she gets away with it, would make her feel worse about herself\\n\\n!Untitled\\n\\nMaking the leap of faith into a virtuous adulthood requires not just an ability to endure pain, but also the courage to abandon hope, to let go of the desire for things always to be better or more pleasant or a ton of fun. Your Thinking Brain will tell you that this is illogical, that your assumptions must inevitably be wrong in some way. Yet, you do it anyway. Your Feeling Brain will procrastinate and freak out about the pain of brutal honesty, the vulnerability that comes with loving someone, the fear that comes from humility. Yet, you do it anyway.\\n\\nKant started with a simple observation. In all the universe, there is only one thing that, from what we can tell, is completely scarce and unique: consciousness. To Kant, the\\xa0*only*\\xa0thing that distinguishes us from the rest of the matter in the universe is\\xa0our ability to reason—we’re able to take the world around us and, through reasoning and will, improve upon it. This, to him, was special, exceedingly special—a miracle, almost—because for everything in the infinite span of existence,\\xa0*we*\\xa0are the only thing (that we know of) that can actually\\xa0*direct existence*. In the known cosmos,\\xa0*we*\\xa0are the only sources of ingenuity and creativity.\\xa0*We*\\xa0are the only ones who can direct our own fate.\\xa0*We*\\xa0are the only ones who are self-aware. And for all we know, we are the only shot the universe has at intelligent self-organization.\\n\\nTherefore, Kant cleverly deduced that, logically, the supreme value in the universe\\xa0*is the thing that conceives of value itself*. The only true meaning in existence\\xa0*is the ability to form meaning*. The only importance is the thing that decides importance\\n\\nAnd, as if that weren’t enough, it explains all of it in a single sentence. The Formula of Humanity states, “Act that you use humanity, whether in your own person or in the person of any other, always at the same time as an end, never merely as a means.”\\n\\nThat’s it. The Formula of Humanity is the single principle that pulls people out of adolescent bargaining and into adult virtue\\n\\nTo transcend the transactional realm of hope, one must act\\xa0*unconditionally*\\n. You must love someone without expecting anything in return; otherwise it’s not truly love. You must respect someone without expecting anything in return; otherwise you don’t truly respect him. You must speak honestly without expecting a pat on the back or a high-five or a gold star next to your name; otherwise you aren’t truly being honest.\\n\\nKant summed up these unconditional acts with one simple principle: you must treat humanity never merely as a means, but always as an end itself.\\n\\nBut what does this look like in day-to-day life? Here’s a simple example:\\n\\nLet’s pretend that I’m hungry and I want a burrito. I get in my car and drive to Chipotle and order my usual double-meat monster that makes me oh so happy. In this situation, eating the burrito is my “end” goal. It’s ultimately why I’m doing everything else: getting in the car, driving, buying gas, and so on. All these things I do to get the burrito are the “means,” i.e., the things I must do in order to achieve my “end.”\\n\\nMeans are things that we do conditionally. They are what we bargain with. I don’t want to get in my car and drive, and I don’t want to pay for gas, but I do want a burrito. Therefore, I must do these other things to get that burrito.\\n\\nAn end is something that is desired for its own sake. It is the defining motivating factor of our decisions and behaviors. If I wanted to eat a burrito only because my wife wanted a burrito and I wanted to make her happy, then the burrito is no longer my end; it is now a means to an even greater end: making my wife happy. And if I only wanted to make my wife happy so I could get laid tonight, now my wife’s happiness is a means to a greater end, which in this case is sex.\\n\\nLikely that last example made you squirm a little bit, made you feel that I’m kind of a dirtbag.\\xa0That’s\\xa0*exactly*\\xa0what Kant is talking about. His Formula of Humanity states that treating any human being (or any consciousness)\\xa0*as a means to some other end*\\xa0is the basis of all wrong behavior. So, treating a burrito as a means to my wife’s end is fine. It’s good to make your spouse happy sometimes! But if I treat my wife as a means to the end of sex, then I\\xa0am now treating her merely as a means, and as Kant would argue, that is some shade of wrong.\\n\\nKant’s Formula of Humanity doesn’t only describe our moral intuition into what’s wrong; it also explains the adult virtues, those actions and behaviors that are good for their own sake. Honesty is good in and of itself because it’s the only form of communication that\\xa0*doesn’t*\\n\\xa0treat people merely as a means. Courage is good in and of itself because to fail to act is to treat either yourself or others as a means to the end of quelling your fear. Humility is good in and of itself because to fall into blind certainty is to treat others as a means to your own ends.\\n\\nIf there were ever to be a single rule to describe all desirable human behavior, the Formula of Humanity would probably be it. But here’s the beautiful thing: unlike other moral systems or codes, the Formula of Humanity does not rely on hope. There’s no great system to force onto the world, no faith-based supernatural beliefs to protect from doubt or lack of evidence.\\nThe Formula of Humanity is merely a principle. It doesn’t project some future utopia. It doesn’t lament some hellish past. No one is better or worse or more righteous than anyone else. All that matters is that conscious will is respected and protected. End of story.\\nBecause Kant understood that when you get into the business of deciding and dictating the future, you unleash the destructive potential of hope. You start worrying about converting people rather than honoring them, destroying evil in others rather than rooting it out in yourself.\\nInstead, he decided that the only logical way to improve the world is through improving ourselves—by growing up and becoming more virtuous—by making the simple decision, in each moment, to treat ourselves and others as ends, and never merely as means. Be honest. Don’t distract or harm yourself. Don’t shirk responsibility or succumb to fear. Love openly and fearlessly. Don’t cave to tribal impulses or hopeful deceits. Because there is no heaven or hell in the future. There are only the choices you make in each and every moment.\\n\\nKant understood that there is a fundamental link between our respect for ourselves and our respect for the world. The values that define our identity are the templates that we apply to our interactions with others, and little progress can be made with others until we’ve made progress within ourselves.39\\xa0When we pursue a life full of pleasure and simple satisfaction, we are treating\\xa0*ourselves*\\xa0as a means to our pleasurable ends. Therefore, self-improvement is not the cultivation of greater happiness but, rather, a cultivation of greater self-respect. Telling ourselves that we are worthless and shitty is just as wrong as telling others that they are worthless and shitty. Lying to ourselves is just as unethical as lying to others. Harming ourselves is just as repugnant as harming others. Self-love and self-care are therefore not something you learn about or practice. They are something you are ethically called to cultivate within yourself, even if they are all that you have left.\\n\\nThe Formula of Humanity has a ripple effect: your improved ability to be honest with yourself will increase how honest you are with others, and your honesty with others will influence them to be more honest with themselves, which will help\\xa0*them*\\xa0to grow and mature. Your ability not to treat yourself as a means to some other end will in turn allow you to better treat others as ends. Therefore, your cleaning up your relationship\\xa0*with yourself*\\xa0has the positive by-product of cleaning up your relationships with others, which then enables them to clean up their relationships with themselves, and so on.\\n\\n*This*\\xa0is how you change the world—not through some all-encompassing ideology or mass religious conversion or misplaced dreams of the future, but by achieving the maturation and dignity of each individual in the present, here and now. There will always be different religions and different value systems based on culture and experience; there will always be different ideas about where we’re going and where we’ve come from. But, as Kant believed, the simple question of dignity and respect in each moment must be universal.\\n\\nRoy Baumeister began researching the concept of evil. Basically, he looked at people who do bad things and at why they do them.\\n\\nAt the time it was assumed that people did bad things because they felt horrible about themselves—that is, they had low self-esteem. One of Baumeister’s first surprising findings was that this was often not true. In fact, it was\\xa0usually the opposite. Some of the worst criminals felt pretty damn good about themselves. And it was this feeling good about themselves in spite of the reality around them that gave them the sense of justification for hurting and disrespecting others.\\n\\nEvil people never believe that\\xa0*they*\\xa0are evil; rather, they believe that everyone else is evil.\\n\\nLeaders need their followers to be perpetually dissatisfied; it’s good for the leadership business. If everything were perfect and great, there’d be no reason to follow anybody.\\n\\nConversely, Nietzsche argued, the “slaves” of society would generate a moral code of their own. Whereas the masters believed they were righteous and virtuous because of their\\xa0*strength*\\n, the slaves of society came to believe that they were righteous and virtuous because of their\\xa0*weakness*\\n. Slave morality believes that people who have suffered the most, those who are the most disadvantaged and exploited, deserve the best treatment\\xa0*because* of that suffering. Slave morality believes that it’s the poorest and most unfortunate who deserve the most sympathy and the most respect.\\n\\nWhereas master morality believes in the virtue of strength and dominance, slave morality believes in the virtue of sacrifice and submission. While master morality believes in the necessity of hierarchy, slave morality believes in the necessity of equality. While master morality is generally represented by right-wing political beliefs, slave morality is usually found in left-wing political beliefs\\n\\nbeing heroic is the ability to conjure hope where there is none. To strike a match to light up the void. To show us a possibility for a better world—not a better world we\\xa0*want*\\n\\xa0to exist, but a better world we didn’t know\\xa0*could* exist. To take a situation where everything seems to be absolutely fucked and still somehow make it good.\\n\\nHopelessness is the root of anxiety, mental illness, and depression. It is the source of all misery and the cause of all addiction.\\xa0This is not an overstatement. Chronic anxiety is a crisis of hope. It is the fear of a failed future. Depression is a crisis of hope. It is the belief in a meaningless future. Delusion, addiction, obsession—these are all the mind’s desperate and compulsive attempts at generating hope one neurotic tic or obsessive craving at a time.5\\n\\nThe avoidance of hopelessness—that is, the construction of hope—then becomes our mind’s primary project. All meaning, everything we understand about ourselves and the world, is constructed for the purpose of maintaining hope. Therefore, hope is the only thing any of us willingly dies for. Hope is what we believe to be greater than ourselves. Without it, we believe we are nothing.\\n\\nTo build and maintain hope, we need three things: a sense of control, a belief in the value of something, and a community.\\n\\xa0“Control” means we feel as though we’re in control of our own life, that we can affect our fate. “Values” means we find something important enough to work toward, something better, that’s worth striving for. And “community” means we are part of a group that values the same things we do and is working toward achieving those things. Without a community, we feel isolated, and our values cease to mean anything. Without values, nothing appears worth pursuing. And without control, we feel powerless to pursue anything. Lose any of the three, and you lose the other two. Lose any of the three, and you lose hope\\n\\nEvery problem of self-control is not a problem of information or discipline or reason but, rather, of\\xa0*emotion*. Self-control is an emotional problem; laziness is an emotional problem; procrastination is an emotional problem; underachievement is an emotional problem; impulsiveness is an emotional problem.\\n\\nThis sucks. Because emotional problems are much harder to deal with than logical ones\\n\\nThe overindulgence of emotion leads to a crisis of hope, but so does the repression of emotion.\\n\\nInstead of bombarding the Feeling Brain with\\xa0*facts*\\xa0and\\xa0*reason*, start by asking how it’s feeling. Say something like “Hey, Feeling Brain, how do you feel about going to the gym today?” or “How do you feel about changing careers?” or “How do you feel about selling everything and moving to Tahiti?”\\n\\nThe Feeling Brain won’t respond with words. No, the Feeling Brain is too quick for words. Instead, it will respond with feelings. Yeah, I know that’s obvious, but sometimes you’re kind of a dumbass, Thinking Brain.\\n\\nThe Feeling Brain might respond with a feeling of laziness or a feeling of anxiety. There might even be multiple emotions, a little bit of excitement with a pinch of anger thrown into the mix. Whatever it is, you, as the Thinking Brain (aka, the responsible one in this cranium), need to remain nonjudgmental in the face of\\xa0whatever feelings arise. Feeling lazy? That’s okay; we all feel lazy sometimes. Feeling self-loathing? Perhaps that’s an invitation to take the conversation further. The gym can wait.\\n\\nIt’s important to let the Feeling Brain air out all its icky, twisted feelings. Just get them out into the open where they can breathe, because the more they breathe, the weaker their grip is on the steering wheel of your Consciousness Car.28\\n\\nThen, once you feel you’ve reached a point of understanding with your Feeling Brain, it’s time to appeal to it in a way it understands: through feelings. Maybe think about all the benefits of some desired new behavior. Maybe mention all the sexy, shiny, fun things at the desired destination. Maybe remind the Feeling Brain how good it feels to have exercised, how great it will feel to look good in a bathing suit this summer, how much you respect yourself when you’ve followed through on your goals, how happy you are when you live by your values, when you act as an example to the ones you love.\\n\\nBasically, you need to bargain with your Feeling Brain the way you’d bargain with a Moroccan rug seller: it needs to believe it’s getting a good deal, or else there’ll just be a lot of hand waving and shouting with no result. Maybe you agree to do something the Feeling Brain likes, as long as it does something it doesn’t like. Watch your favorite TV show, but only at the gym while you’re on the treadmill. Go out with friends, but only if you’ve paid your bills for the month.29\\n\\nWhen you offer something easy with an emotional benefit (e.g., feeling good after a workout; pursuing a career that feels significant; being admired and respected by your kids), the Feeling Brain will respond with another emotion, either positive or negative. If the emotion is positive, the Feeling Brain will be willing to drive a little bit in that direction—but only a little bit! Remember: feelings never last. That’s why you start small. Just put on your gym shoes today, Feeling Brain. That’s all. Let’s just see what happens.\\n\\nIf the Feeling Brain’s response is negative, you simply acknowledge that negative emotion and offer another compromise. See how the Feeling Brain responds. Then rinse and repeat.\\n\\nBut whatever you do,\\xa0*do not fight*\\xa0the Feeling Brain. That just makes things worse. For one, you won’t win, ever. The Feeling Brain is always driving. Second, fighting with the Feeling Brain about feeling bad will only cause the Feeling Brain to feel even worse. So, why would you do that? You were supposed to be the smart one, Thinking Brain.\\n\\nThis dialogue with your Feeling Brain will continue back and forth like this, on and off, for days, weeks, or maybe even months. Hell, years. This dialogue between the brains takes practice. For some, the practice will be recognizing what emotion the Feeling Brain is putting out there. Some people’s Thinking Brains have ignored their Feeling Brains for so long that it takes them a while to learn how to listen again.\\n\\nwe lie to others because we’re in such a habit of lying to ourselves\\n\\n**Our Self-Worth Equals the Sum of Our Emotions Over Time**\\n\\n**Your Identity Will Stay Your Identity Until a New Experience Acts Against It**\\n\\nThese narratives we invent for ourselves around what’s important and what’s not, what is deserving and what is not—these stories stick with us and define us, they determine how we fit ourselves into the world and with each other. They determine how we feel about ourselves—whether we deserve a good life or not, whether we deserve to be loved or not, whether we deserve success or not—and they define what we know and understand about ourselves.\\n\\nThis network of value-based narratives is our\\xa0*identity*.\\n\\nAnd the worst thing is, the longer we’ve held onto these narratives, the less aware we are that we have them. They become the background noise of our thoughts, the interior decoration of our minds. Despite being arbitrary and completely made up, they seem not only natural but inevitable.35\\n\\nThe values we pick up throughout our lives crystallize and form a sediment on top of our personality.36\\xa0The only way to change our values is to have experiences\\xa0*contrary*\\xa0to our values. And any attempt to break free from those values through new or contrary experiences will inevitably be met with pain and discomfort.\\xa0This is why there is no such thing as change without pain, no growth without discomfort. It’s why it is impossible to become someone new without first grieving the loss of who you used to be.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1209f5c5-9ecf-4be2-9fb3-5e6b539ac9bc', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n杂 63\\n\\n**关于佛法、基督教和易经的对话（绝对好文，值得一看）**\\n\\n2010年冬夜, 在华德书院，金博士和陈阳老师之间有一次精彩的对话，现摘录出來，与大家分享。\\n\\n金立佐,英国牛津大学博士.中国国际金融有限公司（CICC）的创建人之一。曾任英国洛希尔商人银行、摩根士丹利投资银行高管。CEA（UK）（英国中国经济学会）首任会长.2009年最具影响力独立董事奖获得者。\\n\\n陈阳，自幼习《易》，继承家学，酷爱传统文化，浸淫《易经》二十多年，始有所得。及长，师从四川成都文殊院释宽霖大和尚，先后受教于多位高僧大德，潜心修行多年。\\n\\n有个很有意思的故事，当年佛祖说过这么一句话：“阿陀那识甚深细，一切种子如瀑流；我于凡愚不开演，恐彼分别执为我。”“阿陀那识”指的就是第八阿赖耶识，“甚深细”指第八识的道理特别深、特别细；“一切种子如瀑流”，佛讲八识中的一切种子如同瀑布一样流淌下来，形成生命界的根本苦况：轮回；“我于凡愚不开演，恐彼分别执为我”，这个第八识并不是普通意义上所谓“灵魂”啦，并不是有个不灭的、单一的轮回转生的主体“灵魂”……\\n\\n佛不开演，他当然不开演，结果佛在世的时候果然没有开演。佛在世的时候，涉及到这个部分的讲法，一直比较含混。唯识的道理，直到龙树菩萨横空出世，他自己在被龙王菩萨度化以后，然后他建立自己的中观唯识系统的时候才建立起来。\\n\\n建立起来以后，就日渐衰微，等传到印度那烂陀寺戒贤祖师手里的时候，已经汲汲可危、后继无人了。戒贤祖师很痛苦，就想涅磐圆寂，但有菩萨在虚空中现身，告诉戒贤要再等等，再过多少年，会从东土大唐来一个叫玄奘的中华僧人，你传给他才算功德圆满，然后你才能离世。\\n\\n当时戒贤祖师年龄已经很大了，身体也不太好，大概有类风湿之类的毛病，所以他只好听菩萨吩咐，一直活到一百五十多岁，一直等到玄奘法师来。玄奘法师穿过西域来到了那烂陀寺，戒贤祖师见到他以后就哭，说你怎么才来呀？然后玄奘法师就花功夫把唯识学通了。学好之后他就回大唐中国了。\\n\\n他回来传唯识学，碰到两个大问题。首先，中国人尤其是汉族，特别善长搞圆融，搞圆融体悟都是一等一的高手，搞逻辑分析差不多都脑筋短路。所以我们中国人悟这个悟那个很灵光，一旦遇到细密、繁锁的逻辑分析立即就大脑短路。其次，玄奘法师的梵文造诣是超时代的，他甚至比印度人都高明，所以他在辩经的时候，全印度没有人能超过他。但是我们可爱的玄奘法师，怎么说呢，他有点像我们这个年代的一些海归，英文很好，中文不灵；玄奘法师他自己的母语——汉语的造诣，实在是不敢恭维。\\n\\n所以玄奘法师在佛经翻译方面，就不如鸠摩罗什法师通达，实在是因为他汉语的造诣比不上不如鸠摩罗什法师。鸠摩罗什法师偏偏是个“老外”，但他的汉语倒比玄奘本人还好。就这么古怪。因为一方面中国人的习惯思维，实在是不善长唯识逻辑；另一方面玄奘法师的翻译实在艰涩难懂，因为这两个因素，唯识学在中国甚至比在印度衰落得更快。玄奘法师建立法相宗之后，到了二代，到他的大弟子窥基法师手里，已经迅速地衰落下去了。\\n\\n窥基法师是玄奘法师的罗汉弟子转世再来。传说玄奘法师取经途中，半路上唤醒过一个远古时代就入定了的罗汉，双方约定，罗汉转世到中国，等玄奘法师回国后即出家做他的弟子。这位罗汉投胎到了大唐著名武将尉迟敬德家，后来就追随玄奘法师出家了，这就是窥基法师的来历。唯识学这一脉，在玄奘法师手里专门立了一派，叫法相宗，所以有时候也称唯识法相宗。这一脉传到了窥基法师就没落了。但是唯识法相的经典论著，因为玄奘法师的特别看重，所以对这个部分的翻译是当时优先进行的。\\n\\n唯识法相这个系统，主要的经论包括这几部：《华严经》《愣严经》《愣伽经》，还有一部特别重要的佛经，就是《解深密经》；论包括《瑜伽师地论》《成唯识论》等。\\n\\n唯识有多难呢？我们举个例子。佛说我们人类一昼夜念头有13亿转，现在地球上有67亿人，作一个假设，这67亿人全部成佛以后有多少位佛？大家肯定说有67亿佛。这个就是我们这些“凡、愚”学理不通，想当然的理解罢了。从唯识的道理，那真正是念念成佛。假设67亿人平均寿命50年，那么大家尽寿命有多少个念头？大致是1300000000×67×50×365＝1589575000000000个。每个念头成佛，所以地球上这些人真正成佛，我们即使假设，那也是1589575000000000数量级的化身佛。\\n\\n这个，对我们人类的普通智慧来说，理解起来实在是太困难了。《六祖坛经》说“自性众生誓愿度”，我们内在的自性众生有多少？佛家讲无量，一定要讲出个量来，至少这一生，以活50年计，也得1589575000000000这么多的众生。每个众生成佛，你算算看？……我们被自己的“一生”这个观念迷惑住了，其实我们念念生死，所以一生一死是一生，念头刹那生灭那也是一生……都是一生，每个一生化身成佛，这就是唯识的道理，绝对不简单。禅宗说“自性众生誓愿度”，自性众生有多少需要度，禅宗说“无量”；但唯识就认为这个有问题，你不要动不动“无量”，凡事要定量、定性分析，所以唯识特别接近现代科学的思维方式。现在，我们能够理解唯识吗？很难。何识佛祖在世的2500年以前，所以面对当时的人，佛也只能说得隐隐约约。\\n\\n关于千百万亿化身佛的事，佛经里有个菩萨就很能表现这一点。这位菩萨就是执行力第一的“大行普贤菩萨”。佛经里，有一次文殊菩萨问众多的菩萨，什么样的法门，是你们认为最好的？最适合生命界的众生去学习、去实践？这就相当于我们学数学，总会有个最好的学习方法。普贤菩萨报告说，他的方法叫“心根圆通法门”。什么叫“心根圆通法门”呢？普贤菩萨说，他在三千大千世界中，以心根谛听，如果有众生发起了修行成佛的正念，乃至有众生发了善念，我就恒时派无量无数的化身到这个众生那里，帮助对方，直到对方超越轮回，成就佛果。这个法门说浅显点，就是他心通配合无量无数的化身做功德，所以普贤菩萨对三千大千世界所有众生的心念都听得到、听得懂。\\n\\n这个当然不是一般人做得来的，然后文殊菩萨只好评价说，你说的这个方法，不是低级别的菩萨能办到的。后来观世音菩萨就报告他的耳根圆通法门，这个无论初级中级高级都能进行，被文殊菩萨评为第一。由这段可以知道，高等级的菩萨都可以达到无量无数的化身成就，何况成佛？\\n\\n佛经还记载，普贤菩萨有一次神识离开他的主体，很好奇去探究他身体里面的奥秘，结果陷进他的毛孔里12年，差点迷路回不来了。期间，普贤菩萨在他自己的毛孔里看到无量无数的佛国，看到无量无数的佛国有无量无数的佛，正在给无量无数的菩萨说法。然后普贤菩萨的主体意识迷到里面差点出不来。\\n\\n大家把这些都当神话看，但在佛法修行的实践中，这都是要“信”的。只有信，才会去仔细思维和研究，然后才能实际达到那个程度，这才能成佛。所以研究唯识学很麻烦，得配合《华严经》，《华严经》的道理是完全颠覆我们对宇宙的既有认识的。而唯识学，你真正修禅宗、修密宗，都是要真正掌握的，而且遵循其道理实际修证的。\\n\\n当然，我们现在在佛理上基本狗屁不通，还谈什么禅宗、密宗。\\n\\n真正大乘，你看《华严经》，一个初地得果的菩萨，化身就能够游行他方世界，旅游到一百个佛国土，亲见一百佛，供养一百佛。就是说他能把一百个念头转化成化身，凭这个旅游与参学一百个外星球。所以看《华严经》，几地菩萨能到多少世界，见多少佛，到了十地菩萨，叫法云地，那可真是亲到无量佛世界，亲见无量佛了。\\n\\n这些你要是结合道家看，就非常有意思。道家先出阴神，然后修炼出阳神，还要渡雷劫，一不小心一个雷劈下来，魂飞魄散，这就完了，叫“退道”。一雷劈散了，不知道灵识散了转世成蚂蚁还是什么，从头再来，要过无量劫再获得人生，再慢慢来。如果渡过去了，就往纯阳转换。每一次渡雷劫的时候，既是一个死的过程，也是一个生的过程，就会把至阴转化为至阳。道书里也会说地仙多少个化身，鬼仙多少个化身，说是渡过了九重雷劫的神仙，叫金仙，金仙有四亿三千万念头，念念化身。这些看着都跟小说里差不多了。\\n\\n这是佛家道家的修行人。那么我们凡夫有没有做功？按照唯识的道理，我们这些凡夫也在做功，我们每一个妄念出来，整个宇宙都在震动——只是我们智慧不够，无法观察清楚而已。然后我们一念为佛，一念为魔，一念在天堂，一念在地狱——这个其实已经是轮回了，我们的念头，我们的这些“化身”，或善或恶都在给整个大千世界的轮转添加燃料。所以，我们功夫和境界要正确，要有理论指导清楚的，所以要了解唯识学。\\n\\n唯识学甚至是小乘修行也绕不开的。小乘沿着当年婆罗门教等外道修证做功夫的路线实证，最后进入罗汉圣人的灭尽定，达到小乘圣人的实际证量。这个修证做功夫的路线主要是四禅八定系统，是通行于外道与佛教的共法。佛教小乘，是苦集灭道“四谛”配合功夫，完全走修证的路线。所以，真正小乘远比后来停留在嘴皮子上的所谓“大乘”更容易证果成圣，佛在世的时候，那些弟子有七天证果的，有四十九天证果的，就是注重功夫、注重实证。\\n\\n那么这个系统也离不开唯识的道理。四禅八定，四禅就是色界天四禅的境界，加上无色界天的四种定，一共八种，所以叫“四禅八定”。依色界天建立的禅定，分别叫初禅、二禅、三禅、四禅。初禅，生命的心识系统可以说恒常浸润在禅定的喜悦中，禅定的喜悦就是生命所需的食物，在这样的禅悦中，修行人任何时候都在享用内外身心、各种感官的极其稳定的喜乐感觉。这种感觉在唯识学中，比如《百法明门论》等等，都有专门的篇幅讨论。\\n\\n色界二禅，才真正谈得到“整体观”，此时修行人在定境中消解了人类身心内部中枢神经与外周神经系统的分立，可以达到是全体神经整体来感受，分立感、差别感大为微弱，这个禅定境界也叫做默然定。默然，也就是任何的语言系统在这里并不需要了。这时候有所谓“了了分明”的觉知，这个觉知能看也能听，能解也能觉，但与依靠外在感官的觉知不同。\\n\\n很多修行人就以为这是“悟道”了，感动得眼泪花花的；尤其是一些西方的修行者偶然达到这个定以后，在整体的感受中，就以为这是“道”了——这个当然错得离谱，但也比我们这些人不修功夫强。怎么区别这个境界，不自误误人，其深刻的道理还得回到唯识学的系统里来。\\n\\n色界三禅，有大乐发生在脑内，那才叫“法喜充满”，这也算我们人类最高品质的快乐享受了，跟这种大乐相比，性高潮简直等于你去奸尸了。这个定境中所有躁动的欢喜起落都已经消失，取而代之的是绵密不绝的快乐。这种纯粹、极致的快乐，在人类的人生历程中，再也找不到出第二个了。佛说：在世间所有的快乐之中，找不到比色界三禅更快乐的了!佛用人类在盛夏时走入清凉的莲花池，来譬喻三禅的快乐。三禅的快乐就像一个被晒得火热的人，步入莲花池后的清凉一般，通体舒适，没有不清凉快乐的地方。\\n\\n人有六根，眼耳鼻舌身意，一般搞唯识学的人不做功夫，所以搞不清楚；其实大脑根本不是意根，大脑是身根，当大乐在它那里发生，那当然远胜过在生殖器这个地方发生的性高潮。\\n\\n我们知道，生命在痛苦中的身心中不容易保持平衡，甚至根本上失去平衡。因此，大部分犯错的人，多半是在痛苦逼迫的仓皇中所铸下，因为跻身在痛苦中的人们，很难静下心来思考抉择自己该怎么做。而身居禅定快乐的人，他们在没有丝毫身心粗猛痛苦逼迫的有利条件下，智慧深细而广大，如果他们以无所求、无所得的态度来面对这种享受，他们所拥有的条件是很容易就解脱的。所以，佛说，三禅的定境是菩萨的禅定境界，对于成佛事业助益极大。\\n\\n四禅叫“舍念清净”，就是舍掉三禅的喜乐之后得到的定境。这个实在不简单，一般人、神总求离苦得乐，因为苦让人受逼迫而不舒服，而乐让人得以在其中享受。离苦得乐是一般世间天神、人、畜……等生命存活的最基本也是最迫切的需求；如果失去了这个理由，一般生灵是活不下去的。但是，四禅就是得舍掉三禅的喜乐才能达到。所以，拥有四禅的定境生在四禅天的天人，是一群软硬不吃、苦乐俱舍，行于清净中道的大神。在佛教中，四禅天色究竟天的神灵，被佛称为“摩稀首罗”，意思是色究竟天国的神明。\\n\\n这些，在修行人做功夫转化身体时，要与唯识学关于心理状态的论述，要配合来修的。\\n\\n这是色界四禅。无色界有四种定。在这四种定里，物质肉身的束缚在心理上大大薄弱，在这四种定里，基本上只有心识和时空的感受，也就是说，这四种定境里只有纯粹的精神、真空和时间的觉受了。\\n\\n首先，是空无边处天定，在这个定境里，只有一片真空的觉受了，所以，依这个定的境界就叫它空无边处定。\\n\\n空无边处定向上一层，就是识无边处定。定境中只有过去、现在、未来时间相续不断的存在，只是时间这单一一维的存在，此时修行人的心识还是相续不断，但其他觉受确实是解脱了。\\n\\n再往上一层，连时间这一维也不存在了，“什么都没有了”，但是心识还在。为什么呢？因为感觉到什么都没有了是还有感觉的——修行人还有“感觉到什么都没有”的感觉，这就是无所有处定了。\\n\\n再往上一层，到连感觉“什么都没有”的感觉也没有了，就进入了非想非非想处定。此时就只有微细的“这个”单一念头，也就进入了唯一的单一作用力，就定在那里。这就是无色界的最高的定，也是我们娑婆宇宙三界的最后处。佛为了说明对人类来说这么难以想象的境界，只能勉强地用“非想非非想”这样一个并不简单直接的词来形容。\\n\\n这些，都得有唯识学的基本理论素养，才能保证修行的顺利。依修行来说，从欲界的粗定，到色界的四禅，再到无色界的四空定，四禅八定就是依次从物质相、能量相、空间相、时间相、心识相破起，直到非想非非想处天的非想非非想处定，只剩下心识本身的单一存在。\\n\\n如果连这一单一存在也破了呢？\\n\\n那么你成佛了。\\n\\n可非想非非想处天真的是往上一步，就能成佛了？\\n\\n答案是：理论上再往上一步就能成佛，实际上这基本上办不到。非想非非想处定，一定八万大劫，约3612800000亿年，这么长时间，基本上等于毫无指望了。所以佛告诫修行人，菩萨要在三禅和四禅的定境中定慧增长，不可轻易进入这个定。\\n\\n那么南老传的实修法门，就是呼吸配合白骨观，白骨观修成以后白骨流光，身见基本解脱了，基本上没什么身体的感觉了，再配合呼吸，就可以进入三禅，再舍念清净，就能达到四禅。可以说是当代最对机的成佛体系了，这个也需要唯识学的理论，才能了解这个法门的创设有多伟大。\\n\\n在真正的基督精神里，耶稣基督也并不承认“信我者得救，不信者不得救”，更不是原教旨主义那个极端逻辑：“信我者生，不信者死。”耶稣并不承认这个。所谓“信主者上天堂，异教徒下地狱”，这也是庸俗化了的基督教。其实你好好看一看英文原著的《圣经》，圣人耶稣说得清清楚楚，与大乘佛法的道理相印证，你会觉得，哦，原来还可以这样理解耶稣，耶稣是一个佛，或者说他是一个菩萨，可能会更接近他的实际品格。耶稣来到世上，首先就是个异端，是个叛逆，他是来颠覆希伯莱的犹太教《旧约》传统的。他在基督教体系中，可以说是个颠覆性的人物，或者叫革命性人物也可以。\\n\\n他的思想和说法，在当时的犹太教传统中，实在是太异端了，结果犹太教教士借罗马政府的力量，借国家机器的力量把他钉在了十字架上。但后来，事情发生了很大的变化。犹太教把他的思想给收编了，直接就表现在《旧约》把耶稣的《新约》给收编了。这个当然不是把耶稣给招安了，他们把他给处死了；是把《新约》招安了，其实《旧约》和《新约》完全是两样思想，耶稣带来了革命性的不同，这一招安，《旧约》《新约》放在一起叫《圣经》。这个手段太高明了，远比他们处死耶稣圣人厉害，将敌人先处死，再把他的思想说成是自己人的思想，然后主要搞的还是《旧约》那一套，这个厉害啊。\\n\\n在《圣经》里，《旧约》很薄，《新约》很厚。本来，耶稣作为“人子”来到世上就是传福音的，这个福音是什么呢？就是广泛告诉大家，过去在《旧约》里，大家对上帝的崇拜和了解是错误的，上帝不是“不信我的让他死”这么神经病，不是这个样子的。但是耶稣圣人他还只能利用，或者是应用当时人们能够接受的语言和比喻来讲他要传的福音。所以他只能这么说：我来做事情，并不是因为派我来的“那个人”（那个大家以为的发神经的人）；派我的那个，其实是大家的“父”。他这个父亲是广义的，不是狭义的，指所有人的父亲，所有人的本源来处。在当时，他只能这么讲，要不然他周围的人听了也听不懂。其实他已经尽了最大的努力，他本人本身就是木匠家庭出身，也不像释迦牟尼佛有那个出身条件，可以学遍所有的老师，可以有系统接受知识训练的经历，所以他举的例子都非常非常的朴素和草根。但是他传递的信息，他传播的福音，明眼人，尤其是真懂佛道的人，来看他，哦，原来他是这个样子，在说这些事情。\\n\\n在《新约》里，耶稣传播的福音里，他特别强调圣灵。圣灵在哪儿呢？他说，圣灵在我这儿，圣灵也在你那儿，可是你也在我这儿，我也在你那儿。他的讲法，是个庄严的互融互通的法界相，这个就要参合《华严经》来看了。他说，你别以为有一个圣灵是你的，有一个圣灵是我的；不是这样的，圣灵普遍在的。他反问，你怎么可以说，有一个上帝，哦，你对我好，我就对你好；你对我不好，我就惩罚你。他否认这个上帝，说那是大家错误的理解。错误的源头，自然是这么传播的犹太教教士们。你想，他能不被那些人恨之入骨吗？\\n\\n那个有条件的，看你的态度的，然后充满了人性的恶劣的，那是《旧约》里头愤怒的上帝，报复的上帝——《旧约》里头的原话，上帝表示说，我是一个爱报复的、嫉妒的上帝。这样子的上帝，还是一种神灵的层次，就是佛家讲六道轮回里头的天人，而且看上去更接近天人里的阿修罗，嗔恨心真是不小。但是耶稣说，不是这样的，你们大家不要听犹太教教士胡说，上帝是普遍的、周遍地爱世人的。他的这个无条件的“神爱”理论，完全颠覆了过去的那种思想，过去的“神爱”可真是功利。\\n\\n《新约》里记载，耶稣对门徒说，你们要真信我，你们就要像我一样去爱世人。怎样像他一样爱世人呢？他特意展示给大家看。有一天非常意外的，大家都回来以后，他把洗脚水给大家都打好，然后给每一个门徒洗脚，洗完脚后帮大家擦好。大家说，奇怪呀，没见他干过这事呀，大家当时不明白，于是耶稣说了要像他一样爱世人。但大家都听不懂，耶稣又说我以后不会跟你们在一起了，我这么做，是要你们也像我对你们这样对待别人。随后他就被罗马政府抓走了。\\n\\n所以他当时传递的信息，那些福音，每一个方面你仔细看，全是对《旧约》的颠覆，是这样子的，他说了很多一段一段的话，你用心看，你就会非常震憾。这就是为什么《旧约》系统的犹太教教士们必须把他干掉的真正原因，因为他是个异端，靠宗教传统吃饭的人受不了。为什么后来《旧约》把《新约》招安了呢？这就是手段高明啊，最妙的方法不是反对自己的敌人，当然刚开始两者水火不容，他们把耶稣给处死了；后来发现拥护《新约》的基督徒越来越多，然后他们就动脑筋搞“兼并收购”——反正大家谈的总归有一个相同的地方，不管是什么样的上帝，总是都在谈上帝。于是本来《新约》是把《旧约》推翻的，后来《旧约》反过来把他容纳在里面，然后把有关的东西不声不响地埋没了。\\n\\n现在我们中国所谓传福音，大多又是《旧约》那一套，福音就是“不要信魔鬼，不要信邪灵，要信上帝，不然就会……”就教化来说，耶稣和释迦牟尼佛是一样的，因为释迦佛教化，是通过他的身教和言教；耶稣也是如此，言教是传福音，身教，打洗脚水给你洗脚，这都是。释迦佛展示神通，耶稣也展示奇迹。不过，耶稣说，因为没人信我，所以我必须用这个方式，意思是唤起众人注意，或者像现在人的说法，来吸引眼球。但是，他马上就会强调，不是你信的那个（不是信我本人），是信我里头的一个（那个大家具备的），然后有时候他叫上帝，有时候叫圣灵，有时候叫至大的……他经常叫不同的词，他不会用同一个词。这跟佛家道家讲“道”、“佛性”、“自性”什么的，有何区别？\\n\\n然后他也说，你们假如有确定无疑的信心，你们做的将会比我做得还要大。他行使神迹的时候，如果对方犹豫不决的时候，他通常是不做的；一点不信的，那就更不可能。他通常是说，我没有救了你，是你的信心救了你，但是你的这个信心是通个我这个中介表达出来的。他的说法，与佛法相通的，全是相通的。我们那些朋友听了都害怕，说是你入佛道了，一个基督徒怎么可以进入佛教了呢？他们美国人，不知道这边文化的情况，结果有一次一个朋友约我去他家，他老怕我入魔道，他们以为佛道就是魔道。他们的说法就是现在到处乱讲的那些人说的什么“邪灵”之类的。后来沟通了几次以后，他们放心了。\\n\\n所以我们可以理解佛法为什么能够在西方传播开，不是说密宗怎么了，是他们通过佛法忽然发现耶稣太寂寞了，要通过东方来真正理解他。\\n\\n现在反而是东方搞着《旧约》的逻辑。当然，宗教中总难免集中一些偏执狂，有些人会进入到一个偏激的信仰中，所以对宗教精神的正信，都需要一个过程。人群里面，永远都是三六九等，有像金博士这样的基督徒，也有满嘴胡说八道的基督徒。老实说，比这样的基督徒更恶心的佛教徒也绝不少。在人世间，这些必然如此，否则不成娑婆世界。这些都是允许的，只要不要集结在一起乱来就可以了。这一点耶稣的精神是好的，他的精神也要发扬光大。他被那么虐待，还说：主啊，请宽恕他们吧，因为他们不知道自己在干什么。\\n\\n释迦佛过去世他也没有怨恨，所以在《金刚经》里被歌利王割截身体，都没有一丝怨恨——境界是一样的，都是大圣人。人类最怕的无非是一个死，这个真不在乎了，不在乎到连怨恨都没有，那差不多不会被死控制了。所以，他们就表达给我们看。连这个都能够超越的话，还有什么不可以的？因为人类实在是不见棺材不掉泪，于是他们就做给我们看。这些圣人，无一例外都是有大情的。\\n\\n我当心理医生的过程，我发现有天下的事情真是“不生不灭”，人内心的能量，绝对是不增不减，不生不灭。有的人混得很窝囊，怕得一塌糊涂，完全就干不了任何一点正事，就等着全世界来拯救他，来施舍给他。他那能量可没少，于是造出病来生着，呆在那里打妄想，想像自己其实是一个圣人，是什么什么转世——那个能量挺大的，能够把清高和愤恨发挥到很高的地步，而且大言不惭。由这个路子去信教，当然就走极端，就反社会，就很有嗔恨心和攻击性，真正宗教精神那里是那个样子。\\n\\n相对来说，基督教跟西方的科技文明和主流社会，经过几百年的磨合，磨合得还算比较好了。这一方面，是佛和道不及的，所以中国本土的宗教需要革新，现在抱着老古书摆清高的搞法，可能不得人心。耍清高玩攻击是不对的，因为我们每个人都对别人不尊重，做任何事情都失信，社会公然行贿、枉法……所以，中国人自己搞得自己有些不值得人尊重。因为我们自己要尊重自己。比如说，每个国家搞政治的，都要搞搞阴谋什么的，这大家觉得还正常一些，因为政治本来就是这样的吗。商人也本来就是牟利的，但是现在我们社会的学者，培养下一代的教师，也差不多了。这还有什么尊重和尊严？过去，像陈寅恪、马寅初他们，不管他们学术对不对，那种对生命的尊重，那种庄严或者神圣精神，才是我们文化命脉延续下来的可靠保证。\\n\\n一种文化的传承，不能全从世俗的角度着手。耶稣的伟大，是脱俗的；佛也是。佛当年也是印度婆罗门教的异端啊，所以很受过迫害，连他的堂兄都背叛他、害他。完全从世俗着眼，比如有助于搞学问出名啊，打坐可以健康长寿啊，这些追求无非是在六道轮回里面达到比较好的一道继续轮回，还是在功利圈里面转，怎么出得来？又怎么谈得上为民族整体谋个终极归宿？这一点，我们真没法跟西方人比，我们真是太实际了，拿世俗当标杆了，这个就差远了。\\n\\n欧美尽管毛病大，但科学研究和创新还在那里。尽管他们的科学越来越偏技术了，越来越跟商业结缘了；尽管如此，为科学而科学的人，人数还是蛮多的。中国不是没有，但是显不出来。但欧美在精神和文化上，真是很困扰，很痛苦。我们甚至还没有发展到那一步，还盯着脚面看呢。\\n\\n人类需要宗教里的真精神，需要圣人的宝贝。但从中国到西方，需要梳理，因为根本就是两个语系。所以文化精神从中国到西方，简直根本没法翻译，无法转换。你对中国文化以及它背后的内涵，稍微有点感悟的话，中国文化讲“中”，意思是有分别就是错误，因为不是“中”了。光是这个思想，就把西方一大堆东西都超越了。这还是老祖宗，那些圣人刚开始有点味的东西，还不是那个“道”。怎么翻？没法翻，英语里没有对应的词，连这个想法都不大有，我只能这么讲。\\n\\n从“中”这个字讲，中国文化看自己，看别人，看世界就站在那个角度上，那个没有地域性差别对待的角度上，光是这一点就是对人类的巨大贡献。因为“中”根本找不到具体的立足点，可以说“中”的全球视野是不立在任何一个观点、地域上的，所以才能包涵地球上任何一个点，这才是真正全球观点，也同时超越了“全球化”，这个即是《华严经》的道理，也是“中观”的观点，其实根本就有观没点，这个怎么告诉西方人？很难。\\n\\n虽然说儒释道是一家，背后的东西都是一样的，但从来没有人跟西方的众人展示过，日本人做得不够，密宗毕竟是宗教中的一派。能够真正展示、能够反映那个韵味的东西，现在还没有一个对应的西方思想和语文体系，这件事情还需要很长时间。\\n\\n日本人虽然做得不够，但他们比我们早一些反省到国民教育、宗教文化的问题。他们后来就兴起了“创价协会”。创价协会是一个入世的团体，当然现在快成了一个商业结合政治的组织，有些违背最初成立的初衷了。这个协会把基督教的形式、方法和佛教精神融合在一起，他的中心不是寺庙也不是教堂，叫总部和分部，但跟教堂的选址是一样的，选在市区。只有在市区才能跟人群接触，才能着手解决普通人的问题。\\n\\n创价协会的创始人被日本人看成是圣人。他当年觉得日本的教育和文化出大毛病了，当时战前、二战期间，日本的国民教育是狂热的军国主义教育。日本的小学生都接受这一套。比如说，日本的小学会给小孩子发一个苹果，然后老师问：好不好吃？大家说好吃，于是老师引导说，这是出产在中国的东西，我们日本地狭人多，随时都有灭亡的危险，只有从支那人那里抢这些东西，大家才有活路和前途。这个当然太可怕了，直接导致二战期间日本民族的全体疯狂，既毁了他们自己几代人，也给周边国家带来了深重的灾难。\\n\\n创价协会的创始人觉得日本堕落了，一代代的孩子学的都是知识，精神上居然是这些政治化的东西，太可怕了。而且日本军国主义教育，就是要孩子长大去侵略在文化上可以说是父母之邦的中国，他认为这样下去日本全完了，全毁了。所以，他在战前就和另一个人搞民办教育，提倡教育革命。你想想看，凡是涉及到人的心灵、性灵这些东西，一定是普世的爱，肯定有真正的宗教品格在，他提倡的精神一定是爱人的，一定是反对战争的，这跟当年日本军国主义的政治意识就起冲突了。他们两个就被逮捕起来，其中一个就被折磨死了。他在监狱里算是幸存了下来。最近有一篇文章特别好——《日本是如何毁灭的》，专门讲日本的教育，然后产生了几代“愤青”，这批人成了狂热的偏执狂，结果把日本折腾毁灭，到现在的日本，也还没有在文化上确立一个方向。\\n\\n愤青，产生于人群中普遍狂热的情绪。现代以来，当政治教育代替了传统的做人教育以后，政治一定会培养出自己将来无法控制的愤青一代。这是政治家的短见与愚蠢，企图通过狂热的排外情绪或者民族情绪来避免民众对于现实政治的关注，无疑是以为暴力情绪永远不会降临在自己头上一样愚蠢。当国民经济，或者重大社会问题集中暴发的时候，这种情绪将不可避免地降临到政治头上，一代或者几代愤青们要求着政治的兑现，政治不能不兑现，于是发动对外战争，或者国内的政治高压，都成了政治不得不配合的事情。因为这种政治教育，会以国家机器的威力普遍的培养出几代愤青出来，那不是政治家当初设想的轻易就可以控制的。\\n\\n《日本是如何毁灭的》，讨论当年日本年轻一代的愤怒情绪，他们说凭什么西方人就行，东方人就不行。中国民族主义者就集中在愤青们中间，一个不小心又变成网络暴民，在现实环境中，已经给政治带来了压力。那篇文章写得很风趣，也很挖苦，是我们中国人写的。\\n\\n日本战败后，创价协会的创始人被放出来了。这个人有信仰，不屈服，出来就继续做，办“教育创造价值学会”，还是从教育起家。但是纯粹教育很难做出来，那怕是这位圣人。他当时很苦恼，因为说来说去，有教材的问题、老师的问题等等，这都是需要钱的。直到这位老兄遇到了日莲宗的传人。日莲宗，就是中国天台宗传到日本后的名字，其实是天台宗的一支。他跟这个人一谈就“好上了”，他俩觉得互相找到魂了，然后就合在一起，也突破了教育的界限，从此就不叫教育创造价值学会，直接就叫“创造价值协会”，简称“创价协会”。从那以后，这个协会广泛地把宗教、教育、生活、工作融汇贯通，普遍地影响人的各个方面。\\n\\n创价协会的创始人见过我们国家的领导人。他说，要向中国人还债，欠了中国人。因为他们弄的这套东西，基本上是从中国传过去的，所以他表示将来要还回去。当时是1975年，周总理说，现在你们不能来；50年以后来。所以他跟我国领导人有一个约定，他要等到2025年来还债。\\n\\n我为了这事专门去考察，创价协会很有意思，他们办幼儿园、小学、中学、大学、社团，他们的文化创意产业搞得很大，很厉害。而且他们台前是日本的第三大在野党，叫公民党。十个日本家庭里，有一个是属于公民党的。他们把基督教组织优点和佛教精神结合得很好，教育和经营结合得也很好。我进去几次参加他们的聚会，其中有一个是茶道，也有教做菜的，都有。然后就把这些完全是生活的东西，跟创造价值结合在一起。所以日本人能够在战后，把一点细微的事情做到极致，跟创价协会的教育相当有关。这方面，比如基督教为什么在中国传得特别快？他解决现实问题啊，心里有难处了，一帮姊妹们帮你做心理辅导、耐心调解，又不收费，甚至帮忙解决剩男剩女的婚姻。因为个人的力量弱，有个上帝在，周围有一群人关心着，人自然认为有得靠，靠得住。所以，对现实的普通人来讲，如果你说最后一个关怀、终极关怀，比如上帝不在了，没有人负责最后对人生的兜底工作，很多人要垮掉的。所以你说什么都没有，没有一个超越于所有人之上的，这个佛教精神肯定干不过那个说还是有个存在，专门兜地的。所以佛教的净土宗在一这点上就比较普世。\\n\\n后来我想再去日本，了解他文化和精神更深层次的东西，但有很多事就暂且搁置下来，又拖了好几年。后来去韩国，感觉韩国人自我的东西非常强、非常刚强，日本人有日本人的执着，但没有韩国人这么执着的。比如说，在物质上韩国工业化，军工这一块做到了亚洲第一，他有那个玩命的精神。基督教的传播，韩国人只用了30年，就成为人均教堂、人均基督徒世界第一。这个民族很有意思，能在短时间里，把干的事情干到极致，还能干过头，人家有这个本事。我们是大陆国家，韩国是半岛，但是太小，小的特点就出来了，他那个抱团，那个志气、锐气，没有包容性就全出来了。\\n\\n韩国挺好玩，他们把自己的历史改了一遍，尽量减少和中国的部分，还在废除汉语。所以不要以为只有我们乱改现代史，近代以来，我们东方集体患上了自卑病。不过，我感觉将来还要改回去，现在这套搞法，毕竟是旁门左道，不是正经正法。还有一个好玩的，他们自己不愿承认，韩国话特别像闽南话，特别像我们那里的蛮话。然后我们就能搭上话，我说你这个话是不是这个意思？韩国人说你怎么知道？韩国人自尊心很过敏，文化自卑的心理比我们还厉害，把几十年前有关与中国的文献、史料都不要，重新弄了一套。相比之下，我们还算稍微好那么一点点，虽然也是五十步笑百步。\\n\\n陈阳还年轻，比起当年的我们，已经算幸运，较早地接触了老祖宗的宝贝。以后你把《易经》、《皇极经世》整理出来，继承发扬，这就是传统文化上一个重大的工作。日本人有没有去讲究《皇极经世》这样的著作？\\n\\n他们研究不了。他们在《易经》方面的研究，比我们这些现代中国人都要强一些，他们在“明治维新”时期，就有规定，一个官员要进入内阁，也就是他们的中央政府，必须要懂《易经》，而我们有文化断裂的大问题。但《皇极经世》太难了，他们没这个智慧和传承。《皇极经世》的传承本身就寥寥可数，佛教是传过去了，但是《皇极经世》没有。别说他们，我们中国现在真懂《皇极经世》的人，也没有多少。\\n\\n今天有一些感悟，所以想问陈阳关于“情”的问题。\\n\\n“情”是这么一个东西，它是唯一沟通人和佛、人和菩萨，以及人和神之间的桥梁。所以基督教说“神是爱”。在佛法来讲，诚所谓“无缘大慈，同体大悲”，在这个意义上，佛陀证悟的境界最后要落在这个“悲”字上，在诸佛的眼里，三千大千世界无非是“大悲”的流行。\\n\\n我们大部分人的误会，就在于以为佛教最后讲的，是一个完全枯寂的“空”，一个什么都没有的“空”。这就是凡夫的邪见，至少也是一种断灭见。如果“空”是什么都没有，那么人不需要信仰、功夫、实证之类的，只消在头上来一棍，昏过去就解脱了。但是我们这些人就以为这是解脱——对痛苦不再有感知，足够麻木，就是解脱。\\n\\n真正佛教精神才不是这样。佛说的“同体大悲”，最后要落实到一个东西上。东西方宗教立足点虽然不同，但讲法一样，一个叫“神是爱”，另一个说“同体大悲，无缘大慈”。它们最后要落实，是有一个真实的东西。这个东西不是麻木，不是无情，不是失去感知能力。这样子见地不正，修行很麻烦的。一个这样的佛教徒，最后会堕进畜生道，变成猪，而且一耽误就是千生万劫，绝不是几十年的事。佛不是我们想像的那样，完全一脸的道貌岸然，一脸的呆板，没什么情绪，没什么情感，这是我们的邪见。佛菩萨也不会如我们所想，我们出一点香火钱，他们就为我们提供大富贵、大健康、大名气、大权力。我们得到这些东西，是要付出相应的东西的，佛菩萨即便满足了我们，他们也得还。所以正如南老所说，这个贪欲其实是想让国王当乞丐，为我们这些区区凡夫去讨饭，这个就是个大问题，滋长着不劳而获的思想。\\n\\n真正佛菩萨有大情大爱，只不过他们确实跟我们不一样。我们在轮回中，现在一个情绪起来，我们根本控制不了，被他牵着鼻子走。想说老子不生气，立即就能转化，这已经就是圣人了。我们凡夫生气是刹不住车的，直到这个情绪走了，你想捞回来它也不会回来。所以这既是一个烦恼，转化得过来也是一个菩提。有情众生有情众生，所以佛祖他不是众生，但他绝对有情，正通过有情，才沟通圣人与凡夫。而且离开了情，也不成世界，也不成佛果。\\n\\n禅宗初祖达摩的一首偈子说得就很清楚了。他说：“有情来下种，因地果还生。一花开五叶，结果自然成”。这五叶，就是沩仰宗、曹洞宗、云门宗、临济宗、法眼宗。而“有情来下种”显然是说无情不能下种。很多人误解解脱、超越轮回是要足够麻木，不再痛苦，这个邪见当然以为可以在无情中下种。这下下去有什么用？等于在沙漠里种庄稼，种子就是焦芽败种。就是芽也焦了，种子炒熟了，根本发不了芽。\\n\\n儒家的学问，在《大学》里叫做：格物致知，诚意正身。格物致知是一个理性的训练过程，“格物”是说把内心的物欲先保持一个间隔，然后才能致力于智慧、致力于理性知识的求知。但接下来儒家立即说到情这个部分，讲诚意正身。人要对别人有诚意，对自己有诚意。对别人有诚意的一个人，就能跟别人感同身受，然后处世断事才能“正身”，不偏不移。这就是《大学》的纲目：“格物致知、诚意正身、修身齐家、治国平天下”关于个人“内圣”部分的纲领。诚意这个部分就相当于有情，因为无情还能称之为诚意吗？那只是麻木不仁。\\n\\n所以在修行的最初发心这个阶段，要看一个人的境界。一个凡夫，因为自己痛苦得不行，发心在这个菩提道上追求的时候，第一念刹那动的时候，动机是什么，就决定他好几十辈子的修证路线和成绩大小。所以南老在他的著作里头，提到佛教徒不要搞麻木和愚痴。太痛苦了，所以要通过某种办法变麻木，要麻醉自己，这个会堕畜生道，会变成猪。\\n\\n很多人认为“有情来下种”是达摩祖师在“秀”他的功绩，意思是你们中国的有些有情众生，因缘成熟了，我老人家过来下种了。他不是这个意思，他讲的是修行解脱可以通约的一个命题。所有有效的修行，都要在“有情”里面下种。在薄情寡义里面，那也叫“种”，但是是焦芽败种。“有情来下种”是一个充分必要条件，然后结果就是“因地果还生”。因为有“有情”这样一个因地，这样真实不虚，最后自然有那样的结果出来。\\n\\n所以“有情来下种”是正信和迷信之间的一个区别，是在自己有情的这颗心里面下种，还是在有求的一颗心里面下种，还是在薄情、寡情、甚至无情的这颗心里面下种，最后都会得到一个果。这个果和最初的那个因完全匹配。而且，这样的习气和业力往小里说，会影响几辈子的道业修行，往大里说会决定千生万劫的生命形态。比方信仰的目的就是追求麻木，沿着这个迷信和愚痴的路线走，万一成功了，就会变成猪，而且还会耽搁千生万劫。\\n\\n在佛的眼里，这个甚至比饿鬼道还差。在佛教的教义里，认为饿鬼虽然更苦，但还是比畜生高级一点。虽然饿鬼的生存环境比畜生恶劣得多，但至少他们有灵性，他们有自我意识，有类似人类的智能，畜生则完全是愚痴。所以在佛经里面，你会发现佛经常呵斥人的时候，说的是“愚痴众生”，他既不是说“嗔恨众生”，也没说“贪婪众生”，反复反复地说“愚痴众生”。\\n\\n具体在修证的路途当中，我们要一关一关地过，最后达到“果还生”的成功。修证中，我们要力求“入道”。入道虽然不是得道，但至少是真正入门了，再也不是一个门外汉了，这才真正明了修行解脱、超越轮回的路线是怎么一回事。入道，达摩祖师跟二祖慧可讲过，就是要“外息诸缘，内心不喘，心如墙壁，可以入道”。他其实指我们内在世界要刹得住车，因为我们的大脑很难摆脱妄想，明明人已经困得不行，还强撑在那，脑子还停不下来，所以就要看电影什么的，硬要把自己搞失眠。\\n\\n但初祖达摩这段话，我读到的时候，就疑心这是句“双头话”，就是除开表面的意思外，还有深层的意思。双头话，你从正向看对，你从负向看也对，所谓“仁者见之谓之仁，智者见之谓之智”。换句话说，从两面的对立角度看都对。后来我看了南老评价的“一花开五叶，结果自然成”，他也谈到负面的那个理解。禅宗在六祖慧能那里集大成，慧能把中华佛教、汉传佛教的顶尖和极致完成了。但这项功绩至少有三分之一是神秀的，不完全是六祖一个人完成的，当然他是集大成者，是最重要的。六祖的弟子青岩行思，再来是马祖道一。禅宗在马祖道一那里放大光明，然后是百丈禅师。百丈解决了禅宗的组织、生产问题，他把人力资源和组织管理搞得最完善。从百丈开始，中国佛教有了丛林制度，禅宗执行“农禅合一”的生产制度，保证基本生活所需。这以后“一花开五叶”，但是自从“一花开五叶”以后，“结果自然成”，你也可以理解为把禅宗的老命差不多也结果了。这也是南老讲出来的另外一个意思。\\n\\n这句话两边看都对，也完全符合中国的根本思维。中华文化的根本思维是从万古经王的《易经》过来的，就是阴阳思维，后来唯物主义者也把他叫做辩证思想。就是凡是道理都是两边的，都是阴阳辩证的。\\n\\n“有情来下种”，如果个人觉得这个世界太苦，令人太不满意了，我自己先跑掉再说，这是小乘的情，出离心绝对够，就配合功夫修“苦集灭道”四圣谛，能成为小乘圣人罗汉。如果觉得这个世界确实是糟糕，但我不能自己一个人跑，我得带上大家一块儿出去，这就是发了菩提心，这就算是大乘。这个菩提心因为在境界上的差别，个人修行时执着的差别，所以有五十二位菩萨、十地菩萨等等区分。如果当初修行发心，一念间三际托空，无过去、无现在、无未来，一念间同体大悲，也就能成佛。这也是密宗“即身成佛”灌顶的真正秘密，因为一念间“同体大悲”，虚空中诸佛都会过来给你灌顶，不是现在北京乱跑的这些所谓“活佛”给你摸摸顶搞一下。\\n\\n“三际托空”和道家的一些说法也非常接近。我个人觉得，这些说法特别从侧面很好地说明了“三际托空”的道理：过去永恒，觉知了了分明。因为过去已经发生了，成为一个确定性事件。未来不朽，犹如真如本性。关于未来，佛也不能说绝绝对对地知道，未来的运作，涉及形而上的绝对，诚如真如本性。再然后：现在无生，念念流注转变。我们现在念念都在变化，真是一切种子如瀑流。但这些流变生灭只是现象，所有生灭的背后都有一个不生不灭的。所以合起来叫做：过去永恒，未来不朽，现在无生。\\n\\n自魏晋以后的道经，基本上走的是佛家和道家融合的路线；尤其是道家正宗，就是仙家一派。所以诸佛三际托空，过去永恒，未来不朽，现在无生，于有情心中兴起无缘大慈、同体大悲，自然超越三界轮回牢笼。我们凡夫一般想像佛菩萨解脱以后，就会坐在喜马拉雅山上说：你看那个那个笨蛋，愚痴；你看那个众生，好贪。就好像啥都不要干，就坐在那里“现”。其实真正的佛可不是这样，因为同体大悲的缘故，这就意味着诸佛永远不会放弃他的本职工作，除非众生灭尽，诸佛的工作才会停止，寿命才会完结。但众生不会灭尽，这个宇宙众生尽，那个宇宙又诞生出一群。所以众生无尽，诸佛的寿命于是永恒，而诸佛的工作更不会停止。《易经》讲圣人境界：天行健，君子以自强不息。道家讲“未来不朽”，这正是一种非常强烈精神，没有止境的自强不息，这也是佛法里“阿耨多罗三藐三菩提”即“无上正等正觉”的佛境界。', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c150bbf8-c144-4148-a182-a89e52c6555e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n是的，在无法成为客体的那一个瞬间，人所产生的，是巨大的无力感，有关自身主体在漫长过程中，所丧失的全部力量，在这一瞬间重新被意识到，再也没法作为客体去利用自己、利用他人，只能坦诚的面对自己的全部问题，然后一一解决，这便是恋爱的、喜欢的，对那个个体而言的全部含义\\n\\n\\n平等关系需要在明明知道有差异前提的情况下去创造；平等的关系，类似天下大同的社会，都不是自然状态，是需要人的努力才有可能诞生的\\n\\n\\n\\n> 因此，我们在这里清楚地说明了意识形态幻象的作用：不是从某个悲剧事件中制造出虚假的冲突，而是用冲突的虚假来源去代替冲突的真正来源。（\\n> \\n> \\n> 齐泽克\\n> \\n\\n我们必须明确如下一个事实，即文化布尔乔亚总是试图推迟创伤之核的来临——惯常的作法是使用新的异化代替旧的异化。在那里，\"精神内耗\"作为主体切身冲突的虚假来源代替了不可言说的主体自身，它作为一个纯粹空无的能指代替主体行动。\\n\\n> 将某事提升为不可能，作为推迟或避免遇到它的手段。（对谈齐泽克,齐泽克&戴利）\\n> \\n\\n悲剧与苦难是的确存在的，然而直面惨淡的人生却反而并不总是被优先考虑，\"精神内耗\"与对他者承受苦难之能力的赞赏也就作为了一种虚假的否定性来将自己排斥在苦难之外，并且将苦难本身及其来源悬置。\\n\\n至于叫好，看客们叫好的原因在于他们以为自己切近了他人的痛苦，可是他们的痛苦是私人的。的确苦难无法量化，无法比较，也就必然是私人的，但苦难不仅仅是私人的，仅仅对于一些人是私人的。单纯的倾听不具备任何意义，倾听的来源是任意的，我们无法关闭耳朵。因此，二舅很苦，还有人很苦，但我们并不真正清楚他们的苦难——我们需要言说，需要分清谁在言说，需要意识到自己仍有喉咙，而不是顺从地将一切苦难与压迫合法化为\"内耗\"抑或其他任何的垃圾桶。\\n\\n> 那么，中心问题...是...这一事实：我们要发现谁在说话，他们说话的立场和观点，促使人们谈论的制度，同时该制度也会进一步存留和传播所说的内容。（福柯）\\n> \\n\\nwhat’s the greatest enemy to human being: the human propensity for evil \\n\\n这同样可以解释为什么喜欢\"买游戏\"而不喜欢\"玩游戏\"。常见的情况是看到就觉得很有趣，进而很想买，买了之后却不想玩，如此反复。因为在浏览商品页面的过程中，因为尚未拥有所以欲望被压抑，但欲望却又不断再生，于是产生了一种创伤性的快感。而当真正去商店页面下单，明明已经入库却又因各种各样的原因没有想玩的欲望。并且至关重要的是，我们明知如此，却偏要如此。\\n\\n因为欲望从来都不欲望它的对象，所谓的目的不过是自欺欺人的骗局，我们至始至终都是在享受\"欲望再生产\"的过程。当目标看似达成时，欲望便奔向下一个目标。只要你不买，商品就会不断的向你呈现由你自身所构建的美丽。这种欲望的再生产是无处不在，避无可避的。\\n\\n至此，我们终于可以回答最初的那个问题：何为欲望？欲望是自我的匮乏，是非我的欲求。欲望存在，但所求是虚构的。\\n\\n我们真的需要一个位置去安置我们无处安放的欲望，黄油与直接浏览色情作品最大的不同就是，它可以给予我们一个\"位置\"。因为人是不可以没有欲望的，欲望的缺乏会导致我们产生一种\"深深的无力、致命的苦闷\"，这或许比深陷于欲望更加可怕。所以即使我们想要逃避现实的一切，认清欲望的空壳也是不可能的。因此，即使欲望是虚假的，并不存在，也比没有欲望要更好。虚假之欲望为幻象，而穿透幻象是痛苦的。黄油正是给予我们一种温和的方式去穿透欲望、安置我们无处安放的欲望。如果仅仅是寻求一个位置，那么这就不算是\"让步\"而是\"和解\"。黄油就好像一个工具，可以让主体游走在幻象的边缘而不至于痛苦\\n\\n同样的，我们应该审慎的看待自己的欲望，黄油给了主体的欲望一个位置并不代表可以高枕无忧。这次是黄油，那么下次是什么？我们要警惕的是对欲望的让步。穿透幻象虽然痛苦但仍有必要，因为我们无时无刻不在这么做。\\n\\n这或许是为什么拉康会说出这句话：\"所以我认为，一个人唯一有罪的事情就是相对于自己的欲望而让步。\\n\\n他似乎有崇高的理想，但是崇高的理想带不来崇高，理想最无用，带来崇高的是洞察，看向真实的能力，罗翔喜欢读书，喜欢说一些名言，他自己的东西几乎听不到，一些非常表面的逻辑可以勾起他洋洋得意的兴致，讲的口若悬河。一个无洞察的人本该迷茫，本该心虚，这是好的。但是罗翔口若悬河好像懂很多，说很多，远比余华能说，好像懂得更多，说的得意洋洋，好像很深刻很有洞见。但是内容不会因为得意的表情变得深刻，你听不到深刻的洞见，只有耳朵都磨出浆的经典理想主义者的那套老话\\n\\n酿酒就是酵母在无氧条件下把糖变成酒精的过程。根据原材料的不同，酒就不同。例如啤酒是谷物发酵而成，而葡萄酒（wine）或苹果酒（cider）是果汁发酵而成，蜂蜜酒（mead）是由蜂蜜发酵而成。酿葡萄酒时候，葡萄不扒皮酿，酿出的是红葡萄酒，味浓就牛肉吃，大杯上酒，饮前先摇以闻味道。葡萄拔了皮酿是白葡萄酒，味淡就海鲜，小杯上加冰镇，控制挥发，不可摇。如果是颜色非常深的黑葡萄，扒皮之后酿还是有颜色，叫粉葡萄酒（pink wine），如果嫌太俗，就用拉丁语族的名字，法语Rosé ，意大利语叫Rosato，叫玫瑰酒。葡萄酒里面加点二氧化碳，或发酵时进了氧气产生了二氧化碳，葡萄酒就成了汽酒，叫香槟，用细长的杯上，为了欣赏气泡款款上升的过程。中国的黄酒是发酵酒，由水稻或麦子发酵而成；稠酒也是发酵酒，有糯米发酵而成，古称醪醴（音老李），就是醪糟，这种酒不易保存，所以不普及；还有青稞酒由青稞发酵而成，也是发酵酒。\\n\\n但是上述发酵生成的酒精会杀死酵母，因此，靠发酵酿出的酒酒精浓度不算高，如果嫌酒不烈，把上述的发酵酒蒸馏，酒精浓度就提高了。标态下酒精沸点78.4度，比水低，所以在加热过程中先汽化，然后把这部分酒精蒸汽冷凝变成酒精，再勾兑酒，酒的酒精浓度就大了。中国的黄酒蒸馏之后就是白酒，也叫烧酒。蒸馏酒的时候往往要蒸馏几次而每次提取不同的馏分。例如在中国，第一次蒸馏出叫头曲，第二次蒸馏出的二锅头（这种方式头锅没有二锅好）\\n\\n西方把谷物酿的酒蒸馏，叫威士忌；由葡萄酒蒸馏出的叫白兰地；有蜂蜜酒或甘蔗酒蒸馏出的叫朗姆酒（rum）。白兰地制好后，越放越值钱，放三年，叫V.S.,写在牌子旁边，very special，三星；放5年，叫VSOP, Very Special Old Pale，五星；放六年以上叫六星吗？不叫，就像汉字一是一横，二是两横，三是三横，万不是一万横一样，放六年以上的叫XO，extra old。干邑是法国的一个地区，Cognac的音译，邑中文意思就是城，翻译得不错，这里的葡萄酿成的白兰地最好，所以以地名作酒名，香槟、波尔多、勃艮地都是以地名作为酒名。威士忌当然也是越久越贵，以j walker为例，是方瓶子，红方，即red label 8年，黑方12年，金方18年，蓝方30多年。\\n\\n上述蒸馏出的酒精含量比发酵酒高，但不是特别高，因此仍然带有发酵酒原料的味道，如白兰地有葡萄酒的味道。如果使用完全蒸馏的办法，原来的发酵酒的味道就会很淡，例如伏特加就是完全的蒸馏酒，是由谷物或土豆先发酵后蒸馏而成，伏特加不会有土豆味；它因为无色，斯拉夫人叫它“水儿”。伏特加在斯拉夫语的意思是“水”（вода），但是是小词形式，表示“一点水”，就像英语中的tablet（小片）和cigarette（小雪茄，即香烟），广东话里的湾仔的仔一样。这种完全蒸馏的酒因为除了酒味没什么其他味道，所以人们往里面兑些果汁啦、草药啦，例如gin（杜松子酒，音译为金酒或琴酒）就是加入杜松的果实，杜松就是一种叫桧（音贵）的植物，中国人讨厌“桧”字，叫“杜松”。\\n\\n除了蒸馏，也可以靠冷冻的办法去掉水，因为标态下水凝固点0度，而酒精是零下114.3度（因此酒精可以做温度计测量低温，水不能），就是说水先凝固成固体，一过滤就除掉了，这种办法叫freeze distillation，中文不知道叫什么，就叫冻馏吧。比如Applejack（苹果白兰地）就是苹果酒经冻馏法制成的。Eisbock（烈性黑啤酒）是啤酒冻馏后制成。\\n\\n中国的白酒英文叫baijiu，因为世界上没有第二种中国白酒，它的酿造方法全世界唯一，分两步，先制曲，再由曲酿酒。这曲就是发酵引发物，曲可以做酱、醋、腐乳等，也可以酿酒。\\n\\n“比起听女人诉说冗长的身世，我觉得听到她们一声叹息更能引起我的共鸣。尽管我一直期待着，但奇怪的是从没有在一个世间女子那儿听到过这样的叹息。不过，眼前这个女人虽然嘴上不说“空虚”，但身体的外部轮廓中却带着一股一寸见方的气流，从这股气流里能强烈地感受到一种无言的空虚。一靠近她，那股气流就会笼罩我的身体，和我身上带有的一些阴郁的气息正好交融在一起，恰似“水底岩石落枯叶”一般，让我从恐惧和不安中得以脱身。\\n与躺在那些白痴妓女的怀中安然入睡的感觉截然不同（最起码那些妓女都很开朗），跟这个诈骗犯的妻子度过的一夜，对我来说是彻底放松的一夜（这么不假思索地、非常肯定地使用这种夸张说法，在我的整篇手记中是绝无仅有的）”\\n\\n我们在美国的屋顶,全部能做的就是大喊大叫\\nWe were on the roof of America and all we could do was yell.\\n\\n我可以给别人的除了自己的迷茫,一无所有\\nI had nothing to offer anybody except my own confusion.\\n\\n两个主角的肉体虽然在路上，但在人生的层面上他们哪都没去；他们从不处理问题。他们只是把问题抛在身后往前看\\n\\nthe road is life\\n\\n我发现自己正在重复穿过美国的城市,好像一个奔波的销售员:旅途是破烂的、货物是劣质的、在口袋里揣着的是烂掉的豆子没有人买我的东西。\\nI realized I was beginning to cross and re-cross towns in America as though I were a traveling salesman - raggedy travelings, bad stock, rotten beans in the bottom of my bag of tricks, nobody buying.\\n\\n萨尔,不管我在哪里,我的旅行箱一定都会从床下露出来。我准备好了离开,或者被赶走。我决定让事情不受自己的掌控。\\nI\\'ll tell you, Sal, straight, no matter where I live, my trunk\\'s always\\nsticking out from under the bed, I\\'m ready to leave or get thrown ouf. I\\'ve decided to leave everything out of my hands.\\n\\n我想真的告诉你,之后吧一一阿肯色、坐火车穿越一吹长笛又长又糟的旅行,五天五夜,只是为了来见你,萨尔。\\nI wanted to REALLY tell you - much later - Arkansas, crossing on the train - playing flute Long long awful trip five days and five nights just to SEE you, Sal.\\n\\n迪恩垮掉了,但垮掉是至福的根源和灵魂\\nhe was BEAT - the root, the soul of Beatific.\\n\\n片刻间,我感受到了一种我一直想要感受的高潮.\\n我意识到这是因为内在意识的稳定……就像风吹在一潭纯净的、好似镜子的水上一样。\\nAnd for a moment I had reached the point of ecstasy that I always wanted to reach... I realized it was only because of the stability of the intrinsic Mind... like the action of wind on a sheet of\\npure, mirror-like water.\\n\\nThey hashed these over. Then Carlo asked Dean if he was honest and specifically if he was being honest with him in the bottom of his soul.\\n“Why do you bring that up again?”\\n“There’s one last thing I want to know—”\\n“But, dear Sal, you’re listening, you’re sitting there, we’ll ask Sal. What would he say?”\\nAnd I said, “That last thing is what you can’t get, Carlo. Nobody can get to that last thing. We keep on living in hopes of catching it once for all.”\\n\\n美国是如何刺激经济的：财政部缺钱了/财政部要搞经济刺激了 —> 财政部发行国债(也就是打借条)，美联储用储备金以及印出来的钱买国债。这下政府有钱了，但这笔钱是抵押未来税收借来的 —> 美联储大量购买国债，导致国债价格升高，国债收益率降低(反比关系) —> 国债收益率降低，杠杆率提升 —> 杠杆率提升，投机活动开始，经济刺激周期开始，资产价格暴涨，包括股市和楼市 —> 财政部为了还钱，需要增加税收，或者发动战争卖武器 —> 财政部如果换不起钱，就提高债务上限，借新还旧；或者提高税收收入，用税收还钱。\\n\\n上述一个闭环就是美国刺激经济的流程。为了能够让美国财政部能够借来足够多的钱，这个世界上必须有足够多的买家来买债券。在世界资源有限的情况下，也就意味着，不能有人去买欧洲国债或者是人民币国债，不能有人去买欧洲资产或者是人民币资产。\\n\\n对成功抱有幻想的人是不会坚持到最后的。不相信自己会成功的人才有坚持的能力\\n\\n六便士的主角从来没想过自己会成功。他之所以画画，是因为他想以这个姿态活着\\n\\n当我清醒时，曙光才会破晓。\\n/梭罗《瓦尔登湖》\\n\\n天才之人就是矛盾之人。超凡的生涯就是矛盾的生涯。与时代相矛盾的就是时代的天才，与凡人相矛盾的就是平凡的哲人。\\n\\n-**芥川龙之介**\\n\\n芥川因为对自己矛盾性的感悟，觉得自己既要烧死女儿才能成就伟大的作品，又不能接受烧死女儿（《地狱变》），所以十分矛盾。矛盾的结局就是怀疑自己存在的价值，结果就是自我了断\\n\\nwe think we want happiness, but we don’t\\n\\nwe dont know ourselves as well as we think. When we do, we struggle to accept it \\n\\n\"Man only likes to count his troubles; he doesn\\'t calculate his happiness.” 陀氏\\n\\n罪与罚主要是讲一个人杀了一个富女人，因为觉得自己的self interest,比如养母亲，去法学院，可以justify自己的morality as self interest，但他最后承受不住压力自首了；反rationalism, egolism 的作品\\n\\n余华说话从来不着调,一件很痛苦的事情,他很幽默的调侃。属实是现在人开放了一点,但是还有很多话不能说。罗翔确实当下很受欢迎,因为他离百姓思维最近,他是隔丝头子。不是说他不好,只是他是普通人的极致,把普通人的困惑思考都放大了。他有极致的困惑,极致的追寻,只是没有答案。他能代表很多人的处境,他是代表,不是答案。在一个年代里,所有人都饿的皮包骨,所有人都想大口吃,都想胖,都想走到他的另一面。他们不想健康的瘦,或者健康的婴儿肥,或者是健康的胖,他们想大口吃,长肉,不想健康。罗翔就是最想变胖的那个人,他没有答案,不知道如何健康。学了很多,读了很多,说了很多。他在寻找健康的方法,但是他还是认为胖才是健康,和其他人一样,比其他人更加坚信胖是真理。心里想着健康,实际上推崇的是发胖。平庸的思维,这种平庸极具代表性,因为所有人几乎都这么想,所以他受欢迎,他似乎有崇高的理想,但是崇高的理想带不来崇高,理想最无用,带来崇高的是洞察,看向真实的能力,罗翔喜欢读书,喜欢说一些名言,他自己的东西几乎听不到,一些非常表面的逻辑可以勾起他洋洋得意的兴致,讲的口若悬河。个无洞察的人本该迷茫,本该心虚,这是好的。但是罗翔口若悬河好像懂很多,说很多,远比余华能说,好像懂得更多,说的得意洋洋,好像很深刻很有洞见。但是内容不会因为得意的表情变得深刻,你听不到深刻的洞见,只有耳朵都磨出浆的经典理想主义者的那套老话\\n\\n轻信的源头是义务教育体系。\\n义务教育起源于普鲁士,最早设立是马丁路德为了推广新教而倡导的。\\n由于是一对多的教育,其中流程化工业化Q的知识灌输和标准化考核,成为了不可缺的特点。那么,为了保持课堂效率,在情景心理和社会心理上,进行了诸多的权威感设计,并沿用至今。\\n在义务教育的教室中,多数情况下,讲台要比地面高出一截,使学生只能仰视黑板和老师,从而在心理上制造老师高人一等的感觉。\\n同时,在学校的走廊灯地方,一般会陈列神圣化Q教师、名人的画像和标语。\\n在综合的心理暗示Q下,当一个人上了讲台,成为了教师身份,他天然的具备权威感和不可置疑感\\n这种权威感和不可置疑感的心理暗示技术,与教堂等宗教场所有异曲同工之妙。\\n这时候,灌输型教育的效率达到了最高,因为学生心理上默认老师说的都是不可置疑的真实的内\\n从而获得了义务教育的最优性价比。\\n因而,义务教育与启发式教育。是冲突的,在心理氛围建设下,老师在人格上是高于学生且高尚于学生的。\\n在这种机制下,有些人从小学开始的义务教育+高中,有12年中,获得信息和知识时,都不会去质疑,因而在潜意识。的信息处理回路中,对于权威没有质疑这个选项,这样他就变成了已经能被社会共识规训、被权威感驯化的人,这样的人显然适合作为社会化大生产的参与者。\\n但是这种轻信权威的脑回路,到了社会上之后,会成为一种被欺骗、奴役、剥削的弱点。\\n比如看到知乎上某些大V的观点,不假思索的同意;看到“专家Q\"的论点,就轻信等等。\\n这种可能性要求我们,在教育他人和自己时,要在义务教育的基础上打补丁,去合理化质疑,以及在灌输式教育以外,进行启发式的启蒙和训练,以对冲义务教育带来的范式思维弱点。\\n\\n至于容易被轻信的谬误的特征,其实很好判断。\\n1,在义务教育和正规教育体系下,内容(教科书、教材、讲义等)的目的是灌输知识,因而这种书和内容看上去像老师;\\n2,一些极其有用的,启发式的知识,能够破冰思维局限性的,看上去像严厉的导师,或者不可立刻理解的智者;\\n3,一些感觉且深有感触的,有点小醒翻灌顶但很容易读懂的,其实看上去像是玩伴或者知己;\\n4,一些看了就兴奋、悲伤、开心,情绪共鸣强烈的,往往是最无用且断章取义或管中窥豹的,它们本质上是“谄媚的小丑\"。\\n以上四种内容:\\n第一种最容易获得且有效;\\n第二种属于智者可遇而不可求;\\n第三种可以作为玩伴舒缓情绪;\\n第四种“谄媚的小丑\"只会通过舔你而获利,对你毫无用处,应该戒除。\\n\\n“我的上帝啊,那是足足一分钟的欣悦啊!这难道还不够一个人受用整整一辈子吗?”\\n\\n陀思妥耶夫斯基《白夜》\\n\\n很抱歉,我不能对您说什么动听的话,因为与梦想中的爱比较起来,切实的爱是一件严酷和令人生畏的事情。梦想中的爱图的是急功近利、立竿见影,渴望做出人人注目的壮举。怀着这样的梦想确实连命也舍得,只要这过程不持续很久,而是像在舞台上那样快快结束,只要人人都瞧着他表示赞许。切实的爱则需要工作和毅力,对于某些人来说兴许还是一门学问。但我可以预言,一旦您惊恐地发现,尽管您作了一切努力,您非但没有向目标靠近,反而像是离得更远了,-恰恰在那个时刻,我可以向您预言,您将一下子达到目的,并将在自己的上方清楚地看到上帝神奇的力量,他一直在爱护您,一直在冥冥中指导您的行动。恕我不能继续奉陪了,有人还在等我。\\n《卡拉马佐夫兄弟》\\n\\n不幸的是,这些青年并不懂得,在很多情况下,舍身也许是所有的牺牲中最轻而易举的,而从自己风华正茂的生命中拿出五六年来埋头苦学,做点学问,哪怕只是为了十倍地增强自己的力量,以便为他追求的真理服务,为他心向往之并且引为己任的大事业服务,-这样的牺牲对于他们中许多人来说几乎完全做不到,实际情况往往如此。\\n《卡拉马佐夫兄弟Q》\\n\\n于是，我想到了王小波给自己的外甥的谈话：\\n\\n> 痛苦是艺术的源泉;但也不必是你的痛苦柴科夫斯基自己可不是小伊万Q;玛瑞凯瑞也没在南方的种植园里收过棉花;唱黄土高坡的都打扮得珠光宝气;演秋菊的卸了妆一点都不悲惨,她有的是钱听说她还想嫁个大款。这种种事实说明了一个真理:别人的痛苦才是你艺术的源泉;而你去受苦,只会成为别人的艺术源泉。\\n> \\n\\n《月亮与六便士》的出现以及持久影响不是偶然,与20世纪人本主义、存在主义以及福柯式生活哲学Q等思想潮流的滥觞互为背景和相关。上述思想认为:人最主要甚至只需要对自己负责,存在就是有价值的、有意义的,生活不是一种发现而是一种发明一没有统一、正确的生活方式,发明你喜欢的、对自己来说独特的才最重要。\\n无论你是否认同和喜欢,这些观点都颠覆了传统的价值观和生活观,深远影响了新人类、新新人类\\n也许没有绝对的正确,只是一种个人选择。对于大多数人来说都不会走两个极端,而是在自我和责任之间寻求一种平衡,即在背负家庭和生活责任的同时,不忘为自己小酌几杯。\\n当然,你如果对那种纯度更高的自我、自由感到恶心, No Problem,它本来就不是为你而设,你只需要知道有这样一些人存在,假如一我仅仅说假如,你全力背负着生活、家庭、单位以致社会的重负度过自己的人生时,回首往事,你不会为没有真正为自己活过而后悔。如此,就足够了,\\n\\n非暴力沟通的第二个要素是感受。心理学家罗洛梅(Rollo May)认为: “成熟的人十分敏\\n锐,就像听交响乐的不同乐章,不论是热情奔放,还是柔和舒缓,他都能体察到细微的起伏。”然而,根据罗洛·梅的叙述,大多数人的感受“像军号声那样单调”\\n\\n黑格尔的历史哲学,因他并不能像中国人般有极长极详的历史材料,可让他凭仗来形成他精美的哲学。所以他并不根据历史来讲哲学,而是根据哲学来讲历史。他说整个人类的历史,就是一部“精神逐步战胜物质”的历史\\n\\n因此中国政府需要一个世袭的元首,但也只许此元首是世袭的,其余中央地方各级政府,一切官员,则没有一个是世袭的。也许又有人要说,既然有一个世袭的皇帝掌握政府最高大权,这已就是专制。但我们要知道,中国政府的一切大权,并不在皇帝手中,皇帝下面有一个宰相,才是实际掌握政府最高大权的。试以唐代为例,唐代最高政令也分有三权:\\n是发布命令权。\\n是审核命令权。\\n三、是执行命令权。\\n发布命令的是“中书省”,审核命令的是“门下省”,执行命令的是“尚书省”。后来中书、门下合署办公,便成为两权。唐代政府最高命令是皇帝的敕旨,但皇帝敕旨并不由皇帝拟撰发出,而是由中书拟撰发出的。由中书发下的皇帝敕旨,又必经门下覆审。所以中书、门下两省,在唐代政府中即等于秦汉以来的宰相。\\n\\n这个道理,自孔子时即开始提出。《论语》里屡次说到,“士志于道,而耻恶衣恶食者,未足与议也”一类的话。孟子也说:“无恒产而有恒心者,惟士为能。”农、工、商生活有私家经济之凭藉,惟士则无恒产而有恒心。其精神所注在于“道”,不在私人衣食\\n\\n衣食。汉武帝时规定做官人不许经商,唐代规定应考人做官人都不能兼营工商业。士人报考,必须声明身家清白,此所谓清白,亦包有不兼营私人生产工作而言。因此中国社会上的士,其身份地位,很有些相当于佛教的和尚或外国的教士。不过和尚是要出家的。在西方,宗教与政治分途,“上帝的事由上帝管,凯撒的事由凯撒管”。传教徒既没有家庭,也不参加政治。而中\\n国的士,则是不出家的,不但有家庭,还要参加政府,要顾到修身、齐家、治国、平天下一套人生的大任务。西方社会里的最高人生理论寄托教会,中国社会的人生大道理,则寄托在士的\\n流。有志做士的,便不该自谋个人生活。他的个人生活该由旁人来替他解决,他则应该专为公众服务。孟子之徒问孟子:“先生后车数十乘,从者数百人,传食诸侯,不太奢侈吗? ”孟子说:“尧以天下让舜,舜受了尧的天下,也不算奢侈。像我这样,怎便算是奢侈呢?”因此中国社会上的士,是可贫可富的。\\n\\n中国历史上所载人物,像伯夷、管宁般无所表现的历代都有,而且都极为后人所重视,正因认为他们在历史上各有他们莫大的意义与价值之贡献。我不是说人不应有表现,人是应该有所表现,但人的意义和价值却不尽在其外面的表现上。倘使他没有表现,也会仍不失其意义与价值之所在。那些无表现的人,若必说他们有表现则也只表现于他们内在的心情与德性上。中国古人说三不朽,立德为上,立功、立言次之,功与言必表现在外,立德则尽可无表现,尽可只表现于其内在之心情与德性上。\\n\\n在中国传统文化之下,任何人在任何环境、任何条件下,都可堂堂地做个人,本无中国、美国之分别。而且做人,可以每天有进步。若一个人能生活得每天有进步,岂不是一个最快乐的人生吗?而且纵说每天有进步,进步无止境,又是当下即是,即此刻便可是一完人。只在当下,可以完成我最高的理想,最完美的人格,而不必等待到以后,自然也不必等待死后升到上帝的天国,才算是究竟。就在这世间、这家庭、这社会里,我当下便可成一完人,而又可苟日新,日日新,又日新,日新其德,作新民,在其内心自觉上,有日进无已之快乐。一步步地向前,同时即是步步地完成。这样的人生,岂不是最标准、最理想、最有意义、最有价值吗?孔子说: “贤哉回也,吾见其进,未见其止。”颜渊正是一天天在那里往前进,没见他停下来。颜子同门再有,他是那时一位大财政家,多艺多能,很了不起。然他内在人格方面却没有能像颜渊般一步步地向前。若仅就表现在外的看,似乎颜渊不如再有。但从蕴藏在内处的看,则冉有远逊于颜子。这意见,在中国一向早成定论,更无可疑的\\n\\n日新其德”的理论,不也是进步吗?又如说“创造”,那么在我们传统文化里,也曾创造出如我上举伊尹、伯夷、柳下惠、屈原、陶潜、杜甫等数不清的人物了。在今天我也可以日新其德,自求进步,终于创造出一个理想的“我”来。说“自由”,这是最自由的,试问作任何事,有比我自己要做一个“理想我”这一事那样的自由吗?说“平等”,这又是最平等的,人人在此一套理论下,谁也可以自由各自做一个人,而做到最理想的境地。说“博爱”,这道理又可说是最博爱的。人人有分,不好吗?此所谓“苟日新,日日新,又日新,作新民”,从各自的“修身”作起点,而终极境界则达于“天下平”,使人人各得其所,还不算是博爱之至吗?\\n\\n我喜欢熊彼特《资本主义、社会主义与民主主义》中的这段话:\\n作任何社会预测,有价值的不是由总结事实与论据所导出的是与否,而是那些事实与论据本身。它们包含着在最后结论中合乎科学的一切东西。此外的一切不是科学而是预 言。(1942, 19)\\n\\n从西方法律上讲,人同等有权利和地位,谁也取消不了谁。从西方宗教上讲,人又都是上帝的儿子。但中国人对这个“人”字却另有一套特别定义。人家尽加分别,中国人不加以分别;人家尽不加以分别,中国人独加以分别。此处实寓有甚深意义,值得我们注意和研究。\\n\\n美国在放水周期，通过放水，主动塑造出一个全球经济的扩张周期，用美元无偿获取各国商品。各国必然因为经济上行周期而不断扩张产能，扩张产能就要扩张债务，扩张债务就指望着能从以后的经济活动当中获取增长的利润来加以偿还。但是，接着，美国就会搞加息周期，来恶意引导全球经济预期，进而再收水抽贷，造成产业资本的经营困难和债务破产：出口订单减少，债务危机爆发。\\n\\n那么，这些肥美国家的产业资本所经营的生产资料，就会因为债务危机而价格跳水，那么，在枯水期唯一还保有流动性的地方，也就是美国，自然就成了这些价格缩水的生产资料的唯一买家了。\\n\\n之前，你卖粮给我，我给你纸片。\\n\\n后来，你贷款去开荒，置办了耕牛和农具，开垦了新耕地。\\n\\n再后来，我加息了，我催贷了。\\n\\n粮食价格下跌，你还不上贷。\\n\\n最后，不仅粮食在我粮仓里，连耕牛农具和耕地，也因为你的债务危机而抵押给了我。\\n\\n商品是我的，生产资料也是我的。\\n\\n但是你看，你自由了呀。\\n\\n美元不同于其他国家主权货币的一点就在于：它凭借着它的霸权，也就是它在全球调度资源的能力，使得它可以凭借自身的货币政策转向，就足以“创造”经济周期，既创造新的经济上行周期，也创造新的经济下行周期。\\n\\n货币政策周期催动产业周期，产业周期沉淀为债务周期，而债务周期，就会在政策周期的变动中，被迫爆炸。\\n\\n但是，美国在它亲手以货币政策来创造出来的全球经济衰退所造成的民不聊生中，是不负有伦理责任的。\\n\\n我调整我自己的货币政策，造成你破产了，干我屁事啊。\\n\\n谁叫你用美元，赚美元，借美元，该啊，怪不得你睡不着你，怪不得你抑郁了，该啊。\\n\\n而且，美元加息造成的其他国家产业资本价格缩水，其收益对于美国来讲，并不是第一位的。因为美国的去工业化造成的美元的寄生性，使得美元长期必然存在一种从各国的真实物质交换活动中被剔除的自然倾向，也就是美元放水所引起的人嫌狗憎，各国会对美元放水引起的美元信用稀释不满，进而乐于去构造围绕着真实物质交换的区域经济联合，而为了维持美元的货币红利，美国就需要去做恶，去使坏，去毁灭，他见得不别人好，因为其他国家相互间稳定的区域经济发展，必然要压制美元这种本无如此必要的货币的变态的世界贸易结算份额，那是美国所不能接受的。\\n\\n所以，他要摔摔打打，往各家水井里投毒，破坏各国各区域之间日渐增长的区域内经济一体化，如亚洲区域经济，欧洲区域经济，都是他眼中钉。\\n\\n只有把这个正常发展的区域经济一体化拆解掉，维持它们的碎片化，美国才能予取予求，生杀予夺。这才是美元的收割呀。\\n\\n所以，他要用加息来爆破各国，进而用债务重组去控制，实现它的超经济控制，进而实现其目的。\\n\\n发展能解决障碍，而对于一心想要拥抱障碍，甚至是“它自己就是那个最大的障碍本身”的美国来讲，其对策自然就是“不发展，甚至倒退”。\\n\\n这样拆解开来看一看也就明白了，只要谁能够以自己的主权货币来进行投资扩张，且这种投资扩张不会引发通货膨胀，那么这个国家，就无惧于美元的把戏。\\n\\n谁能够一边投资，一边不爆发通货膨胀呢？那就需要这个国家能够提供源源不断地充足的生产资料和消费品，那就可以。', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8889a010-999f-477b-acba-23b7dab5bd3f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n杂 70\\n\\n如果你看懂了上面这个基础，你应该会注意到，大部分人（俗人）思考问题，都解决不了这个问题：“现实和欲望的关系是什么？“，因为别人的欲望就是你的现实，你自己的欲望很容易改变，然后你就以为别人的欲望是一样（容易改变）的，却不明白别人（们）的欲望是一个“妙”，《道德经》给出了一个很好的基础概念空间。清晰地定义了这两者的分野，并用大量的实例来强化这几个概念之间的关系。\\n\\n但这个基础哲学交到俗人的手中，他们是理解不了的。虽然这个东西其实真的很简单，但这个世界就是有很多人是抛不开自己的（这可能是造物保护人的需要）。你给他讲“事实”是什么，他永远要关联到自己身上，然后以这个事情（理论上）对自己好不好来判断对错。这样，在这些人身上就存在一个很大的矛盾区：他们发现自己的处事原则常常解决不了问题，但他们不知道为什么。他们的“理论”什么`都解决不了。他们无论成败，都不知道为什么，不知道是什么造成了成功。但他们又向往成功者的策略。而成功者的真正策略是无为（参考这里：老子的「无为」是一种什么样的态度，对任何事都「无为」么？ - in nek 的回答），这种策略被他们用他们个人观点来解释，他们的世界就不完满了。就会进化出所谓：存天理，去人欲这样的理论来。\\n\\n这类理论认为，这个世界是存在绝对真理的，事君有事君的真理，交友有交友的真理，这些道理，是可以被遵循的，只要你按这些道理去做，就能达到成功的彼岸。你要努力去学习这些道理，并去践行，如果你没有做好，是因为你还有人欲，有私心，有私欲，所以才没有做好。只要你做好了，自然问题也就解决了。你看知乎上很多人，努力去说服别人，却没有什么兴趣去知道别人的实际情况，都是这种心理在作祟。\\n\\n说到底，他们是认为这个世界上有“绝对真理”\\n\\n这种观点只能骗那些不真诚的人（就是我前面举的那些“相信”的例子），因为你发现用这样的策略来做事，怎么都做不成。只是看到一堆道貌岸然的伪君子，他们说的话，他们自己都不信。王守仁那个时代的知识分子，在这个问题上，非常痛苦，主流学说只能用来说，不能用来用。对一个真诚，而且摆脱了经济压力的人来说，这是非常非常痛苦的事。这个社会的主流人生观不自恰，整个社会都在等待一种力量来把大家拉到一起去。\\n\\n这个问题和我们今天遇到的问题是一样一样的，你说我们今天的主流价值观是什么？我们努力的目标是什么？共产主义？估计不是，你要共产主义干什么？为人民服务？说不通的，凭什么你要为人民服务？说出来好听的东西，不见得是你内心的真心观点。民主自由？真诚一点说吧，如果没有了“当今“，民主自由你真的关心？（请搞明白这句话什么意思，不要上来就喷），民主自由是用来寒摻执政者用的好吧，没有了执政者在那里，你该换一个口号了，所以那个仍不是你的欲，不是这个社会的欲，然则，我们的欲应该是什么？这是我们不少知识分子的烦恼，也是是王守仁那个时代知识分子们的迷茫……说远了，说这么多，是让读者们可以近距离体验一下，为什么我们会说王守仁是圣人。他做了一件什么事。\\n\\n王守仁怎么解决这个问题的呢？为了理解他的观点，首先你要理解一下《道德经》的局限在什么地方。《道德经》对道理（规律）的论述已经非常自恰了，即使面对相对论这样的现代学问，《道德经》都没有不自恰的地方，虽然它不能解决相对论的问题，但任何理论都要最终反映为“对我的影响”，反映为“眼前此刻”，《道德经》在解决眼前此刻的问题上始终没有什么落后和过时的\\n\\n但及时如此，它没有解决另一个关键问题：我个人到底应该追求什么欲？我已经知道如何做成一件事了，但我不知道我为什么要做成那件事。\\n\\n王守仁说，这你得问你自己的本心。因为世间的一切都是你本心的反映。你觉得你做得对，那只是你自己觉得你做得对，不是因为别人说你做得对。所以，所有的道理不过是你内心的反映，心即理也。你事君，交友，治民是你觉得这样做是对的，是好的，你不会去让“君“说你好来实现“事君“这件事的成功，你“弑君”也可以认为是“事君”的成功，因为你可以认为这样是为了他少做点坏事，让他青史留名。现在多少父母会“修理”他们的孩子，他们还会心安理得地说，“这是为你好”。“好坏”这种东西，这些都是你内心的感受，所以，你应该做什么，要问你自己的本心。\\n\\n这就和上面那个俗人的观点有本质区别了，俗人的观点看起来更唯物，认为真理在物上，物上有正确的行为这个属性。而王守仁的观点显得更唯心，他说，物可以有规律，但最终好和坏，却是内心的反映，所以，规律你可以求诸外，但良知你要求诸內。你要问你的心\\n\\n而你的本心其实是无所谓的，因为你的心在没有和外部信息交互的时候，他什么都没有。这就好像一个程序，没有任何输入的时候，计算机在死循环等待输入，程序（逻辑）虽然在哪里，但他们没有运行，有和没有无所谓。而你和这个世界开始互动了，这称为意，或者意动，这时就会和心发生关系。而心对于什么是好的什么是坏的其实是知道的，在当你按着无为的策略去执行的时候，最后你会知道你的心到底要的是什么。正如我在那个《道德经》中解读的那样，有的人说，我就是想要有钱，但实际上你不是。你也可能说，我想要的就是要压倒别人，权倾朝野，但如果你按无为的策略来做事，你会知道这个代价是什么，同时最后你会推演出这个是否是你真正的欲\\n\\n这看起来就好像你的心是另一个人（或者计算机），你没有和他交往，你不知道他的特点，等你和他有很多很多的交往了，你就明白它的特点是什么了。王守仁的修炼，你让你不断面对自己的内心，通过做事（格物），最终理解你心中的良知是什么。这个思维，就叫无善无恶心之体，有善有恶意之动。你不去做事（无论是行善还是行恶），你就没有意动，也就不会真正知道你的心，也不知道你把什么认为是善，什么是恶。从道德经的角度上说，这个世界没有善恶（其实这个应该算是现代社会的所谓“普世价值”了），但你的心有，人人说你是个坏蛋，你还是会觉得你在行恶，外界的输入会让你真正面对你的内心，也许有人会在被人人唾骂的时候仍找到自己心中的善，王守仁不反对，你真的承认就好，他不在乎，他只是说，你“真的这样想才好”\\n\\n现在你明白什么是知行合一吗？身之主宰便是心，心之所发便是意，意之本体便是知，意之所在便是物。这些全部都是你心灵中的东西，是你的心灵和世界的互动。你的视，听，言，动决定你的欲，无论你用“精一“（深度）还是“博约“（广度）来从这个世界吸取信息，这些信息本身就是物，是一种心对世界的抽象\\n\\n俗人认为知和行是分离的，要让行向知靠拢。而知行合一是说，这两者是一个东西，你做了一件事，那既是你的知，也是你的行，这两者共同映射到你的心里面。如果你愿意去面对你自己的内心，找到你真的觉得快乐的东西，知道什么是你不希望看到的东西，那个东西叫“良知”，（这个概念可不是我们以为的那个强加给别人的“良知”的概念，那是自己的概念，自己真心觉得这个是自己想要的），而要达成这个问题的唯一方法是“做事”，也就是所谓知善知恶是良知，为善去恶是格物。\\n\\n这个逻辑是很“完满”的，如果你还处于中二阶段，或者满脑子自己（的绝对真理），你当然不能感受到它有多完满，但对于找不到人生目的做事者来说，这已经是我能想到的最优解了。否则你还能相信什么？如果你一直找不到，就只能寄望宗教了。但对于聪明人来说，相信一个别人制造和解释出来的东西，这是无法自己骗自己的。一般的宗教，或者类似宗教式的鼓动宣传，只属于俗人和控制者，它无法让你找到你的真正追求的。至此，你就可以理解，为什么说王守仁是圣人了。他建立了一个不需要你崇拜的宗教啊。\\n\\n存在主义有一个基本的假设前提：“只有意识到生命本无意义，才能赋予它任何你想要的意义”。每个人的“心（或者欲）”都只有自己才有可能去弄清楚到底是什么，追求这个目标的过程，王阳明管它叫“致良知”，而实现这个目标的操作手法，叫“格物”，或者再具体点叫“世上练”。这个方法有点类似于数学上的穷尽法，在不断的尝试、比对、觉知的过程之中，我们最终在不断逼近那个我们所期望得到的“真相”：找到我们所真正喜欢的，真正快乐的，真正心安理得的。\\n\\n“意”是在我们的身心上不断的变化和流动的感觉，而在那些“意”的背后才是藏着一个真正的本体“心”，与这些“意”的相处某种意义上也是这个过程的一部分。（这一段是我推导出来的，我个人的体悟还处在这个阶段之中，并非完全明晰，很多时候会执着于“意”之中而痛苦万分。）“阳明学”所说的“此心不动，随机而动”，讲的大概就是要去训练一颗“心”不被“意”所随意影响吧。\\n\\n首先应明确的是，萨特在《存在与虚无》中论证的自由不是人追求的东西，它本身就是人的存在。如果说，人现在的行动赋于过去以意义，萨特由此得出\"存在先于本质\"的结论，那么，自由则是行动的首要条件，甚至可以把这句著名论断改为\"自由先于本质\"。\\n\\n人所以是自由的，首先是因为，人在行动中表现出来的并不是他自身，而是一个面对自我的在场，他总是要向外超越，自由就是人的实在核心中的虚无，它迫使人不断地自我造就，而不象自在那样单纯地\"是\"。所以，不应把萨特的自由理解为人的本质，而应说人就是自由，\"自由先于人的本质，并使人的本质成为可能。\"(《存在与虚无》第495页)\"人命定是自由的\"这句名言集中体现了这种自由的意义，这句话直译为\"人被判决为是自由的\"(L’homme est condamnéà être libre)。我们可以从两方面来理解它：一是命定意味着人不能自己创造出自己的肉体来，他是被无缘无故地抛到世界上来的；二是人一旦被抛到世上，就享有绝对的自由，就应为他所做的一切负责任。自由对人是与生俱来，无可逃避，它就是人的宿命。自由对于人与其说是一种幸福，毋宁说是一种痛苦。\\n\\n自由的本质，就是按你的意志来做事。而且这里的意志应该是理性意志。你按照自己的意志做事，必然要自己给自己立规矩，然后遵守自己的规矩。遵守规矩的过程就是自律。其实整件事不如说是“自主”更恰当一些。自主是一枚硬币，自由和自律是硬币的两面\\n\\n如果行为和想法违背自己订立的这种理性的意志，人就会痛苦；如果没有违背，就会开心。这里的痛苦和开心，都是非表面的\\n\\n我们在此找到了所有错误的“生活意义”和正确的“生活意义”的共同尺度。所有失败者——神经症患者、精神病患者、罪犯、酗酒者、问题儿童、自杀者、堕落者以及妓女——之所以都是失败者，是因为他们缺乏同伴感和社会兴趣。他们在处理职业、友谊和性等问题时都缺少通过合作加以解决的自信，他们赋予生活的意义是一种个人的意义：没有哪个人可以从实现自己的目标中获益，他们的兴趣也只停留在自己身上。他们成功的目标对自己而言，只是一种虚构的个人优越感目标，他们的胜利只对自己有意义\\n\\n人的思想同样如此。古人云“君子一日三变”，意思是说，君子一天之中三次改变其想法。正是因为能够发现并创造新生事物，他们才能成为君子。由此可知，一天连一变都不愿做的想法是要不得的。对于变化，每个人都会心存畏惧和不安。这是人本性的一种体现。不过，这也可以说是人心有旁骛的一种体现。一变二变是进步的姿态，而三变四变则是发展的动力\\n\\n温饱解决后贵族们陷落到空虚无聊之中。摆脱空虚无聊有两个出口：一个是堕落，一个是升华\\n\\ndesirability \\xa0feasibility 决定了人们有没有 motivation\\n\\n洛夫的诗：“如果你迷恋厚实的屋顶，你就会失去浩瀚的繁星。”躲在“无为”背后，是深深的畏惧。就像我回答了这么多问题，因为回答问题这件事，不用和人交流，不用思前想后，或者干脆说可以轻松逃避职业失败这个事实。\\n\\n这是不对的。精神麻醉只是麻醉，没有办法把人带到更高的精神境界。\\n\\n飞的八分比走的十分看似没什么不同，但就算只差两分，能够飞上天，不是很令人开心的事吗？\\n\\n说八分跟十分没有差别，就好像是说每个人迟早会死，怎么活都没有差别。──虽然迟早会死，但过什么样的生活却是天差地远。\\n\\n简单说，那段经历让我适应了宁静，学会了看淡一切，但也磨灭了我的斗志和野心。\\n\\n后者固然不可取，但对于前者，倒是可以作为诸位的借镜。当你为自己的生活感到疲倦、需要宁静的时刻，不妨试试本能的沉默，并在自己的角落好好休息。你的生命能量会一点一滴、一日一夜递增，不假多时，便能恢复渴望已久的生机盎然。\\n\\n《禅与摩托车维修的艺术》曾提到:\"人的倾向是只以单一的模式来思考、感受、置身单一模式的人\\n\\n往往误解、低估另一模式的真谛,却没有人愿意放弃眼前的真理。\"不论人生境遇如何,我总是以此为鉴,并致力于理解差异说法之所由,用心地追求会通两端的可能,在此所谓的“会通\",并非韦迳取折衷与调和,所重当在通过此更了解彼,通过彼更了解此。\\n\\n但伴随这可怕一面而来的，还有一个美好的前景：萨特的存在主义暗示的是，只要你一直努力，那就有可能获得真实与自由。这有多令人激动，也就有多令人惧怕，而且二者的原因还都一样。正如萨特在演讲结束后不久的一次采访中总结的那样：没有任何划定的道路来引导人去救赎自己；他必须不断创造自己的道路。但是，创造道路，他便拥有了自由与责任，失去了推脱的借口，而所有希望都存在于他本身之中\\n\\n少有人走过的路\\n\\n爱，是为了促进自己和他人心智成熟，而不断拓展自我界限，实现自我完善的一种意愿\\n\\n律能够让我们承受问题带来的痛苦，并最终解决问题；而心灵在承受痛苦和解决问题的过程中，则会不断地成长和成熟。所以，自律是人们心灵进化最重要的手段和工具。那么，我们为什么愿意通过自我约束去承受人生的痛苦呢？因为有一种力量在推动着我们，这种力量就是爱。爱是人们自律的原动力\\n\\n真正意义上的爱，既是爱自己，也是爱他人。爱，可以让自己和他人都获得成长。不爱自己的人，绝不可能去爱别人。父母缺少自律，心灵不能成长，就不可能让孩子学会自律，获得心灵成长。我们在推动他人心智成熟之时，自己的心智也不会停滞不前。我们为了他人去努力自律，与为了自己去努力自律一样，这二者之间并没有太大的区别。我们强化自身成长的力量，才能成为他人力量的源泉。我们最终会意识到，爱自己与爱他人，其实是并行不悖的两条轨道，随着时间的推进，两者不但越来越近，其界限最后甚至会模糊不清，乃至完全泯灭。\\n\\n第四，爱需要付出努力。由于爱是不断扩展自己和他人自我界限的过程，所以，爱意味着我们要不断付出努力，去跨越原来的界限。爱不能停留在口头上，而要付诸行动；爱不能坐享其成，而要真诚付出。我们爱自己或爱某人，就要持续地努力，帮助自己和他人一起获得成长\\n\\n爱，最重要的体现形式，就是关注。我们爱某个人，一定会关注对方，细心照料对方，进而帮助对方成长。我们必须把成见放到一边，调整心理状态，满足对方的需要。我们对对方的关注，一定是一种发自内心的行为，这种行为不仅能促进对方心智成熟，还可以对抗自己内心的懒惰，让我们付出努力。著名心理学家罗洛梅说过：“如果用现代心理分析工具去分析每个人爱的意愿，我们就会发现，爱的意愿的本质，其实是一种关注。为了完成意愿所需要的努力，就是对关注的努力，也就是努力去关注。我们要让头脑清醒，让心智健全，这是体现关注的最基本要素。”\\n\\n勤于自省，才能走出这种境地。如果你具有爱心，而且想帮助对方，首先必须进行自我反省，确认自己的观点是否有价值。“我看清了问题的本质吗？”“我的动机是为对方着想吗？”“我发现了问题的症结，还是出于模模糊糊的假想？”“我是否真正了解我所爱的人？”“他的选择可能是正确的，我是否因经验有限才觉得他的选择不够明智呢？”“我想给所爱的人提供指导，是否是出于一己之私？”真正以爱为出发点的人，应该经常反思上述问题。\\n\\n对别人提出批评，通常有两种方式：一种是仅凭直觉就坚信自己是正确的；另一种是经过反省，确认自己有可能正确。前一种方式给人以高高在上的感觉，父母、配偶或者教师常常采用这样的方式，这很容易招致不满和怨恨，而不会给对方的成长带来帮助，甚至还会产生意想不到的消极后果。第二种方式给人谦逊而谨慎的印象，它需要批评者首先自我完善，由此让很多人知难而退。但与第一种方式相比，这种方式更有可能带来成功，而且，根据我的经验，它通常不会产生破坏性的后果\\n\\n也有相当多的人宁可压抑自己批评他人的冲动，对他人的问题视而不见。他们过于谦虚，总是三缄其口，从不给所爱的人指导和建议。这种人不具备真正的爱。\\n\\n人不应被情感所奴役，也不能把情感压抑得荡然无存。我有时候告诉患者，如果感情是他们的奴隶，自律就是管理奴隶的法律。感情是人生活力的来源，它让我们体验到人生的乐趣，满足自我的需求。既然感情可以为我们服务，我们就应该尊重它的价值。不过，作为感情的主人，我们却经常犯两个错误：其一，我们可能对奴隶不加约束，听之任之。我们从不给予管理和指示，长此以往，奴隶也就不再工作，而是闯进主人家里，为所欲为。它们搜光橱柜，砸烂家具。不久以后，我们就发现自己成了奴隶的奴隶\\n\\n加缪辞世后，阿尔及利亚的友人在蒂巴萨为加缪树立了一块纪念碑，碑上镌着加缪的一句话：“在这儿我领悟了人们所说的荣光，就是无拘无束地爱的权利\\n\\nIn the midst of winter, I found there was, within me, an invincible summer.\\n\\nAnd that makes me happy. For it says that no matter how hard the world pushes against me, within me, there’s something stronger – something better, pushing right back.\\n\\n哀恸的人有福了，因为他们必得安慰。\\n\\n怜恤人的人有福了，因为他们必蒙怜恤\\n\\n理性是人类后天的训练成果，不是本能（所以说理性=神性）\\n\\n在长期的从医生涯中，我发现几乎每一位患者都会刻意隐瞒某些真相，因为那些真相曾经让他们非常痛苦，他们虽然用说谎的方式暂时避免了痛苦，却不得不去承受另一种长期的痛苦——心理疾病无尽的折磨\\n\\n恶，可以定义为：为了维护病态的自我\\n\\n为了追逐快乐，人会努力；为了逃避痛苦，人会不遗余力。所谓“不遗余力”，就是不择手段，用谎言颠倒是非，混淆黑白，不计后果，只要是能避免痛苦，任何伤天害理的事情都可以干得出来\\n\\n从前面的案例中，我们发现这些邪恶的人都是控制别人上瘾的人。比利和鲁克的父母控制孩子上瘾，桑德拉控制亨利上瘾。如果不让他们去控制别人，就如同不让烟鬼吸烟，不让酒鬼喝酒一样，他们会十分难受。那么，为什么他们要乐此不疲去控制别人呢？因为他们找不到真实的自己。寻找真实自己的过程，是一个痛苦的过程，需要接纳自己的不完美，承受罪恶感所带来的痛苦。凡是控制别人上瘾的人，都是不敢面对真实自己的人，他们对良心的谴责充满了恐惧，不敢面对自己的内心，不敢正视自己的“恶”，极力逃避罪恶感所带来的痛苦\\n\\n积极的痛苦是人生必须承受的；而消极的痛苦像头疼，应该尽力摆脱\\n\\n然而，要达到这样的认识高度，必须彻底转变对痛苦的看法，同时也要彻底转变对意识的看法。在伊甸园的故事里，人类吃了善恶树上的禁果后，就有了意识，有了意识，也就有了痛苦。所以，意识是我们痛苦的源泉。倘若没有意识，也就无所谓痛苦。但意识并不只是给我们带来痛苦，它同时还会给我们带来摆脱痛苦、获得救赎的动力。而救赎本质上就是治疗。\\n\\n意识是痛苦之源，没有意识，就感觉不到痛苦。我们帮助别人减轻身体上的痛苦，最常用的方法就是麻醉他们，让他们暂时失去意识，感觉不到痛苦。\\n\\n痛苦完全由意识引起，但救赎的动力也来自于意识。拯救的过程就是意识逐渐增强的过程。随着意识的增强，我们就不会像那些不愿成熟的人一样，畏缩在洞里止步不前，我们会一步一步地进入沙漠。在继续前行时，我们会承受越来越多的痛苦，但我们也因此变得越来越成熟\\n\\n我把心理健康定义为：不惜任何代价不间断地致力于面对真实。“不惜任何代价”意味着，不管事实让我们多么不舒服、多么痛苦，自己都要勇敢地面对\\n\\n这些都没有错。无论我们走得多高多远，我们都无法摆脱心灵发展早期阶段残留下来的遗迹。所以，如果正自鸣得意，觉得你自己已稳步地走在第四阶段的正确道路上，那么赶快检查一下你的地牢。相反，如果你感到自己的不足还有很多，这将有助于你自醒，认清我们身上残留的遗迹，向更高级的阶段拓展。就像奥斯卡·王尔德所说：“每个圣徒都有过去，每个罪人都有未来。”\\n\\n既然我肯定是要死的，那么我总是不能放弃愚蠢而陈旧的自我，又有什么意义呢\\n\\n稣既是人也是神，但不是各占一半，如同教义所说：一个完完全全的人和一个完完全全的神\\n\\n我对“地狱”的看法也不是传统基督教式的，但我还是要因此而感谢C.S.刘易斯，本世纪最伟大的基督教作者。他的长篇小说《大离异》，写的是在地狱里的一群人的故事。故事发生在一个凄惨、黯淡的英格兰中部城市，这群人设法登上一辆去往天堂的公交车。天堂是一个明亮的、充满欢乐的、可爱的地方，他们受到了亲戚朋友们的热情问候和殷勤招待。一天结束的时候，除了一个人之外所有人都回到了公交车上，有点不清楚的是，除了一个人之外，所有人都选择了回到地狱！\\n\\n为什么？刘易斯用了许多例子。长话短说，我引用下面这个例子作为代表。且说在公交车上有一个人受到了他侄子的欢迎，他对在天堂里遇见侄子感到很奇怪，因为他认为他侄子在地球上的所做所为，绝不会有这样的结果。但是他侄子正在热情地欢迎他，天堂也是这么的明亮和欢乐。那人说道：“这似乎是一个非常好的地方，我想要留在这儿。但是你知道的，我是哥伦比亚大学的一名历史教授。你们这儿有大学吗？”\\n\\n侄子答道：“当然有了，叔叔。”\\n\\n“我想要得到一个终身教职。”\\n\\n“你当然会得到终身教职啦，在天堂里每个人都有终身职位。”\\n\\n叔叔很是吃惊。“每个人都有终身职位？这怎么可能？难道你们不辨别一下，谁能胜任、谁不能胜任吗？”\\n\\n侄子回答：“在这儿每个人都是能胜任的，叔叔。”\\n\\n叔叔还是不放心，继续追问侄子：“你知道的，我是系主任，所以我认为我在这儿也应该是主任。”\\n\\n“我很抱歉，我们没有主任。这里不是以那种方式运作的。我们每个人都很负责，工作起来协调一致，所以根本不需要负责人。”\\n\\n叔叔有点气急败坏，“如果你认为我打算参加某种幼稚的组织，不区分有能力之人和乌合之众，那你就想错了。”于是，他登上公交车返回了地狱。\\n\\n我对“地狱”的看法跟刘易斯很像。地狱之门是大开着的，人们能够走出地狱，而他们之所以留在地狱的原因是他们没有选择离开\\n\\n再说说“知晓”。在我信教前，我的主要身份是科学家。科学家又被称为“经验主义者”。在经验主义者看来，人类实现认知的最佳但并非唯一途径就是“经验”。所以科学家们只能借助实验去掌握一些经验，除此之外，我们从哪儿还能够学习并最终知晓呢？我正是通过自己的人生经历，才对上帝有一点点“知晓”的。\\n\\n在我看来，思考用的时间一般很短暂，如果用的时间很长，就是沉思，而如果你思考你的思考，这就是冥想。沉思和冥想是深入心灵的桥梁，能够挖掘事物的根本，直抵内心的深处，探讨和解决生命中最重要的问题\\n\\n但是当我说人生痛苦时，并不意味着我们永远无法得到快乐。实际上，人生在充满痛苦的同时，也充满了幸福；在最困难的时候，也最接近希望。如果我只强调人生的苦难，而忽略了所有的快乐、善良和温暖，忽略了心灵成长的机会，以及生命宁静和美好的特质，那么我的思考就是草率的，不完整的，也是不正直的。的确，现实的神秘与矛盾在于，生命虽然会带来焦虑和痛苦，不过，一旦我们超越焦虑和痛苦，随之而来的就是无可限量的幸福和快乐\\n\\n所有人都有矛盾思考的能力，但是我们忽略或使用这项能力的程度则相差很大。这不是由我们的智商所决定，而是由我们的态度所决定\\n\\n**一旦心智开始扩展，它就永远不会回到从前的限制**\\n\\n迈向心理健康的道路是承认死亡，而不是否认死亡。在这个课题上我所读过的最好的书，是约瑟夫·锋利写的《生活在死亡的边缘》。与我一样，他相信死亡不是意义的剥夺，而是意义的赋予。不管我们是年轻还是年老，对于死亡的意识能够引导我们走上追寻意义的道路\\n\\n**不成熟的人最大的特征是，他们总是坐在那里抱怨生命没有满足他们的需要。而少数成熟的人则将生命视为自己的责任，努力满足生命的需要。**实际上，当我们明白发生在自己身上的一切事情，都可以成为锻炼的机会时，我们也就可以用截然不同的态度来看待生命了\\n\\n对于“建设性”与“非建设性”这两个词，我喜欢用“存在性”和“神经官能性”这两个词来代替。存在性的痛苦是生存本来的一部分，是不应该躲避的。例如，放弃旧习惯的痛苦，改变的痛苦，重新学习的痛苦，面对打击和挫折的痛苦，面对衰老与死亡的痛苦等。从这么多的痛苦中，我们可以学到许多东西。承受这些痛苦，我们的心灵就能获得成长，变得越来越强大；逃避这些痛苦，则会让我们的心灵退化、人格萎缩。\\n\\n而神经官能性的痛苦就是指逃避生存性痛苦所带来的痛苦，这些痛苦是我们自己衍生出来的，多余的，没有必要的。之所以说它们是非建设性的痛苦，是因为它们不仅不会帮助我们的心灵获得成长，反而还会阻碍我们，让我们不堪重负。这就像打高尔夫球，我们只需要14只球杆，而你偏偏要背上98只杆，让自己举步维艰\\n\\n自律有四个原则：推迟满足感、承担责任、忠于事实、保持平衡\\n\\n托马斯·潘恩在《理想时代》中说：“为了人类的幸福，一个人在思想上必须对自己保持忠诚，所谓不忠诚不在于相信或不相信，而在于口称相信自己实在不相信的东西。”\\n\\n懒惰是人类的原罪，而改变则是我们的原恩。正是因为这种与生俱来的能力，我们才能不断改变自己，升华自己。所以，根本就没有人性这回事情，是改变把我们与其他动物区分开来。人类与其他动物最大的区别，不是我们能抓住东西的拇指，不是我们灵巧的声带，也不是我们巨大的脑容量，而是我们极端缺乏的动物本能，我们没有太多预先设置并遗传下来的行为模式。比起其他动物，我们非常缺乏固定不变的本性。但值得庆幸的是，我们懂得改变，并拥有更多的选择。社会性的选择，心理性的选择与生理性的选择，这些选择使我们能够更有弹性地应付各种不同的情况\\n\\n虚假的平等使我们产生了虚假的共同体观念——所有人都是一样的！当这种观念破灭时，我们就会以强迫性手段来达到平等：先是温和地劝说，然后越来越严厉，完全误解了我们的责任。**我们的社会责任不是去建立平等，而是去发展系统，合乎人性地处理我们的不平等。**在合理的限度内，这种系统应该赞赏并鼓励多样化\\n\\n在我看来，追求真理远远比追求幸福更重要，在追求真理的路上，我们一定会遇到幸福，但在追求幸福的路上，我们却常常遭遇不幸', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ac2b9ef7-820d-4cc7-a2c4-5871808dc704', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n杂 54\\n\\nThe test of a first-rate\\xa0**intelligence**\\xa0is the ability to hold two opposed ideas in the mind at the same time, and still retain the ability to function\\n\\n我问查理：“你有自己的私人飞机，伯克希尔也有专机，你为什么要到商用客机机场去经受这么多的麻烦呢？”\\n\\n查理答：“第一，我一个人坐专机太浪费油了。第二，我觉得坐商用飞机更安全。”但查理想说的真正理由是第三条：“我一辈子想要的就是融入生活（engage life），而不希望自己被孤立(isolate)。”\\n\\n查理最受不了的就是因为拥有了钱财而失去与世界的联系，把自己隔绝在一个单间，占地一层的巨型办公室里，见面要层层通报，过五关斩六将，谁都不能轻易接触到。\\n\\n认为不做好人也可以成为伟人的想法真是大错特错，我敢保证，人世间真正的伟人同时也必定是真正道德高尚的人。\\n\\n——本杰明·富兰克林\\n\\n恐怕事情就是这样的。假如有20种相互影响的因素，那么你必须学会处理这种错综复杂的关系——因为世界就是这样的。但如果你能像达尔文那样，带着好奇的毅力，循序渐进地去做，那么你就不会觉得这是艰巨的任务。你将会惊讶地发现自己完全能够胜任。\\n\\n——查理·芒格\\n\\n查理自己也说过：“我最反对的是过于自信、过于有把握地认为你清楚你的某次行动是利大于弊的\\n\\n最成功的投资必定是最像生意的投资。\\n\\n投资者购买股票应该像购买日用品——而不是像购买香水。\\n\\n——本杰明·格拉汉姆\\n\\n如果说我们有什么本事的话，那就是我们能够弄清楚我们什么时候在能力圈的中心运作，什么时候正在向边缘靠近。\\n\\n——沃伦·巴菲特\\n\\n如果你确有能力，你就会非常清楚能力圈的边界在哪里。如果你问起（你是否超出了能力圈），那就意味着你已经在圈子之外了。\\n\\n——查理·芒格\\n\\n你必须意识到生物学家朱利安·赫胥黎的那句话是千真万确的——他说：“生活无非就是一个接一个的联系。”所以你必须拥有各种模型，你必须弄清楚各种模型的相互关系以及它们的效应。\\n\\n有性格的人才能拿着现金坐在那里什么事也不做。我能有今天，靠的是不去追逐平庸的机会。\\n\\n——查理·芒格\\n\\n泰德·威廉姆斯是过去70年来惟一一个单个赛季打出400次安打的棒球运动员。在《击球的科学》中，他阐述了他的技巧。他把击打区划分为77个棒球那么大的格子。只有当球落在他的“最佳”格子时，他才会挥棒，即使他有可能因此而三振出局，因为挥棒去打那些“最差”格子会大大降低他的成功率。作为一个证券投资者，你可以一直观察各种企业的证券价格，把它们当成一些格子。在大多数时候，你什么也不用做，只要看着就好了。每隔一段时间，你将会发现一个速度很慢、线路又直，而且正好落在你最爱的格子中间的“好球”，那时你就全力出击。这样呢，不管你的天分如何，你都能极大地提高你的上垒率。许多投资者的共同问题是他们挥棒太过频繁\\n\\n乌龟若能找到某些特别有效的方法来应用前人最伟大的工作，或者只要能避免犯下常见的错误，这种情况就会发生。我们赚钱，靠的是记住浅显的，而不是掌握深奥的。我们从来不去试图成为非常聪明的人，而是持续地试图别变成蠢货，久而久之，我们这种人便能获得非常大的优势。\\n\\n说真话，你将无须记住你的谎言。就是这么简单的一个概念。\\n\\n在未来五到十年里，如果美国没有遇到（和衍生品相关的）大问题，我将会非常吃惊。\\n\\n我认为我们是美国惟一甩掉衍生品账本的大型公司。\\n\\n这两种看法之间的分歧经久不衰，威廉·詹姆斯和大卫·休谟（David Hume）正是为此分道扬镳的。当我们围绕本话题进行讨论时，常常会忽视休谟的存在。詹姆斯明确指出，他所提出的自我概念具有坚实的生物基础。他所说的“自我”并不是形而上学的觉知力。但他还是认识到自我具有觉知功能，即便这种功能难以觉察。而休谟摒弃了自我，甚至达到了要消灭自我的程度。下面这段话体现了休谟的观点：“任何时候，如果失去了知觉，我就绝不可能觉察到我自己。除了知觉以外，我们也绝不可能观察到其他东西了。”他还说：“我可能要斗胆对其他人类断言，他们不过是一大批各种知觉的集合。这些知觉以不可思议的速度快速交替出现，不断变化着、运动着。”\\n\\n休谟否认自我的存在，这让詹姆斯大为震动，在对休谟的观点进行评价时，他的质疑令人印象深刻。他申明自我是存在的，并强调自我中奇特地混合着“单一性和多样性”，号召人们关注贯穿自我成分中的“同一性内核\\n\\n迄今为止，大多数有关有意识心智的神经生物学研究进展都是基于以下三种研究角度的结合而进行的。\\n\\n**1．直接见证人角度：对于我们每个人而言，个体的有意识心智是个人的、私密的、独一无二的；**\\n\\n**2．行为角度：我们有理由相信，他人也同样具有有意识心智，我们能够从他们的行为中观察到有意识心智的蛛丝马迹；**\\n\\n**3．大脑角度：在假设个体的有意识心智存在或不存在的状态下，我们都可以研究大脑功能的某些方面**\\n\\n现在，从第四种研究角度出发，我可以基于演化生物学和大脑研究的相关证据，对之前提出的观点作出些许修正：百万年以来，无数生物的大脑中都出现过活跃的心智，但严格地说，唯有在大脑产生了具有见证能力的主人公之后，才产生了意识，并且唯有在大脑产生语言后，心智的存在才广为人知。见证人是一种附属物，它解释了内隐的脑活动，即心理的存在。对于意识的神经生物学来说，重要的目标是理解大脑是如何产生这一附属物的，它是与我们如影随形的主人公，我们称其为自我\\n\\nPeople are afraid of disappointing themselves, so they procrastinate the action. The truth is that not doing anything is even more disappointing, but we learn it too late\\n\\n爱因斯坦说他那些成功的理论来自“好奇、专注、毅力和自省”。他所说的自省，就是不停地试验与推翻他自己深爱的想法。\\n\\n最后，尽可能地减少客观性，这样会帮助你减少获得世俗好处所需作出的让步以及所要承受的负担，因为客观态度并不只对伟大的物理学家和生物学家有效。它也能够帮助伯米吉地区的管道维修工更好地工作。因此，如果你们认为忠实于自己就是永远不改变你们年轻时的所有观念，那么你们不仅将会稳步地踏上通往极端无知的道路，而且还将走向事业中不愉快的经历给你带来的所有痛苦。\\n\\n那就像谚语所说的：“在手里拿着铁锤的人看来，每个问题都像钉子。”当然，脊椎按摩师也是这样治病的。但这绝对是一种灾难性的思考方式，也绝对是一种灾难性的处世方式。所以你必须拥有多元思维模型。\\n\\n这些模型必须来自各个不同的学科——因为你们不可能在一个小小的院系里面发现人世间全部的智慧。正是由于这个原因，诗歌教授大体上不具备广义上的智慧。他们的头脑里没有足够的思维模型。所以你必须拥有横跨许多学科的模型。\\n\\n你们也许会说：“天哪，这太难做到啦。”但是，幸运的是，这没有那么难——因为掌握八九十个模型就差不多能让你成为拥有普世智慧的人。而在这八九十个模型里面，非常重要的只有几个。\\n\\n长远来看，股票的回报率很难比发行该股票的企业的年均利润高很多。如果某家企业40年来的资本回报率是6%，你在这40年间持有它的股票，那么你得到的回报率不会跟6%有太大的差别——即使你最早购买时该股票的价格比其账面价值低很多。相反地，如果一家企业在过去二三十年间的资本回报率是18%，那么即使你当时花了很大的价钱去买它的股票，你最终得到的回报也将会非常可观。\\n\\n如果把沃伦·巴菲特看作普世智慧的典范，那么有个故事非常有趣：沃伦敬爱他的父亲——那是一个了不起的人。但沃伦的父亲有强烈的意识形态偏见（正好是右翼的意识形态），所以跟他交往的都是些意识形态偏见非常严重的人（自然都是右翼分子）。\\n\\n沃伦在童年时就观察到这一点。他认为意识形态是危险的东西，决定离它远远的。他终生都离意识形态远远的。这极大地提高了他认知的准确性。\\n\\n如果你把准确、勤奋和客观当成你笃信的意识形态，那倒不要紧。但如果你们因为受到意识形态的影响，而确凿无疑地相信最低工资应该提高或者不该提高，并认为这种神圣的想法是正确的，那么你们就变成了傻子\\n\\n这种背道而驰的历史，我想说我同意彼得·德鲁克的观点：跟其他利益相比以及跟大多数其他国家相比，美国的文化和法律制度特别照顾股东的利益。实际上，在许多国家，股东权益并没有得到很好的保护，有许多东西比股东权益更重要。我想许多投资机构低估了这个因素的重要性，也许是因为人们很难用现代的金融工具来对它进行定量分析。但有些因素并不会因为“专家”无法很好地理解而失去它们的重要性。总的来说，相对于对国外的企业进行直接投资，我倾向于伯克希尔通过投资类似可口可乐和吉列那样的公司来参与全球经济\\n\\n凯恩斯指出的，在依靠劳动换取收入的原始经济中，当女裁缝把一件衣服以20美元的价格卖给鞋匠时，鞋匠就少了20美元可以消费，而女裁缝则多了20美元可以用。总消费支出并没有受到lollapalooza效应的影响。但如果政府印刷了另外一张20美元的钞票，用它来买一双鞋，鞋匠多得到了20美元，可是没有人觉得自己的钱变少了。当鞋匠下次再买一件衣服的时候，这个过程就重演了，不会无休止地持续放大，但会产生所谓的凯恩斯乘数效应，这是一种促进消费的lollapalooza效应。同样地，和同等规模的诚实交易相比，尚未败露的贪污得来的钱对消费的刺激效应更大\\n\\n印度哲学家克里希那穆提（J.Krishnamurti）曾经说，“不带评论的观察是人类智力的最高形式。”\\n\\n非暴力沟通的第二个要素是感受。心理学家罗洛·梅（Rollo May）认为：“成熟的人十分敏锐，就像听交响乐的不同乐章，不论是热情奔放，还是柔和舒缓，他都能体察到细微的起伏。”然而，根据罗洛·梅的叙述，大多数人的感受“像军号声那样单调”。\\n\\n如孟德斯鸠所言，“有商业的地方就有美德”。\\n\\n——随失败而来的是耻辱感：一种腐蚀性的意识产生了，那就是我们没能使世界信服我们自身的价值，并因而获到怨恨成功者且自惭形秽的境地。\\n\\n个成年人的生活可以说包含着两个关于爱的故事。第一个就是追求性爱的故事，这个故事已经广为人知，并且得到人们详尽的描述，构成了音乐和文学的根本主题，被社会普遍接受和赞颂。第二个就是追求来自世界之爱的故事，这一故事更为隐蔽、更加让人难为情。人们提到它的时候往往采用刻薄的、讽刺的语言，好像只有那些生性嫉妒和有心理缺陷的人才会产生这样的需求，或干脆把对身份的追求简单地解释为对财富的追求。但第二个关于爱的故事在强烈程度上一点不亚于第一个，在复杂性、重要性和普遍性上也是如此，而且一旦失败，所导致的痛苦不会比第一个少。在第二个故事中也有令人心碎的时候，这一点可以从那些被世界定义为小人物的人们空洞、绝望的眼神中得到证明。\\n\\n他人对我们的关注之所以如此重要，主要原因便在于人类对自身价值的判断有一种与生俱来的不确定性——我们对自己的认识在很大程度上取决于他人对我们的看法。我们的自我感觉和自我认同完全受制于周围的人对我们的评价。如果我们讲出的笑话让他们开怀，我们就对自己逗笑的能力充满自信；如果我们受到他人的赞扬，我们就会对自己的优点开始留意。反之，如果我们进了一间屋子，人们甚至不屑于瞥上我们一眼，或者当我们告诉他们我们的职业时，他们马上表现出不耐烦，我们很可能会对自己产生怀疑，觉得自己一无是处。\\n\\n几乎所有18世纪及其后的伟大经济学家和政治家都接受了他的观点。休谟在《论奢华》（1752）一文中重复了曼德维尔的观点，认为是富人对财富的追求和对奢侈品的消费，而不是穷人的劳作最终给社会创造了财富。他说：“在任何一个国家，如果人们无意追求奢华的生活，那么，人们会变得懒惰，失去对生活的乐趣，他们对社会也将毫无益处，而由他们组成的社会甚至不能维持这个国家的海、陆军队。”\\n\\n24年后，休谟的同胞亚当·斯密写成了《国富论》一书。斯密进一步发挥了曼德维尔的观点。他的理论也许是对富人的社会功用最具煽动性的辩护。在书的开始，斯密承认大笔的财富并非总能带来幸福：“有了财富，人们一样焦虑、恐惧和悲伤，有时甚至较先前为甚。”他还不无嘲讽地指出有些人终其一生只是追求那些“花哨却一无是处的玩意儿”。然而，笔锋一转，他随即对这些人表达了无尽的感激。人类的整个文明，社会的任何福利都离不了人们的欲望，离不开人们贪求过量财富的能力，同时也需要人们对其财富进行炫耀。正因为如此，“人类才会耕垦土地，才会去搭建房屋，才会建立城市和国家并创造各种科技和艺术产品来点缀和提升他们的生活。这样，整个世界才得以完全改观，荒莽的原始丛林被改造成肥沃宜人的平原，浩瀚无际的海洋也为人类所用，成为各种保障人类生存的新的资源宝藏”。\\n\\n旧的经济学理论认为，富人们占有了一个国家有限财富的过多份额因而应该受到谴责。斯密承认，人们很容易把一个拥有“万贯家财”的人视为“社会害虫、凶残的巨兽和吞噬所有小鱼的大鱼”。但是，他们错了，因为社会的财富并非是有限的，只要实业家和贸易商有雄心且有毅力，社会的整体财富总是能够无限地扩大\\n\\n“他人的头脑太过恶毒，不能作为我们自己真正幸福的栖身之所。”\\n\\n——叔本华《附录与补遗》（1851）\\n\\n“大自然从来没有命令我：‘要摆脱贫穷。’大自然也从来没有命令我：‘要尽力致富。’她只是请求我：‘你一定要自立。’”\\n\\n——尚福尔《格言录》（1795）\\n\\n“决定我幸福的不是我的社会地位，而是我的判断；这些判断是我能够随身携带的东西……只有这些东西才是我自己的，别人无法从我身边拿走。”\\n\\n——爱比克泰德《谈话录》（约公元100年）\\n\\n叔本华的话说：“只有击中目标的谴责才能使我们受到伤害。一个人如果真正知道他不应该受到某种谴责，那么他可以满怀把握地对此不屑一顾。\\n\\n自己身体里有上千个声音同时吵闹，相反，我们的体验是统一的。意识流轻松、自然地从这一刻涌向下一刻，并有着独立、统一而连贯的叙述。我们所体验到的心理统一，来自名为“解释器”的专门系统，它对我们的感觉、记忆、行动及其之间关系构建解释。这就带来了一种个人叙事，把我们意识体验的不同方面整合成有机的整体：从混乱中诞生出秩序。阐释模块似乎为人类所独有，而且专属左半脑。它驱动人提出假说，触发信念，而这些信念反过来又限制了我们的大脑。\\n\\n解释器欺骗了我们。它创造了自我的幻觉，有了它，我们人类就拥有了力量感，以为可以“自由”地决定自己的行为。从许多方面而言，这是人类具备的一种了不起的正面能力。智力越来越高，又有了洞穿表面之外联系的能力之后，人类过了多久便开始琢磨这一切的含义，琢磨生命的意义呢？解释器提供故事情节和叙事，我们相信自己是自由意志、做重要选择的行动载体。 这种错觉如此强大，以至于再多的分析也无法改变我们的感觉——自己是在有意识、有目的地采取行动。这里有一个最为浅显的事实：即便是个人心理层面上最为顽固的决定论者和宿命论者，也并不真心相信自己是大脑棋盘上的一颗走卒。\\n\\n传统哲学中，自由意志是这样一种信念：人类行为是个人选择的表达，不由物理因素、命运或神祗来决定。发号施令的，是你。你的自我，有中央指挥中心。你掌控全局，不受因果关系支配，自由地做事情。你有自由免受外人的控制，免遭胁迫、勉强、欺骗，你的行动只受内心的约束。然而，按我们在上一章所了解到的知识，现代观点认为，大脑促成了思维，你，就是你庞大的并行分布式大脑，没有中央指挥中心。机器里没有幽灵，没有一个能称为是“你”的秘密物体。你引以为傲的“你”，是你的阐释模块编造出来的故事。这个故事竭尽所能地整合了你的大部分行为，而对其余的行为加以否认，要不就是寻找理由\\n\\n过程无需大脑参与，纯属反射动作。你先动。疼痛感受器的信号同时也会发送到大脑。大脑处理信号，并将之阐释为“疼痛”之后，你才意识到痛。意识需要时间，意识到痛之后，你并没有做出“挪开手指”的有意识决定：抽开手指是一种条件反射，早就自动完成了。痛感意识信号是在受伤后自大脑产生的，该信号指向手指，但手指其实已经抽走了。你的解释器必须把所有观察到的事实（疼痛和挪开手指）整合成一个合理的故事，解答“为什么”的疑问。因为痛而抽开手指合乎情理，于是它就虚构了时间。简而言之，解释器编造了与情况相配的故事，让人觉得是自己出于自我意志采取了行动\\n\\n洛伦茨以为他会得到跟上一次运算相同的结果——毕竟，计算机代码是确定的。可等他端着咖啡回来时，结果却完全不同！毫无疑问，他很生气。起初，他以为这是硬件问题，但后来，他回想到之前输入数据时，输入的不是完整的“0.506 127”，而是四舍五入，只输入了“0.506”。由于庞加莱的混沌系统有半个多世纪都没见过天日，当时的人觉得这么小的差别无所谓。但对这个有着诸多变量的复杂系统，差异可大了！洛伦茨重新翻出了混沌理论。\\n\\n现在，我们把天气理解为一个混沌系统。由于变量太多，不可能一一准确测量，即便测量出来，初始测量上任何一点细微的不准确，也会让最终结果产生巨大的变化，所以，长期预报根本做不到。1972年，洛伦茨做了一番讲演，论述细微的不确定性最终会影响一切计算，让长期预报的准确性报废。这次讲座的标题是：“可预测性：巴西的一只蝴蝶拍了拍翅膀，会在得克萨斯引起一场龙卷风吗？”人们将之简称为蝴蝶效应，它吸引到了决定论者们的注意力，推动了他们的势头。\\n\\n混沌并不是说系统的行为是随机的，而是说，由于变量太多太复杂，无法进行测量，即便进行了测量，从理论上来说，测量也不可能完全精确，最微小的误差也会让最终结果产生极大的变化，故此，系统无法预测。\\n\\n薛定谔方程以确定性方式描述了波函数怎样随时间改变（且具可逆性），但却无法预测任一时间状态下电子在轨道上所处的位置：这是个概率。如果要实际测量电子的位置，那么测量这一行为，则将扭曲初始值（和不进行测量时相比）。\\n\\n某些成对的物理性质存在如下一种关系，使人无法同时测量两者：你越是（凭借测量）精确地知道性质之一，则另一性质就越是无法精确得知。就轨道中的电子而言，这组配对的性质是位置和动量。如果你要测量电子的位置，就会改变电子的动量，反之亦然。理论物理学家海森堡将之称为不确定性原理。\\n\\n你其实可以自己做一个小实验。该实验表明意识乃是事后体验。用手指摸鼻子，你的鼻子和手指上会同时产生感觉。然而，承载从鼻子到大脑处理区域的感觉神经只有大概7厘米长，而从手指到大脑处理区域的神经却有106厘米长，神经冲动的传输速度始终相同。两处感觉抵达大脑存在几百（250到500）毫秒的时间差，但你意识不到这一时间差。尽管大脑并未同时获得冲动，但它收集感官输入的信息，进行运算，做出决策：“对两个地方的抚摸是同时发生的。”这之后，你才获得了认知体验的感受。意识要花时间，可它出现在工作完成之后\\n\\n我并不认为，留心处理既得遗产有什么不应该。因为一个没有生计忧虑的人，一开始就有这样多的钱，正可以无忧无虑地过真正独立的生活，这是解除人生穷困忧愁的特权，他可以从人类宿命般的奴隶生活中求得解放。唯有获得了这种好运气的人，从降生之日起才是一个真正自由的人，因为他能够主宰时间，每天早上，他可以说“这一天是我的”。\\n\\n遗产若能遇到一位有高尚品性的主人，便可发挥其最大效用，因为他能从事不同于一般“为糊口而生”的工作，这样就能各得其利，就他个人来说，虽有独享安逸生活的闲适心情，但他能创造对社会有价值的东西，能以百倍的代价来偿还对其他人的亏欠。\\n\\n思考兴趣发生的原因可分为两类：一是纯粹客观性的，一是主观性的。后者是在有关自我的事件时引发了思考的兴趣，前者是对宇宙万物产生兴趣。这一类人之所以思考，就如同我们的呼吸一般，纯属生理的自然现象，当然，这类人并不多见，连一般的所谓学者，真正在思考的，也少得可怜。\\n\\n现实世界中，不管举出多少理由来证明我们过得怎么怎么幸福、怎么怎么愉快，但事实上，我们只是在重力的影响下活动而已，战胜了它，才有幸福可言。但在思想的世界中，只有精神，没有肉体，也没有重力的法则，更不会为穷困所苦。所以，有优美丰饶心灵的人，在灵思来临的一刹那得到启示，其乐趣绝非俗世所能比拟\\n\\n生存空虚，在以下几点中都能很明显地表现出来：\\n\\n第一，在生存的完整形式中，“时间”与“空间”本身是无限的，而个人所拥有的极其有限；\\n\\n第二，现实唯一的生存方式，只是所谓“刹那现在”的现象；\\n\\n第三，一切事物都是相关联、相依凭的，个体不能单独存在；\\n\\n第四，世上没有“常驻”的东西，一切都在不停地流转、变化；\\n\\n第五，人类的欲望是得陇望蜀，永远无法满足的；\\n\\n第六，人类的努力经常遭遇障碍，人为了克服它，必须与之战斗并予以剪除。\\n\\n常人的智慧为意志所支配，且严重地被束缚着，只能受容“动机”而活动，意志这东西，在世界的大舞台中，可把它比之为一大把穿在木偶上而使木偶活动的铁线，凡人就像木偶，他们一生之所以枯燥无味、严肃认真，就是为了这点。那种认真相，越上一层越认真。动物的面貌也如此，冷冰冰地煞有介事，从不露出笑容。然而，从意志解放出来的智慧之主——天才，却成了著名的米兰木偶戏的操纵者，舞台上能够感知一切的只有他一个人，希望暂时离开舞台到观众席上看戏的也只有他一人\\n\\n正如尼采所言，一个伟大的人格，可以在自己的身上克服这个时代。\\n\\n个体心理学是一种社会心理学，它把人们看作一个需要共同生活在这个星球的完整单元。无可争议，我们需要彼此。因为没有彼此，我们就不能生存。所以个体心理学的准则之一就是社交兴趣。我们的社交兴趣是我们对一个团体的积极归属。我们能察觉到自己的力量、资源和能力，能察觉到社会的需要，能用同情心把自己置于同伴的处境，从而做出贡献以改进每个人的生活。社交兴趣是“我和你”的叠加，而不是“我反对你”（阿德勒对于神经衰弱的定义）。\\n\\n朝着目标移动这一概念，是个体心理学的基础。勇气是我们用来向社会有效目标移动的燃料。勇气也是一种自信，可以用“我行”来描述，而不是“我不行”。勇气暗示着“去试试”的意愿，去做我们能做的，聚焦于做出贡献的意愿与为此付出的努力，而不是聚焦于一个完美、完整的产品或行动。阿德勒的辅导和治疗，可以看成一个人获得勇气的过程，是对自己和自己能力的信仰。辅导过程是从“只有当我确信自己能成功的时候我才会做”移动到“我会尽最大的努力”。勇往直前，相信自己，暗示自己在成功和失败中保持独立，也就是我们的自我价值不依赖于做得有多正确，而是在情况所需时——做我们能做的。失败对于阿德勒主义者而言不是“不成功”，而是“不尝试”。\\n\\n陆澄懊恼道：“人为什么要有七情啊，做个无情的人该多好，就不必因遇到不幸的事而哀伤忧愁了。”\\n\\n王阳明正色道：“话可不是这样说。那群朱熹门徒就是你这种心态，希望能把七情从我们的心灵中驱赶出去。可是，七情是人心与生俱来的，所以它的存在就是合理的。只是你应该用你的良知来清醒地认识它们，不要被它们控制。如果良知是太阳，那么七情就是浮云。太阳是移动的，不可能总停留在一处，无论何处，只要有一线光明，就全是阳光所在。天空即使布满乌云，可你还是能看得清，这就是良知的妙用。而这妙用无非是掌握一个度罢了。按你所说的，因为云能遮日，就要抹去天生的浮云了么？”\\n\\n他的弟子薛侃接口说：“是啊，闻誉而喜，闻毁忧郁，就是好名的毛病在发作。但是该怎么治疗这种病呢？”\\n\\n王阳明给出了方法：“名与实相对。务实的心重一分，求名的心就轻一分。若全是务实的心，就没有一丝求名的心。如果务实的心犹如饥而求食，渴而求饮，还哪里有时间和精力好名？”他接着说道，“过度追求‘名’就会把‘实’忽视，名和实不相符，活着的时候还可以弥补，如果死了那就真来不及了。”\\n\\n“所谓的心，并非专指那一团血肉。所谓的真正的心，是那能使你视、听、言、动的‘性’，有了这个它，才有了生生不息之理，也就是仁。性的生生之理，显现在眼时便能看，显现在耳时便能听，显现在口时便能说，显现在四肢便能动，这些都是天理在起作用。因为天理主宰着人的身体，所以又叫心。这心的本体，本来只是一个天理，原本无非礼存在。这就是你真实的自我。它是人的肉体的主宰。如果没有真我，也就没有肉体。你若真为了那个肉体的自我，必须依靠这个真我。做到戒慎于不视，恐惧于不闻，害怕对这个真我的本体有一丝损伤。稍有丝毫的非礼萌生，有如刀剜针刺，不堪忍受，必须扔了刀、拔掉针。如此方是有为己之心，方能克己。你现在正是认贼为子，反而说什么有为自己的心，但为何不能克己呢？”\\n\\n王阳明马上解释说：“天生万物和花园里有花又有草一样。哪里有善恶之别？你想赏花，花就是善的，草就是恶的。可如有一天，你要在门前搞个草坪，草又是善的，草里的花就肯定被你当成恶的了。这种‘善恶’都是由你的私意产生，所以就是错误的。”\\n\\n薛侃吃惊地问：“这不就是无善无恶了吗？”\\n\\n王阳明正色道：“天下任何事物本来就没有善恶，它所以有善恶全是你强加给它的。我问你，黄金是善还是恶？”\\n\\n王阳明就解释说：“人为什么会常常感到不幸福？表面看是因为我们的身体总受到束缚，精神也不能自主，我们受到了客观条件的种种限制。实际上，我们之所以受到客观条件的限制，是因为我们和外物产生了对立。我们所以和外物产生对立，是因为我们总是以自己的标准来衡量外物，于是，就有了是非好恶之情。当我们对外物有了是非好恶之情，就是给外物贴上是非善恶的标签。一旦你给它们贴上标签，它们就有了生命，反过来干扰你。也就是说，我们被客观条件所限制，全是我们自己搞出来的。”\\n\\n薛侃茫然。\\n\\n王阳明就举例子说：“比如你刚才对野草发出的感叹，你就是给它贴上了‘恶’的标签，对于‘恶’的东西，人人都会动气，一动气，心情就受到干扰，你心情不好，还\\n\\n谈什么幸福！不仅仅是被你评价为‘恶’的事物会对你产生干扰，就是被你评价为‘善’的事物也会对你产生干扰。比如被你评价为‘善’的黄金，表面上看是你喜欢它，你拥有它，实际上，当你喜欢上它时，它已经控制了你，时刻干扰你。它在你手里，你就过度兴奋，可当它遗失时，你必然过度地忧伤，你已经成了它的木偶和奴隶，你如果被这样一个‘善’的东西所左右，失去自主力，也是没有幸福可言的。”\\n\\n王阳明说：“我说不刻意为善去恶，并非说全无‘好恶’，如果全无好恶，没有是非之心，那连和尚都不如，你就会成为一个麻木不仁之人。所谓‘不刻意’，就是说‘好恶’全凭天理，再无他意，就是不要刻意和事物对立。你现在是为了保持花园，花园里有草，这就妨碍你了，它妨碍你，你就该把它拔除。如果没有拔除干净，你也不要放在心上。比如你今天拔了一天草，可还没有拔完，那你也不要晚上想着草，一想草，就会想到它是恶的，如此，你就和草对立起来，它主导了你的情绪。你不能控制情绪，自然会被情绪所控制。”\\n\\n“才力虽然不同，可他们的良知却同，所以都可以称为圣人。正如黄金的分量不同，但只要在成色上相同，就可以称为精金。把五千两重的一块金子放到一万两重的金子里，从成色上而言，没有不同。把伯夷、伊尹和尧、孔子放在一块，他们的纯是天理的心也没有任何不同。\\n\\n“精金所以为精金，在于成色足，而不在分量的轻重。这就如圣人之所以为圣人，在于良知光明，而不在‘才力’的大小。因此，平常人只要肯学，使自己的良知光明，同样可以成为圣人。正如一两重的精金，和万两重的精金对比，分量的确相差很多，但就成色足而言，则毫不逊色。‘人皆可以为尧舜’，根据的正是这一点。学者学圣人，只不过是去人欲而存天理罢了。\\n\\n“后世之人不理解圣人的根本在于纯是天理，只想在知识才能上力求做圣人，认为圣人无所不知、无所不会，我只须把圣人的许多知识才能一一学会就可以了。因此，他们不从天理上下功夫，白白耗费精力地从书本上钻研，从名物上考究，从形迹上摹仿。这样，知识越渊博而人欲越滋长，才能越高而天理越被遮蔽。正如同看见别人有万镒之精金，不肯在成色上锻炼自己的金子只妄想在分量上赶超别人，把锡、铅、铜、铁都夹杂进去，如此分量是增加了，但成色却愈低下，炼到最后，不再有金子了\\n\\n后世儒者只在分量上比较，所以陷入功利的泥潭之中。如果剔除比较分量的心，各人尽己之力与精神，只在此心纯是天理上下功夫，就能人人知足，个个功成，如此就能大的成就大的，小的成就小的，不必外求，无不具足。这就是实实在在的明善诚身的事。后儒不理解圣学，不懂得从自心的良知良能上体认扩充，却还要去了解自己不知道的，掌握自己不会做的，一味好高骛远。不知自己的心地宛如桀、纣，动不动就要做尧、舜的功业，如此怎么行得通？终年劳碌奔波，直至老死，也不知到底成就了什么，真可悲啊！”\\n\\n我们还可以用一个浅显的例子来说明朱熹和王阳明对“格物致知”大相径庭的解释。比如孝顺父母，朱熹认为，孝顺父母是个复杂的活，你必须要探究学习孝顺父母的各种知识，最后得出一套理论，然后再开始孝顺父母。而王阳明则认为，只要在孝顺父母这件事上端正好态度（正念头），良知就会指引你去如何孝顺父母，这些孝顺父母的行为是不必向外学习的。\\n\\n朋友指着岩石间一朵花对王阳明说：“你经常说，心外无理，心外无物。天下一切物都在你心中，受你心的控制。你看这朵花，在山间自开自落，你的心能控制它吗？难道你的心让它开，它才开的；你的心让它落，它才落的？”\\n\\n王阳明的回答很有味道：“你未看此花时，此花与汝心同归于寂；你来看此花时，则此花颜色一时明白起来。便知此花不在你的心外。”\\n\\n这就是王阳明心学中诠释“心外无物”最漂亮的乐章。它的意思是这样的：你的眼睛受心的控制，你未看那朵花时，你的心在花身上就没有动，于是你的心和它一样，都处于沉寂状态，由此可知，花不在你的心外，因为它和你心的节奏是一起的，这其实就是万物一体，只不过万物和我都在沉寂状态；当你来看它时，你的心在它身上，你的心动了，而花也映入你的眼，所以它的颜色和你的心一样，都鲜艳、动了起来，这还是万物一体，也就是动的状态的万物一体。花和你的心处于同等频率和状态中，请问，花在你心外还是在你心内？\\n\\n其实，这正是王阳明“知行合一”的注脚，你的心一动（知），其实就已“行”了。不要认为一个小私心无伤大雅，时间一久，肯定会出大问题。\\n\\n不过，美国的上层与所有贵族共享一种一望而知的特性：他们对形形色色的思想无动于衷，毫无兴趣（这也是“看不见的顶层”的标志，科尼里斯·文德比尔特·惠特尼拙劣的文字表演就曾证实过这一点）。马修·阿诺德之所以把他们称作“野蛮人”，正是由于他们对思想的漫不经心。他明确地将他们的安详归咎为“从来不让任何人的思想烦扰自己”。尽管如此，他们仍不失为一个不错的阶级。只要对任何才情焕发、独树一帜的说法闭目塞听，他们的生活就是舒适、优裕甚至妙趣横生的。\\n\\n中上层阶级喜欢通过给自己的宠猫起名为斯宾诺莎、克吕泰墨斯特拉或者赣第德，以此显示自己昂贵的教育水准。正如你已经领悟到的，这种做法同时意味着，他们几乎完全吻合1980年那本无人不知的《权威预科生手册》（Official Preppy Handbook，莉莎·伯恩巴赫与他人合著）中所描绘的阶级特征\\n\\n绝大部分中产阶级或以下的美国人宁愿成为中上层阶级，也不愿成为上层阶级或“看不见的顶层”。一次最近的路易斯·哈里斯民意测验显示，当要求回答“你愿意成为哪一阶层的成员”时，大部分人选择中层。当问题是“你愿意属于中层的哪一部分”时，大部分人的回答是“中上层”。成为中上层阶级是人们谙熟而可靠的梦想：这个阶级的习俗较之中产阶级稍显尊贵，易于辨别，便于习得。而如果做一名上层阶级成员，你可能会因不知如何食用鱼子酱和某道法国菜，或如何使用一只洗手指的碗而脸红心跳。很少有美国人私下里不愿意成为中上层阶级的。\\n\\n据《白领》（White Collar，1951年）和《权力精英》（The Power Elite，1956年）的作者C·赖特·米尔斯称，“地位恐慌”是最具中产阶级色彩的焦虑形式，所以他们才需要申请越来越多的信用卡，订阅《纽约客》杂志，因为他们估计这表现了中上层阶级的品味。中产阶级对这份杂志，或其中的广告的热爱，恰好印证了米尔斯对他们的描绘：“不从更高的社会环境借来地位，他们就会不得其所。”《纽约客》杂志的广告商看来早就对这批读者的心态了如指掌，他们在这些中产阶级面前的“准中上层阶级”姿态有时简直逗噱。比如，他们最近用一张打印的请帖大肆抨击使用昂贵信笺的恶习。跃入眼帘的第二个单词格外醒目，造作地遵循英式拼法将美国通用的honor拼成honour\\n\\n吸烟和毒瘾，有几个条件必须满足：\\n\\n1.想要改变的行为的好处和功能都被打乱，要改变的行为不再给人带来满足感和其他可怕的副作用（疏远家庭、收入减少、增加痛苦、健康风险）。\\n\\n2.减少或消除对之前行为的选择。因为可能的后果，按以前的行为行事不再是一种选择。\\n\\n3.产生旧行为的根本原因（孤独、紧张、厌倦、恐惧）已通过其他方式解决。\\n\\n4.改变被赋予了一些意义，允许人们去寻找更大的生活目标。\\n\\n第一，本书对未来不确定性的基本观点并非基于Markowitz (1952)所倡导的均值／方差法选择投资组合的观点，其中均值／方差法是当初Sharpe(1964)、Lintner(1965)、Mossin(1966)以及Treynor (1999)发展出资本资产定价模型（Capital Asset Pricing Model，简称CAPM）的基础。相反，我们直接以Arrow (1953)的延伸研究——基于Arrow (1951)和Debreu (1951)——为基础，使用状态／偏好法分析不确定性。\\n\\n第二，我们大量使用了一个模拟资本市场达到均衡过程的程序，并对由此而得到的资产价格和未来价值的关系进行了大量的研究。\\n\\n道行之而成，物谓之而然。\\n\\n（《齐物论》）\\n\\n注释：\\n\\n道：道路。\\n\\n物：这里指被语言指称的事物。\\n\\n今译：\\n\\n道路是人们行走之后而形成的，事物是被人们称谓而确定如此名字的\\n\\n君之所读者，古人之糟魄也夫！\\n\\n（《天道》）\\n\\n今译：\\n\\n国君您读的，是古人的糟粕啊！\\n\\n庄子对言语表达精微意义的能力，一直持怀疑态度。他在《秋水》篇中，已经明确表示了言语之类对“道”是无能为力的，只有对现实世界中属于“物之粗”的部分，言语才有效，“物之精”那一部分，主要靠意想（“可以意致”）。\\n\\n知道者必达于理，达于理者必明于权，明于权者不以物害己。\\n\\n（《秋水》）\\n\\n注释：\\n\\n权：权变，顺应时势而变化。\\n\\n今译：\\n\\n领悟大道的人必定通达事理，通达事理的人必定明晓权变，明晓权变的人，就不会让外物伤害到自己。\\n\\n庄子在濮水上垂钓。楚王命两名大夫去传达自己的旨意说：“请让我把楚国的政事托付给您！”庄子拿着鱼竿，看也不看他们，说：“我听说楚国有一只神龟，已经死掉三千年了。楚王把它用布巾包着，用竹盒盛着，珍藏在庙堂之上。这只乌龟，到底是希望自己死掉，只留下一副骨甲来被人珍藏呢，还是宁愿活着，拖着尾巴在泥水里爬来爬去呢？”两名大夫说：“当然是宁愿活着，拖着尾巴在泥水里爬来爬去。”庄子说：“你们回去吧！我也是想要拖着尾巴在泥水里爬来爬去啊。”\\n\\n这个故事或许是庄子生平事迹中最有名的一个了，司马迁著《史记》为他列传，就记叙了类似的情节。\\n\\n庄子究竟为什么要拒绝出仕呢？\\n\\n庄子将受到万千尊崇的死龟和曳尾涂中的生龟对举，让楚国的两位大夫做选择题，透露出他的根本考虑是以生命为本位。曳尾涂中，看似卑下，但这其实是龟的自然生活常态；庙堂中的“神龟”虽然受到尊崇，但那绝不是龟命定的结果，而是人类意志的赋予，并且以丧失生命为前提。不能保持生命的本然，不能保全自己的生命，这两点都是违反庄子基本信念的，可谓庄子拒绝出仕的理据。\\n\\n惠施的问题是：你不是鱼，怎么会/怎么能（“安”）知道鱼的快乐呢？庄子则悄悄将“安”字的意义转换成“哪里”即“从什么地方”了，所以他回答说：“我知之濠上也。”庄子的论辩不能说逻辑周洽，更多显示的是机智。\\n\\n然而，庄子便不对吗？\\n\\n世间不仅是现实，世间不仅有逻辑。庄子展示的是一个通达天地自然，与万物沟通无碍的心灵。鱼游水中，我游梁上，同样的自在率意，鱼我双方是融通的。鱼乐，实是我乐的映射；我乐，故而鱼亦当乐。杜甫有两句诗：“感时花溅泪，恨别鸟惊心。”（《春望》）或许可以移来为证，只是一哀一乐而已。\\n\\n庄子坚持自己的观感，反对的正是惠子的细琐分辨。\\n\\n以利合者，迫穷祸患害相弃也；以天属者，迫穷祸患害相收也。\\n\\n中原之国的君子大人，虽然精通礼义，却不能够了解人的心性。\\n\\n这是庄子批评孔子的话。礼义是儒家所提倡的，不过，在庄子看来，这并不是人性的必然组成部分，而只是构成社会的过程中建立起来的外在规定。儒家的错误即在将外在的礼义认作根本，轻略了人心、人性的根本。\\n\\n哀莫大于心死，而人死亦次之。\\n\\n（《田子方》）\\n\\n今译：\\n\\n最悲哀的莫过于心死，连人死都不如心死悲哀。\\n\\n天地有大美而不言，四时有明法而不议，万物有成理而不说。\\n\\n（《知北游》）\\n\\n今译：\\n\\n天覆地载，有最伟大的美德，却沉默不语；春秋四时更替，有最明确的法则，却不会议论；万物生成，都有其背后的原理，却不曾言说。\\n\\n以贤临人，未有得人者也；以贤下人，未有不得人者也。\\n\\n（《徐无鬼》）\\n\\n注释：\\n\\n贤：贤良的名声。\\n\\n今译：\\n\\n凭借着自己贤良的名声而自居于他人之上，没有能得到人心的；拥有自己贤良的名声，却自居于他人之下，没有不能得到人心的。\\n\\n设下捕鱼的荃，目的是为了鱼，捉到了鱼，就可以把荃忘掉了；设下捕兔的蹄，目的是为了兔，捉到了兔，就可以把蹄忘掉了；说话的目的是所要表达的意义，明白了意义，所说的话就可以忘掉了。\\n\\n虽然言语的传达不能充分、真正传达出真意，但这是一个无可奈何的途径，只要不执着于这个手段，心中清楚，追求的根本目标是“意”而不是“言”，即“言者所以在意”，因“言”而窥“道”，就是了。而在“得意”之后，即可放开语言（“忘言”），不要死于句下，不要纠缠于手段。试想：你过河走的是桥，已然渡过，但还在桥头徘徊，你算真正过河了吗？\\n\\n人上寿百岁，中寿八十，下寿六十，除病瘦死丧忧患，其中开口而笑者，一月之中不过四五日而已矣。\\n\\n真者，精诚之至也。不精不诚，不能动人。故强哭者，虽悲不哀，强怒者，虽严不威，强亲者，虽笑不和。真悲无声而哀，真怒未发而威，真亲未笑而和。\\n\\n（《渔父》）\\n\\n今译：\\n\\n所谓真，就是精心诚意的极致。不精心诚意，就不能打动人。所以，勉强哭泣的人，虽然悲伤却并不哀痛；勉强发怒的人，虽然严厉却没有威势；勉强表示亲切的人，虽然笑脸相迎却并不和气。真正的悲痛，就算没有哭声，也是哀伤的；真正的发怒，还没有发作就已经威势逼人；真正的亲切，还没有笑就已经让人感到和气了。\\n\\n不重外在形式，关键在内心的实质。《庄子》后文中还提到：饮酒以快乐为关键，居丧以哀伤为关键。（“饮酒以乐为主，处丧以哀为主。”）故而饮酒意在追求快乐，所以不必讲究酒具如何；居丧以悲哀为要，所以不必讲究礼法。（“饮酒以乐，不选其具矣；处丧以哀，无问其礼矣\\n\\n由此我们可以看到现代工业社会瓦解的两个症候。\\n\\n第一，不快乐的人增多了。没有了直接真实的社会责任，自己的问题自己解决，每个人都沉浸于自己，成为孤独的现代文明的俘虏。很早以前巴特勒主教（Bishop Butler）就曾说过：“……一个人尽可以自爱自私，但这样做只能令他变得可悲。”\\n\\n第二，现代工业社会的组织水平还远低于我前面所讲到的简单社群。这种不幸的特点即是，当各式各样的组织形成之后，组织间并未形成真诚的合作，相反，它们经常互相戒备，彼此敌视。这就致使整个社会陷入停滞状态——组织间相互碾压，集团间混乱竞争。这就如斯坦利·卡森所说，是灾难临近的先兆。\\n\\n任何社会团体，不论是处于哪种文化阶层，都必须面临两种管理问题。它必须使其成员：\\n\\n·在物质和经济上的需求得到满足\\n\\n·保证整个组织的自发协作\\n\\n第四个故事，是我的博士论文。我在论文中回顾了国家从1776到1976这200年间所谓的“成功”文学。我发现，在前150年里，文学的重点集中在我称之为“道德伦理”上，即关于永恒、普遍和不言而喻的原则和自然法则。在后50年里，重点开始转向“人格伦理”，即主要基于形象和个性影响力的技巧。在过去的35年里，自从开始我的博士论文以来，与人格伦理的分离更为明显。即使有大量的表面文章涌出，但是我们商业的重点主要是外表、政治、财务收益，以及新技术及其应用。\\n\\n远在当代信息革命到来之前，哈耶克（Hayek 1945）沿袭冯·米塞斯（von Mises 1981）的看法指出，现代经济的技术复杂性不断增长，需要更高程度的分散经济决策。他指出，一个经济体所使用的绝大多数信息都是局部性的，与当地的特殊情况息息相关，也只有当地人才熟知。哈耶克认为，这就是社会主义的中央计划在技术复杂的情况下无法运行的原因：任何计划者都不可能会吸收并处理在现代经济中产生的全部本地知识。\\n\\n相反，在这一章中，我要提倡酒神式生命情调的正当性，它体现在“能量”、“热情”、“魅力”甚至“迷狂”这些动态而非静态的隐喻、观念中。这也是荷马（Homer）、拜伦（Byron）和艾伦·金斯堡（Allen Ginsberg）所倡议的充满爱欲的生活概念，这种生活或许有时因绝望和忧世而难以承受，但也会因快乐和勃勃生机而振奋。绝大多数哲学家更加熟悉的维特根斯坦就是一个不寻常的例子，众所周知，他的生活很神经质，却也令人羡慕（不只是因为他的天才）。确实，拒绝在幸福与苦楚（还有“善”与“恶”）之间做最终的区分，可能是我想要辩护的观点之一，但这个不同寻常的论点不在这里要说的范围之内。（不过\\n\\n在第五章对此有一次拙劣的尝试。）我较为温和的观点是：一种有德性的生活可能不只是许多哲学家和当代道德权威敦促我们的那样，成为好相处的邻居、受人尊敬的公民、有责任心的同事，以及有爱却毫无生气的人。\\n\\n显然，人们谈及的许多（哪怕不是绝大多数）德性都与情感有所关联，不过常常是以负面的方式关联在一起。比如，勇气与克服恐惧有很大关系，亚里士多德在《尼各马可伦理学》（Nicomachean Ethics）中对此有详细阐述。菲利帕·福特（Philippa Foot）曾有一个著名的说法：德性是情感的“矫正”，是把较为粗俗、自利的情感约束起来。绝大多数传统的恶习（贪婪、纵欲、自傲、愤怒，或许还有嫉妒，不过值得注意的是，懒惰不在其中）都可以轻而易举地被定义为情感过度。可是，上述任何情感的缺乏则常常被当作德性（禁欲、贞洁、谦虚等等）。因此，尼采（在多处）警告我们说，莫把“缺乏情感的人”等同于好\\n\\n就是我在此要探寻的主张：激情本身就可以是德性。（当然，它们也可以是恶习。）在《伦理学》（Ethics）第二卷中，亚里士多德问到，德性应被当作激情，还是更确切地说，被当作“人格状态”；他坚决反对前者而主张后者。我并不否认，一般而言，德性是人格状态（或者就此来说，激情可以是人格状态），但在我看来，激情（比如爱）也可以是德性。\\n\\n在尼采更早的著作《快乐的科学》（The Gay Science）中，题目本身就显示了对激情生活的辩护：la gaya scienzia（快乐的科学），游吟诗人的生活，充满柔情和爱的生活。因此，尼采的“非道德主义”常常被认为与唯美主义关系密切，即认为伦理学和伦理判断可还原或转化为审美和审美判断。这不是我在本书中要持的立场，虽然我认为这话说得挺有道理。相反，我想追究尼采对激情的强调，尤其是他那激人奋进又令人毛骨悚然的“权力意志”观念，它所强调的不是美学而是某种别的东西：“能量”、“热情”、“力量”以及“自制”，不过这种自制不是说要克服激情，恰恰相反，是要培育激情。很显然，这不只是在反对康德的实践理性、功利主义的计算以及享乐主义，还要反对伦理学中由道德情操理论家和（较为贴近尼采所想的）叔本华所主张的较为温和的情感概念。不过\\n\\n灭绝激情和欲望，仅仅为了预防它们的愚蠢以及这种愚蠢的不快后果，这在我们今天看来，本身就只是一种极端的愚蠢。\\n\\n——尼采，《偶像的黄昏》\\n\\n哲学家常常把“理性”与“激情”对举，典型的做法是捍卫前者而反对后者，并主张哲学本身即理性之爱。\\n\\n我们无法再生活于如此匆忙且艰难的世界。然而，我们必须有所行动。因此，我们要试着改变世界，像变魔术一样。\\n\\n——让—保罗·萨特，《情感》（The Emotions）\\n\\n是如今依然强势的将情感“原始化”的倾向——我认为，这种做法等于否认我们对情感负有责任。我承认，说我们“选择”自己的情感有点过激。但是，若把它看作一种自我实现的预言，一种控制情感选择并日益意识到我们确实拥有情感选择的方式，那么在我看来，就是一种非常宝贵的存在主义智慧\\n\\n我喜欢熊彼特《资本主义、社会主义与民主主义》中的这段话：\\n\\n作任何社会预测，有价值的不是由总结事实与论据所导出的是与否，而是那些事实与论据本身。它们包含着在最后结论中合乎科学的一切东西。此外的一切不是科学而是预言。（1942，19）\\n\\n人生有两种悲剧：一种是没有得到你心里想要的东西；另一种是得到了。（萧伯纳，1902：220）\\n\\n快乐必须足够短暂，以使我们可以将注意力集中到下一项任务上。大脑通过两种方式将快乐加以限制……进化使得人不可能有永久的快乐——太多的快乐只会使我们无法专注于基本的生存。（科布，2003）\\n\\n我将之改造成：舒适，牛逼，刺激。\\n\\n他的第一项“个人舒适”，我将之称为“舒适”。就是排除了生理上的痛苦，没有饥饿、寒冷、性欲的长期饥渴，等等。温饱曾经是不容易得到的东西，所以舒适的满足曾经是人类最大的问题。\\n\\n他的第二项“社会舒适”，这四个字不具备望文生义的清晰性，不是成功的关键词，其意思相当于黑格尔所说的“为被承认而奋斗”。但是“被承认”也还是有欠明晰处，\\n\\n这一社会的精神特质宣告：假如你心情低落，那就吃。（鲍曼，1998：79）\\n\\n这不是玩笑，而是这个世界，特别是今天中国社会中，相当多数的人排遣无聊的手段——扩大物质资源的消费。我们竟然没有看到，这里发生了一个悖论：因为温饱的解决，发生了空虚和无聊的问题；我们却在解决温饱上面加大砝码，来应对空虚和无聊的问题。\\n\\n所以在必需品中，我的解释，不但包括那些大自然使其成为最低阶级人民所必需的物品，而且包括那些有关面子的习俗，使其成为最低阶级人民所必需的物品。（斯密，1880下：431）\\n\\n发达国家中的贫困标准早已不是生理上的最低必需品的满足，而是“最低社会面子”。……不管如何任意、专横和无聊，一种生活风格是成为某个社会成员的必须条件。而如果成员身份是生存的必需，就有理由将洗去贫困污名的身份消费视为生物意义上的必需品。……在一个社会平均生活标准提升，贫穷标准也提升的发展的经济中，可以清楚地看到一代代人的成员身份成本的提高。（Scitovsky，1976：116）\\n\\n假如我们承认需求从来都不是对某一物品的需求而是差异的“需求”（对社会意义的欲望），那么我们就会理解永远都不会有圆满的满足，因而也不会有需求的确定性。（波德里亚，1970：68）\\n\\n右翼思想家乔治·吉尔德则认为，供应不是需求拉动的，而是企业家的天才创造\\n\\n生产和消费应该是生活的手段，而不是生活的目的。但是当前的消费早已导致了本末倒置，手段压倒了目标。一方面，商人们的目标就是赚钱。对他们来说，可赚钱无意义的事情做，不赚钱有意义的事情不做。另一方面，他们竭力影响民众用“消费”取代“生活”。最终，很多实际上不需要的东西充塞进生活，很多需要的东西无人供应、逐渐消失，乃至人们放弃或遗忘了最初的渴望。\\n\\n2000年美国专栏作家布鲁克斯写出了《布波族：一个社会新阶层的崛起》，颇有影响。他实际上讲述的不仅是一个新阶层，更是一个新时尚的兴起。他说：\\n\\n20世纪90年代的高学历精英分子的最大成就，就是创造一种生活方式，既可以让人享受富裕的成功，同时又不失叛逆的自由灵魂。……这些受反精英教育的精英阶级，他们生活富裕却反对物质主义。……（他们）创造出了第三种文化。高学历精英们并未主动去创造这样的融合，它是数以百万计想两全其美的个人共同努力的结果。但是，现在它已经成为主导我们这个时代的基调了。文化和反文化已经两相融合，要指出谁吸收了谁根本是不可能的。因为波希米亚人和布尔乔亚阶级已经彼此吸收了。他们从这个过程中，产生了所谓布尔乔亚式的波希米亚人，也就是我们所称的布波族（Bobos）。（布鲁克斯，2000：\\n\\n布波族的价值观是兼有不菲的收入和反物质主义的态度，二者加在一起才构成他们心目中一个人的地位，只具备一样则无地位可言。“如果你是庸俗物质主义的信徒，你将被他们排挤。”（同上书：43）他们有钱，却故意在“打扮上总要比周围人差上一级”。（同上书：47）在他们眼中，“一个年收入百万美元的小说家，比一年赚五千万美元的银行家更受到尊重”。（同上书：45）因为最好是“顺手发财”，不应该牺牲个人的趣味和创意。\\n\\n第一，现代人的自由时间几乎肯定比原始时代少。第二，现代生产力的发展并没有同比例地减少人们的工作时间。原因耐人寻味\\n\\n消费不是自由时间的朋友，而是休闲的对立面。……有说服力的论证说，工商界与政府并不喜欢让人们拥有真正的休闲，只希望我们把休闲时间用于购买物品和他人的服务，或为政府纳税。（古德尔等，1988：100，171）\\n\\n正是在大萧条时代，美国人养成了运动的习惯，养成了读书的习惯，养成了家庭园艺的习惯。大萧条使美国人的休闲方式大大改变。大萧条造成了文化上的进步。而说到根本，那是失业造成的，是人们被迫获得的更多的闲暇造成的。\\n\\n温饱解决后贵族们陷落到空虚无聊之中。摆脱空虚无聊有两个出口：一个是堕落，一个是升华。\\n\\n古典教育与现代教育的最大差别是后者是教人们如何工作的，而前者是教人们如何生活的。\\n\\n人文教育最主要是训练消费技巧，为享受各种刺激做准备。\\n\\n正如古林蔡斯（Gourinchas）和雷（Rey）论述的，美国已从世界的商业银行家变成投资银行家了。美国从亚洲盈余国借入廉价资金，拿到亚洲和其他地方重新投资，在举债经营的基础上获取较高的收益。1952年时，美国是净债权国，债权占GDP10%，到2004年，却成为净债务国，债务占GDP22%。美国之所以能承受这么多的债务，因为它是储备货币国和“世界银行家”，拥有“大得离谱的特权”。此外，它与不能印发货币去偿还外债的东亚不一样，美国大部分外债都是用美元标价的，所以通过美元贬值，就可把调整的负担转嫁给这些债权的持有国。\\n\\n但是这种情况是不可持续的。由于消费水平进一步提高，国民储蓄率下降，所以美国变得越来越靠举债度日。在2007年，美国非金融部门债务增加到占GDP226%，而10年前占183%。金融部门债务差不多翻了一番，从占GDP64%上升到114%。储蓄率的下降反映在经常项目出现大笔赤字上，2006年经常账户赤字已增加到8570亿美元（占GDP6.5%）。\\n\\n形势已经转变，现在应当在世界经济中心作出调整了。不过，由于亚洲是一个大债权人，它还是逃不脱背负美国政府那些负担的大部分。\\n\\n禅宗的“禅”字本来是从梵文“禅那”音译而来，意为“静虑”“思维修”，它指一种集中精神并不断提高层次的冥想。“禅”是佛教很重要而且很基本的修行方法。禅宗的中心思想是：“不立文字，教外别传；直指人心，见性成佛。”意思就是说传教不需要文字，它是判教以外的传法法门，做法就是直接把人心中那个佛性点出，一点顿悟，就能成佛。禅宗之所以流行，是因为他们认为只要透过自身实践，从日常生活中直接掌握真理，最后就可以达到真正认识自我的成佛境界，这对当时大多数不识字的中国人民而言可是非常便利的成佛方法。\\n\\n讨论“体知”与“实行”之间关系的有《尚书》、孔丘、孟轲、朱熹，但这些人都是把体知和实行分开看。到了王阳明，他提出的是更先进的看法，他认为知行本就是一体，学行也是一体，这就是著名的“知行合一”理论。王阳明也讲“格物致知”，但他把“致知”又往上提了一层，叫“致良知”，不是只知道道理，还要因为知道道理而发扬自己的善良本性。\\n\\n船山和朱熹一样，彻头彻尾地检视了儒学传统和中华文化，希望从中总结出一个对国家人民有利的学问，因此他的思想是极具包容性的。从中国思想史的角度来看，他的重大贡献是他提出心性既是受天而生，人都可以为善的看法——为善的重点在于养成良好的习性。而且这个良好习性必须时时日日去注意，因为“天日命于人，而人日受于天”（每日上天所交付给人的天性都不同，人每日都自上天得到不同的天性）。船山指出了人性养成的动态性，这在中国人性论的演变过程中是很大的突破。有人认为在这一波宋明新儒学思潮里，王船山的地位可以和著作等身的朱熹相比拟\\n\\n如果历史自发地走向民主，那么就不会发生农业和工业革命，这两类革命都以承受巨大危机的痛苦为代价跳跃到了未来，这使得绝大多数人想回到过去。在人类危机的时刻，把危机当作未来实验室也许能够使人心里得到一丝安慰。\\n\\n所有形态的社会都是借助于剩余产品和妥协（关于分配的）两种产生过程而向前发展的。然而，在资本时代这两种过程之间的结合达到了新的高度。新兴商品化的浪潮使得金融业日益兴旺发达。这与一种更加微妙而强有力的妥协是息息相关的。这里存在一种有趣的悖论：妥协度越高，金融化程度也就越高，我们的生活就越容易遭受经济危机。因此，按照以上逻辑可以观察到一个有趣的现象：在现代社会里越容易达成妥协，严重的危机就越频繁。\\n\\n黑格尔的历史哲学，因他并不能像中国人般有极长极详的历史材料，可让他凭仗来形成他精美的哲学。所以他并不根据历史来讲哲学，而是根据哲学来讲历史。他说整个人类的历史，就是一部“精神逐步战胜物质”的历史\\n\\n旅行更深的意义是什么？是加缪说的吧，旅行中最有价值的部分是恐惧。旅行者远离了家乡，一种模糊的恐惧随之而来，他本能地渴望旧环境。正是在恐惧中，你变得敏感，外界的轻微变动都令你颤抖不已，你的内心再度充满疑问，要探询自身存在的意义。人类的所有知识、情感、精神世界，不都因这追问而起？\\n\\n**奈特将风险与不确定性加以区分。他认为，风险是一种人们可知其概率分布的不确定，具有预测特点；而不确定性则意味着人类的无知，因为不确定性表示着人们根本无法预知没有发生过的将来事件，它是全新的、惟一的、过去从来没有出现过的。【3】**\\n\\n**在奈特看来，利润来自真正的“不确定性”，而不是“风险”。我认为，在这个问题上奈特搞反了。**\\n\\nLife is what happens to you while\\n\\nyou are busy making other plans\\n\\nJonn Lennon\\n\\nThe Meaning Of Life according to: Platonism - “learn more.” Aristotelianism - “be good.” Cynicism - “be self-sufficient.” Hedonism - “have pleasure now.” Epicureanism - “free yourself from pain now.” Stoicism - “be logical, don't suffer.” Classical Liberalism - “defend individual liberty.” Kantianism - “do as you’d have others do (your duty).” Nihilism - “do anything (life has no meaning).” Pragmatism - “bring the most good to humans.” Theism - “follow God’s will.” Existentialism - “make decisions and be positive.” Absurdism - “stop making sense of life. Just live.” Humanism - “Act in self-interest and common good.” Logical Positivism - “life has no meaning until you give it one.” Natural Pantheism - “care for nature.” Mohism - “love people impartially.” Confucianism - “Live an ordinary life.” Legalism - “learn practical things.” Utilitarianism - “greatest good for greatest amount of people.” Cyrenaicism - “live in the present and seek maximum pleasure in the NOW.”\\n\\n**公共选择理论创始人布坎南指出科斯的方法不可行（交易费用太高）。他主张求诸于公共政策解决“公地悲剧”，可以狭义地理解为“宪政”。布坎南是独立于自由放任派、政府干预派的公共选择派。**\\n\\n**布坎南的观点容易理解，言论自由、自由市场在法律的框架下进行。哈耶克曾引用了康德的话定义自由：“如果一个人不需要服从任何人，只服从法律，那么他就是自由的。”**\\n\\n**自上个世纪70年代之后，西方经济普遍取得共识：布坎南等人主张的宪政、制度及公共政策，成为庇古资源最优配置的保障，即用法律维持边际私人纯产值与边际社会纯产值相等。**\\n\\n**到了15世纪中期开始，印刷机打破了精英阶层的信息垄断权，欧洲书籍的出版不再局限于大学和修道院。**\\n\\n**1460年，约翰·福斯特完成了《圣经》的印刷。威廉·卡克斯顿在1476年将第一台印刷机引入英格兰。1490年，每座欧洲大城市至少在使用一台印刷机。**\\n\\n**廉价印刷品为普通大众提供了识字机会，民众在学习中逐渐摆脱愚昧。近代印刷技术成为宗教改革、文艺复兴及科技革命的推动力。正如哈耶克所言：“观念的转变和人类意志的力量，塑造了今天的世界。”**\\n\\n**世界上的罪恶差不多总是由愚昧无知造成的，没有见识的善良愿望会同罪恶带来同样多的损害。**\\n\\n**——《鼠疫》加缪**\\n\\n当心中的厌恶与嫉妒汹涌翻腾时，格洛丽亚马上坚决压抑这些“不良”的情绪。“亲爱的，我很抱歉。你是对的——我的怒气和强烈的占有欲是不对的。我知道你是迫不得已。”\\n\\n“资本家们自然要行动起来。它向下层同胞呼吁，开始高呼祖国，把自己的私事冒充全民的事情”——斯大林《马克思主义和民族问题》\\n\\n**成年人的生活，都是要不停地被锤的，但是如果有目标和意义和爱的人，被锤的过程就是有价值的；如果没有，就只是被锤而已。。。**\\n\\n“人就是人。他不仅是自己认为的那样，而且也是他愿意成为的那样。”语自萨特。\\n\\n就算某一天你凌晨三四点才睡，你也要起床进餐，充分醒过来，再回去睡。\\n\\n失去的睡眠是补不回来的，晚点起只会破坏原有的生物钟，让已经建立的一切秩序陷入紊乱\\n\\n绝望的人没有故乡。——加缪\\n\\n我习于冷，志于成冰。——木心\\n\\n人生如逆旅，我亦是行人。——苏轼\\n\\n**明天醒来我会在哪只鞋子里。———海子**\\n\\n4、如入火聚,得清凉门。—《华严悲智揭》\\n\\n当把自己名誉的全毁得失置之度外的时候,就如同在烈火之中找到了通往清凉的门径。第一眼看见\\n\\n这个句子就立马发了朋友圈,每次看到都有一种神清气爽的感觉\\n\\n20、我本想这个冬日就死去的,可最近拿到一套鼠灰色细条纹的麻质和服,是适合夏天穿的和服,\\n\\n所以我还是先活到夏天吧。\\n\\n太宰治\\n\\n我对太宰治只有同理心没有爱心。虽然被他的直白露骨的丧搞得很不舒服,却又不得不承认这实在\\n\\n是个敏感又别扭的、让人忍不住想要拥抱他的可怜家伙。\\n\\n哈耶克在美国期间写作了一本著名的《自由宪章》。在这本书中，哈耶克强调法律之于自由的作用。他将自由定义为“服从于法律，而不是任何人，那就是自由”。他引用了约翰·洛克话说，“何处无法律，则亦无自由。”【8】\\n\\nThree key conditions differentiate days when you have a full charge from typical days:\\n\\n- **Meaning**: doing something that benefits another person\\n- **Interactions**: creating far more positive than negative moments\\n- **Energy**: making choices that improve your mental and physical health\\n\\n孔老夫子讲的“巧言令色，鲜矣仁”\\n\\n王小波在他的《黄金时代》中说：我年轻的时候，有好多奢望。我想爱，想吃，还想在一瞬间变成天上半明半暗的云。后来我才知道，生活就是个缓慢受锤的过程，人一天天老下去，奢望也一天天消失\\n\\n在考虑如何自信以前，我们需要先弄懂什么是自信。研究表明，自信与许多行为相关联。威尔休·阿林德尔与其同事在尼德兰对自信行为进行厂研究，归纳出以下四点：\\n\\n1．表达消极情感：例如，敢于要求搅扰你的人改变他的行为，表达你的愤怒，争取自己的权利，拒绝他人的请求。这 就是自信首先让人想到的内容。\\n\\n2．接受并积极应付个人极限：敢于承认自己忽略了某事，犯了某种错误，敢于接受批评，能够不耻下问。\\n\\n3．敢于表达自己：敢于表达不同见解，接受自己与他人观 点上的差异。\\n\\n4．积极的行为：能够发现他人的天赋或成就，善于表扬他人，也能够接受他人的赞扬。\\n\\n．我给人脸色是因为我受到了伤害，想惩罚他人。\\n\\n．尽管有时有效，但一般而言，它对人际关系没有益处。\\n\\n．尽管我曾认为，他人应当满足我的意愿，但我应当改变这 种观念。\\n\\n．我要学会认识到，他人的态度是我所不喜欢的，我要自信 起来\\n\\n当涉及到钱的问题你会十分小心、不想冒险，还是主动去学习管理风险？\\n\\n从长线上来看，每个人经历的好事坏事数量和客观强度应该是差不多的；但每个人intepretation的方式是极其不一样的导致心理上的强度很不一样\\n\\n不但幸运本身是盲目的，而且使享用它的人也成为盲目的，世上没有比交好运的白痴更不可容忍的了。\\n\\n——西塞罗\\n\\n心灵的快乐就是对肉体快乐的观赏。\\n\\n——伊壁鸠鲁\\n\\n我们通常得到的欢乐总是不如我们期望的那样动心，相反，我们所遭遇的痛苦却比我们预料的更为深重。\\n\\n——叔本华\\n\\nPleasure is never as pleasant as we expected it to be and pain is always more painful.\\n\\n- Schopenhauer\\n\\nIs America still the world's last superpower with global policing obligations? Or should we shuck off this imperial role and make America, again, in Jeane Kirkpatrick’s phrase, “a normal country in a normal time”?\\n\\n帕特里克·J·布坎南（Patrick J.Buchanan）\\n\\n**亨廷顿认为，西方的衰落的起点，不是在21世纪，而是在20世纪，1919年，劳合乔治，克里蒙梭，威尔逊，三个人在巴黎和会就能控制整个世界，可以决定哪些国家存在哪些国家不存在，可以决定新的国家的诞生，可以决定边界如果划分，决定从哪些国家榨取经济特权，但是现在，不行。**\\n\\n**从北约轰炸南联盟，催生出来新的国家科索沃，科索沃不过区区185万，弹丸小国，居然到了2019年还有90多个国家不承认，而且还不是联合国成员国，而在巴黎和会，说创造出波兰，就创造出波兰，不但创造出波兰，而且还创造出一大堆的新国家。**\\n\\n**对比西方在20世纪的力量，亨廷顿认为西方的力量是下降的，而且还会继续下降，不过他认为下降的速度和上升的速度可能同样，只不过这种下降是曲折的，可能半路还会短暂的复兴，但是大的趋势不变，西方力量上升需要400年，下降可能也需要400年**\\n\\n作者说：“生活不是我们活过的日子，而是我们记住的日子，我们为了讲述而在记忆中重现的日子” - 马尔克斯自传\\n\\n其实真正有价值的观点，我觉得一定是有一些冒犯性的。他一定会冒犯到一些人，因为它会挑战人的固有认知，会让人有一点不舒服的地方。但是我觉得这样的观点可能随着人们变得越来越喜欢挑刺，逐渐地就消失了。他在说之前会去先权衡一下这个观点，环保的人会不会不喜欢，女权主义者会不会不喜欢，虎扑直男会不会不喜欢。而当你权衡了所有的观点的时候，你得出来的就是一个无聊的观点。并且会养成一个习惯，时刻去维护这个观点，所有的观点都是以这个东西作为一种延伸，所以就形成了人设\\n\\n你很难拒绝这种诱惑。如何去抵抗诱惑呢？王尔德说，除了诱惑我什么都可以抵抗，但抵抗诱惑最好的方式就是臣服于它。但是我不同意王尔德，我觉得臣服于诱惑之前，我还可以坚持一会儿。\\n\\n畸形的投放逻辑\\n\\n其实流量造假和维护数据在行业里早就是公开的秘密了。\\n\\n当下的很多企业的广告投放，尤其是大公司，很多工作都是“汇报导向”的，我把这个钱花出去，就要有对应的“效果”，直接效果是很难量化的，所以各种传播数据就很重要，真实效果几何，执行层的员工不会真正关心，但是代表自己成绩的PPT，数字一定要漂亮。甚至有的甲方会计算“单次曝光成本”，平台为了吃掉更大的投放预算占比，也在迎合甲方的“数据需求”，放宽统计口径，营造一种虚假的数据繁荣。\\n\\n很多人都会把eefit这位老哥比作皇帝的新装里，敢于喊出真相的那个单纯正直的孩子，但是eefit哪有那么无辜，他也只不过是无数希望通过1块钱投放，产生1000块回报的不切实际的甲方群体的一份子罢了。\\n\\n按照弗洛姆的说法，“生产性的人”才是人的本质之所在， “只有在生产性的活动中，人才能使人生有意义，虽然他在这一过程中享受人生，但他并不贪婪地想保住这人生。他戒绝了占有的贪婪欲望，他已被存在所满足；他是充实的，就因为他是空虚的，他之所以享有许多东西，就因为他没有多少东西”。\\n\\n**但王尔德说过一句很聪明（或许是他说过最聪明）的话：“生活模仿艺术远胜于艺术模仿生活。”\\xa0 我发现自己在生活里对爱情的理解与做法其实是对大众传媒作品拙劣的想象和模仿，在生活中，爱而不得才是常态，并不值得自怨自艾或是憾恨他人，认为自己是世界上最受伤的人。**\\n\\nLIFE IMITATES ART\\n\\nFAR MORE THAN\\n\\nART IMITATES LIFE\\n\\n我也知道还有一个补过的方法的：去讨他的宽恕，等他说，“我可是毫不怪你啊。”那么，我的心一定就轻松了，这确是一个可行的方法。有一回，我们会面的时候，是脸上都已添刻了许多“生”的辛苦的条纹，而我的心很沉重。我们渐渐谈起儿时的旧事来，我便叙述到这一节，自说少年时代的胡涂。“我可是毫不怪你啊。” 我想，他要说了，我即刻便受了宽恕，我的心从此也宽松了吧。\\n\\n“有过这样的事吗？”他惊异地笑着说，就像旁听着别人的故事一样。他什么也不记得了。\\n\\n全然忘却，毫无怨恨，又有什么宽恕之可言呢？无怨的恕，说谎罢了。\\n\\n我还能希求什么呢？我的心只得沉重着。\\n\\n现在，故乡的春天又在这异地的空中了，既给我久经逝去的儿时的回忆，而一并也带着无可把握的悲哀。我倒不如躲到肃杀的严冬中去罢，——但是，四面又明明是严冬，正给我非常的寒威和冷气。\\n\\n鲁迅《风筝》\\n\\n走上人生的路途吧。前途很远，也很暗。然而不要怕，不怕的人面前才有路\\n\\n知交零落实是人生常态，能够偶尔话起，而心中仍然温柔，就是好朋友\\n\\n所谓有趣的灵魂，实际上就是这个人的信息密度和认知水平，都远高于你，并愿意俯下身去听你说那毫无营养的废话和你交流，提出一些你没有听过的观点，颠覆了你短浅的想象力及三观\\n\\n改正一个错误，是需要付出代价的。但只要是改正了，不管是多大的代价，其实都是最小的代价，你改得越晚，你付出的代价越大\\n\\n多读点书，要不然你的三观是由你的亲朋好友决定的\\n\\n科学不天然具备合法性\\n\\n**但是真情不应该受到嘲讽和否定。爱情是会过期，生活消磨感情，消磨激情，制造摩擦。情势往往不受个人控制，总有一些留不住的人终将远去**\\n\\nThis is called American Dream. You have to be asleep to believe it\\n\\n美国总统罗斯福79年前讲过：“四大自由”，即言论和表达自由、宗教信仰自由、免于匮乏的自由和免于恐惧的自由，是文明社会的基础\\n\\n有一个互联网上请教别人的小技巧叫 Cunningham's Law，说的是“在互联网上获得正确解答的最好方法并不是去问一个问题，而是发布一个错误的答案，等别人来纠正你。”\\n\\n**学习本身会给人很爽的感觉呀，**\\n\\n**只不过现在，你的学习中夹杂了太多的杂质，比如：**\\n\\n**分数、排名、个人评价、家长期待、是否毕业、社会认同、与人竞争、别人家孩子……巴拉巴拉。**\\n\\n**所以你讨厌的并不是学习，而是学习的一些副产品**\\n\\n你要明白国内的教育制度是为了分配有限的教育资源，也就是选拔，也就是为了难而难。现阶段如果课程和考试设计的让每个人都喜欢学习，都能有学习欲望，都能拿高分，那就是设计失败了，得推倒重来\\n\\n生活是残酷的，喜剧只不过是展示这些残酷，甚至化解一下\\n\\n很多时候我们努力，就是为了放弃\\n\\n**一件奇怪的事是，我们为什么没人肯低头呢。很多段我看重但又破碎了的关系，假设当时有一人往前多走那么一步，也会变得完全不一样。我本来觉得应当是对方，对方也许也是这样想，但其实不论是谁都无所谓**\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4b1fa2cc-b299-4c44-9dae-8660cba9b653', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n杂 穷查理\\n\\n国王和熊一样，总让身边的侍奉者提心吊胆。\\n\\nKings&Bears often worry their keepers.\\n\\n若交往中没有友情，有友情却无热情，有热情但缺少坚持，有坚持却不见成效，有成效又没有收益，有收益但无德行，那么，这些都毫无价值。\\n\\nRelation without friendship, friendship without power, power without will, will without effect, effect without profit, & profit without virtue, are not worth a farto.\\n\\n填饱肚子是为了活着，但活着并不是为了填饱肚子。\\n\\nEat to live, and not live to eat.\\n\\n伟人的魅力并不是靠遗传获得的。\\n\\nThe favour of the Great is no inheritance.\\n\\n当心年轻的医生和年老的理发师。\\n\\nBeware of the young Doctor & the old Barber.\\n\\n穷人拥有极少，乞丐一无所有，富人应有尽有，但总不知足。\\n\\nThe poor have little, beggars none, the rich too much, enough not one.\\n\\n宁肯要一匹全盲的马，也不要一匹独眼残马。\\n\\nHe has chang'd his one ey'd horse for a blind one.\\n\\n以烈火考验真金，以真金考验女人，以女人考验男人。\\n\\nThe proof of gold is fire, the proof of woman, gold; the proof of man, a woman.\\n\\n狼吃狼之时，大饥荒之日。\\n\\nGreat famine when wolves eat wolves.\\n\\n愚者的心挂在嘴上，智者的嘴藏在心中。\\n\\nThe heart of a fool is in his mouth, but the mouth of a wise man is in his heart.\\n\\n和恶棍交易，既不体面也无收益。\\n\\nThere is neither honour nor gain, got in dealing with a villain.\\n\\n清者自清。\\n\\nInnocence is its own Defence.\\n\\n记住穷理查的话：任何事，若始于愤怒，必将终于羞愧。\\n\\nTake this remark from Richard poor and lame, Whate's begun in anger ends in shame.\\n\\n哪里有尸体，秃鹰就在哪儿聚集；哪里有良好的法律制度，人们就往哪儿靠拢。\\n\\nWhere carcasses are, eagles will gather, And where good laws are, much people flock thither.\\n\\n掩盖过错总要煞费苦心，而改正过错却只需花一半的心思。\\n\\nWhat pains our Justice takes his faults to hide, With half that pains sure he might cure ‘em quite.\\n\\n对收获的期望会减轻当下的痛苦。\\n\\nHope of gain Lessens pain.\\n\\n勤奋万事易，懒惰事事难。\\n\\nAll things are easy to Industry, All things difficult to Sloth.\\n\\n骑马时应紧贴马背、牢牢控制，驾驭别人时则应放松、给其自由。\\n\\nIf you ride a Horse, sit close and tight, If you ride a Man, sit easy and light.\\n\\n虽然傻瓜们不承认，但真理再新，也是真理；错误再老，仍是错误。\\n\\nA new truth is a truth, an old error is an error, Tho' Clodpate wont allow either.\\n\\n予人欢乐者，自己也会获得快乐。\\n\\nWho pleasure gives, Shall joy receive.\\n\\n有无爱的婚姻，也就有无婚姻的爱情。\\n\\nWhere there's Marriage without Love, there will be Love without Marriage.\\n\\n疾患不要来得太晚，幸福也别来得过早。\\n\\nBe not sick too late, nor well too soon.\\n\\n不要愚蠢，也不要狡诈，但要做个明白人。\\n\\nBe neither silly, nor cunning, but wise.\\n\\n在开始讨价还价后，堡垒和处女膜也就维持不了多久了。\\n\\nNeither a Fortress nor a Maidenhead will hold out long after they begin to parly.\\n\\n有些人因学得过多而发疯，但没有人因为学到好知识而发疯。\\n\\nSome men grow mad by studying much to know, But who grows mad by studying good to grow.\\n\\n不懂服从的人，也难以指挥他人。\\n\\nHe that cannot obey, cannot command.\\n\\n法律如同蜘蛛网，只能抓住小苍蝇，却眼睁睁看着大苍蝇逃之夭夭。\\n\\nLaws like to Cobwebs catch small Flies, Great ones break thro' before your eyes.\\n\\n有学问的笨蛋比没有学问的笨蛋更愚蠢。\\n\\nA learned blockhead is a greater blockhead than an ignorant one.\\n\\n穷人为了充饥而四处奔波，富人为了消食而不停散步。\\n\\nThe poor man must walk to get meat for his stomach, the rich man to get a stomach to his meat.\\n\\n给予的东西熠熠生光，而收受的东西往往容易生锈。\\n\\nWhat's given shines, What's receiv'd is rusty.\\n\\n懒惰和沉默是傻瓜的美德。\\n\\nSloth and Silence are a Fool's Virtues.\\n\\n用语言报复不了他人，但却可能招来他人极大的报复。\\n\\nThere's small Revenge in Words, but Words may be greatly revenged.\\n\\n即使受到许多伤害，也不要去伤害任何一个人。\\n\\nIt is better to take many Injuries than to give one.\\n\\n空凭希望活着的人，死时连个屁都不如。\\n\\nHe that lives upon Hope, dies farting.\\n\\n财富不属于占有它的人，而是属于享受它的人。\\n\\nWealth is not his that has it, but his that enjoys it.\\n\\n羡慕出于无知。\\n\\nAdmiration is the Daughter of Ignorance.\\n\\n上帝帮助那些自助者。\\n\\nGod helps them that help themselves.\\n\\n理智就是力量。\\n\\nForce shites upon Reason's Back.\\n\\n欲壑难填。\\n\\nIf you desire many things, many things will seem but a few.\\n\\n我发现，很少有人饿死，但撑死的人却很多。\\n\\nI saw few die of Hunger, of Eating 100000.\\n\\n把秘密告诉他人，就等于向对方出卖了自己的自由。\\n\\nTo whom thy secret thou dost tell, To him thy freedom thou dost sell.\\n\\n爱情和权力不能与他人分享。\\n\\nLove & lordship hate companions.\\n\\n认清哪些事情是荣耀的，是获得荣耀的捷径。\\n\\nThe nearest way to come at glory, is to do that for conscience which we do for glory.\\n\\n不要在烛光下挑选精致布料、漂亮姑娘和闪闪发光的黄金。\\n\\nFine linnen, girls and gold so bright, Chuse not to take by candle-light.\\n\\n越破的车轮，噪音越大。\\n\\nThe worst wheel of the cart makes the most noise.\\n\\n过度享乐会带来更大的痛苦，过分自由会带来更多的束缚。\\n\\nNothing brings more pain than too much pleasure; nothing more bondage than too much liberty. (or libertinism.)\\n\\n写作要引经据典，说话则应通俗易懂。\\n\\nWrite with the learned, pronounce with the vulgar.\\n\\n古人告诉我们什么是最好的，但我们必须懂得什么最适合现代。\\n\\nThe ancients tell us what is best; but we must learn of the moderns what is fittest.\\n\\n我们必须对每一句闲话负责，也必须对每一次虚度的时光负责。\\n\\nAs we must account for every idle word, so we must for every idle silence.\\n\\n让房子因你而生辉，而不是凭漂亮房子为自己添彩。\\n\\nGrace thou thy House, and let not that grace thee.\\n\\n开玩笑无法化敌为友，但却可能化友为敌。\\n\\nYou canst not joke an Enemy into a Friend; but thou may'st a Friend into an Enemy.\\n\\n不要吝啬那些让你毫无损失的东西，比如待人礼貌、给人忠告和鼓励。\\n\\nBe not niggardly of what costs thee nothing, as courtesy, counsel, & countenance.\\n\\n不要把自己所知、所欠、所有、所能都全盘托出。\\n\\nProclaim not all thou knowest, all thou owest, all thou hast, nor all thou canst.\\n\\n观察所有人，尤其是你自己。\\n\\nObserve all men; thy self most.\\n\\n当你被人善待，铭记之；\\n\\n当你善待他人，忘却之。\\n\\nWhen befriended, remember it:\\n\\nWhen you befriend, forget it.\\n\\n只要有一方承认错误，争吵就不可能持续很久。\\n\\nQuarrels never could last long, If on one side only lay the wrong.\\n\\n懒惰像锈蚀，比劳动更消耗身心。常用的钥匙总是铮亮发光。\\n\\nSloth (like Rust) consumes faster than Labour wears: the used Key is always bright.\\n\\n管理好你的工作，否则它会奴役你。\\n\\nDrive thy Business, or it will drive thee.\\n\\n雇主的眼睛比手更忙活。\\n\\nThe Eye of a Master, will do more Work than his Hand.\\n\\n想被爱，就要让自己和善可爱。\\n\\nIf you'd be belov'd, make yourself amiable.\\n\\n很多人都抱怨自己的记忆力差，却少有人反省自己的判断力。\\n\\nMany complain of their Memory, few of their Judgment.\\n\\n没有礼貌，任何别的优点都难以弥补。\\n\\nA Man without ceremony has need of great merit in its place.\\n\\n买东西的人需要100只眼睛，而卖东西的人只要一只眼睛就够了。\\n\\nHe who buys had need have 100 Eyes, but one's enough for him that sells the Stuff.\\n\\n防止坏习惯比戒除坏习惯简单。\\n\\nIt is easier to prevent bad habits than to break them.\\n\\n指责能带来刺痛，是因为它的真实。\\n\\nThe Sting of a Reproach, is the Truth of it.\\n\\n理智是所有人都需要的，但少有人拥有，并且没有人认为自己缺乏。\\n\\nGood Sense is a Thing all need, few have, and none think they want.\\n\\n悠闲的生活和懒散的生活是两码事。\\n\\nA life of leisure, and a life of laziness, are two things.\\n\\n有时要眨眼假装看不见，有时要睁大眼睛看清楚。\\n\\nThere's a time to wink as well as to see.\\n\\n幸与不幸总是来得猝不及防。\\n\\nNone know the unfortunate, and the fortunate do not know themselves.\\n\\n诡计必须穿上外衣，而真理却能赤身示人。\\n\\nCraft must be at charge for clothes, but Truth can go naked.\\n\\n很多人败家，是因为总买用不上的便宜货。\\n\\nMany have been ruin'd by buying good pennyworths.\\n\\n当你缺少理智时，更需要信念的支撑。\\n\\nYou've just so much more Need of Faith, as you have less of Reason.\\n\\n怀疑或许没错，但四处散布你的猜疑则可能铸成大错。\\n\\nSuspicion may be no Fault, but shewing it may be a great one.\\n\\n傻瓜可能会犯两次同样的错误，他改正到一半就放弃了。\\n\\nTwo Faults of one a Fool will make; He half repairs, that owns & does forsake.\\n\\n大多数傻瓜都认为他们只是无知而已。\\n\\nMost Fools think they are only ignorant.\\n\\n本性善良却鲁莽缺少审慎，是一大不幸。\\n\\nGreat Good-nature, without Prudence, is a great Misfortune.\\n\\n智者从他人遭遇中吸取教训，愚者则只能从自身遭遇中吸取。\\n\\nWise men learn by others harms; fools by their own.\\n\\n狂热结束之日，悔恨开始之时。\\n\\nThe end of passion is the beginning of repentance.\\n\\n语言可以卖弄才智，但只有行动才能表达真意。\\n\\nWords may shew a man's wit, but actions his meaning.\\n\\n贫穷本身并不可耻，以贫穷为耻才是真的羞耻。\\n\\nHaving been poor is no shame, but being ashamed of it, is.\\n\\n忽视可以消减伤害，\\n\\n而报复只会增加伤痛。\\n\\nNeglect kills Injuries,\\n\\nRevenge increases them.\\n\\n侮辱敌人，你比敌人更低劣；报复敌人，你也只是和他同属一类；宽恕敌人，你将超越于他。\\n\\nDoing an Injury puts you below your Enemy; Revenging one makes you but even with him; Forgiving it sets you above him.\\n\\n问心无愧，就会无所畏惧。\\n\\nKeep Conscience clear, Then never fear.\\n\\n零让其他数字更有意义，谦逊使其他美德更加突出。\\n\\nA Cypher and Humility make the other Figures & Virtues of ten-fold Value.\\n\\n很多人以为自己在花钱买快乐，殊不知自己已委身其中，被快乐奴役。\\n\\nMany a Man thinks he is buying Pleasure, when he is really selling himself a Slave to it.\\n\\n能经受住他人责难并努力改善的人，即便不聪明，也离聪明不远了。\\n\\nHe that can bear a Reproof, and mend by it, if he is not wise, is in a fair way of being so.\\n\\n打翻朗姆酒，仅仅只是损失了酒；而贪饮朗姆酒，不仅没了酒，可能连自己也要搭进去。\\n\\nHe that spills the Rum, loses that only; He that drinks it, often loses both that and himself.\\n\\n得意时露恶癖，逆境中显德行。\\n\\nProsperity discovers Vice, Adversity Virtue.\\n\\n知道自己裤子臭的人，对他人每个皱鼻的动作都耿耿于怀。\\n\\nHe that is conscious of a Stink in his Breeches, is jealous of every Wrinkle in another's Nose.\\n\\n抑制最初的欲望容易，满足随之而来的诸多欲望难。\\n\\nIt is easier to suppress the first Desire, than to satisfy all that follow it.\\n\\n如果财富无法使我免于死亡，那么它们也不应阻碍我获得永生。\\n\\nIf worldly Goods cannot save me from Death, they ought not to hinder me of eternal life.\\n\\n美食家难遇美食。\\n\\nNice Eaters seldom meet with a good Dinner.\\n\\n高傲自负的人厌恶其他人的高傲。\\n\\nThe Proud hate Pride-in others.\\n\\n谁能最准确地判断一个人，他的敌人还是他自己？\\n\\nWho judges best of a Man, his Enemies or himself?\\n\\n挥霍比贪婪更无道义。\\n\\nThe Prodigal generally does more Injustice than the Covetous.\\n\\n亲兄弟不一定是好朋友，但好朋友往往亲如兄弟。\\n\\nA Brother may not be a Friend, but a Friend will always be a Brother.\\n\\n严苛往往意味着仁慈，反之亦然。\\n\\nSeverity is often Clemency; Clemency Severity.\\n\\n大人和小孩一样也有自己的玩具，只是价格不同而已。\\n\\nOld Boys have their Playthings as well as young Ones; the Difference is only in the Price.\\n\\n客套不是礼貌，礼貌也不是客套。\\n\\nCeremony is not Civility; nor Civility Ceremony.\\n\\n若想实现一半的理想，就要付出加倍的努力。\\n\\nIf Man could have Half his Wishes, he would double his Troubles.\\n\\n成功毁掉了许多人。\\n\\nSuccess has ruin'd many a Man.\\n\\n失意时，没有人了解你；得意时，你不了解你自己。\\n\\nWhen out of Favour, none know thee; when in, thou dost not know thyself.\\n\\n说大话的人可能不傻，但相信他的人肯定是傻瓜。\\n\\nA great Talker may be no Fool, but he is one that relies on him.\\n\\n很多人为宗教信仰辩护，但从未践行过自己的信仰。\\n\\nMany have quarrel'd about Religion, that never practis'd it.\\n\\n突如其来的权势易使人傲慢，从天而降的自由往往让人莽撞，最好的做法是一点一点增加。\\n\\nSudden Power is apt to be insolent, Sudden Liberty saucy; that behaves best which has grown gradually.\\n\\n愤怒总有自己的理由，但很少有正当理由。\\n\\nAnger is never without a Reason, but seldom with a good One.\\n\\n一双善于倾听的耳朵，胜过一百张嘴巴。\\n\\nA pair of good Ears will drain dry an hundred Tongues.\\n\\n侍奉上帝要与人为善，但祈祷被认为是更简单的行为，因此更多人选择祈祷。\\n\\nServing God is Doing Good to Man, but Praying is thought an easier Service, and therefore more generally chosen.\\n\\n最受期待的礼物是自己买来的，而不是他人给予的。\\n\\nGifts much expected, are paid, not given.\\n\\n在世事纷扰中，人们得以救赎，不是靠信仰，而是靠对信仰的渴望。\\n\\nIn the Affairs of this World Men are saved, not by Faith, but by the Want of it.\\n\\n友谊里不能有虚伪客套，但又不能没有礼貌。\\n\\nFriendship cannot live with Ceremony, nor without Civility.\\n\\n如果你想被爱，就先爱人并成为可爱之人。\\n\\nIf you would be loved, love and be loveable.\\n\\n诚实者经历艰辛，然后享受欢乐；奸诈者纵情享乐，随后便遭受苦痛。\\n\\nThe honest Man takes Pains, and then enjoys Pleasures; the Knave takes Pleasure, and then suffers Pains.\\n\\n谁是智者？向每个人学习的人。\\n\\n谁是强者？能控制自身情感的人。\\n\\n谁是富人？懂得知足的人。\\n\\n谁是这样的人？没有人。\\n\\nWho is wise? He that learns from every One.\\n\\nWho is powerful? He that governs his Passions.\\n\\nWho is rich? He that is content.\\n\\nWho is that? Nobody.\\n\\n勤奋能克服困难，懒惰则制造困难。\\n\\nDiligence overcomes Difficulties, Sloth makes them.\\n\\n活得长并不一定活得好，活得好多长都足矣。\\n\\nA long life may not be good enough, but a good Life is long enough.\\n\\n对智者来说，命运沉浮如同月亮阴晴圆缺，并没有多大影响。\\n\\nA Change of Fortune hurts a wise Man no more than a Change of the Moon.\\n\\n吃得不多的人，永远不会懒惰。\\n\\nHe that never eats too much, will never be lazy.\\n\\n没有什么比眼泪干得更快。\\n\\nNothing dries sooner than a Tear.\\n\\n掩饰错误比改正错误更劳心费神。\\n\\nMen take more pains to mask than mend.\\n\\n一个今天胜过两个明天。\\n\\nOne To-day is worth two To-morrows.\\n\\n个人的力量是极其有限的，所以单凭一己之力干不成大事，需要借助他人的力量，合力完成。个人必须先获得他人的信任，才能得到他人热情主动的帮助。而人们总是要感受到他人对自己的真诚，才会去信任对方，所以伪君子一旦露馅，就将一事无成。一个人若得不到他人的信任，也就无法获得他人的帮助，只得孤军奋战。即便有所成绩，也是可鄙的。\\n\\nSince Man is but of a very limited Power in his own Person, and consequently can effect no great Matter merely by his own personal Strength, but as he acts in Society and Conjunction with others; and since no Man can engage the active Assistance of others, without first engaging their Trust; And moreover, since Men will trust no further than they judge one, for his Sincerity, fit to be trusted; it follows, that a discovered Dissembler can achieve nothing great or considerable. For not being able to gain Mens Trust, he cannot gain their Concurrence; and so is left alone to act singly and upon his own Bottom; and while that is the Sphere of his Activity, that entire he Call do must needs be contemptible.\\n\\n像能活100岁那样努力工作，像明天就要死去一样虔诚祈祷。\\n\\nWork as if you were to live 100 Years, Pray as if you were to die To-morrow.\\n\\n冒着失去一个朋友的风险来开玩笑，是非常愚蠢的行为。然而很少有人意识到，朋友很容易因为一个玩笑而反目成仇。有些人认为朋友了解包容自己，因而无所顾忌地拿朋友开涮，开不敢与他人开的玩笑，殊不知，这会给朋友造成巨大的伤害。即便再亲密的朋友，也无法保证能肆无忌惮地开玩笑，除非你肯定朋友这么做丝毫不会伤害到你，否则，认为朋友之间能随便开玩笑实在是愚蠢荒谬的。\\n\\nIt is generally agreed to be Folly, to hazard the loss of a Friend, rather than lose a Jest. But few consider how easily a Friend may be thus lost. Depending on the known Regard their Friends have for the, Jesters take more Freedom with Friends than they would dare to do with others, little thinking how much deeper we are wounded by an Affront from one we love. Bat the strictest Intimacy can never warrant Freedoms of this Sort; and it is indeed preposterous to think they should; unless we can suppose Injuries are less Evils when they are done us by Friends, than when they come from other Hands.\\n\\n乐善好施并不会让自己变穷。\\n\\nGreat-Alms-giving, lessens no Man's Living.\\n\\n沉默不一定是智慧的象征，但喋喋不休必然是愚蠢的表现。\\n\\nSilence is not always a Sign of Wisdom, but Babbling is ever a Mark of Folly.\\n\\n有关公共事务的第一大错误，就是盲目参与。\\n\\nThe first Mistake in public Business, is the going into it.\\n\\n喝酒时商量事情，清醒后再做决定。\\n\\n**Take counsel in wine, but resolve afterwards in water.**\\n\\n一半的真理往往是弥天大谎。\\n\\nHalf the Truth is often a great Lie.\\n\\n通向信仰之路的方法，是闭上理性之眼。只有熄灭蜡烛时，晨光才更清晰明亮。\\n\\nThe Way to see by Faith, is to shut the Eye of Reason: The Morning Daylight appears plainer when you put out your Candle.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5d449dab-f8ea-4453-a89e-60cc27f02ffd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n杂 35\\n\\n夫轻诺必寡信，多易必多难。《道德经》\\n\\n有些通过肢体语言体现的正反馈环也可以发生在社交当中。如果你垂头丧气、膘不振,那么你也会感到自己渺\\n\\n小和挫败,而他人的反应更会放大你的这种感觉。人和龙虾一样,都会根据身体姿态来评估彼此,如果你显得失\\n\\n败,那么别人也会把你当失败者对待;如果你笔挺站立,人们也会用不一样的态度对待你。\\n\\n你可能会反驳说,失败和失败者的身份是真实存在的,仅靠调整体态不足以改变这早已固化的现实。如果一个人\\n\\n身处底层还想要显得挺拔强势,那么只会招致更多打压。这的确有可能,但是笔挺站立、昂首挺胸的不光是身体。\\n\\n因为你不单只是一具肉体,你也有思想,物理层面的挺立也可以激发精神层面的挺拔。\\n\\n挺拔的你会直面人生的重负,随之你的神经系统会产生完全不同的反应,你会更加积极地迎接挑战,而不是坐等\\n\\n灾难降临。你会看见恶龙镇守的黄金,而不是被恶龙的存在吓退;你会在支配等级中找准自己的位置,占据领\\n\\n地,并且做好防守和扩张的准备。\\n\\n为了最崇高的理想而牺牲并不等于心甘情愿、默不作声地接受他人持续的剥削和压榨,否则无异于是在支持暴\\n\\n政,允许自己变成奴隶。一味地忍受欺凌是不道德的,即使那个欺凌你的人就是你自己。\\n\\n关于“己所不欲,勿施于人\"和“爱人如爱己这样的道理,我从著名的瑞士心理学家卡尔荣格那里学到了很重要的\\n\\n两点。第一,这些道理与是否亲善友好没有关系;第二,这两者并非单向的指爷,它们反之亦成立。面对朋友、家\\n\\n人或恋人,我在道德上有义务尽量维护自己,否则我就会成为奴隶,而对方则会成为暴君。当你被折磨奴役时,\\n\\n站起来为自己说话和为别人说话没什么区别。正如荣格指出的那样,你需要接纳和爱护拥有原罪的自己,就像包\\n\\n容和帮助那些不完美的人一样。\\n\\n恰如其分地热爱自己\\n\\n我在人生中的至暗时刻,总能被人们想方设法关爱至亲的行为所震撼。我认识一个因车祸致残的人,他和另一个\\n\\n患有神经退行性疾病的人并肩工作了许多年,他们彼此合作,共同完成水电线路修理工作。我认为,这样的英雄行为应该成为常态,而不是例外。许多身患重病的人都毫无怨言地努力生活着,如果你刚好幸运地拥有一具健康的身体,那么至少你也有过一位亲戚會在危机中挣扎过。人们总能排除万难,支撑起自己、家人和社会。对我而言这是个惊人的奇迹。\\n\\n失败和崩溃很容易发生,但受伤的人们总是会坚持下去,这种奇迹般的坚韧值得受到由衷的赞赏。\\n\\n在咨询工作中,我经常鼓励人们认可自己和对自己体贴照顾、真诚关怀的他人。人们确实被存在的种种局限折磨着,但是克己和利他为我们带来了集体供暖、自来水、电力和无限的电子计算能力,每个人都能填饱肚子,并且有机会思考社会和自然的问题。维系我们安居乐业的所有复杂机器都会因为熵定律而不断趋近故障,多亏了细心之人的持续关注,一切扌能始终保持良好的运转。面对痛苦、失望、损失、匮乏和丑陋,有的人坠入了怨念和仇恨的深渊,但大多数人还是拒绝放弃。\\n\\n面对生命的有限、暴政的威胁和自然的掠夺,不论是人类还是每个个体都在负重前行,这种努力值得同情。自我同情是治疗羞耻、内疚和自我轻视的良药,不过这只是故事的一半。\\n\\n如果你抱着感恩之心看待传统、国家、日常生活中的小事和爷人惊叹的成就,那么你对自我和人类的仇恨就能得到平息。\\n\\n人类值得尊重。你值得尊重。你对于自己和他人来说都很重要,你在世界的发展中扮演着重要的角色,因此你在\\n\\n道德上有义务像照顾至亲一样照顾自己。你需要尊重自己的存在。每个人都有难以改变的缺陷,会在比自己优秀\\n\\n的人面前相形见绌,如果这意味着我们可以推卸照顾自己的责任,那么世界将会变得更加糟糕,每个人都会因此\\n\\n饱受折磨。显然这不是正确的方向。\\n\\n待己如助人,这意味着你需要选择对自己真正有好处的事物,即使这些事物不一定是你想要的或是合你快乐的\\n\\n给孩子糖果可以让他快乐,但这不代表你只需要给他吃糖就可以了。“快乐″和“好\"绝不是同义词。你需要让孩子学\\n\\n会刷牙,或者让孩子在天冾的时候穿上他们不爱穿的羽绒服。你需要培养孩子具备道德感、责任心、自我觉察的能\\n\\nDON T THINK THAT IT IS EASIER TO SURROUND YOURSELF WITH GOOD HEALTHY PEOPLE THAN WIT\\n\\nBAD UNHEALTHY PEOPLE. IT\\'S NOT A GOOD, HEALTHY PERSON IS AN IDEAL. IT REQUIRES STRENGTH AND DARING TO STAND UP NEAR SUCH A PERSON\\n\\n和善良上进的人为友\\n\\n并不比和糟糕颓废的人为伍容易,\\n\\n因为前者代表了一种理想,\\n\\n和他们同行需要力量与勇气\\n\\n给你个提示:未来和过去很相似。唯一的区别是,过去是固定的,但未来有可能变得更好。这种好或许可以是你在一天之内花费最小努力所能达到的。当下永远都是有缺陷的,但是你前行的方向比你的起点更重要。\\n\\n也许快乐总是产生于改善的过程,而不是目标达成时那转瞬即逝的满足感。有希望就是快乐的,不论这希望产生于多么黑暗的深渊。\\n\\n在适当的时候,內心的批评家会建议你整改那些你能够和愿意,甚至乐意去改变的事情。问问你自己,在生活的所有混乱中,有什么是你能够且愿意去厘清的?你有能力也有意愿去修复这个看似不起眼的问题吗?你能现在就行动吗?想象一下,你需要和“自己\"谈判,而”自己\"是个懒惰、易怒、爱埋怨并且难以相处的人。要让这样一个人行动起来并非易事,你可能需要用个人魅力和一些幽默去打动他。或许你需要真诚地对他说:“不好意思,我打算减少一些痛苦,希望你可以帮帮我。不知道有没有什么事情是你愿意做的?对你的帮助我会很感激。\"\\n\\n坦诚谦虚地那个声音属于一个會经被伤害而且井常羞涩的人,所以你必须认真地回复:“我真的会。我可能做得不太好,也可能不是个好伙伴,但是我愿意对你好,我保证。”一点小小的善意就可能带来长远的回报,审慎的奖励则可以作为\\n\\n强大的激励。然后你就可以牵着\"自己\"的手去把碗洗了,但最好别继续清理卫生间,而忘记喝咖啡、啤酒或者看电\\n\\n影,否则当你下次再想从内心深处请出被遗忘的\"自己\"时,就不太容易了\\n\\n你可以问问自己:“如果我明天要去改善和他人的关系,比如和我的朋友、兄弟、老板或者助理之间的关系,我应\\n\\n该说些什么?如果我想更好地迎接明天,今晚应该清理家里书桌上或者厨厉里的哪些混乱?我应该从我的衣柜或\\n\\n者内心驱逐哪些蛇虫鼠蚁?\"你的今天和其余每一天都是由无数小的选择和行动构成的,你能试着把其中一两件\\n\\n事做得更好吗?这里的\"更好\"是从你自己的角度和标准来定乂的。你能把属于自己的特定明天和特定昨天做比较\\n\\n吗?你能否运用自己的判断,问问自己那个更好的明天是什么样的?\\n\\n你要把目标定得小一点。你的扌能有限,你已经习惯了自欺欺人、心怀怨恨、逃避责任,所以一开始不要给自己太\\n\\n大负担。你应该这样设定目标:到今天晚上为止,我希望自己的生活比早上有一丁点进步。然后问问自己:“我能\\n\\n够并且愿意做哪些事情来实现这一点?我希望得到怎样的奖励?\"执行你的选择,不诒做得好不好,然后用咖啡\\n\\n庆祝自己的胜利。你可能会觉得这么做有点傻,但是没关系,明天你还这样做,后天,大后天,一直持续做下\\n\\n去。随着时间的推移,你的比较基线会神奇地提升,就像复利一样。坚持三年,你的生活捋会完全不一样。然后你\\n\\n就可以设定更大的目标,更远大的理想。你的双眼也会跟着清晰起来,并逐渐能够看清世界。\\n\\n人类值得尊重。你值得尊重。你对于自己和他人来说都很重要，你在世界的发展中扮演着重要的角色，因此你在道德上有义务像照顾至亲一样照顾自己。你需要尊重自己的存在。每个人都有难以改变的缺陷，会在比自己优秀的人面前相形见绌，如果这意味着我们可以推卸照顾自己的责任，那么世界将会变得更加糟糕，每个人都会因此饱受折磨。显然这不是正确的方向。\\n\\n**待己如助人，这意味着你需要选择对自己真正有好处的事物，即使这些事物不一定是你想要的或是令你快乐的。**\\n\\n成功和美德的秘诀难以捉摸，失败则只需要你养成几个坏习惯然后任其发酵，接下来你就会陷入恶性循环。失败会不请自来，而人的罪恶则会加快它的进程。\\n\\n**救赎并不是徒劳的尝试，但是将人拉出壕沟容易，救出深渊却很难。处在深渊底部的人可能已经没有什么救赎的价值了。在帮助一个人之前需要确认他是不是真的想要被帮助**\\n\\n你是谁？你以为你知道，但也许你并不知道。比如，你既不是自己的主人，也不是自己的奴隶。你不能轻易命令自己必须做什么，或者强迫自己服从，就好像你无法用这样的方式对待你的妻子和儿女一样。你对不同事物的感兴趣程度是不一样的，你可以在一定限度内培养自己对某些事物的兴趣，但有的事物总是会很吸引你，而有的事物无论如何也不会。\\n\\n如果你粗暴地压制天性，它也一定会不断反抗。你能强迫自己努力工作到什么程度？这努力的动力能保持多久？你对伴侣牺牲和付出到什么程度会让你的心甘情愿转变成怨恨？你真正渴望的是什么？在明确自己的价值标准之前，你应该先把自己当作陌生人去了解。什么对你有价值或者能令你快乐？你需要多少享乐和奖励才能弥补自己因忍辱负重所受的委屈？你应该如何对待自己，才不至于想要挣脱羁绊，放弃一切？你可以强迫自己过完痛苦的一天并在回家之后烦躁地踹自己的狗，眼睁睁看着日子一天天过去，但你也可以学会引导自己去做可持续的、有价值的事情。你会问自己想要什么吗？你可以和自己公平地谈判吗？抑或你就是个暴君，你的自我则是暴君手下的奴隶？\\n\\n什么情况下你会讨厌自己的父母、伴侣或者孩子？这样的情况又应该如何改善？你对朋友和生意伙伴有怎样的期待？这并不只是你“应该”想要什么的问题，这里讨论的不只是他人的期待或者你的义务，而是你应该担负起的对自己的道德责任。“应该”可以是这个问题的一部分，因为你生活在一系列社会责任当中，但这不意味着你需要扮演哈巴狗一样顺从无害的角色。只有独裁者才希望自己的奴隶是这个样子\\n\\n给你个提示：未来和过去很相似。唯一的区别是，过去是固定的，但未来有可能变得更好。这种好或许可以是你在一天之内花费最小努力所能达到的。当下永远都是有缺陷的，但是你前行的方向比你的起点更重要。\\n\\n**也许快乐总是产生于改善的过程，而不是目标达成时那转瞬即逝的满足感。有希望就是快乐的，不论这希望产生于多么黑暗的深渊。**\\n\\n**和昨天的自己比，别和今天的别人比。**\\n\\n***IF YOUR LIFE IS NOT GOING WELL, PERHAPS IT IS YOUR CURRENT KNOWLEDGE THAT IS INSUFFICIENT, NOT LIFE ITSELF.***\\n\\n**生活的不顺或许不是因为生活本身，**\\n\\n**而是源自你的无知。**\\n\\n***YOUR HEAD WILL START TO CLEAR UP, AS YOU STOP FILLING IT WITH LIES. YOUR EXPERIENCE WILL IMPROVE, AS YOU STOP DISTORTING IT WITH INAUTHENTIC ACTIONS. YOU WILL THEN BEGIN TO DISCOVER NEW, MORE SUBTLE THINGS THAT YOU ARE DOING WRONG. STOP DOING THOSE, TOO.***\\n\\n**停止用谎言填充头脑，**\\n\\n**头脑就会变得更清晰；**\\n\\n**停止用不坦诚的行为扭曲生活，**\\n\\n**生活就会得到改善。**\\n\\n**随后你就能发现和纠正那些更微妙的错误**\\n\\n无论是精神、身体还是心智上的痛苦，都不一定会带来彻底否定人生价值、意义和愿望的虚无主义。这样的痛苦总是可以从不同角度去理解的。\\n\\n这是尼采的观点。5他认为面对过邪恶的人的确有可能继续推进邪恶，让它持续存在，但是经历邪恶也有可能让人学会善良。一个被欺凌的男孩可以选择模仿折磨他的人，也可以从自己的经历中明白，欺负他人和制造痛苦是错误的。比如，一个被自己母亲折磨的人能够体会到做个好家长的重要性。许多虐待孩子的成年人小时候也被虐待过，但是大多数儿时被虐待过的人并不会虐待自己的孩子。这个显而易见的事实可以用很简单的计算来证明：如果一个家长虐待了3个孩子，然后每个孩子长大后又生了3个孩子，以此类推，那么第一代子女里有3个虐待者，第二代有9个，第三代有27个，第四代有81个。一直这样发展下去，到了第20代就有超过100亿人在童年时被虐待过，这比整个地球的总人口还要多。但事实恰恰相反，虐待会在代际间消失，因为人们会限制它的延续。这证明人心中的善是能压倒恶的\\n\\n**如果你停止说谎，遵从自己的良心，那么即使面对死亡也能保持高贵。如果你真诚勇敢地追寻最崇高的理想，获得的安全感和力量将远胜于任何目光短浅的自我保护。如果你以正确、充实的方式生活，就能发现你已强大到足以克服死亡的恐惧**\\n\\n罗马剧作家特伦斯（Terence）曾经说过：“我对人类的一切都不感到陌生。”伟大的精神分析家卡尔·荣格补充道：“只有根基深入地狱的树才能生长至天堂\\n\\n如同《道德经》所说：\\n\\n为者败之，执者失之。是以圣人无为故无败，无执故无失\\n\\n尼采认为保罗以及后来被新教徒追随的路德推卸了道德责任。尼采写道：“基督徒从未遵循耶稣规定的行为准则，对于至高无上的‘因信称义’，之所以会出现不恭又喋喋不休的讨论，无非是因为教会缺乏勇气和意愿来信奉耶稣所要求的善行\\n\\n对尼采有深远影响的陀思妥耶夫斯基也批判过制度化的基督教。在陀思妥耶夫斯基的杰作《卡拉马佐夫兄弟》里，无神论者伊凡讲述了一个“宗教大法官”的故事，让我们来简要回顾一下这个故事。\\n\\n伊凡瞧不起兄弟阿廖沙做修道院院士的追求，所以跟阿廖沙讲了一个耶稣在西班牙宗教裁判所存在的时代回归地球的故事。救世主的回归引起了轩然大波，他医治病人，复生死者，而这一切很快引起了宗教大法官本人的注意。宗教大法官立刻拘捕了耶稣，将他投入牢笼。随后大法官在狱中拜访了耶稣，告诉他，世人已不再需要他，而他的回归对教会来说是巨大的威胁。大法官说耶稣要求人类虔诚而真实地活着，这对凡人来说太难做到。出于怜悯，教会淡化了这一点，不再要求信徒成为完美之人，允许他们在简单仁慈的信仰和后世中寻求解脱。这样的工作耗费了好几个世纪，而教会最不愿意看到的就是那个要求人类担负责任的人又回到世间。耶稣默默地听着，当大法官要离开时，他拥抱和亲吻了大法官。大法官震惊得面色惨白，没锁牢门就离开了。\\n\\n这个故事的深刻性和作者伟大的创作精神令人叹为观止。作为伟大的文学天才之一，陀思妥耶夫斯基在他所有的作品中都选择义无反顾地直面那些最巨大的存在主义问题。在《卡拉马佐夫兄弟》里，无神论者伊凡以无比清晰热忱的方式反驳了基督教的预设，而支持教会的阿廖沙根本无法反驳兄弟的任何一个论点。陀思妥耶夫斯基承认基督教已被理性打败，而且毫不避讳这个事实\\n\\n教条的死亡带来的是更加糟糕的虚无主义，以及对乌托邦的危险期待。陀思妥耶夫斯基和尼采都预见到了这样的结果。尼采认为人们必须在上帝死后发明自己的价值观，但从心理学上来说，这是他思想最薄弱的部分：人们无法发明自己的价值观，因为我们无法将自己的信念强加于心灵之上。这是荣格在深入研究了尼采提出的问题之后做出的伟大发现。\\n\\n人们会像反抗极权主义一样反抗自己。一个人无法要求自己或者他人唯命是从。“我要停止拖延”“我要坚持健康饮食”“我要停止酗酒”，人们虽然这么说，却不一定这么做。一个人也没法把自己变成思想中构建出来的样子，尤其是当他的思想受到意识的影响时。\\n\\n**每个人都有天性，而我们必须发现这种天性，只有与之抗衡，我们才能与自己和解**\\n\\n就如同波普尔所说，人们能够让想法为了自己的利益而死。然后那个创造想法的自我就可以摆脱先前错误的限制继续前进。**只有相信自我的某些部分不会因为这一系列死亡而改变，自我才能够开始思考**\\n\\n**如果不好的事情存在，那么好的事情也一定存在。如果最糟糕的罪恶是为了制造痛苦而折磨他人，那么善就是与之截然相反的、阻止这种罪恶的东西**\\n\\nDistress, whether psychic, physical, or intellectual, need not at all produce nihilism (that is, the radical rejection of value, meaning and desirability). Such distress always permits a variety of interpretations.\\n\\nNietzsche wrote those words\\n\\n**逃避或者说出真相，不仅仅是两个不同的选择，更是两条人生道路，两种完全不同的存在方式**\\n\\n**愿景能够将当下的行为与长远的基本价值观相联系，赋予当下行为非凡的重要性和意义，提供限制不确定性和焦虑的框架**\\n\\n心怀坦诚的人类有能力将存在的痛苦减少到可以忍受的程度。存在的悲剧源自人类经验中固有的局限和脆弱，甚至可以说这种悲剧是我们为存在付出的代价，因为存在必然伴随着局限性。\\n\\n我曾经见过一位男士诚实而又勇敢地面对自己妻子病情逐步恶化的过程。他拒绝逃避，优雅地调整心态去适应，并同时接受必要的帮助。在妻子即将离世的时候，整个家庭带着支持和关怀之心聚集在她的床前，家庭成员之间也因此建立了新的联结。我女儿的髋部和脚踝曾经遭受过严重损伤，之后的两年里，她承受着持续不断的剧痛，却依然保持着良好的精神状态。她的弟弟也心甘情愿地放弃了许多社交机会，选择陪伴并支持她。一个拥有爱、力量和完整人格的人可以承受难以想象的痛苦，然而由欺骗和悲剧带来的毁灭性打击则是难以忍受的\\n\\n我们可以再次通过约翰·弥尔顿的诗作《失乐园》来理解这点。在弥尔顿看来，代表理性精神的主人公是最奇妙的天使。我们可以从心理学出发解读这一点。理性是有生命的，它存在于每个人心中，却比任何人都年长。我们应该将它视作一个人格，而不是一种官能。它有自己的目标、诱惑物和弱点，能比其他天使飞得更高、看得更远。但是理性会爱上自己，也会爱上自己的创造，它会抬高自己创造的事物，并将它们奉为绝对的真理。\\n\\n再次强调，理性面对的最大诱惑就是美化自己和自己的创造，并且宣称自己的理论能够诠释一切，任何超越或超出其理论范畴的事物都是无须存在的\\n\\n**你需要与未知为友，在行动的同时保持自我觉察。你需要先处理好自己的痛苦，再去担心他人。这样你才能够强化自己，承担起存在的重担，使生活重新焕发活力**\\n\\n人们有责任勇敢地看清当下，并从中吸取教训，即便当下看上去很可怕，或者看清当下所造成的恐惧会伤害我们的意识和视觉。“看见”这个行为会给个体带来新知，虽然它会挑战我们熟悉和依赖的事物，给我们带来烦恼和不稳定，但它尤为重要。因此，尼采才认为一个人的价值是由他可以承受多少真相来决定的。\\n\\n**你绝不仅仅由那些你已知的部分构成，你也包括那些你只要愿意就可以获得的部分。你永远不应该为了当下的自己而牺牲未来你可能成为的样子。**\\n\\n你永远不应该为了当下的安全而放弃更好的未来，尤其是当你已经瞥见了一些无法忽视的可能性\\n\\n**专注能够让你朝着目标前进，更重要的是，专注带来的信息能够让你及时调整目标。**\\n\\n一个强权主义者从不会问：“如果我现在的志向错了呢？”他会把自己的志向视为绝对的存在，这个志向会成为他的神，他的终极价值，会决定他的情感、动机和思想。每个人都会服务于自己的志向，只不过有的人明确地知道他们的神是谁，而有的人却不知道。\\n\\n如果你彻底盲目地为单一目标服务，那就永远无法发现那些对你和这个世界的发展来说更好的目标，你的不坦诚会牺牲掉这种可能性。如果你实事求是，在奋进的过程中允许自己看清不断显现的现实，那么你对于事物重要性的理解就会发生转变。你会重新调整方向，这种改变有时候可能是缓慢的，有时却可能是突然的\\n\\n**存在究竟是好是坏？你需要冒很大风险，活在真相或谎言中，直面后果，然后才能得出结论。**\\n\\n这就是丹麦哲学家克尔凯郭尔所强调的“信仰行动”（act of faith）。你没法预知结果，个体间的差异巨大，他人的正面例子不足以作为证据，你必须拿自己的人生做赌注去得出答案。你相信命运会把你带到更好的地方，相信改变的过程可以不断纠正存在，这就是探索精神的本质\\n\\n他人的观点无法被伪装成真相，因为真相不是一串口号。相反，真相是个人的。只有你能够识别基于自己独特人生情境的真相。理解你的真相，谨慎而又清晰地将它传达给自己和他人，这会让你当下的生活安全而富足。当你的未来能够在过去的确定性之上展开时，它也必将是充满善意的。\\n\\n真相从存在的最深处不断涌现，让你的心灵在面对生活不可避免的悲剧时免于枯萎和死亡。它能让你优雅地承担起存在的重担，而不是试图报复，这样存在就可以继续\\n\\n**如果你的生活不尽如人意，试着说真话；如果你拼命地坚持某种意识或者沉迷于虚无主义，试着说真话；如果你感到脆弱、无用、绝望和困惑，试着说真话。**\\n\\n说真话，或者至少别撒谎\\n\\n20世纪最伟大的心理咨询师之一卡尔·罗杰斯很了解聆听这件事，他写道：“我们大多数人无法坚持聆听，总是忍不住要去评价，因为聆听太危险了。聆听的首要要求是勇气，而我们并不总具备勇气。”4罗杰斯知道，聆听可以改变一个人，所以他评论道：“你们有的人以为自己善于聆听，却从未改变过别人。这很有可能是因为你们的聆听并不是我所描述的那种聆听。”罗杰斯提议读者做一个小实验，在下次遇到争执时，先停止对话，并且立这样一个规矩：每个人必须先准确反映对方刚刚表达的想法和感受，直到对方满意，然后才能表达自己的观点。这个技巧在生活和工作中都非常有用，我经常总结他人对我说的话，确认我是否准确理解了他们。有时我理解得很准确，有时却会有些小偏差，甚至还有些时候我完全误解了对方。不过，这些全都是很有价值的反馈\\n\\n人类只需要看到足够自己实现计划、达成目标的东西就行了，人类生存在这种“足够”当中。这是对世界的一种激进而又无意识的功能性简化，而且人们很容易把简化后的世界和世界本身混淆。另外，人们看见的对象也并不仅仅是为了人类简单直接的感知而独立存在的，它们之间还以复杂多维的方式彼此关联着。人们感知的不是对象本身，而是它们的实用功能，这样的简化能让人充分理解感知的对象。\\n\\n**一个人必须拥有精确的目标，否则就会淹没在世界的复杂性当中**\\n\\n有的人可能觉得这样的行为很蠢。也许是，但我认为那些孩子们很勇敢也很了不起，值得人们鼓励和钦佩。这样的行为的确是危险的，但孩子们想要战胜危险的心才是重点。如果穿上护具，他们会更安全一些，但那样就没意思了。孩子们并不在意安全，他们在意的是能力的提升。\\n\\n**能力才能让一个人在最大程度上感到安全**\\n\\n乔治·奥威尔很了解这类问题。他在1937年创作了《通往维根码头之路》奥威尔将视线转向了另一个问题。虽然当时英国各地都明显存在令人痛苦的不平等现象，社会改革的思潮却并不怎么受欢迎。奥威尔认为原因是那些衣着考究、热衷思考批判、满心怜悯和同情的社会改革派并不喜欢穷人，他们只是仇恨富人，并且在用虔诚和自以为是来伪装自己的怨恨和嫉妒\\n\\n**对目标的追求能赋予生命持久的意义，那些让生命显得深刻和迷人的情绪几乎都产生于我们朝着理想成功前进的时刻，而我们为此付出的代价就是成功的等级之分和结果的差异。**\\n\\n绝对的平等只有在放弃了价值本身后才能实现，但那样的话，人生就没有追求了。也许更好的方式是，心存感激地承认一个复杂而先进的文化可以提供许多游戏，让每个成员都参与竞争，并且以各种方式赢得胜利\\n\\n**怨恨的产生只能有两个原因：一是自己被占便宜了，二是自己不愿意承担责任**\\n\\n到了20世纪80年代，超人已经严重面临“机械装置之神”（deus ex machina）\\xa0(6)的问题。这个术语专门用来描述古希腊和古罗马戏剧中当主角危在旦夕时，全能神明突然出现并且将其拯救的反转剧情。即使在今天的许多故事里，当主角身陷重围，或者剧情难以继续发展的时候，也会出现观众预期之外的神奇反转。比如，漫威就使用这个方法挽救了《X战警》的剧情，其中的救生员（Lifeguard）就能够发展出任何拯救生命所必需的能力；斯蒂芬·金在《末日逼近》的结尾，让上帝亲手毁灭了小说中的反派角色；在1985年播出的《达拉斯》第九季，所有剧情后来被发现只是一个梦。观众们反感这样的“欺骗”行径，他们只愿意相信剧情连贯一致的故事，当作者“作弊”的时候，观众就会愤然离去。\\n\\n这就是超人面临的问题。他拥有的超能力使他可以极端到在任何危机下做自己的“救世主”，以至于在20世纪80年代，超人这个品牌几乎破灭了。之后，约翰·伯恩（John Byrne）改写了超人的故事，在保留他身世的情况下，去除了许多新添加的超能力。超人不再能撬动星球、抵御核弹，他也需要依靠太阳获得能量。超人有了合理的局限性。一个无所不能的超级英雄就不再是英雄了，他的能力并不针对任何特定问题，所以他没有什么特别之处。他没有任何要努力抗争的东西，所以他无法被人们钦佩\\n\\n知人者智，自知者明。\\n\\n胜人者有力，自胜者强。\\n\\n知足者富，强行者有志，\\n\\n不失其所者久，死而不亡者寿\\n\\n《道德经》第三十三章\\n\\n我应该如何使自己变得更强大？不要撒谎或做你鄙夷的事情。\\n\\n我应该如何使自己变得更高尚？只遵照你心灵的旨意行事。\\n\\n我应该如何面对最棘手的问题？将它们视为通向人生之道的大门。\\n\\n我应该如何帮助落魄的人？用正确的示范来鼓舞他，使他重新振作起来。\\n\\n当众人皆醉我独醒时应该做什么？坚定勇敢地说出事实的真相', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='456ba416-3b76-477a-aad1-6ace429345d7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n杂 96\\n\\n一个人只有潜意识觉得自己是安全的，他才能安下心来学习，定下心来做事。才能静下心来，深谋远虑。\\n\\n要是一个人潜意识安全感缺失，别说深谋远虑了，下一秒要干嘛他都不知道，一天到晚拿个手机，刷个不停。\\n\\n你会发现很多人是没法定下来，没法静下来。不是他不想，是根本做不到。只要一闲下来，心里就莫名的躁动，你也不知道在燥什么，反正心里就是不舒服。\\n\\n心里不舒服就想办法转移注意力，吃、喝、玩、乐就开始了，必须整点刺激的东西，刺激一下身体，刺激一下大脑，缓解内心的那种不舒服感。所以这样的人是没法做事业的，因为内心总有股不明的力量在干扰他，看不到它，干不掉它，很多人一辈子都在和这股力量对抗，痛苦不堪。\\n\\n怎么破?首先看到这股力量，承认这股力量，然后把缺失的安全感补回来，疗愈内心的创伤，让心健康成长起来，心成长起来后，内心这股恐惧的力量就自动消失了。\\n\\n这个时候你才能把梦想在心里投影出来，你才知道什么是人生蓝图，你才知道人生要何去何从\\n\\nfinance需要解决的本质问题is a principle agent problem：owners don’t have info about company. Managers do; \\n\\nwhy we love cash more in the past 30 years: because it can’t be manipulated \\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n“一个社会的落后首先是精英的落后，而精英落后的标志是嘲笑民众落后。”\\n\\nQ: What do you think about GE and Immelt?\\nAckman: \"A lot of very talented CEOs have come out of GE. GE is one of the best training programs for running a business, [but] I think it\\'s been a terrible place to learn how to invest money.\\nAckman: \"GE is actually an extremely well-operated company, but it\\'s a company that\\'s been an absolute failure in terms of the way they\\'ve allocated capital.\" Ackman: \"They\\'ve not done a good job with capital generated by the core\" but now\\n\"I think GE appears to have gotten religion.”\\n\\nFor CEOs, capital allocation is the most important thing from an investor perspective\\n...and capital allocation is a \"religion”\\nMost dramatically, this signals the Rise of Investor Capitalism -From Financial Capitalism to Managerial Capitalism to Investor Capitalism\\n...the growing dominance of investors and the investor mindset\\n\\n要爱具体的人，不要爱抽象的人； 要爱生活，不要爱生活的意义。 ——陀思妥耶夫斯基 《卡拉马佐夫兄弟》\\n\\n他是如此清醒地认定，自己必得独处，注定要在水中游泳，此生已无根无着，以致当他偶尔见识到市民的某项日常之举时，比如说我准点去办公室上班的那种守时，或者一位家仆或电车售票员的客套话，他都可以真真切切，不带丝毫讽刺地为之着迷\\n\\n直到今天我都不知道还有什么词更适合这样一个人物。一头迷茫地闯到我们之中、闯入城市、闯入牧群生活的荒原狼——再没有什么图像更能让他形影毕现，现出他那怯然的孤独，他的野性，他的惶恐，他的乡愁和他的漂泊失所了\\n\\n个体自己是感受不到意义的，但会觉得别人的生命是有意义的？会觉得和别人建立关系是有意义的？\\n\\n思想很有趣，但人更有趣，存在主义咖啡馆的封面\\n\\n思想都是诞生于人的活动和对世界的体悟\\n\\n哈勒是一个善于受苦的天才，按照尼采的某些说法，他在其自身形成了一种天才的、无限的、可怕的受苦才能。同时我也认识到，他的悲观不是以厌世而是以厌己为基础，因为他尽管谈起机构或人毫不留情，贬损至极，却从来不忘提及，他舌箭所向的第一个总是他自己，他第一个憎恨和否定的是他自己\\n\\nQuirrel at Blue Lake. He says:\\xa0*\"To live an age, yet remember so little... Perhaps I should be thankful?All tragedy erased. I see only wonders...\"*\\n\\nwe suffer, so we bargain with the future so we can minimize suffering \\n\\nwhat cures in therapy is truth \\n\\nif u get ppl to face the things they are avoiding, they get better \\n\\nwhat makes human unique is being conscious that one can die; humans are so self conscious that they discovered boundraries of time and space;  \\n\\nthe ppl I see mostly hurt was hurt by deceit and betrayal \\n\\n凸显存在偶然性的恰当哲学方式就是说：自在的存在（being-in-itself）。存在的偶然性在于这样一个事实——不能从一个物体衍生出另一个物体；它们之间不存在一种必然的关系。不仅物体是多余的，罗冈丹本身也是多余的。当“物体”失去功用，它们的存在就会凸显出来；同样，当罗冈丹无法在撰写有关罗尔邦的作品的过程中确定他存在的目标，他的存在也就凸显出来\\n\\n“民族”现在代表着一切合法权威的来源——这就是说，他们可以以它的名义，推翻一个已经持续了几个世纪的、其辩护者坚称是神定的政治体系。\\n\\n西方的民族主义、爱国主义与宗教，就像弯曲的藤条一样相互纠缠。它们中的每一个都为了超越个体生命，甚至值得为此放弃生命的意义提供了来源。\\n\\n若缺少对自己的爱，博爱也无法实现；对自我的憎恨正是如此，它最终会如极度的自私一般，造就同样悚然的孤立与绝望\\n\\n人大多不愿入水，在他们学会游泳之前。’这难道不是一句妙语吗？他们当然不愿意游泳！他们是为土地而生，不是为水而生的。他们当然不愿意思考；他们是为生活而造的，不是为了思考！是啊，谁如果把思考当作第一要务，他固然可以让思考这么继续下去，但是他也就把水错认做了土地，不知何时就会溺水而死\\n\\n我学会了如果你必须离开一个地方,一个你曾经住过、爱过、深埋着你所有过往的地方,无论以何种方式离开,都不要慢慢离开,要尽你所能决绝地离开,永远不要回头,也永远不要相信过去的时光才是更好的,因为它们已经消亡。过去的岁月看来安全无害,被轻易跨越,而未来藏在迷雾之中,隔着距离,叫人看来胆怯。但当你踏足其中,就会云开雾散。\\n\\n现在恐惧已经消散了一不是被克服的,也不是靠说理摆脱的。它消失了,因为有些别的东西替代了它的存在:自信与依赖,对脚下那片土地与生俱来的依赖一如今已转化为对飞机的信赖,因为大地已经消失不见,没有其他事物可做寄托。飞行,不过是短暂的逃离,逃离来自大地的禁锢。\\n\\n这一切并没有什么非凡之处。我掌握了一项技能,曾费尽艰辛才得以掌握它。我的双手学会了驾驭飞机的技能,这技能凭借的是熟能生巧。现在它们已游刃有余,就像鞋匠的手指操纵锥子。只有“操控”才能为人类的劳动带来尊严。当你的身体体验到你赖以谋生的工具带来的孤独感,你就会明白其他的事物:那些试验、无关紧要的职位、你曾紧抓不放的虚荣,对你来说都是虚妄。 - 《夜航西飞》\\n\\n这里人们所过的生活很简单，悉达多想，这种生活没有任何难题。当我还是沙门的时候，一切都是艰难和烦恼，最终是绝望。而现在一切都非常容易，容易得就像伽摩拉的亲吻指令。我需要衣服与金钱，仅此而已。这都是不会搅乱一个人睡眠的简单目标。\\n\\n他听得很多，说得极少。他一直记着伽摩拉说过的话，在伽摩湿瓦弥面前从未有任何奴态。他迫使这位商人待他如平等的对手甚或如酋长一般。伽摩湿瓦弥谨小慎微地经营自己的生意，而且常常带着一种冲动和激情。而悉达多则视这一切为游戏；他只是尽力去学会游戏规则而已，对于游戏本身他却无动于衷\\n\\n“这位婆罗门并非真正的商人，将来他也不会成为商人。他从未专心于做生意。可是他似乎有什么秘诀，成功总是自然而然地降临到他们这种人身上。或许他生来吉星高照，或许他身怀魔法。或许他从那些沙门那儿学到了什么诀窍。他好像总是在玩弄生意游戏，生意从未对他产生任何影响，从未能控制他；他从来不怕失败，他从来不担心生意上的损失。” \\n\\n那位朋友建议道：“你可以把他为你经营的那部分生意的三分之一赢利归他所有，同时要他承担同样比例的损失，这样他也许会更热心一些。”\\n伽摩湿瓦弥接受了这一建议。但悉达多仍旧漠不关心。如果赢利，他淡然处之；如果蒙受损失，他只会笑一笑，道：“噢，这笔交易真糟糕。”\\n\\n他的确无心于生意，生意之所以有意义只是因为能使他有足够的钱送给伽摩拉；而且实际上他从生意所得的钱超过他自己的重要。悉达多对世上的人感到非常同情和好奇，他觉得世人的劳作、烦恼、享乐与蠢行对于他仿佛天上的月亮一般神秘而遥远，尽管他发觉自己很容易与所有的人交谈，与所有的人相处，向所有的人学习。然而他清楚地知道有一点把他与世人分离开来——因为他曾经是沙门：他看到世人如无知的孩童，如动物一般地生存，这使他既羡慕又鄙弃。他看到人们不停地劳作，为了金钱，为了微不足道的享乐和无足轻重的荣誉而经受痛苦，华发早生；而对他来说，那些世俗的名利似乎不值得付出如此的代价。他看到人们彼此责骂，彼此伤害；他还看到人们为了一个沙门只会一笑了之的痛苦而悲伤不已或者为了一个沙门根本感觉不到的丧失而烦恼不堪。\\n\\n伽摩拉，大多数人都像一片片落叶，在空中飘浮、翻滚、颤抖，最终无奈地委顿于地。但是有少数人恰如沿着既定轨道运行的星辰：无常的命运之风吹不到他们，他们的内心有着既定的航程\\n\\n悉达多自己也染上了一些庸常人们的特征，也有了一般常人所特有的幼稚和忧虑。然而他却仍然羡慕世人，他越是变得与世人相似，他就愈加羡慕他们。他尤其羡慕常人具足而他所缺少的那一点：世人对自己的生活所持的那种重大感，他们深刻的欢乐与忧伤，以及那种无休止地推动他们去爱的力量所带给他们的焦虑而甜美的幸福。这些人永远爱着他们自己，爱着他们的孩子，爱着荣誉和利益，爱着对未来的筹划和企盼。悉达多没能学会这些孩童般的快乐与蠢行，他只学会了他所鄙弃的令人讨厌的东西\\n\\n悉达多则轻声道，仿佛在自言自语：“什么是冥想? 什么是对肉体的弃绝? 什么是斋戒和调息? 那只不过是在逃离自我，只不过是对自我所受苦难的一种短暂的逃避，只不过是针对生命荒谬与痛苦的一副暂时的麻醉剂。一个牧牛人在小酒馆里喝了几碗米酒或椰子奶，他也在做同样的逃离，也在用同样的麻醉剂，于是他不再感觉到自我，不再感觉到生命的苦难，于是他体验到了暂时的逃避。那碗米酒使他昏然沉入睡乡，他同样找到了悉达多和侨文达长时间修行之后逃离肉体并住于非我之境时所找到的感觉。”\\n\\n我耗费了很长时间才终于明白：一个人无法通过学习得到任何东西，所以我相信在万物的本质之中存在着某种我们不能称之为学识的秘密，我的朋友，世上只存在一种知识，即阿特曼，它无处不在，它存在于你，存在于我，存在于每一种造物之中；而我已经开始相信这种知识的最大敌人莫过于博学之士，莫过于学识本身\\n\\n因为在他看来，认识到事物的因缘就意味着思想，而只有通过思想，感觉才成为知识并且不会失去，而是变得真实并开始成熟。\\n悉达多在自我的路上深沉地思索。他意识到自己已不再是青年，他已经是一个成熟的男人。他感到某种东西已然脱离了他，仿佛一条蛇已蜕去了旧皮。那种伴随他整个青年时代并一直是他自我之一部分的因素已被抛在了身后：这就是寻觅导师和聆听教义的愿望\\n\\n悉达多也终于明白为什么他作为婆罗门或苦行者与自我的争斗会徒劳无功，过多的知识阻碍了他，过多的圣诗，过多的献祭，过多的禁欲，过多的造作和追求阻碍了他。他过去一生充满了傲慢；他永远是最聪明和最急切的一员——永远比他人先行一步，永远那么博学和理智，永远是祭司和哲人。他的自我潜入了他祭司的身份，潜入了他的傲慢与理性，在那儿盘踞并生长；同时他却幻想着自己正以斋戒和忏悔来摧毁自我。现在他清醒地意识到他的自心之声是对的：没有任何导师能够给予他救赎。这就是为什么他必须进入尘世并沉湎于权力、女人和金钱；这就是为什么在他自心中的祭司与沙门死去之前他必须成为商人、赌徒、酒鬼和富人。因而他也必须经历那些可怕的岁月，遭受恶心的折磨，彻底认清尘俗生活的空虚和疯狂，直到他陷入痛苦而绝望的境地；只有如此，他自心中的浪子悉达多与富人悉达多才能死去。事实上他已然死去，一个新生的悉达多已从他的睡梦中觉醒；这新生的悉达多也同样会衰老和死去。悉达多本为无常，一切形态皆为无常；然而今天他很年轻：他只是一个孩子——新生的悉达多——而且他也非常快乐\\n\\n他们都是为生意和金钱而奔忙，或是参加婚宴，或是外出游玩。这条河挡住了他们的去路，而船夫就是要尽快带他们渡过这一障碍。但这千万人之中会有几十个人，也许只有四、五个，对他们来说，这条河并非阻碍。他们听见了河水的音声并且用心去谛听，于是河水对于他们成为神圣之物，正如河水对于我一样\\n\\n他的确从未对一个人爱到完全投入自我以至于忘却自我的程度；他还从未做到这一点，而在他看来，这似乎是他与世人之间的最大差异。但是现在，他的儿子就在身边，他，悉达多，由于爱与忧伤，已经完全成为尘俗中的一员。他疯狂地爱着，由于爱而痴迷。在他的生命中，他初次体验到了这迟到的最强烈、最奇异的激情；由于这种激情他承受了巨大的痛苦，却也得到了升华。在某种意义上他获得了新生，生命变得更为充实。\\n他真切地体会到他对儿子的那种盲目的爱是一种极富人性的情感，这种爱本身就是轮回，就是从深水涌出的烦恼之泉；同时他也感到这种爱并非毫无价值与意义，因为那毕竟源于他的本性，即使是这样的情感、这样的痛苦和蠢行也需要亲身去体验。\\n\\n他不再去分辨不同的音声——诸如愉悦之声与哀泣之声，童稚之声与雄浑之声；所有思慕者的哀叹，智者的欢笑，愤怒者的叫喊，濒死者的呻吟都融入彼此，互为纠结与交织，以千万种方式缠绕在一起，而所有的音声，所有的目标，所有的渴望，所有的善与恶，悲伤与欢乐，所有这一切共同构成了统一的世界，所有这一切共同交融成万物奔流不息的进程，所有这一切共同谱成了生命永恒的旋律。当悉达多凝神倾听这万音交响的河水之歌，当他不再着意分辨悲叹与欢笑，当他的心灵不再执着于任何一种特定的音声并不再任其占据他的自我，当他倾听所有的一切，倾听圆融与统一，正当此时，那宏大的万音交响之歌只包含一个字“唵”(Om)——圆满之音\\n\\n当一个人有所追寻，”悉达多道，“他只会看到他所追寻之物。他之所以无所发现，无所获得是因为他只专注于他所追寻之物，因为他执迷于自己的目标。追寻意味着有了目标，而寻见则意味着自由、包容，摒弃一切目标。尊贵的人，您也许的确是一位追寻者，由于您的追寻过于急切，您没有看到许多眼前的事实\\n\\n“我没有任何财产，”悉达多道。“如果你是指这一点，那么我当然一无所有。但我是甘愿如此，所以我并不窘迫。”\\n\\n世界并非不完善，或者正沿着通向完善的漫漫长路缓缓发展。不，世界在每一瞬间都是完美的：所有罪孽都已然领受神恩，所有孩童都是潜在的老人，所有婴儿都已打上死亡的印记，而所有的垂死者——必获永恒的生命。一个人不可能认清另一个人已然修到何等境界。佛存在于劫匪与赌徒身上，而劫匪亦存在于婆罗门身上。 在极深禅定之中，人可以除灭时间并同时经历所有过去、现在与未来，于是一切皆善，一切完美，一切即梵。因此，我认为一切的存在皆为至善——无论是死与生，无论罪孽与虔诚，无论智慧或是蠢行，一切皆是必然，一切只须我的欣然赞同，一切只需我的理解与爱心；因而万物于我皆为圆满，世上无物可侵害于我。我通过我的灵魂与肉体得知，我之堕落乃为必需，我必然经历贪欲，我必然去追逐财富，体验恶心，陷于绝望的深渊，并由此学会不再去抵制它们；学会热爱这个世界，不再以某种欲愿与臆想出来的世界、某种虚构的完善的幻象来与之比拟；学会接受这个世界的未来面目，热爱它，以归属于它而心存欣喜。侨文达，这就是我头脑中的一些观念\\n\\npeople can’t think if they can’t talk. If they don’t have someone to talk to, they won’t talk, and therefore won’t think\\n\\n艺术要表现主要特征,因此事物各个部分的关系都要围绕它发生改变,使主要特征居于支配地位。这样的逐层递进不是一级否定一级,而是一级修正一级,使之更准确。- 丹纳《艺术哲学》\\n\\n人类了解万事万物最根本的主要特征的方法，一是靠科学，二是靠艺术；艺术是诉诸于人的感情的\\n\\n全神贯注的标志是，自我和时间的消失\\n\\n忘记结果，可以让认知带宽负担降低，让你把手头的事做得更好\\n\\n!Untitled\\n\\n那天我忽然明白了，世界上并不存在一种救赎，只要完成了就能过上好日子。考好了高考不是救赎，申到了名校不是救赎，成年不是救赎，当上CEO迎娶白富美也不是救赎。我现在选择过的每一天，就会是我往后余生过的每一天；不存在一个所谓的节点，能让你只要在那个节点来临之前拼命工作，那么过了那个节点就可以开始享受人生。\\n\\n成年早期我学到的重要的一件事，就是把自己当成一个人，一个有限的、有七情六欲的、并非全知全能的人；去接受常识，关于任何的挑战都会有成功和失败的常识，关于风险和收益并存的常识，关于人被逼到极致会崩溃、人会生病、人需要休息、今天牺牲一切去工作意味着往后余生都会牺牲一切去工作的常识。\\n\\n把自己当成一个人，把人当成目的而不是手段，是我与世界和解的最重要的一步。- ZY\\n\\n但那段消沉的时光里，我竟慢慢明白了那句话：对一个好人来说，最大的回报就是他是好人本身；是做一个好人才能体会到的，那贪嗔痴的肉身和世俗的名利享乐无可比拟的，灵魂的安定。是胆敢献祭所有的情感，并毫不畏惧可能的伤害；是可以识人断物、里仁为美，但即便真的遇人不淑，也可以以直报怨泰然而处的底气。 - ZY\\n\\n弗洛伊德提出潜意识是革命性的，说明了human beings are influenced by motivations and thoughts thats beyond their conscious control \\n\\nthe ancient ppl thought that force to be god\\n\\n荣格认为dreams are the root of mythologies; \\n\\n!Untitled\\n\\na philosophical or moral idea that manifested itself first as a concrete pattern of behavior that’s characteristic of a single individual, then its a set of individuals, then its an abstration of that set. This become god  \\n\\nim not telling u what i know; im trying to figure things out; this is part of the process of me doing that. im trying to stand on the edge of my capacity to generate knowledge to get to the buttom and make this clear continuously \\n\\npresent can influence past, because u can interpret ur past in another way, so its an entirely different thing \\n\\nwhy not lie if it can get u closer to what u want? the answer is not self evident\\n\\none reason is if u lie, u distablize urself by giving up noble aim. The shallow stuff don’t sustain u \\n\\npain argues for itself; pain is the fundamental reality because no one disputes it \\n\\nwhat we need to answer is where r we going and why, not how the objective world is. Its different question with different answers\\n\\nphenomenology 现象学 is interested in the structure of ur subjective exp; everything u exp is true; \\n\\nphenomenology is the study of what matters; it is given in the theory that things has meaning; \\n\\nconsciousness is the prerequisite of reality; \\n\\n!Untitled\\n\\n!Untitled\\n\\npropogenda never lasts more than 10 years; \\n\\nhow does a story last > 1000 years unless its a story that u can remember\\n\\n尼采 gathered air of his time and articulated that god is dead; \\n\\nspeech/writing is different from thoughts because its public in nature; its shaped by the existence of everyone else\\n\\nspeech is a causal element in society; it shapes people in mutual relationship; its a process that brings habitable reality out of inhabitable chaos \\n\\nwhat is human transcendency? what persists between all human exp; 荣格叫这个archetype; \\n\\n但丁的地狱底层就是betray, 倒数第二层是lie;   if u speak truth, I can take ur word and know u; if not, everything happened between us means nothing, its just a big mystery;', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e0a986e9-f735-44c2-863c-cb7301b85887', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n杂 68 奶牛danny\\n\\n今天是22岁的最后一天。几个月前，我从沃顿商学院毕业，用文凭上“最高荣誉毕业”的标签安抚了已经年过半百的老妈，然后转头辞去了毕业后的第一份工作，跟一家很受尊敬的公司、还有150万的年薪道了别，回到了上海，加入了“刚毕业就失业”俱乐部，开始了一天三顿盒饭的新生活，中间许多精彩剧情暂时略过。\\n\\n我肯定不是第一个做过这样事的人，也肯定不会是最后一个。所以在说自己的一些有趣故事前，我想借用大家（包括30岁甚至40岁以上的朋友）的一点时间和一点平和的心态，和大家分享过去一年以来一直没说的一些话。所以前两部说的是对于一些一直困扰着我们的关键词的理解和体会。他们是：欲望、外界、标签、天才、时间、经历、人生目标、后悔、和现实。\\n\\n这可能会是一篇科普文，也可能会是一篇长篇小说，但我不想这篇文章变成一篇励志文，大家都审美疲劳了。所以我想忽略阳春白雪，尽管信息量很大，但是至少说一些实实在在的经验和故事，说一些效果立竿见影的观点，再说说活捉林志玲什么的，总之让大家多看一点就多获得一点实际的价值。\\n\\n第一部：那些最容易被理解错误的事\\n\\n关于欲望\\n\\n这些是我们内心里和人生理想一样真实的东西：学历、工作、房、车、财富、以及爱。我们每个人都愿意为了这些欲望去付出，无论付出的是汗水、鲜血、还是身体健康、又或是其它你懂的。尽管我们付出的方式可能不被社会主流认同、可能没那么具有有戏剧性，但你和我、北大图书馆里的学生和网吧中奋斗的少年、职场杜拉拉和夜场里跳舞的小姐、韩寒和芙蓉凤姐（韩少躺着也中枪-_-），我们谁没有为了一个目标连续熬夜奋斗过呢？我们谁没有为了得到一样东西而撕心裂肺地付出过呢？谁没有过那种拼命得快受不了的感觉呢？所以我们最不缺励志的故事，因为我们每个人都是付出领域的专家。\\n\\n真正的问题是，当我们跑得越快，越是无法考虑我们是否在朝着正确的方向奔跑。\\n\\n北野武讲过一个很有趣的故事。他说他没出名之前想有一天有了钱，一定要开跑车，吃高档餐厅，跟女人们睡觉。而真正功成名就的时候，他发现开保时捷的感觉并没有那么好，因为“看不到自己开保时捷的样子”。结果他就让朋友开，自己打个出租车，在后面跟着，还对出租司机说：看，那是我的车。\\n\\n我想说，过去几年里我认识的、深交的、共事过的所有人，包括身边一批又一批二十出头收入一百多万的金融朋友、三十岁左右收入几百万的前辈朋友、以及简历金碧辉煌得已经不在乎收入的大BOSS、以及我自己的经历告诉我两件事：\\n\\n一，顶级学校的文凭、顶级公司的工作、顶级的收入、顶级的房、顶级的车、顶级的声望，这些都无法满足人类。\\n\\n二，无论是通过爸妈，通过运气，还是通过奋斗得到这些顶级的东西，人类都不会得到更多的幸福感。\\n\\n接着北野武的故事说下去。想象一下：你今天骑在一辆助动车上，一个小山村来的年轻人经过，说你的车好帅，你不会有任何的满足感。十几年的奋斗后，你坐在一辆你今天都叫不出型号的保时捷的驾驶位上，一个路人经过，说你的车好帅，相信我，你也不会有任何的满足感。你不在乎他，就像你今天不在说你助动车帅的人。你的视角在变。每当我们考虑许多年后能够取得的成就，我们总是习惯站在今天的角度去衡量幸福感和满足感。你今天的视角只是错觉，却让你相信自己的目标是正确的。这是我们最容易跑错方向的时候。\\n\\n人类的需求是很奇特的。我们吃第一个面包的时候的幸福感，和我们吃第一千个面包的时候的幸福感，是差不多的，前者甚至比后者还多一些。同样的感觉适用于我们赚到的第一笔一万元和第一笔一千万元，第一辆十万的车和第一辆一千万的车，第一个女孩和第十个女人，第一个男生和第十个男人。“生理需求、安全需求、归属与爱的需求、尊重的需求和自我实现的需求”--在著名的马斯洛五大需求中，你从任意一个细分需求里获得的幸福感只能有那么多。\\n\\n我们清楚地知道快感和幸福感的不同，我们也知道欲望和需求是两个东西（你从来没有听说过“马斯洛五大欲望”对不对？），但是我们的不幸福却是因为不小心把快感当成了幸福感，把欲望当成了需求，而这就是因为我们常站在现在的视角去想象未来的感受。事实是，就好像我们不需要很多的面包一样，我们不需要很多的财富，不需要很多的爱。因为他们很难给你带来更多快乐。当然，我们也不需要去拔高理想和自由的重要性。你可以尝试着停下来思考一下，这五种需求是否真的有高低之分，思考一下，是否连最贫穷最饥饿的人们，都一直在生活中同时追求着这五个高低层次的需求。你会发现其实这五种需求一样真实，离你一样近，也一样远。然后你需要找到一个可以同时实现这五种需求的平衡点。这个平衡点就是只属于你的奔跑方向。这篇文章会实实在在地帮你找到这个方向。但在这之前，我们先谈一些别的。\\n\\n关于外界\\n\\n外界带给我们生活最大的影响是嫉妒和比较。\\n\\n我们一直高估了嫉妒。举个例子，没有人嫉妒雷帝Gaga。雷帝Gaga应该要比我们都更有名、更有钱、坐更好的车、住更大的房子，比我们更随心所欲，而且也比我们更有才华。但你不嫉妒她，对么？我们没有人嫉妒雷帝Gaga--因为她实在是太雷了。她奇怪得让我们完全不能把我们自己跟她联系在一起，所以我们在名利和才华面前没有自卑，也没有嫉妒，更没有仇恨。反而，我们会去思考，觉得她挺有趣的，挺发人深省的，不是么？\\n\\n所以当你见到好事情发生在了那个他或者那个她身上，嫉妒的小火苗在你心中扑哧扑哧的时候，不如把TA当成那个很奇怪的雷帝Gaga吧。因为这样的时候，我们就会懂得抛开个人的杂念，去真正思考别人的亮点。\\n\\n至于比较（Social\\xa0Comparison），我们可以选择努力向那个绩点4.0的同学看齐，努力向那个年薪几十万的旧识看齐，努力向那个不断得到提拔的同事看齐。或者，我们也可以选择看看外面更大的世界，那些和我们一样年轻的人们。看上去像是有30岁阅历的阿呆Adele，19岁时出了张白金专辑《19》，21岁时出了全销量1200万张的专辑《21》，拿了两座格莱美。她出生于1988年。眼神和心态似乎已经像中年人那样淡定的杜兰特和德里克罗斯，两个毫无疑问的超级球星，他们也出生于1988年。如果你喜欢实用一点的，那么iPhone上用户量最大的个人开发第三方浏览器猛犸浏览器的开发者，是一个1992年出生的北京少年。如果你的视线中有一个世界舞台，那么你会看到上面的人物已经越来越接近你的年龄。\\n\\n我们不需要去看齐，我们只需要去“看”。去看到这个世界除了你现在正处在的那个若干平米的封闭空间以外，还有许多许多精彩的事正在发生。当你发现这个世界的深度和广度，你就会发现你跟你身边的那些“同类人”根本没什么好比的。这个世界太大了。你不是你自己的标杆，别人也不是。谁都不是你的标杆，这是一个没有标杆的时代。\\n\\n我们要做的是试着不去嫉妒，不去比较，更不要批判，但要试着去观察、去倾听，然后去思考、去沉淀、去让所有外界的信息在你大脑里经历一个长时间的处理过程。在你的大脑还没有沉淀出你自己对一件事的观点前，不要发表观点，不要给出你的定论。我们可以不断在大脑中质疑我们所看到的、听到的，我们可以不断挑战自己的想法、挑战任何理所当然的存在，只要我们保证我们的大脑一直在思考，独立地思考。要记得，你和世界上所有人都不一样。\\n\\n关于标签\\n\\n“牛逼”是过去几年里笔者听到的比较多的一个形容词。当我们喜欢的人称赞我们的时候，我们总是P颠P颠的。在这里为自己开脱一下，觉得这挺好，说明活得挺真实。\\n\\n但笔者想用一个很好的朋友（自己来认领）去年当着我面描述我听的原话，来翻译一下这个已经被用得和“帅哥”“美女”一样烂俗的词。她说，“你想太多了（这是她一贯的开场白）。你只是有很多很牛的标签--上海中学、沃顿商学院、最高荣誉、黑石的全职Offer、百万年薪。至于你本身么，牛不牛就说不清楚了。”\\n\\n这个故事告诉我们：一、“牛”和“帅哥”“美女”一样，是一种打招呼的方式，二、“牛”的从来都是那些标签，那些改变了金融产业的企业，那些通过培养人才改变了世界的学校，那些定义了时尚的品牌。虽然我无意改变大家打招呼的方式，但对于还没奔到三的人类来说，“高档”“精英”“牛逼”其实不如“做得不错”或者“挺有意思的”来得更实在。当然，等奔到了三，我们就更不想用这些词了。\\n\\n如果你曾经或者将来获得了任何标签，不管是高盛中金麦肯锡，还是北大清华常春藤，又或是Gucci\\xa0Prada\\xa0Armani，有两件事值得思考一下。\\n\\n第一件事用来提醒自己：撕去这些标签，我们或许还未能为500强的客户们创造等同于我们年薪的价值，我们或许还未能用知识改变世界，我们或许还未能把那件衣服穿出五位数价格的范儿。\\n\\n第二件事用来看清自己：这的确是一个人人都用标签来识别对方的社会，但是我们要记住我们的价值和我们身上的标签没有半毛钱关系。成功不是你有什么标签，而是你用这些标签做了什么。(是的，文章开头的“沃顿商学院”“150万”“最高荣誉毕业”这些个标签让一部分人把这篇文章看到了现在，但无论如何，对于心理上被冒犯到了的人，在此致以诚恳的歉意）。\\n\\n总之，把标签用在正确的地方，创造一些价值，虽然不是大到改变世界，也至少带来一些存在的意义。就不展开来说了。\\n\\n关于天才\\n\\n不要去考虑什么天赋异禀，一切都来自经历和渴望。特别是这些年，当我认识了一些全中国、甚至全美国最“天才”的年轻人以后，才发现哪有什么天才，如果把他们的经历一个个说出来，大家肯定觉得完全就是一群苦逼啊。但这些苦逼有一个共同点，他们很清楚的知道自己究竟需要什么，并且很嗨地追求着。\\n\\n第二部：那些最重要的事\\n\\n关于时间\\n\\n时间是唯一的货币。你所拥有的财富很重要，因为你可以用它用来换很多东西。你所拥有的时间远远更重要，因为你可以用时间来换这世界上的任何东西，包括财富，包括成就感，包括幸福感，包括其他那些我们都清楚的、比财富更让我们的生命有价值的东西。是的，每个人拿时间换每样东西的汇率都不同，有些人可以用很少的时间换到很多的财富，有些人需要用很多的时间换到很少的幸福。但是事实是，只要你愿意花时间，你可以换到任何东西。所以你要想清楚，你到底要用时间来换取这世上无限可能中的哪些。打开你的视野，你会发现有太多经历和体验可以让你去换取。但你的时间银行里每天只存了24个小时。你可能以为你还有一辈子的时间去做一些你想做的事，但事实是，没有人可以保证明天上帝是否会往你的银行里存另一个24小时。所以，你要想清楚。\\n\\n关于经历\\n\\n如果你今天能从这篇文章中带走任何一样东西，我希望会是接下来关于经历的这一段。\\n\\n经历的英文叫什么？如果你曾经玩过角色扮演类游戏（RPG），你会知道有一个概念叫EXP，全称叫Experience，这就是经历的英文。人生就是一场巨大的RPG，你扮演你自己。你唯一升级的方法，就是不断地积累EXP。\\n\\n我们都了解那些故事，我们都懂那些道理，看了那么多励志贴，我们甚至都快知道为什么乔布斯会成为乔布斯。但只有经历才能让我们真正把那些道理变成意识。那些改变我们一生的道理，都是不是别人教会的。\\n\\n所以即使你有最完美的理论，你都没有把握说服那些还没有开上保时捷的人们，让他们懂得保时捷不是他们想要的，也没有把握去说服那些还没有在投行工作过的孩子，让他们懂得去放弃投行（更何况，对于那些热爱金融的孩子来说，你的劝诫极有可能是错的）。所以哪怕这篇文章非常努力地想要往实用的方向靠拢，可能你看完以后还是没有任何领悟。这一切就像你无法说服还没有吃过很多很多面包的人们，让他们懂得吃一千个面包是要反胃的。\\n\\n在人生的每个阶段，只有我们已经拥有的那些经历，决定了我们下一步会做什么。所以很多时候，你只要记得一件事，那就是:\\xa0去体验不同的经历。去爱，去恨，去在热恋中没心没肺地笑，去在失恋后声嘶力竭地哭，去翘课，去打架，去拼了命的读书，去让自己真的领悟那些道理。你所尝试的事，你所认识的人，都是你经历的一部分。他们帮助你去理解你一只知道但是不曾真正理解的事，他们帮助你去看到一直存在着但是你不曾看到的世界。\\n\\n但是，你的人生很短，你的时间货币只有那么多。所以除了乔布斯已经告诉你的“不要生活在别人的世界里”你还要记得，永远不要重复一样的经历，因为你不会从第二次一样的经历中收获到更多，更因为这个庞大的世界有太多有趣的人等待着我们去认识、太多截然不同的经历等待着我们去体验。\\n\\n这篇文章也会实实在在地帮助你探索不同的经历。你只要记住，如果你每个星期都在做着差不多的事情，那么一年以后你还是一年前的你，只是老了一岁。如果你愿意每个星期、或者每个月都去尝试一种新的体验，或者认识一个来自完全不同背景的朋友，那么一年后你和一年前一样年轻，只是比别人多活了一年，多了一年的阅历和对世界的认知。\\n\\n关于是否会为了去经历、去追随感情和理想而后悔\\n\\n我们一定会后悔。但我们不会为了作出追随感情、或者追随理想的决定而后悔。事实是，如果我们有努力追寻、不愿放弃的梦想、如果我们有深爱的、不想伤害的人，那么在这条道路上，我们必然会为我们曾经做过的某些事而后悔。当我们离理想和真爱越是近，我们越是容易后悔。就好像晚了一分钟错过飞机的人会比晚了一个小时错过飞机的人更后悔懊恼--因为我们会清楚地看到如果自己不曾犯下某些错误，如果我们再多那么一点点的坚持，就或许已经实现了理想。所以后悔其实是一个信号，它告诉我们，离目标已经很近了。更重要的是，我们活着不是为了追求什么瞎扯的“无悔的生活”，我们不用为那些后悔而伤心痛苦。因为在我们选择的这条道路上，后悔不是告诉我们曾经做错了，而是告诉我们怎样可以做得更好。\\n\\n关于如何找到人生目标\\n\\n兑现承诺的时候到了。我想用一种最简单、直接、有成效方法来解决那些励志文章和成功故事的一个通病：就是他们一直鼓励我们“做我们想做的事”，但从来不告诉年轻迷茫的我们怎么去找到“我们想做的事”（以至于误导了很多朋友以为那就是“我想一觉睡到国庆节”或者“我想做个吃货”之类的意思）。\\n\\n我要说的这个方法在我认识的许多人身上成功过，但它不是我想出来的。知名博客写手Steve\\xa0Pavlina在它的博客中对这个方法有很详细的描述，但似乎也不是Steve\\xa0Pavlina自己想出来的。网上也有不少中文翻译版本，有可能你曾经看到过，但那些翻译都有失偏颇，以至于让读者很难理解精髓。所以在这里把原文重新编辑，结合以上的经验分享，再用比较适合中国人的陈述方式分享给大家。如果你愿意尝试，愿意按照要求去做，或许我们可以用接下来的不到500个字，帮助你在20分钟到1个小时内找到你的人生目标。\\n\\n我们开始吧。\\n\\n(1)\\xa0先在你忙碌的生活中找出一个小时的完全空闲的时间。关掉手机，关掉电脑，关上房门，保证这一个小时没有任何打扰。这一小时只属于你，和你要找到人生理想这件事。你要记住，这可能是你人生最重要的一个小时。你的生命可能在这一个小时候变得不同。如果一个小时的时间货币只能用来换一样东西，那么就是找到你的人生目标绝对是最值得的。\\n\\n(2)\\xa0准备几张大的白纸，和一支笔。\\n\\n(3)\\xa0在第一张白纸上的最上方中央，写下一句话：“你这辈子活着是为了什么？”\\n\\n(4)\\xa0是的，接下来你要做的，就是回答这个问题。把你脑中闪过的第一个想法马上写在第一行。任何想法都可以，而且可以只是几个字。比如说：“赚很多钱。”\\n\\n(5)\\xa0不断地重复第4步。直到你哭出来为止。\\n\\n是的，就是这么简单。尽管这个方法看上去很傻，但是它很有效。如果你想要找到人生目标，你就必须先剔除脑中所有那些“伪装的答案”。你通常需要15-20分钟的时间和过程去剔除那些覆盖在表面上的那些受到外界观念、主流思维影响而得出的答案。所有的这些伪装的答案都来自于你的大脑、你的思维、和你的回忆，但真正的答案出现时，你会感觉到它来自你的内心最深处。\\n\\n对于从来没有考虑过这类问题的人来说，可能会需要比较长的时间（一个小时或者更多）才能把脑子里面的那些杂物剔除掉。在你写到50-100条的时候，你可能会想放弃，或者找个借口去做别的事。因为你可能觉得这个方法没有任何效果，你的答案很杂乱，你也完全没有想哭的感觉。这很正常。不要放弃，坚持想和写下去，这个抵触的感觉会慢慢地过去的。记住，你坚持下去的决定会将这一个小时变成你人生最重要的一个小时。\\n\\n当你写到第100个或者第200个答案的时候，你可能突然会有一阵内心情感上的涌动，但还不至于让你哭出来。这\\xa0说明那还不是最终的答案。但是把这些答案圈起来，在你接下来的写的过程中你可以回顾这些答案，帮助你找到最终的答案，因为那可能会是几个答案的排列组合。但无论如何，最终的答案一定会让你流泪，让你情感上崩溃。\\n\\n此外，如果你一开始不相信人这辈子活着有什么目的，你也可以写下“1.\\xa0活着不为了什么。”\\xa0没关系，只要你愿意坚持想和坚持写下去，你也会找到让你哭出来的答案。\\n\\n作为你的参考，Steve\\xa0Pavlina在做这个练习的时候，花了25分钟在第106步找到了他的最终答案。而那些让他有一阵情感涌动的答案分别出现在在第17，39，53步。他将这些抽出这些答案重新排列，最后在第100步到第106步答案得到了升华。想要放弃的感觉出现在第55到60步（想站起来做点其他事情，感觉极度没有耐心等等）。写到第80步的时候，他休息了2分钟，闭上眼，放松大脑，然后重新整理自己的思绪。这么做很有效果，在那2分钟的休息后，他的思路和答案变得更加清楚。\\n\\n如果你一定要拿笔者来做参考，那么答案是我当时比较无知，还不知道这个系统的方法，所以我用了四个月的摸索和迷茫，撞了很多墙，才找到了最终的答案（在第二章个人故事里会提到）。但经过笔者核实，这个方法科学有效，只因为它提炼出了关键的原理。\\n\\n无论你愿意用什么方法，你最终的答案一定会是一句比较长的句子，或者几句句子的组合。这个答案在外人看来一定非常的空洞，就像是我前面所说的那种“谁都知道，但是只有少数人真正理解的大道理”。但是这几句空洞的句子会对你有非常丰富而且有意义的含义--因为这是你自己用了至少一个小时的时间和精力去整理你过去所有的经历，去思考，去判断，去剔除，去整合，去沉淀，最终领悟出来的。如果你认真看完了从文章开始到这里为止所有的分析，你就会理解为什么这个方法是非常有效的。\\n\\n关于为什么要有个人生目标（以及它和活捉林志玲的关系）\\n\\n这是个好问题。所有人的终极目标其实都一样，就是用有限的人生货币去换最多的幸福感（这个幸福感可以来自内在的、外在的、和世界上任何人和物）。但大部分人都觉得这是件很困难、而且不知道如何下手的事。最大的问题其实就是，如何最大化人生幸福感是一个几万行的方程式，当中你要做出数亿个选择，而我们却指望用逻辑去解决它。你也知道，逻辑是多么不靠铺的一个东西。很多时候，你往往觉得你已经把脑子想炸了，但还是做不出一个选择，这是大脑逻辑功能达到处理极限的问题，它只能解决绕五个弯的问题，面对绕一百个弯的问题它弱得和奔2一样；又有的时候，你的逻辑很容易被你的欲望给废掉了，这个情况最常出现在早上起床的时候--“我该起床么？”“Hmmm...睡着挺舒服的，不起了。”--你以为你用逻辑完美地解决了问题，其实你只是让欲望解决了问题，然后用逻辑完美地说服了自己。所以我们经常在过了一段时间后，突然发现，我们的欲望挂着“逻辑”的羊头“解决”了所有问题，但是自己却空虚得没有任何幸福感。我们不想这样，所以我们需要把你的大脑处理每一个选择的过程变得非常简单正确。\\n\\n确立一个人生目标为什么可以解决这个问题？很简单，人生目标把你那个不知道是什么火星进制的大脑逻辑简化成了二进制。假设你的人生目标是“活捉林志玲”（当然，这只是一个不可能发生的例子，千万不要有人因为写到这个目标哭了出来），那么你每天早上的起床的时候的过程就是：“我该起床么？”“Hmmm...继续睡下去能帮助我活捉林志玲么？”“很明显不能。起床！”\\n\\n这就是你听过很多励志演讲者会说：“究竟是什么让那些幸福快乐的人每天一大早醒来想也不想得就冲下床去做他们要做的事情？”--是林志玲。噢不，是他们的人生目标。其他事情也一样：“我要吃饭么？”“不吃饭我能活捉林志玲么？”\"不能，所以我要吃饭。”“我要去夜店么？”“去夜店能帮我活捉林志玲么？”“不能，锻炼好身体一定可以。所以我还是用去夜店的时间货币去换强健的体格和咏春拳吧。”你会发现你不用再去依赖不靠谱的复杂逻辑，做任何决定都很简单而且正确。当然，你的人生目标会“活捉林志玲”看上去高尚、空洞很多，它也一定会涵括你对自己、对身边亲人好友、对世界的考量。但记住无论如何，你那外人看似空洞的目标曾让你哭出来，所以它对你来说一定有极为丰富的含义。\\n\\n最后，你可能会问：“我怎么能确定一直按照人生目标做出选择，我一定能最大化幸福感呢？”\\xa0（其实这个问题看上去不怎么需要解释的）那是因为你的人生目标是你自己剔除了你欲望带来的杂七杂八的“伪装的需求”，经过沉淀以后得出的你内心最深处最想要的东西，它是你真正的需求。跟随着它你会在短期获得应该获得的快感，更会在长期得到你需要的幸福感。\\n\\n关于现实和人生目标\\n\\n我想给所有已经、即将、或者希望找到人生理想的人，和大家分享两个很平凡的故事，作为结束。\\n\\n我想讲的第一个故事来自我大学最重要的两个导师之一。他是沃顿的一个明星教授，麻省理工本科，哈佛法学院毕业，五十多岁，教了十七年谈判学的课程。尽管他的课作业量很大，但每一年他的课都已几乎满分的学生评分位列沃顿所有课程的前三甲。\\n\\n在我大学毕业前，我约他在费城附近的一个小镇吃了顿午饭。他跟我讲他年轻时候的故事的时候，我问他，他这辈子做出过得最让他后悔的决定是什么？\\n\\n他说，他从小一直很想当老师，特别是小学老师。当他二十多岁从麻省理工毕业的时候，他有一个很好的机会，去家里附近的一家他很喜欢的小学做老师。但即使在美国，小学老师也几乎是待遇很低、不受尊重的一个职业。而同时，他拿到了哈佛法学院的Offer。最后他去了哈佛法学院,而这就是他这辈子做出过最让他后悔的决定。他后悔，不仅仅因为他后来发现哈佛法学院是那么的无聊而且勾心斗角，更因为他当时为了一个被社会所尊重、所仰慕的选择，放弃了一个被社会遗弃、看不起的选择。\\n\\n他说他很幸运，一直那么喜欢当老师，在从法学院毕业许多年的颠沛流离以后，终于如愿以偿成为了一个老师。当我和一些人说起这个故事的时候，他们的第一反应就是，这不是乱说么？如果不是去了哈佛，他可能现在还只是个小学老师，根本不可能成为沃顿教授啊。我想，现实和理想的意义对于每一个人都是不同的，我们只需要理解并不是所有人都觉得成为成为名校的教授是比普通学校的小学老师更伟大、更幸福的成就。\\n\\n第二个故事开头，我想问一个问题：你有没有考虑过我们每天上校内上微薄，看到很多人分享各种励志、免俗、追求梦想的文章，但他们最后究竟做什么去了？你可能以为他们马上回归现实去了。但其实他们很多时候，是怀揣着那些道理，继续去做他们知道怎么做的事情。这就有了第二个故事。\\n\\n每一个二十岁左右的年轻人都像一台高速运行的电脑。一代比一代运转地更快。我们从懂事开始就有别人告诉我们要运行各种程序，上幼儿园，上小学，上初中，上高中，上大学，工作，等等等。我们停不下来。关键是，我们很难运行自己想要运行的程序，因为过去二十年里面我们运行的所有程序都是别人编好以给我们的--我们自己不会编程序。\\n\\n如果有一天，有一台电脑突然下了决心，要运行自己的程序，他就必须先停下来。这时，他会看着周围所有的电脑依然在高速运行着，甚至嘲笑他怎么不动了，然后把他远远地甩在后面。而他，需要慢慢地开始学习自己编程，这个过程很漫长，很痛苦，因为从来没有人教过他。这就是为什么世界上只有少数人在运行自己的程序。\\n\\n说这两个故事不是为了励志，而只是为了告诉大家如果今天或者明天你找到了人生目标，将会发生一些什么：一、即使你内心已经明确地知道你想要什么，依然会有一些更为社会认同的东西来诱惑你，要永远记得坚持。二、如果你坚持了，你一定会经历一个学习自己写程序的过程，这个过程会是痛苦并漫长的。总有一天我们会愿意去面对这个过程。好消息是，我们都还年轻。所以不如趁着现在还有那些热情和勇气，去撞一撞那些墙，用最少的代价。\\n\\n第三部：过去一年里的个人故事，给所有十年来认识的、和喜欢听故事的朋友们\\n\\n辞职前的故事\\n\\n我从去年暑假结束，拿到回黑石的offer后，就开始了寻找自己人生目标的旅程。2010年的九月到12月，我过得挺糟糕的。因为我每天起来都在想我接下来这辈子要干什么。我可以很清楚地看到如果我接受了那个offer，我未来两年的前景。我们办公室里有一个韩国人Jay，我实习的时候是他做分析师的第三年。每年的反馈中，他都是黑石他那一届全球所有分析师里最强的那一个。我没有怀疑自己能够成为这届最好的分析师，但同时，我也可以很清楚地看到，J是我能成为的极限。但仔细想想，J也不过只是那样，像永动机一样地在办公室努力工作，像尊贵的孩子一样在夜店潇洒地玩耍。J是最出色的，但也是黑石所能创造的最出色的。\\n\\n后来我想到了环境的局限性，想到了密集网络。我在上中的时候，我这届最好的学生去了北大和清华。而在沃顿时，最好的学生去了高盛直投、贝恩资本、凯雷、KKR、Jane\\xa0Street等买方。我想到我们是不是已经成为模式化思维的牺牲品（victims\\xa0of\\xa0stereotypes)。\\xa0我们的社交圈里都是与我们同类的人，我们互相交流、竞争、鼓励、启发，处于所谓的密集网络。我们自以为我们充分见识了整个世界，但其实我们只是在重复肯定同一类信息。所以如果你是“最出色的”那一个，那么你极有可能就是所有和你同类的人当中最出色那一个。但这也就是你的极限。而有另外一群人，他们只是想和别人有点不一样，他们想去外面看看，去见识见识这个世界究竟有多大，他们想要找到自己独特的生活。对于这些人来说，天空才是极限。说实在的，所有当年选择DIY出国的朋友们，如果今天你有幸拿到了让那些当年去北大、清华的那些同学羡慕的Offer（再次向躺着也中枪的北大、清华同学致以崇高的歉意），如果你有了比同龄人更多的见识，那绝对不一定是\\xa0因为你比他们更出色，很大程度上是因为在那个出国还没有像今天一样流行的年代，你没有被那个上北大、上清华的模式化思维所套住。所以老天很弄人，因为所有一直在追求“出色”和“卓越”的人最后都在他们最坚信的标准上“输”给了那些只是想过自己独特生活的人。\\n\\n当然，2010年末的时候，我只是确定了自己是被老天玩弄的人哪。但幸好我还有一年时间，我决定一定要要找到一个属于自己的生活目标，然后坚定地走下去。一开始，我和很多人一样，觉得人生的终极目标就是要多走走，去见识这个世界，活出自我。但后来我发现这个目标其实只是说着好听，但是其实不能给人带来持续的动力，然后我就很伤心。再然后，我好不容易想出了一个有点与众不同的目标，就是“做个有意思的人”（Be\\xa0an\\xa0interesting\\xa0person）。因为对我来说，这是我当时能给另一个人的最高评价。但后来我又想了想，这个目标用管理学的标准来说，就是太不具体太不精确所以很难提供持续动力。然后我就更伤心了。所以从九月到十二月的四个月里，每天起来就因为找不到人生目标而痛苦。因为自己跟自己的内心对话太多，经常一不小心就错乱了。当时也没有人告诉我什么20分钟就可以找到人生目标的这种好事。于是我就上了很多奇奇怪怪的课，和各种奇奇怪怪的人交流，希望从他们的经历中获得一些启发。那段时间我过得真的很彷徨也很烦躁，好在我坚持了下来。我谈判课上的教授成为了我很重要的一个导师--尽管他从来没有一对一给予我任何指导。但就像我前面提到的，那些改变我们人生的道理，都不会是别人教会的。进入到十二月以后，我的目标慢慢找到了我。\\n\\n四个月里经过无数内心挣扎之后沉淀下来的思想最终被我总结成了两句很简单、看似和“做个有意思的人”一样不具体、但对我而言包含了丰富含义的话：\\n\\n\"To\\xa0grow\\xa0and\\xa0to\\xa0help\\xa0others\\xa0grow.\\xa0To\\xa0live\\xa0and\\xa0to\\xa0help\\xa0others\\xa0live.\"\\n\\n\"成长，并帮助别人成长。体验和经历生活，并帮助别人体验和经历生活。\"\\n\\n这两句话就成了我的人生目标。它能让我感动得哭，也能让我感动得笑。最重要的是，尽管这两句话在外人看来可能莫名其妙，但我发现这两句话解释了过去二十多年里自己做的许多事情背后的原因，其中包括了我为什么从小一直都不好好读书，为什么选择出国，为什么一直逃课，为什么在2009年和一群朋友一起创建了BIMP这样一个神奇的项目，等等等等。\\n\\n关于辞职的决定\\n\\n在确定了人生目标以后，我的思路和视野都变得清晰了很多。我很快找到了我想要做的事。和身边许多的朋友一样，创业也曾经是我大脑中的考虑过的一个想法。但我一直想不到任何我愿意用我几乎所有的时间货币去换的一个创业项目。但在确定了人生目标的今年一月份，我几乎没有花什么时间就确定了一个项目的大方向，这个商业项目的创意像是奔着我而来的。然后再通过不断的完善从一个不成熟的产品渐渐变成一个成熟的产品，一个真正可以持久给所有人带来价值的产品。\\n\\n所以，可能和许多我很尊敬的朋友不同，我的出发点并不是“慈善”和“义务服务”，“创业”也从来都不是我的目标（一个学了四年金融的人怎么可能一直心存“创业”这个目标呢），我的目标就是实现“成长，并帮助别人成长。体验和经历生活，并帮助别人体验和经历生活。”\\xa0简单的说，我的内心并没有一个声音告诉我“你一定要创业、你一定要创业”，只是碰巧创造一个商业化的项目是实现这个目标最好的方式，而创立一个商业项目这件事碰巧叫做创业。\\n\\n而另一方面，在黑石工作可以帮助我“成长”和“经历”，但是我觉得在黑石的一个暑假实习里，我用20%的时间经历了接下来的两年里可能会经历的80%的体验，对我来说已经很值得了。我也一定会“成长”，但是未必会比创业成长得更快、更深刻、更理想、更多样化（比如说我就没有办法做我一直很想做的美工设计工作了！）。最重要的是，我意识到在黑石我基本上不能实现我人生目标的另外50%--“帮助别人成长。帮助别人体验和经历生活”。所以结果就是，“是否辞去毕业后的第一份工作，直接成为无业游民”这么重大的一个选择，被我用人生目标给瞬间解决了。有多瞬间呢？我后来发现了个有趣的巧合。\\n\\n四年前，我曾经尝试着去写一篇回忆录，来回忆出国两年多的旅程，然后这篇回忆录不幸地才写到出国的第一年就没有后来了。尽管写回忆录是一件有点折磨人的事情，但读回忆录绝对是件超开心的事。当中我写到过六年前我决定放弃轻松进北大清华的机会，毅然决定出国念高中，因为上海中学不支持孩子们申请国外大学。原文如下：\\n\\n“北大清华这种学校我肯定不去!”我当时的有两个很简单也很清晰的想法：一，I\\xa0deserve\\xa0the\\xa0best\\xa0in\\xa0the\\xa0world，二，也是更重要的想法，我想，就算最终在美国毁了，我至少做了一个帅到五体投地的决定，我鄙视了北大清华。更离奇的是，从那以后的两年至今，我几乎从来没有为这个决定后悔过，也不觉得这有什么好想的。仿佛这道选择题是在侮辱我的智商而不是测试我的智商一样。无论如何，两年后的现在，我相信，这个帅到五体投地的决定，是我一生至今最正确的决定。”\\n\\n这个故事告诉我们：人是不会变的。把上文中的北大清华换成黑石，就是我的大脑在半秒中以内做出辞职这个决定的思考流程。可见大脑在考虑一些人生大事上是不怎么需要运作的，让心去运作就足够了，而你的人生目标就是你的心。\\n\\n如果说这六年里，相比上面这段话我又多了什么领悟，那就是（1）一个人生目标（2）人生没有任何决定是错误的，因为你永远无法知道另外一个选择是否是正确的。\\n\\n撞上的许多堵墙\\n\\nRandy\\xa0Pausch在他著名的“最后的演讲”中提到过一个很实在的观点。他说，在我们追寻理想的道路上，我们一定会撞上很多墙，但是这些墙不是为了阻挡我们，它们只是为了阻挡那些没有那么渴望理想的人们。这些墙是为了给我们一个机会，去证明我们究竟有多想要得到那些东西。\\n\\n我撞上的第一堵墙，就是我没有如我所愿地一毕业就辞职。考虑到团队开发的进度，个人诚信问题方面带来的压力，家庭的压力，以及很多直接辞职可能带来的负面因素，我最终还是回去工作了四个月才得以正式辞职，其中包括一个月的培训。很长一段时间里，大老板都不允许我告诉任何人我辞职的事情，但大老板自己却没有做好保密工作，以至于同事们最终都知道了我一个小小的分析师要辞职。但我又被规定不能公开，所以在我座位附近的办公室气氛很糟糕，上班感觉度日如年。当中还穿插了许多压力山大的故事，比如我遇上了公司最高管理层一年一度的3v1谈话，在一个阳光明媚的下午被三个在华尔街响当当的名字各种拷问，因为我光荣成为了公司历史上第一个干都没怎么干就宣布不干了的分析师（从小到大，坏孩子光荣榜上真是永远有我的名字）。又比如曾经跟我关系很好的一个VP整整四个星期把坐在整个办公室出入口的我当空气。但是无论当时多煎熬，现在想来都是非常独特的人生经历。\\n\\n其实我很感谢和尊敬黑石，不仅因为我仍然是个热爱金融的家伙，更因为每一个我接触过的同事的做事风格都对我的个人风格产生了一定程度的影响。从情感的层面上，我最感激的是负责团队人事的韩国VP，在我辞职的过程中帮我做了许多疏通的工作。在我离开的前两天的晚上，他说了一句我印象很深的话。他说，“Denny，你知道，作为你的上司，这次我面对着一个选择，是照顾公司的利益还是你一个年轻人的利益。我选择了后者。我希望你以后不用面临这样的选择。但如果你有一天遇上了，我希望你可以跟我做一样的决定。”\\n\\n我离开的那一天，我的同事和几个以前一起共事过的朋友给我发来了道别邮件。让我很高兴的是，他们在祝福中都用了同一句话“You\\xa0are\\xa0very\\xa0brave”（“你很勇敢”）。之所以高兴，是因为无论今后的道路如何艰难，至少在旅程的起点我实现了奥巴马用来形容乔布斯一生的第一个形容词。对于一个活在当下的傻子来说，这已经足够了。\\n\\n现在我在上海的家中，和我非常喜欢而且非常有创造力的人们一起工作。虽说生活条件很普通（以银行家的标准来说的话简直是糟糕透了），虽说工作强度和时间依然和在黑石的时候差不多（以银行家的标准来看的话处于中上水平），但回到上海后的这段日子确确实实是我人生中自我学习曲线上升最快的一段日子。所以顺便说一个建议，当那些备受尊敬的金融机构告诉你为什么要选择他们的时候，特别是关于学习曲线的那些理由，不要那么快就为之屈服。他们不仅有可能（虽然也仅仅是可能）在推销给你一些你并不需要的东西，并且他们永远不会带你看清楚这个世界上全部的可能性。你要跳出“密集网络”，自己去看清楚。这个建议出自依然热爱金融的笔者。\\n\\n我一年的故事就这么讲完了。如果回顾总结过去一年的人生，那么最好的形容就是从一年前我确定了人生目标的那天起，一切就开始失控。但我想在这个回顾的最后，和所有已经确定了自己人生前进方向的朋友，分享这一年最大的感想：你的理想就像一辆车，如果你觉得这辆车的一切都在你的控制之中，那么可能说明你开得还不够快\\xa0(Your\\xa0dream\\xa0is\\xa0like\\xa0your\\xa0car.\\xa0If\\xa0you\\xa0are\\xa0in\\xa0full\\xa0control\\xa0of\\xa0it,\\xa0you\\xa0are\\xa0not\\xa0driving\\xa0it\\xa0fast\\xa0enough)。\\n\\n关于感谢\\n\\n感谢所有支持你、欣赏你、否定你、看低你的人。\\n\\n我一直说，永远不要忘记你从哪里来，要到哪里去。我不是一出生就上了好到可以改变我的学校，一直到六年前，我都不算是个好学生，学生生涯当过的最高的职位是小队长，期中期末考试好像从来没有进过班级前三，有一年甚至还是全校倒数10%，更不知道自己要什么。感谢自己不知为什么突然一根筋地开始愿意好好努力，自从那以后就知道实现梦想就靠坚持付出，没有别的秘诀。后来我出国，看到了一个很大很大的世界，在一路的坚持中，遇上了许许多多带给我灵感的人，他们用他们的经历影响和改变了我。这就是为什么我一直告诉自己不要忘记你从哪里来，这也是为什么我想继续传播我受到的影响，可能是作为一种感谢。\\n\\n今年上半年还在上创业课的时候，我一边要照顾自己的项目的开发，另一边又创业课项目团队中的其他四个成员眼看即将毕业，完全不作任何事情。我的教授Gelburd，一个前创业家，也是我在沃顿的第二个导师，他并没有因为我一个人担纲整个项目的开发和准备而减轻对我们团队的项目的要求，但是他给了我很多鼓励。期末演示日的那天，我在一天有三个期末演讲的情况下，被迫一个人完成了80%的项目演示。没有什么奇迹，我们的质量肯定不是最好的。但在我毕业的前几天，我收到了这门课的成绩。Gelburd给了我A+。他写了一封感谢信给他，他回复我说，每一年上这个课的学生中，真正去创业的不出三个，I\\xa0think\\xa0you\\xa0will\\xa0be\\xa0come\\xa0very\\xa0successful。\\n\\n收到他的邮件，我告诉我自己，绝对不能辜负曾经看好你的人。哪怕只有一个看好你的人，为了那一个人，你都必须要坚持下去。\\n\\n同样地，过去的许多年里，我被许许多多人否定过，甚至包括身边很好的朋友。从五年前的：“就你也想进沃顿？”一直到几年的：“你还是别创业了吧”，“你肯定不会辞职的”等等。这些否定和质疑一路上给我很大的鼓舞，让我很清楚的知道什么是我真正想要的。\\n\\n在美国的这六年，我最大的幸运就是遇到了许许多多强大的人，他们强大的地方可能是一些人生经验，可能是一个很偏门的技巧，有或者是一个很奇怪的逻辑，一个坚持了几十载的生活细节。今后我会一一道来。\\n\\n在这里，我想特别感谢Stacy，你是我出国最早认识的朋友之一。是你对音乐的坚持让我看到这个世界是如此的精彩。还想特别感谢袁帅、甄欢、柳潼、质含、瑞之、盛杰、和筱纯，感谢2009年的时候你们愿意和我一起把BIMP这个项目做起来。这是理论上我的第一个创业项目，按照BIMP现在的强大程度，它必然只会越来越强大，我真心希望它会给更多的对金融真正感兴趣的孩子带去帮助。\\n\\n最后，特别匿名感谢所有从今年一月开始到今天，给目前还处于隐形状态的小网站提供过帮助的人们。无论你现在是否在和我们一起并肩作战，我们始终是一个团队。\\n\\n最后，两个改变你生活的礼物\\n\\n其实我一直准备了两个礼物。这篇这么长的文章，所有因为一些共鸣或者因为一丝共同的信念而坚持看到了这里的人们，这两个礼物会改变你们的生活的。\\n\\n拆开第一个礼物，是一首旋律很简单的歌，来自Cat\\xa0Steven，叫做\"If\\xa0You\\xa0Want\\xa0To\\xa0Sing\\xa0Out,\\xa0Sing\\xa0Out.\"\\n\\n歌词简单的甚至不需要任何中文翻译：\\n\\nWell,\\xa0if\\xa0you\\xa0want\\xa0to\\xa0sing\\xa0out,\\xa0sing\\xa0out;\\xa0And\\xa0if\\xa0you\\xa0want\\xa0to\\xa0be\\xa0free,\\xa0be\\xa0free;\\n\\nCause\\xa0there\\'s\\xa0a\\xa0million\\xa0things\\xa0to\\xa0be,\\xa0you\\xa0know\\xa0that\\xa0there\\xa0are.\\n\\nAnd\\xa0if\\xa0you\\xa0want\\xa0to\\xa0live\\xa0high,\\xa0live\\xa0high;\\xa0And\\xa0if\\xa0you\\xa0want\\xa0to\\xa0live\\xa0low,\\xa0live\\xa0low;\\n\\nCause\\xa0there\\'s\\xa0a\\xa0million\\xa0ways\\xa0to\\xa0go,\\xa0you\\xa0know\\xa0that\\xa0there\\xa0are.\\n\\nYou\\xa0can\\xa0do\\xa0what\\xa0you\\xa0want,\\xa0the\\xa0opportunity\\'s\\xa0on;\\xa0And\\xa0if\\xa0you\\xa0can\\xa0find\\xa0a\\xa0new\\xa0way,\\xa0you\\xa0can\\xa0do\\xa0it\\xa0today.\\n\\nYou\\xa0can\\xa0make\\xa0it\\xa0all\\xa0true,\\xa0and\\xa0you\\xa0can\\xa0make\\xa0it\\xa0undo.\\n\\n如果你还很难看到\"there\\'s\\xa0a\\xa0million\\xa0things\\xa0to\\xa0be,\\xa0and\\xa0there\\'s\\xa0a\\xa0million\\xa0things\\xa0to\\xa0do\"，那么请拆开第二个礼物。\\n\\n第二个礼物将帮助你看到生命中的无限可能。这是一个明年一月才会开始邀请测试的网站，这个网站是我为我自己创造的，但如果你看到了这里，那么它也是为你创造的。怎么来形容这个网站呢？\\n\\n对于互联网行业研究者来说，它和移动互联网、移动应用、云服务等当下潮流一点关系都没有，也没有任何国外的成功案例或者相同模式可以C2C（Copy\\xa0to\\xa0China）。所以，它可能很无聊。\\n\\n但对于普通用户来说，我的目标是让这个网站做到以下三点：\\n\\n1.\\xa0它要很好看、很酷、很好玩，像一个游戏一样\\n\\n2.\\xa0它要颠覆现有网站的传统，要很真实，几乎和现实一样真实，真实得以至于它根本不像一个网站\\n\\n3.\\xa0它要实实在在地帮助用户获得让生命更有价值的经历\\n\\n它叫做“连客”，来源于英语单词“Link”。它会帮助用户将过去和未来的经历连接在一起，让用户看到生命中的无限可能。就像乔布斯说得那样，“连接人生中的那些点”。\\n\\n我知道，以上的一切听上去很不靠谱。但没关系，它若存在一天，就会给用户带来一天价值，给这个社会带来一天的活力。\\n\\n过去一年里大家都有各种各样精彩的故事，而我所有的故事都是围绕着这个网站。', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d6bc588f-8c2c-4e21-868c-1fa3f3f4fca2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n杂 20\\n\\n去不久的一个晚上，由于下大雪，没有人上班，民宿老板自己来店里看店，夜晚我们两个人就喝着茶攀谈了起来。他跟我说，其实人不一定非要前往什么地方，前方的生活也不一定就会和眼下的生活不一样。\\n\\n他说，很多人随波逐流，不过是用大多数人走的路来缓解一下内心的焦虑。迷茫是人的常态，而破解的方法就是专注当下，过好眼下每一刻。因为眼下每一刻就是你的全部生活，而生活就是全部的意义。\\n\\n你遭受了痛苦，你也不要向人诉说，以求同情，因为一个有独特性的人，连他的痛苦都是独特的，深刻的，不易被人了解，别人的同情只会解除你的痛苦的个人性，使之降低为平庸的烦恼，同时也就使你的人格遭到贬值。 --尼采 《快乐的知识》\\n\\n“早晨，我睁眼醒来翻身下床，又变成了原来那个浅薄无知、善于伪装的滑稽角色。胆小鬼连幸福都会惧怕，碰到棉花都会受伤，有时也会被幸福所伤。趁着还没有受伤，我想就这样赶快分道扬镳。我又放出了惯用的逗笑烟幕弹。”--太宰治《人间失格》\\n\\nO Liberté, que de crimes on commet en ton nom!（“自由，多少罪恶假汝之名以行！”）\\n\\n真实的、永恒的、最高级的快乐，只能从三样东西中取得：工作、自我克制和爱。\\n\\n—— 罗曼•罗兰《托尔斯泰传》\\n\\n想象世界是我们用以应对和思考外界事物的媒介，它是现实世界的简化版。\\n\\n前面的“人如同被设定好的程序”这种消极的认知，其实是因为我在“想象世界”中进行思考和推导的产物。\\n\\n如果人的思考始终只停留在自己简化的想象世界的逻辑中，那么他的思维就容易变得越来越狭隘，越来越抽象，越来越形而上。\\n\\n同时，当我们试图表达自己的某种人生体悟时，往往这种体悟都会附带着一种“情绪”。一种或是平静，或是淡薄，或是忧伤的、总之好像产生这种体会就代表着你似乎找到了人生真理式的情绪。\\n\\n但其实不是。\\n\\n这样的“顿悟”我已经经历过很多次了。所以它告诉了我一个经验:如果你在某种特殊情境下产生了某些特别的感触和见解，其实一般这不代表着你找到了真理。\\n\\n这种情况往往有可能是类似于文艺青年读了一本浪漫的书而产生的自我感动。\\n\\n那是一种“自我感动”，不是“真理”。\\n\\n真理无法用语言描述。\\n\\n而且，真理也无法用简短的几千几百个字就表达出来。\\n\\n它是一种复杂的完全超越语言和顿悟的东西。\\n\\n我还意识到，其实我一直都在潜意识里不自觉的追求这种“顿悟”，和“找到了真理”的感觉。\\n\\n而这种一直深藏在潜意识里的追求，其实是龟毛兔角，试图蒸沙成饭。\\n\\n真理是不需要、也没办法主动的刻意追求来的\\n\\n**WHEN IT CAME**\\xa0rolling in, the\\xa0money\\xa0affected us all. Not much, and not for long, because none of us was ever driven by\\xa0money. But that's the nature of\\xa0money. Whether you have it or not, whether you want it or not, whether you like it or not, it will try to define your days. Our task as human beings is not to let it.\\n\\n当事情不断向前发展的时候，金钱对我们所有人都产生了影响。影响不是很大，而且持续时间也不长，因为我们都没有被金钱控制。但这是金钱的本质，无论你是否拥有，你是否想要，你是否喜欢，金钱都会对你的生活进行定义。我们的任务就是阻止它\\n\\n「鞋狗」\\n\\n对于快乐，因为我们能够意识到它的虚幻性，所以会更加合理的去追求。\\n\\n这个合理是什么意思呢？\\n\\n1. 我们会保证我们所追求的快乐不会伤害到我们自己。\\n\\n谈到这一点，很多人都能理解追求吸毒的快乐会伤害自己，这是显而易见的。但其实还有很多本质上和吸毒是类似的东西我们并没有意识到，比如说追求一个不可能和我们在一起的人，刷微博上瘾而过度熬夜等等。\\n\\n1. 我们会拥有耐心。\\n\\n比如说一个目标的达成需要半年的时间，过去我们可能往往还不到一个月就开始心急如焚，迫切的恨不得直接跳到未来去。\\n\\n这种急迫的只想要结果的心情，会大大的破坏我们做事情的质量，同时也会给我们带来很多的焦虑。\\n\\n但是当我们用正念的态度活在当下时，那种迫切的想要达成目标的心态就会慢慢消失。其实用“拥有耐心”这个形容词并不准确，因为耐心这个词还是在暗示着我们在忍受着想要尽快达成目标的心情，但是实际上，正念并不会令我们拥有“耐”心，因为我们根本就不是在忍耐，我们只是更清楚的看清了事实，知道了迫切的心情是有害无益的，所以我们从根本上就没有了这种对目标的迫切需求。\\n\\n1. 我们会有意识的从对无常快乐的追求循环中逐渐跳脱出来。\\n\\n跳脱出来不意味着不再追求快乐，而只是说我们不再用无常的心态去追求无常的快乐，而是用恒常的心态去追求恒常的快乐。\\n\\n想要理解这一点，我们可以想一想我们每个人身边都有的学霸，或者是那些做出了巨大成就的人，他们无一不是拥有超强的坚持的能力，对着自己的目标从不放弃的不懈的追求，而且很能忍受无聊和痛苦。\\n\\n在别人看来，可能会认为这些人是忍受了巨大的痛苦才能保持这样的自律，但在我看来恰恰相反，他们是因为比别人更智慧的，掌握了追求更大快乐的方法，所以才会那么的自律。\\n\\n他们知道散乱的目标只会带来微小的快乐，但付出的代价却是N倍的焦虑和人生的空虚；\\n\\n他们知道一时的懒惰和放弃带来的享受，还比不上坚持之后获得收获快乐的万分之一；\\n\\n别人认为他们日复一日的在那里学习，创业，做研究是很枯燥的事情，但在他们的自己感受中，那并不是为了成功而付出的代价，而是他们在做的那些事情本身就在给他们带来快乐，只是别人认为那是枯燥的而已。\\n\\n同时，我们每个人也都在潜意识里特别的害怕浪费时间，害怕虚度了自己的人生。所以我们想要同时做很多事情，当你在跑步的时候听歌，看电视的时候玩手机，听课的时候打游戏，这会给你一种好像是一份的时间却赚到了两份的体验的错觉。\\n\\n这种错觉是如此的强大，虽然实际上因为我们的精力没有充分的投入而导致两件事情都没有做好，但我们还是会乐此不疲。\\n\\n当然分心也有一定的好处，就是在你做一件不喜欢，但又不得不做的事情的时候，分心可以令你感受不到逼迫自己的痛苦，能够令难熬的时间变得更快一些。\\n\\n但事实上这种所谓的好处也根本不值一提。因为真正解决这种问题的方法，是要么我们令自己喜欢上并且去享受这件事，要么是我们尽可能的减少生活中的这些不得不做的事情\\n\\n这反过来也可以解释为什么那些目标明确，执行力很强的人一般都内心强大。因为他们意志力集中，直接就把很多的心理和情绪上的问题给覆盖了。而不是像我们一般人一样，整天在很多的问题里纠结着而什么都不去做，于是你越是什么都不做，堆积的问题和负能量就越多。\\n\\n并不是说你开始行动了就能解决或者消除那些负能量，而是说当你开始专注的行动时，那些负能量对你来说就是不存在了的\\n\\n这也是为什么正念是一种“不加评判”的觉察，如果带有“评判”的话，那就还是“你的自我”在看，这种形式的觉察就不是一种正念，它就还只是一种大脑的思维活动和分析。\\n\\n当你不加评判的去觉察的时候，这时候就不是你在第一位去观察，而是把客观事实放在第一位，你自己的感受和思维放在第二位，然后通过第一位的客观事实，来纠正你的思维和感受\\n\\n在这里还要再提一句，很多正念练习都是从让你观察自己的想法或感受入手，但如果说，你之前就是一个有着过度的自我关注的人，你总喜欢想很多，总是喜欢分析自己，那么你的正念练习就不要从对自己思维和感受的观察入手，因为这很可能令你进行不下去，或者给你带来更多的分析和混乱。\\n\\n这个时候你可以去观察一些和自身没有直接相关，或者关联不大的外物上入手，比如尝试着不加评判的去观察你家的猫，观察外面的楼房，观察抽烟冒出来的烟雾等等。\\n\\n这样的练习首先就是在训练你把过去那种总是无时无刻都在关注自己的思维倾向给平衡过来。\\n\\n一个总是关注自己的人不仅会有很多痛苦，而且在他的内心当中会是几乎没有和外界的联结的。这种没有联结，就是由于缺乏对外界和客观现实的关注所直接导致的。\\n\\n另一方面，观察外物也是在清空我们自己的内心。你必须树立这样一种意识，对于一个总是过度自我关注的人来说，他对自身的分析和思考至少有60%以上是无用的、浪费时间的、甚至是有害的。\\n\\n因为如果你有着较强的执行力，那么你根本没有那么多的时间用在自我分析上；同时你也可以回忆一下，很多时候你对自己的分析只是为了缓解当时那一时的焦虑，在之后的很多时候你都是在不断地重复思考着已经想过很多次的东西，或者是制定着根本就不会去执行的计划。\\n\\n但是你也不要试图直接和自己过度的自我关注去做对抗，不要把它当成一个缺点或者弱点，忘掉它，忽视它，慢慢的训练自己关注自身和关注外界的平衡，只是要投入时间去进行自我训练，那么你过度自我关注的减少，这是一定的\\n\\n1. 正念不是一种能够直接消除你的负面情绪，不是能直接令你获得平静的方法。\\n\\n所以如果你是在心情烦躁的时候练习正念，不要有一种我是在用正念来消除负面情绪的这种自我暗示，正念只是令你保持客观。对于所有的负面情绪，我们是要去接受它，承受它，而不是逃避它。如果你把正念当成一种可以为你带来平静从而可以对抗和逃避痛苦的方法，那你的认知就出现偏差了。永远记住这一点，正念是面对，而不是逃避。\\n\\n1. 不仅仅局限在负面情绪这个层面，包括我们对自己的期许，对未来的渴望等等，很多的因素都会导致我们在进行正念练习的时候，不自觉的对正念产生很多的期待和自我暗示。如果你在练习正念的时候，发现自己在自我暗示可以通过正念不断变好，那其实你也是有一些走偏了。正念的确能够为我们带来很多的好处，但如果在练习的过程中你带着一种为了达成目的的期望去进行，这就不是正念了。\\n\\n在练习的过程中，你越是无所求，越是只是在纯粹的练习正念，纯粹的感受当下，正念的效果也就越好，你能够获得的利益也就越多。这是一个悖论，但事实又的确是这样。\\n\\n1. 正念不是在给你增加什么，恰恰相反，它是一个逐渐减少的过程。损之又损，以至于无为。\\n\\n对于小孩和和比较幼稚的人来说，他们会喜欢简化的思维方式。喜欢用单一的因素和变量去考虑问题。\\n\\n我举一个不恰当的例子，这个例子我们只是用来举例说明的，希望大家不要介意。\\n\\n比如说种族歧视，因为黑人和印度人的名声都不太好，所以有些人会避免聘用黑人和印度人，生活中也会避免和印度人接触。\\n\\n同样是对这两个种族的规避，有的人是因为觉得黑人和印度人都不好，是低劣的种族。\\n\\n而有的人这样做则是因为：黑人和印度人中，不靠谱的几率会更大，为了避免这种风险，所以我少和他们接触。\\n\\n虽然在最后的做法上，这两类人的行为是一致的。但是其中的逻辑完全不同。第一种人是基于一种幼稚的偏见，而第二种人是基于理性的思考\\n\\n再比如说我们玩游戏的核心目的是什么？如果你的第一目的就是随便玩玩，那么当队友不给力，队友喷你的时候，你可以随心所欲的喷回去；\\n\\n但如果你的根本目的是为了取得胜利，那么队友不给力的时候你要鼓励，别人吵架的时候你去调解，别人骂你的时候你不回喷。\\n\\n第一性原理不仅仅是我们用来分析问题的方法，它更重要的是，可以应用在我们自己的生活中，令我们的生活回归理性，拥有一个根基。\\n\\n我们自己打算做一件事情的时候，常常问自己一个问题：我做这件事情最根本的目的是什么？\\n\\n因此假如是我们自己就是这个从贫穷到富有的主人公，在过程中我们的心智只会紧紧地抓住年薪百万这个最终的结果，我们对结果的强烈执着，会令我们迫切的想要赶紧结束当下，迅速的获得那个最终的回报。\\n\\n而“渐进式的思维”就是通过有意识的自我提醒，令我们从对最终结果的执着当中解脱出来。怎么去形容这种感觉呢？就像是以前你是在悬崖上通过一根钢丝想要到达彼岸，而渐进式思维给我们的感觉是在造一座桥，不着急，非常稳的一步步达到那个结果。\\n\\n结果是重要的，但过程也同样重要，在“渐进式思维”中过程和结果是一体的，做好过程就是获得好的结果，获得好的结果就要做好过程。\\n\\n在最终的结果上，渐进式思维和活在当下那节课一样，都能够令我们专注于当下，但是这两种理念令我们到达当下的逻辑和路径是不一样的。\\n\\n渐进式思维是通过强调当下和未来之间的联结，令我们意识到做任何事都不能一蹴而就，当下就是未来，未来就是当下，从而令我们不再为未来感到焦虑\\n\\n云认知就是对思考的思考，有点类似于正念的一边生活一边对自己的念头保持觉察。\\n\\n但是元认知和正念的区别在于，元认知更加强调思考的实用性。\\n\\n元认知主要包含两个方面，第一个方面就是我们前面提到的对思考的思考。具体又可以分为三块：对自己的思考，对思考对象的思考，对思考过程的思考。\\n\\n对自己的思考指的是我们对自身的认识，我们的优势是什么，缺点在哪里，兴趣有哪些，不同兴趣的强度有什么区别等等，基于对自身各种特质的了解，我们才能更好的去扬长避短。\\n\\n对思考对象的思考，我要做的是一件什么性质的事，做这件事的目的是什么，达成这个目的可以分为哪些步骤，其中有哪些障碍，做这件事我有哪些优势可发挥，哪些劣势需要规避等等。\\n\\n对思考过程的思考：我思考问题的过程中有哪些倾向，有哪些不足，我容易忽略什么地方，我容易过度看重哪些地方，我的思考策略有哪些地方可以优化等等\\n\\n最后我想总结一下，虽然我们这个系列课看似谈了很多东西，但是贯穿在这些内容背后，我真正想强调的就三个东西：建立明确的、但不急于求成的目标，慢慢来，和坚持。\\n\\n这三个东西已经泛滥在上个世纪的心灵鸡汤中了。现在的很多人喜欢听新潮的，包装的很华丽的知识。但是永远要记得，我们学任何东西的根本目的是为了我们自己，有没有用这才是最根本的，什么是有用的这个也只有我们自己知道。千万不要沉迷于知识的华丽而让自己成为了银样镴枪头。一个人只要不欺骗自己，他就处在了最好的状态之中\\n\\n对 经 理 的 行 为 感 到 如 此 反 感， 以 至 于 失 去 了 工 作 的 欲 望， 而 这 份 工 作 正 是 自 己 心 仪 已 久 的。 她 正 计 划 辞 职， 离 开 她 所 喜 欢 的 工 作。 但 当 她 冷 静 下 来， 客 观 地 进 行 思 考， 发 现 她 的 经 理 也 有 自 己 的 优 点。 但 是， 细 想 一 下， 经 理 有 些 时 候 确 实 显 得 斤 斤 计 较， 令 她 实 在 反 感， 比 如 他 只 知 道 训 诫 他 人， 把 他 人 拒 之 门 外。 对 贝 伦 妮 丝 来 说， 要 她 从 另 一 个 角 度 看 待 此 事 似 乎 不 太 现 实。 而 且， 从 这 样 一 个 动 作 迟 缓 的 人 身 上 找 到 优 点 并 向 他 学 习， 更 是 一 个 不 可 能 完 成 的 任 务！ 然 而， 她 尝 试 着 接 受， 并 且 开 始 训 练 自 己 冷 静 下 来。 在 此 期 间， 即 便 是 与 自 己 的 价 值 观 有 所 违 背， 她 也 能 够 发 现 经 理 身 上 的 优 点， 并 且 认 识 到 他 动 作 迟 缓 这 一 缺 点。 随 后， 她 恢 复 了 平 静， 意 识 到 经 理 的 沟 通 方 式 确 实 欠 缺 了 一 点 分 寸。 对 她 来 说， 她 只 是 单 纯 觉 得 经 理 有 点 没 礼 貌。 她 也 知 道 经 理 就 是 这 样 的 人， 也 不 会 去 改 变 这 点， 不 过 经 理 也 有 不 错 的 品 质， 比 如 率 性， 在 老 板 面 前 很 忠 诚， 工 作 上 又 很 勤 奋。 现 在 回 想 起 来， 她 可 能 意 识 到， 是 自 己 的 敏 感 导 致 她 有 如 此 强 烈 的 反 应， 然 而 她 的 这 种 一 贯 敏 感 的 反 应 让 自 己 完 全 无 法 注 意 到 经 理 的 优 点。 仿 佛 有 一 团 浓 厚 的 云 雾 围 绕 在 她 的 上 司 周 围， 遮 盖 住 了 其 优 点， 而 只 突 显 出 了 他 行 动 缓 慢 的 一 面。 对 于 她 对 经 理 如 此 断 章 取 义 的 理 解， 她 笑 了。 她 意 识 到 自 己 发 现 了 经 理 身 上 的 一 个 优 点， 这 看 起 来 是 有 点“ 强 迫”， 但 是， 毕 竟 她 做 到 了 这 一 点， 要 知 道， 要 想 他 们 之 间 进 行 交 流 是 非 常 困 难 的。 另 外， 贝 伦 妮 丝 成 功 摆 脱 了 因 为 自 己 的 敏 感 所 带 来 的 负 面 影 响： 她 找 回 了 原 来 的 冲 劲， 继 续 留 在 她 所 热 爱 的 工 作 岗 位 上， 并 承 诺 要 有 礼 节 地 跟 她 那 动 作 缓 慢 的 经 理 进 行 沟 通。\\n\\n第 二 阶 段 中， 当 你 能 忍 受 与 自 己 价 值 观 相 反 的 人 见 面 的 频 率 从 5 次、 10 次、 15 次 增 加 到 20 次 的 时 候， 你 的 思 维 就 自 然 而 然 地 发 生 了 转 变。 通 过 大 量 实 践 训 练， 你 重 新 获 得 了 平 静。 一 步 一 步 地， 你 的 姿 态 开 始 发 生 改 变。 当 你 面 对 与 你 价 值 观 相 反 的 人， 总 是 保 持 着 笑 脸 相 迎， 你 能 够 感 觉 到 自 己 在 尝 试 改 变， 然 后 你 很 快 就 会 问 自 己：“ 这 样 做 的 好 处 是 什 么 呢？” 这 种 思 考 已 经 形 成 了 一 种 正 能 量， 它 能 缓 解 你 的 压 力， 从 而 使 得 沟 通 更 加 顺 畅。\\n\\n这 种 情 况 只 发 生 在 第 三 阶 段。 当 我 们 定 期 训 练 改 变 自 己 的 想 法 时， 这 种 心 理 锻 炼 也 就 成 为 一 种 习 惯。 在 面 对 与 自 己 价 值 观 背 道 而 驰 的 人 时， 我 们 采 取 客 观 的 态 度， 保 持 镇 静 的 方 式。 刚 开 始 时， 你 需 要 3 个 小 时、 5 小 时、 24 小 时 甚 至 更 长 的 时 间， 才 能 达 到 平 和 的 状 态， 现 在 你 能 在 2 分 钟 内 就 完 成 这 项 任 务。 然 后， 你 可 以 在 15 秒 内 控 制 压 力 上 升， 找 到 另 一 种 降 压 的 方 式。 直 到 你 的 反 价 值 观 念 已 经 没 有 当 初 那 么 强 烈。 随 后， 你 获 得 了 一 种 能 力， 那 就 是， 在 与 他 人 一 同 工 作 时， 即 使 他 们 的 态 度 让 你 不 开 心， 你 也 不 会 因 此 生 气， 因 为 你 采 取 了 相 应 的 行 动。 这 说 明， 无 论 在 什 么 情 况 下， 你 都 已 经 学 会 了 如 何 管 理 自 己 的 压 力， 并 保 持 愉 快 的 心 情。\\n\\n1. 首 先， 我 们 必 须 意 识 到 他 正 处 于“ 红 灯” 状 态 之 中。 要 做 到 这 一 点， 你 需 要 用 眼 睛 留 心 观 察， 用 耳 朵 仔 细 倾 听。 通 常 情 况 下， 在 面 对 没 有 开 口 讲 话 的 人 时， 我 们 会 意 识 到 他 所 处 的 状 态。 但 很 多 时 候， 有 一 种 人 所 处 的 状 态， 我 们 很 难 觉 察 出 来。 他 们 对 一 件 事 情 的 表 现 常 常 很 极 端， 比 如 态 度 模 糊、 自 我 反 省、 紧 张、 纠 结 和 不 安。 他 的 行 为 死 板 且 固 执， 他 的 想 法 就 更 不 用 说 了。 值 得 注 意 的 是， 如 果 你 自 己 处 在“ 红 灯” 状 态 中， 可 能 需 要 花 费 更 多 的 时 间 去 观 察。 当 你 被 自 己 的 行 为 冲 昏 头 脑 时， 你 将 无 法 进 入 到 自 我 调 节 系 统。 当 出 现 这 种 情 况 时， 你 就 要 打 起 精 神 了： 你 们 之 间 的 沟 通 开 始 变 得 不 顺 畅， 甚 至 气 氛 有 点 紧 张， 矛 盾 会 像 触 电 一 样 一 触 即 发。 你 想 说 你 是 对 的， 但 他 的 话 使 你 很 生 气， 又 或 者 让 你 很 想 哭、 想 要 放 弃…… 请 记 住：“ 说 服” 一 词 分 开 来 讲 的 话， 要 先“ 说”， 对 方 才 会“ 服”。 2. 重 新 寻 找 自 身 的 平 静： 深 呼 吸， 暂 停 几 秒 钟 或 者 选 择 适 合 你 自 己 的 训 练。 3. 听 听 他 人 怎 么 说。 4. 告 诉 他， 你 明 白 他 的 意 思。 第 一 时 间 站 在 他 的 角 度 思 考， 不 要 吝 惜 多 说 几 次：“ 是 的， 我 明 白 了。” 5. 采 用 调 解 和 谦 虚 的 态 度 来 促 进 自 身 平 静。“ 是 的， 你 说 得 对。” 6. 当 他 在“ 红 灯” 模 式 时， 你 需 要 引 导 他 学 会 跳 出 自 己 的 情 绪， 进 行 理 性 思 考。 对 此， 带 着 善 良、 智 慧 且 沉 着 的 心 态， 问 他 一 些 开 放 性 问 题， 引 发 他 进 行 思 考。 为 了 进 一 步 推 动 训 练， 你 需 要 观 察 自 己 面 前 这 个 人 所 处 的 状 态， 并 根 据 他 压 力 的 大 小， 采 取 正 确 的 态 度 和 沟 通 方 法。 在 这 种 情 况 下， 以 下 管 理 手 段 对 你 应 该 会 有 所 帮 助。\\n2. 告 诉 这 个 人 你 理 解 他， 给 他 一 些 时 间 和 空 间。（ 比 如： 打 开 窗 户， 或 让 他 出 去 走 走， 透 透 气。 告 诉 他， 如 果 没 有 什 么 紧 急 状 况， 看 看 是 否 可 以 把 会 议 推 迟 几 分 钟， 以 便 他 可 以 放 松 一 下。）“ 别 着 急， 给 你 一 点 时 间， 你 先 冷 静 一 下。 你 可 以 出 去 转 一 圈， 晚 点 我 们 再 谈 这 个 问 题。 我 随 时 有 空。” 即 使 你 没 有 时 间， 但 眼 前 最 重 要 的 是， 你 要 告 诉 他 这 件 事 并 没 有 那 么 紧 急， 这 是 安 抚 他， 使 他 冷 静 下 来 的 最 佳 方 法。 相 反 地， 你 越 对 他 施 加 压 力， 对 他 说：“ 加 油， 专 心 一 点， 打 起 精 神 来， 我 们 没 有 时 间 了， 你 现 在 必 须 镇 定 下 来！ 别 无 选 择。” 他 就 会 变 得 越 发 烦 躁 不 安， 并 且 会 产 生 抵 触 情 绪。 2. 把 自 己 当 作 他 的 同 伴， 帮 助 他 组 织 自 己 的 想 法， 并 且 把 这 些 想 法 表 达 出 来：“ 我 理 解 你。 我 们 都 清 楚， 这 真 的 太 复 杂 了。 我 是 你 的 盟 友。 要 知 道， 我 是 不 会 站 在 你 的 对 立 面 的…… 你 想 向 我 解 释 一 下， 到 底 发 生 了 什 么 吗？ 你 在 担 心 什 么？ 怎 么 才 能 看 清 这 一 切？ 你 想 要 做 笔 记 吗？ 或 者 列 个 单 子？” 3. 通 过 问 他 一 些 开 放 性 问 题， 让 他 回 答 得 更 加 清 楚 详 细， 通 过 他 的 回 答， 帮 助 他 找 到 关 键 点 和 解 决 方 案。 记 住 要 友 好 一 点， 因 为 任 何 带 有“ 威 胁” 的 迹 象， 都 会 让 他 再 一 次 陷 入 压 力 之 中。 例 如“ 有 什 么 可 以 帮 助 你 的 吗？ 你 需 要 什 么 吗？ 你 认 为 呢？ 怎 么 才 能 帮 助 你 安 排 好 这 一 切？ 你 接 下 来 打 算 怎 么 做？ 你 需 要 借 助 什 么 工 具， 才 可 以 开 始 这 项 工 作？”\\n3. 首 先， 从 头 至 尾 认 真 地 倾 听 他 人 说 话。 2. 跟 着 对 方 的 方 向 走， 并 且 告 诉 他：“ 我 明 白 了。 我 知 道 你 很 生 气。” 并 指 出 你 觉 得 他 在 意 的 点：“ 如 果 我 理 解 得 没 错 的 话， 当 这 样 或 那 样 的 事 发 生 后， 你 认 为 这 是 不 公 平 的/ 没 办 法 让 人 信 服/ 结 果 不 那 么 令 人 满 意。” 你 将 会 听 到 令 自 己 很 意 外 的 一 句 话， 那 就 是：“ 你 说 得 对。” 思 想 辩 论 的 目 的， 并 不 是 弄 明 白 谁 对 谁 错， 而 是 要 让 人 摆 脱 抗 争 状 态， 进 而 冷 静 下 来。 当 他 重 返 平 静 时， 可 以 继 续 之 前 的 辩 论。 赞 同 某 人 观 点 的 好 处 是： 让 他 变 得 不 再 咄 咄 逼 人； 能 够 引 导 他 冷 静 思 考。 请 记 住： 如 果 我 们 处 于 一 种 愤 怒 的 状 态， 那 么 我 们 就 会 觉 得 自 己 是 受 到 不 公 平 对 待 的 受 害 者。“ 你 是 对 的” 这 句 话， 相 当 于 已 经 承 认 了 这 种 不 公 平 的 存 在。 必 要 的 时 候， 你 可 以 向 对 方 表 达 自 己 的 歉 意， 承 认 你 确 实 做 错 了， 对 他 说 道：“ 对 不 起， 我 错 了。 我 知 道 自 己 犯 了 一 个 严 重 的 错 误， 我 很 抱 歉， 请 原 谅 我。” 或 者 你 提 出 主 动 分 担 对 方 的 责 任：“ 我 承 认 在 这 点 上 我 弄 错 了。 请 原 谅 我。 我 马 上 改 正。”\\n\\n世上根本没有失败这回事，每一次所谓失败，其实都是在告诉我们哪些做法行得通，哪些做法行不通，仅此而已。就这样，不知不觉间，我学会了自我觉察，并开始运用练习的心态\\n\\n这时你可以运用我在《练习的心态》中所说的DOC法则：先做（Do），边做边察（Observe），然后修正（Correct），反复如此，不断提升水平\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4a227c26-f8a1-48bc-8d16-3e9f7e4649ae', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n你不能依赖于对一种思想进行纠错，来改变思想背后代言的东西。所以需要探究的不是，什么是真实的，而是我们如何相信和思考什么是真实的\\n\\n人们究竟是为了实现目标，还是只是为了借达到目标，享受中途的驱动力\\n\\n在古代人的历史观念中，“当下的时代”不过是以往时代的延续和重复，没有什么新奇之处，也就不值得特别的关注。在古代世界，包括中国，人们感知到的时间是不断在循环的。许多直接的自然经验都和人们的这种感受相吻合，比如日复一日的太阳升起又落下，年复一年的春夏秋冬四季轮回，王朝的由兴而盛、盛极而衰的更替……这些都对应着循环的时间意识，它们在学术界被称为“循环历史观”。\\n\\n**但到了文艺复兴，特别是在启蒙时代和工业革命之后，西方社会发生了剧烈变动，上述的“循环历史观”被改变了**。这在很大程度上是因为，人们对“当下的时代”表现出了越来越强的敏感。“当下的时代”不再是以往的延续和重复，而是前所未有的，是崭新的。因此，时间不再是循环往复的，而是线性展开的——从过去、到现在，然后通向未来，时间成为一个有方向的矢量概念。\\n\\n这种新的时间观念非同小可，带来几个重要的变化。\\n\\n第一，如果生活不是循环往复的，而是日新月异的，那么过去积累的经验就很可能是靠不住的，我们不能完全依靠传统习俗来引导生活。“当下的时代”，也就是“现代”，代表着一种对传统的否定甚至决裂的态度。 \\n\\n第二，时间观念的转变，推动我们从“厚古薄今”转向“厚今薄古”，认为当下以及未来要比过去重要。在这种时间坐标中，社会是从低到高、从野蛮向文明、从落后到先进发展的，具有不断进步的可能性。这就形成了所谓“线性进步的历史观”。\\n\\n第三，和这种时间观念相呼应的是，“现代”意味着对人的创造性和主体性的肯定，人类从循环历史宿命的束缚中解放出来，成为自由的、有目的的创造者，成为主宰自己命运的主体\\n\\n但我们现代人不太相信什么自然给定的意义了，哪有什么“天生如此”的事情呢？我们现在相信的是“我命由我不由天”。没错，我们抛弃了自然秩序这个神话，得到了自由。\\n\\n这并不是没有代价的。如果所有人都相信一个共同的神话，我们就有了关于好坏对错的共同标准。但失去了共同神话，无论是上帝也好，传统也好，天道也好，我们就会遇到一个问题：在价值与价值之间很难区分高低优劣，每一种价值都有自己的道理，彼此冲突的观念，常常谁也说服不了谁。\\n\\n共同的神话束缚了我们，却也让我们有了共同的准则。摆脱这个神话之后，我们有了自由，却又陷入混乱和茫然之中\\n\\n个人主观价值绝对提升，自然秩序被打破，理性秩序建立，这些都是古今之变的一部分。简单地说，古今之变，就是自然变成了不自然。\\n\\n这并不是说客观世界从自然变成了不自然，而是说我们看待世界的方式改变了。古代人相信有一个外在于人的自然秩序，这个秩序有它自身的目的和意义。但现在我们不再相信有什么上天注定的意义，我们相信意义是由人赋予的\\n\\n在科学研究中，人类是考察者，是主体，是英文中的“subject”，而自然世界成为我们考察的对象，是客体，对象和客体的英文都是“object”。人类身处自然之中，但在科学主导的思想观念中，人类从自然之中脱离出来，站在了自然世界的对面，形成了面对面的关系，而且人类处在积极主动的主体地位，自然处在消极被动的客体位置，人类与自然变成了主体与客体的关系。\\n\\n这带来了一系列的重要后果——人类开始探索自然，发现自然规律，利用自然资源，改造自然环境；最后人类征服了自然，成为自然世界的主人——这就是“人类中心主义转向”的意思\\n\\n**现代性带来的个体和群体层面的两个问题**\\n\\n第一个改变和挑战与个人生活的意义有关。如果人们不再相信神、不再相信传统、不再相信天道，那么该信仰什么呢？换句话说，人生活的意义是什么呢？我们用理性去回答这个问题，会发现非常困难，甚至无能为力，所以我们时常感到焦虑和空虚。我们该怎么面对这些精神困境呢？怎么找到生活的意义和理由？这是一个难题。\\n\\n第二个改变和挑战是社会生活的秩序。在以理性为基础的新秩序中，自然等级已经被瓦解，我们相信人人都是自由平等的，那么谁应当来统治谁呢？这时候统治和服从都需要理由，那么这个理由经得起理性的质疑和讨论吗？社会秩序就建立在我们对这些问题的回答之中。这是另一个难题\\n\\n和上述思想家相较，韦伯现代性论述的完整性体现在两个方面。首先，他不是简单肯定或全盘否定科学理性的意义，而是深入地分辨科学理性能做什么和不能做什么，而且在科学能做到的理性化后果中，同时阐明其正面的成就和负面的问题。其次，他对现代理性化的分析，涵盖了人类生活的三种主要关系——人与自然的关系，人与人的关系以及人与自我的关系，同时切入了现代人的精神信仰领域与现代社会的制度结构领域。\\n\\n总之，韦伯深刻把握了理性化对现代世界的塑造力量及其巨大成就，也敏锐地觉察到理性化造成的缺憾与弊端。在个体生活领域，理性化让现代人的心灵生活失去了对传统信仰的可靠倚傍，甚至会陷入精神危机；在公共生活层面，理性化倾向于将社会政治秩序蜕变为“现代的铁笼”，隐含着多种困境。而这正是现代性问题中两个重要的维度\\n\\n工具理性的问题是什么呢？它发展得太强大了，压倒、淹没了价值理性。社会的理性化发展，变成了工具理性的单方面扩张，理性化变成了不平衡的“片面的理性化”。在实践中，对手段的追求压倒了对目的的追求。\\n\\n比如，对于“人生目标”这种大问题，我们会发现，太沉重也太困难了。于是我们犯了拖延症，把目标问题不断向后推，先去加强工具和手段，转到工具理性的逻辑上来。\\n\\n就像现在流行说：先实现“财务自由”，再去追求“诗和远方”。但在实现财务自由的漫长过程中，我们关心的都是成本收益计算、效率最大化这些问题。结果是，这个漫长的过程会反过来塑造我们自身，最后我们变得只会赚钱。赚钱这件事，本来是手段，但我们为了找到实现目标的最优手段花费了太多的时间精力，陷入太深，以至于忽视了，甚至放弃了最初的目标\\n\\n到底什么是虚假的思想呢？尼采给出的答案是四个字，“形而上学”。你也许知道，形而上学是西方哲学最早的一个术语。“形而上”，顾名思义，就是“在实体之上”。尼采概括说，形而上学有三大信念：\\n\\n第一，相信在感知的表象世界背后有一个更真实的本质世界；第二，相信这个混乱的世界实际上是有目的的；第三，相信这个纷乱多样的世界背后有一种统一性。\\n\\n这和前面讲过的祛魅之前的世界有点像，都是说世界背后有一个更伟大的意义。当然，二者并不完全一样，但尼采要做的事情和祛魅很相似。尼**采认为，那个所谓更真实的、有目的的、有统一性的本质世界根本不存在。哪有什么比现实更真实的世界，有人真正看见过吗？根本没有，这就是形而上学的编造。\\n\\n我们之所以会编造这些东西，是因为人的心灵很脆弱**。在这个纷乱繁杂的世界中，我们需要安慰。虚假思想虽然能带来安慰，但最终会带来恶果。比如说，尼采认为人为了生命的欲望奋力拼搏是一种生命的本能。但在奋斗中，人总会遭遇挫折与痛苦，感到无力和卑微。为了缓解痛苦与自卑感，基督教就造出了禁欲主义，宣称禁欲是高尚的。于是，人就可以通过否定生命欲望来逃避拼搏，继而逃避那些负面的感受。这就好比一个人本来很爱钱，但因为贫穷感到自卑，于是他就去信奉一套所谓“高尚的人应该视金钱如浮云”的说辞来躲避自己的自卑感。\\n\\n但尼采认为，生命欲望是真实的，也是正当的。即使因为挫折而痛苦，我们也应当直面它们\\n\\n尼采认为，生命本身是强健有力的。这就是超人学说的起点。在这个起点，首先要转变对虚无的态度，从消极的虚无主义转向积极的虚无主义。\\n\\n什么是消极的虚无主义呢？就是面对虚无的真相，陷入悲观和绝望。可是你想过没有，为什么没有上帝的世界就会让人悲哀？为什么没有意义的人生就会令人绝望呢？\\n\\n**虚无这个真相并不直接导致消极。从虚无到消极，有一个必经的中间环节，那就是一种虚幻的信念：认为在世界的表象背后还存在绝对的本质，并且认为人生必须依靠这个绝对的本质才能找到价值和意义**。就像前面说的那个不存在的奖杯。如果你相信了这种虚幻的信念，那么虚无的世界对你来说就是毁灭性的，你就会感到悲观绝望。这就是消极的虚无主义。\\n\\n**但如果你从幻觉中醒来，看到从来就不存在什么绝对的本质或者真理，人生的意义也并不依赖于它，那就没有什么好绝望的**。而且，认识到世界本无意义，这恰恰带来了创造的自由。在尼采看来，价值不是现成在哪里等你“发现”，所有的价值都是人主观创造出来的，生命活动的标志就是能够自己确立价值，这是生命本身的力量。\\n\\n所以，尼采认为：面对无意义的世界和无意义的生命，人应该立足于现实，直面无意义的荒谬，以强大的生命本能舞蹈，在生命活动中创造出价值。用尼采的话说，就是“成为你自己”。这样一来，虚无不再会让你沮丧和绝望，反倒会给你最广阔的创造自我意义的空间，**虚无让人变成了积极的创造者，这就是积极的虚无主义**\\n\\n但是伯林认为，一元价值只是一种幻觉。他说，多元价值之间的冲突是不可能消除的。如果执着于价值一元论，就很容易会去压制其它的价值和理想，干涉甚至毁灭多种多样的生活方式，结果往往在实践中造成巨大的灾难。\\n\\n那么，价值一元论究竟错在哪里呢？\\n\\n伯林拿出了杀手铜，他提出了一个叫作“多元价值的不可公度性”（又译为“不可通约性”）的概念。这是什么意思呢？所谓“不可公度”，就是说你找不到一把能够通用地衡量多种不同的价值，把它们排出上下高低的尺子。\\n\\n伯林说，我们追求的许多价值，它们之间是不能换算的，都是彼此独立的“终极价值”。你不能说，自由是0.5份的平等，正义是1.5份的仁慈。一种价值有它独立的内涵，不能被换算为其它价值。你不能说实现了自由其实就实现了某种程度的平等，反过来也一样。换言之，某一种价值并不是其它价值的派生物，不能被还原为其它的价值。\\n\\n因为多元价值之间不可公度，我们往往无法同时实现多种价值，这个时候价值之间的冲突就无法避免了。\\n\\n有一个真实的案例：几年前在某个大学，一个学生在宿舍的饮水机里投毒，导致他的室友中毒身亡。死者的家属要求判投毒者死刑，而投毒者的父母百般希望获得被害者家人的宽恕，说死者已死，让另一个年轻人偿命也无济于事一两对父母依赖的出发点就是两种价值。一种是正义，杀人偿命嘛；另一种，是仁慈，宽恕我们吧，人死不能复生。\\n\\n如果让伯林来面对这个问题，他会怎么说？他会说，原则上无法解决。正义与仁慈都是人类的终极价值，但这两种价值不可公度，也常常无法调和：如果实现了正义，就无法同时满足仁慈；而如果用仁慈来宽恕凶手，那么就必须牺牲正义。\\n\\n强调价值冲突无法根除，这是伯林价值多元论的重要特征。伯林有一段话说得触目惊心：\\n\\n……我们要在同等终极的目的、同等绝对的要求之间做出选择，且某些目的之实现必然无可避免地导致其它目的之牺牲\\xa0\\xa0\\xa0\\xa0所以，（我们）需要选择，需要为了一些终极价值牺牲另一些终极价值，这就是人类困境的永久特征。\\n\\n注意，这不只是说“三观不合”的人才会发生冲突，即便是价值观完全相同的一群人，甚至一个人与自己，也可能陷入这种左右为难的局面。这才是价值冲突最深刻的困境\\n\\n由于大家的偏好不同，所以价值自然是多元的。所有价值都只对特定的个人或文化才有效，“你喜欢番茄，我喜欢土豆”，除此之外没有什么好说的。\\n\\n但伯林反对这种主观主义的立场。他的价值多元论有一个与众不同的特点，就是价值客观论，他反对价值主观主义和相对主义。伯林强调，价值虽然是多元的，但仍然是客观存在的，不是主观想象的随意构造\\n\\n首先，价值虽然多样，但不是无限的。他说，人类信奉的价值可能有几十种或者上百种，但无论如何也不能说，70亿人就有70亿种不同的价值。\\n\\n其次，伯林坚持认为，人类具有某种最低限度的“共通性”。即使双方价值追求不同，甚至可能会因此开战，但你我仍然可以想象、可以理解对方为什么会追求这种价值。\\n\\n比如，中国人说“忠孝不能两全”。有人会选择忠，有人则偏向选择孝，但两个人都会理解对方的选择。如果这时出现的第三个人是外国的传教士，他说虔诚敬神才是最重要的，前两个人也有可能能够理解他。但如果来了一个人说，你们都不对，杀人才是最高价值。“杀死一个人和踢开一块石头就是一回事，根本没什么区别”，这种说法就完全不可理喻了，只有疯子或者非人类才会说出这样的话。这种不可理喻的感觉，恰恰表明人类具有最低限度的共通性。\\n\\n伯林的价值多元论很容易和主观主义、相对主义混为一谈，但伯林自己一直在努力澄清，他的主张不是主观主义，也不是相对主义。\\n\\n总结一下，伯林认为，人类的生活世界存在着多种不同的终极价值，这些价值是客观的或真实的，但它们之间常常无法公度，不能彼此兼容，甚至可能发生严重的冲突，导致某种无可挽回的损失，这是深刻的人类困境。价值一元论试图克服这种困境，但它本质上是一种概念错误\\n\\n为什么要“造反”，为什么要呼吁根本性的社会变革呢？就算我们承认，资本主义贿赂了人民大众，用满足消费欲望收买了普通人，让人们服从于社会的控制之下，那又怎么样呢？\\n\\n有人会质疑就算这是贿赂，只要人们心甘情愿，愿意接受这种贿赂，这就不过是一桩你情我愿的交易罢了，又有什么不能接受的呢？\\n\\n对此，马尔库塞的回答是，因为这桩交易根本不公平，简直就是欺诈！富裕的生活和舒适的享受本身并没有错，但我们为此付出的代价太高了，几乎是无法承受的代价。这个代价，就是我们作为“人”的身份。接受这桩交易，我们就被“物化”，或者说几乎沦为了动物，不再是完整意义上的人\\n\\n在马尔库塞看来，这份手稿中充满人道主义精神，这种精神贯穿了马克思的整个思想。他认为，马克思所说的人类解放的理想，就是要克服人的异化，这是一种人道主义的理想\\n\\n事实上，自由主义倡导一种特定的自由，是个人自由，特别重视保障个人权利，视其为优先甚至首要的价值。这可以作为这个家族的共同相似特点。\\n\\n其次来看看两个维度，我认为可以从空间和时间的维度考察自由主义家族内部的差异。\\n\\n第一个维度是从空间上看地域差别的类型，主要是英美自由主义和欧洲大陆自由主义的传统，前者的代表人物是休谟和洛克，后者的代表人物是法国的卢梭和德国的康德。前者有很强的经验主义取向，强调免于强制的消极自由；后者有很强的理性主义取向，强调自我主导的积极自由。当然，这个说法是比较粗线条的。\\n\\n我认为第二个维度更重要，它是从时间上看代际差别，你可以把它理解为自由主义家族不同辈分的特性。我们曾在前文提到，这个家族辈分最高的成员是17世纪英国思想家洛克，他强调个人自由和基本权利，主张国家最少干预，在政治上提倡宪政自由原则，被称为古典自由主义。后来到了19世纪，英国的约翰•密尔那一代，出现了所谓现代自由主义，他们非常注重社会公正和平等的价值，转向强调政治民主\\n\\n约翰•罗尔斯，是20世纪最伟大的政治哲学家，甚至可以不加之一。美国前总统克林顿说，罗尔斯“几乎以一人之力，复活了政治和道德哲学”。\\n\\n要说罗尔斯具体的研究成果，最著名的当然就是他在1971年发表的《正义论》。这本书正文开篇是这样一句话：“正义是社会制度的首要价值，正像真理是思想体系的首要价值一样”\\n\\n在《正义论》这本书里，罗尔斯用详细而严密的推理，考虑了各种不同的选项，得出了契约的基本内容。他六万多字的论证，最后得出来的关键原则，主要就是两条。\\n\\n第一条原则叫作“平等的自由”原则，就是每个人都平等地享有一系列基本的自由，包括言论自由、信仰自由以及拥有个人财产的自由，等等。\\n\\n这个原则是怎么得出来的呢？很简单，在无知之幕后面，大家最关心的事情，就是签订了这个契约之后，揭开了无知之幕，这个契约会不会让我活得很惨啊？因为你是理性而自利的人，最重要的计算就是规避风险，这就会淘汰许多选择。\\n\\n这样一来第一个原则就很好理解了。比如，你肯定不会同意搞奴隶制，要是无知之幕一揭开，你碰巧是个奴隶，那就惨了。同理，你不会选择任何等级制的社会。当然，你也不愿意把基督教定成国教，因为无知之幕揭开后，万一你是个伊斯兰教徒，那就对你很不利。你知道社会是多元的，但不知道自己属于哪一元，所以要让每个人都有平等的自由权利，自主地选择自己的生活方式。\\n\\n总之，为了确保自己特定的生活目标和方式不会低人一等，无知之幕后的人都会同意要保障每个人平等的基本自由。这就是第一条“平等的自由”原则。\\n\\n第二条原则稍微复杂一些，它和社会经济的分配原则有关。罗尔斯得出的结论是，默认的选项就是应当完全平等分配。但他做了重要的补充，我们能够接受某些不平等分配，但必须满足两项限制条件。\\n\\n第一项限制是，相关的职位和工作必须在“公平的机会平等”（fair equality of opportunity）前提下，向所有人开放。直白地说，你要是说一家企业的CEO应该多赚钱，那前提是，所有人都有平等的机会成为CEO。\\n\\n这里特别要强调，罗尔斯说的“公平的机会平等”要比一般的机会平等条件更严格。比如，大家想做公务员，机会平等就是大家都有资格去参加公务员选拔考试，然后择优录取。这不就满足了机会平等的要求吗？\\n\\n但罗尔斯认为，这还不够公平。为什么呢？因为考生的背景差异太大了，比如家庭出身、居住地点、教育资源甚至身体状况等差异，这些都会影响考试的结果。而且关键在于，其中有很多因素是你自己无法控制的。罗尔斯认为，如果让这些自己无法掌控的因素影响了你的处境和命运，那仍然算不上公平。\\n\\n换句话说，公务员考试设置了一条平等的起跑线，这是普通的机会平等。但仔细一想你就会发现，这条起跑线根本不是平的，它其实是前一场赛跑的终点，已经有人在你前面，也有人落在你后面了。\\n\\n罗尔斯认为要从最早的比赛开始就设置尽可能平等的起跑线，或者说，要把起跑线的平等往前延伸，从幼儿园开始一直到大学，来保障你成长背景的公平。这样才能满足“公平的机会平等”这个限制条件\\n\\n罗尔斯把这些自己无法掌控的运气或偶然因素，称作“道德上任意武断的因素”。如果这种偶然因素让你在竞争中落败了，难道你就活该受穷受苦吗，难道应当让你自己来承担所有不利的后果吗？无知之幕背后的人们不会答应，因为揭开了无知之幕，自己完全有可能就不幸地属于当今社会的天赋最差、最弱势的群体。\\n\\n所以，罗尔斯认为，社会经济的不平等分配，还需要满足第二个限制条件。罗尔斯把它叫作“差异原则”，就是这种不平等，能够让处境最糟糕的人改善状况。也就是说，除非不平等的分配能使得最弱势群体的处境得到改善，否则，不平等在道德上就是不可接受的，是不公平的，也是无知之幕背后的人不会接受的\\n\\n到这里，我们来做一个总结：罗尔斯通过无知之幕的思想实验，推理论证了一个正义的社会契约中最关键的两条原则。第一条原则是要保障平等的基本自由，第二条原则是，社会经济的不平等分配，必须满足两个限定条件，一个是“公平的机会平等”，一个是要满足差异原则\\n\\n在这里，桑德尔采取了一种不同的理论视野，就是“共同体主义”，也常常被翻译为“社群主义”（Communitarianism）。社群主义强调，“个人是社会构成的”，先有社群，社群造就了个体，而不是先有孤立的个人，然后再由个人组成社群。这对自由主义所依据的个人观念提出了挑战。\\n\\n桑德尔在后期的著作中引用了另一位社群主义哲学家麦金泰尔的一个观点：人类是一种“讲故事的存在”。如果你要回答“你是谁？需要什么？以及想做什么？”诸如此类的问题，那么答案就在你的故事之中。只有讲通了自己的故事——理解自己成长的过程，以及这些经历如何形成了你的目标，后来又发生了什么变化——你才能真正回答这些问题。\\n\\n但是，任何一个人的故事，从来都不是孤立的个人故事。离开了社会关系的塑造，你就讲不通自己的故事\\n\\n由于前面所说的四种流动性始终存在，现代社会永远都处于运动之中，沃尔在把这种特征称作“后社会的状况”（post-social condition）。\\n\\n从这个角度来分析，那种孤立的、近乎原子化的自我，就并不是自由主义虚构出来的“先于社会的自我”（pre-socid self）观念，而是“后社会状况”造就的。沃尔泽称之为“后社会的自我”（post-social self）观念。这种自我观念反映了自由流动社会的现实，它从根本上失去了确定性和统一性，个人不得不随时重新创造自己。\\n\\n许多自由主义者赞赏这种“后社会的自我”，这让我们可以“自愿地联合”并不断“自我创造”，去追求自己喜欢的生活。但大多数社群主义者却为此悲叹，这种失落感也是现实的反映。\\n\\n那么，有没有可能恢复传统的社群？让人们重新获得那些稳定的依恋关系、深刻的归属感以及可靠的生活理想呢？\\n\\n这很难做到，原因很简单——除非我们在根本上改变现代社会的基本结构，否则我们无法限制那些自由的流动：移居自由、社会阶层流动的自由、婚姻自由以及政治认同的自由。\\n\\n正是在这个意义上，沃尔泽说“社群主义不可能战胜自由主义”。但与此同时，自由流动社会造成的忧伤、失落和孤独，以及政治冷漠等后果也会如影随形。所以，社群主义对自由主义的批判不会消失，它注定会周期性地出现\\n\\n泰勒说得有道理吗？想想我们在生活中，到处可以听到各种本真性的宣言：“成为你自己”“做真实的自己”“忠实于自己”“实现自己”……这些口号听起来非常励志，肯定了独特个性的优越性，显示了特立独行，但也会让我们产生一种错觉，**好像自我的独特性只有从自我的内部才能获得**。\\n\\n**现代社会有一种很流行的看法，认为事物的价值是主观的，是“自我”赋予的。我珍视或看重某种事物，不是因为它本身内在固有的价值或意义，而恰恰是因为我看重它、珍视它，它才变得有价值**。\\n\\n但是，这种价值主观论可以成立吗？你可以问问自己：“你为什么会珍视或看重它？”你当然可以回答说，“我认为”“我相信”“我感觉”或者“我决定”。\\n\\n但这类回应完全没有回答“为什么”。如果你进一步去追问来龙去脉，只要你认真给出理由来回答，那么就会显示，那个单独的“自我”实际上并没有独自赋予或创造价值。\\n\\n**比如，你有一盏自己非常喜欢的台灯，从来不让人碰，有人问你为什么要这么宝贝一个台灯，你第一反应也许会说，我就是喜欢，没有什么为什么！但如果你仔细一想就会发现，其实你能够解释为什么，你喜欢是有理由的**：也许是因为这盏灯曾陪伴你度过无数孤独的夜晚，也许它是家人送给你的礼物，也许它是你用第一笔工资买给自己的奖励……于是你会发现，那些看似“自我赋予”的价值和意义，实际上仍然是有渊源和来路的，是由许多经历和故事造就的，也是在社会生活的关系中形成的。\\n\\n所以泰勒认为，自我无法凭空创造发明自己的价值和意义标准。自由选择和价值判断需要依据价值尺度，而价值尺度不可能由“自我”来发明创造，我们只能“选用”和“改造”价值尺度，这正是泰勒的社群主义观点带来的启发：个人自主性的来源不可能是“唯我论”的独白，而只能来自关系性的对话。\\n\\n在祛魅之后的现代世界，我们好像失去了任何标准，但泰勒告诉我们，意义和价值的标准依然存在，就存在于我们生活的共同背景之中。\\n\\n但现代和古代不一样，这个共同背景并不是一套清晰固定的规则或公式化教条，而是一种资源。它有着丰富的多样性，为意义和价值的选择标准提供了资源；它并没有机械地决定我们具体的生活理想和选择。正因如此，个人的选择仍然必要，对话和反思才有意义\\n\\n那么，什么是韦伯难题？我们先简单回忆一下前面讲过的韦伯对现代性的诊断：世界祛魅了，现代社会越来越理性化了，但理性本身却分裂了，分成了工具理性和价值理性。\\n\\n工具理性很好懂，就是把理性计算作为工具，去实现给定的目标。工具好用不好用，是一个事实判断，很容易达成共识。所以，工具理性能够大行其道：建立普遍通用的规则与统一标准，广泛应用于科学技术、经济生产和官僚管理系统等领域。工具理性大大提升了现代化的速度、规模和效率，这是了不起的成就。\\n\\n但相比之下，价值理性却严重衰落了。因为事实判断有统一标准，而价值判断则各有各的尺度。在祛魅之后的现代社会，人们失去了古代人信奉的自然、天道和上帝等客观标准。在价值问题上，大家很难达成共识。\\n\\n你要“诗和远方”，他要功成名就；你要个人自由，他要集体温暖。现代社会在人生理想、道德规范和政治生活这些涉及价值判断的领域，陷入了相互争执的多元主义，韦伯把这个局面叫作“诸神之争”。\\n\\n在这个意义上，启蒙理性主义的雄心抱负只实现了一半一韦伯看出了这个问题，但他找不出解决办法。只好说，让我们接受工具理性的巨大成就，同时承受价值理性衰败的后果。他告诉我们，这就是现代性让人悲喜交加的命运，我们没有更好的办法，只能看清世界，放弃幻想，然后从容面对', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cc116644-8b45-43e7-bf40-c20d6245c6cc', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"可与言之而不与之言，失人\\n不可与言而与之言，失言  - \\n\\nA man was sayingmust be true,\\nbut he did not need to say that he knows it. - 康德\\n\\n\\n\\n“statistically speaking, an average American is physically unhealthy, emotionally anxious and depressed, socially lonely and financially in debt”\\n\\n\\n我认为你缺少这些能力，是因为从你的表述来看，你目前没法分辨“**知识，观点，情绪**”这三样事物。情绪每时每刻都在产生，对人的影响最直接也最有效。你在大部分渠道接收到的信息都是**情绪**，这东西极度廉价，缺少营养而且过期作废，收集多量情绪对人几乎没什么实质帮助。\\n可是活着的人和仍有时效性的消息，他们无法不夹杂情绪。只有当你把这些情绪剥离掉的时候，你才能稍微收集到一点有用的内容——就是**观点**。观点里仍然含有立场，但是至少它真的包含了对人有帮助的成分。常见的处理（情侣间，网络上）吵架的手段里，必须至少有一步是**从情绪里找出观点**。一堆情绪的对撞只会撞出更多更没用的情绪来，但是观点好歹能让你知道现在究竟发生了什么事，我们到底在讨论什么东西，以及下一步该怎么做。\\n\\n再接前文，说到**观点**。观点仍然不够，观点能解决一些眼前的问题，真的想学到长久有效的内容，还要把观点再提炼一次，把观点排除立场干扰之后仍然成立的事情，才有可能成为**知识**。\\n\\n**知识是观点的基础**。你必须确实地**知道**某事，才能基于这件事情形成有价值的观点。你没有知识，就只能输出情绪，情绪没什么用。你可以毫无理由地喜爱或厌恶某事，没人有权利阻止你形成这种情绪。但是你想要以此来施加影响力（比如你要使另一个人也成为跟你同样的立场），你就得面对一个质问叫做“**凭啥**”。\\n\\n你要回答这个“凭啥”至少要有一个**观点**。观点能成立多半需要一份**知识**。最好这个知识是你跟我共同保有的，这样事情的推进才有根据。\\n\\n  **读书有助于你获取知识**。这个说法是我的观点，但这也是一个被人类文明建立以来，这几千年中我也不知道多少亿人反复反复反反复复提出或者继承的观点，如果地球有知乎的话，这个观点被点赞的次数至少超过当前地球人口的总和。我觉得咱们可以认为这是个**知识**了，不用怀疑它，更是大可不必用某人那点小情绪去试图撬动如此庞大的根基。\\n\\n**知识有助于你成为更好的人**\\n\\n\\n\\n\\n为什么我们过不了“摆烂”的生活？\\n因为当你说“摆烂”这个词的时候，你就已经知道是一种烂生活。\\n人不能过自己看不起的生活。\\n如果有一个人，第一眼看到游戏就说，太好了，这就是我这一生想要的。我可以不娶妻不生子一辈子吃糠咽菜，只要我能有游戏，我就过的快乐，安稳。\\n这种理念能践行一辈子，就是庄子说的“真人”。\\n在这里，游戏对这个人形成了某种超越现实生活的意义，而这种超越性，就是大多数哲学和宗教追求的东西。(就是以自己当尺子，有自己绝对的主体性，就是成神)\\n超越了现实生活的人，当然不会认为自己在过一种“摆烂”的生活。\\n但是大部分人都是这样的，他也痴迷游戏，不工作不上学天天打，但是打起来他不敢停。\\n他怕自己一停下来就会想到自己真正在乎的那些东西。\\n\\n\\n写了很多，想想又删了，只留一段话，能懂的朋友自然懂:\\n\\n人一定要注意保持自我的完整性，不要割裂的生活。\\n\\n你去博物馆看梵高，你是知道它“应该”是美的，还是这种美绕过逻辑，直接击中了你的内心。这两者的关系，一定要辨析清楚。\\n\\n我们知道了一种道理，一种生活态度，亦是如此。\\n\\n  \\n  \\n\\n\\nloser and winner are both just manifestations of ego; fundamentally im the same person i have always been\\n\\n我们思考生存的意义，是因为我们能思考\\n\\n\\n“我顺次恭听了这三个人讲的故事，既不感到可笑，也不感到可悲。我想人这种东西，为了消磨时间，硬要运动他们的嘴巴，把本来不可笑的事笑上一番，把本来没有多大意思的事，津津乐道一番，除此之外，再无别的本领了。”  -- \\n\\n\\n\\n你需要钱和地位和爱情，来满足你（从下往上）第二层到第四层的需求。只是因为你有这种需求， 你就会以为这就是你的意义。\\n\\n也就是说，意义只不过是心理需求的一种体现而已。\\n\\n事实上当我几年前发现真正神圣而终极的意义只有宗教的时候，我开始明白意义不是一个固定的东西，它是一个可变的东西，在不同的人生阶段代表不同的东西。后来的学习研究也证实了我的观点，它真的是随着需求而变的。\\n\\n正确的顺序是：先存在=>因为活着而有需求=>把需求当做意义。\\n\\n首先你要接受一个事实，而不是逃避这个事实，那就是你活着。只有当你真正接受了这个事实，才会开始想：\\n\\n**既然现在我活着，我可以做点什么？**\\n\\n为什么一定要接受这个事实？就像心理学上的接纳不完美的自己Q一样，接纳自己不完美的生活， 接纳自己不需要意义的存在。你不是因为什么意义才出生的，你就这么突然地活着了，你坚持活下去的原因就是你不想死而已。无论你这一生的意义是什么，你活着的这一生所做的选择都是根据短期长期快感决定的，而且在不久的将来，你也会突然地死去。整个人类的进化？也许有意义，但是这个视角太大了基本上和你没什么关系。你就这么活着，就这么存在着，也将会就这么死去。你没有别的选择，这是事实。你就是你，你活着就是活着。\\n\\n举个最简单的例子，你不会因为2+2=4°而苦恼，你不会想2+2为什么等于4。因为为2+2=4而苦恼是没有意义的，因为你接受了2+2=4。（理论来源：耶鲁公开课《死亡》Q）\\n\\n没有意义会让你变空虚了吗？\\n\\n**不，一旦接受了这个设定，你就从意义中解放了出来。没有了意义的束缚，你应该更自由**，因为你可以做自己想做的事情而不是意义让你做的事情，你可以真正掌控自己的生活，让它按照你设想的方式发展。\\n\\n所以不是先找到意义然后再活着，而是先努力生活，然后你自然会给自己赋予一个自己满意的意义。\\n认真赚钱，认真去爱，认真体验生命的美好，认真学习，认真见识新世界。你想让你的人生在哪方面拓展就在哪方面加油。\\n这就是热爱生活。\\n然后你就会明白为什么活着。\\n\\n\\n\\n\\n在我的旧日记中我找到了这一段——汉弗莱·特里维廉评论歌德的文章：“如果一个伟大的艺术家直到生命的尽头仍然保持着创造力，双重性格对他来说是必要的；他要对生活保持异常的敏感，他必须对生活永不满足，必须对不可能的事永远有一种执著，一旦得不到，他定会绝望。他必须昼夜有一种不可名状的负担，在赤裸的不可弥补的事实面前他必须颤抖。这种神圣的不满足、不安稳，这种内在的冲动是艺术创造的来源。许多不成功的诗人只在年轻时拥有它。甚至有些伟大的诗人在中年时就丧失了它。华兹华斯失去了面对绝望的勇气，而正是绝望使他的诗富有魅力。但更多的时候，内在的骚动不安如此有力，在一个人未达到成熟前就已被它摧毁。\\n\\n\\n我是主体，所以可以度量价值，不是被度量的客体\\n多数人的价值体系都是用的别人的\\n我是度量万物的尺子，尺子不能度量自己；每个人的价值体系就是他自己抱持的偏见，价值论的愚蠢在于他不认为自己是偏见而是真理和天然正确\\n\\n\\n\\n正在看《明治维新史再考》，读到一句非常简明的总结：之前的学者只聚焦于最后的胜利者一方，把历史集中表现为那些主角以其意志与努力实现理想的过程。\\n\\nWe have action control, but no direct, action-independent feeling or desire control. That is why we are morally responsible for our actions, and not for our feelings and desires.\\n\\n\\n\\n托斯妥耶夫认为人类能实现互相之间大爱的源头在于同罪论，就是JP说的carry the burden of the entire humanity，比如一个人因为饥饿偷了食物，那是因为我没有把这个世界打造成让人免于饥饿的社会；圣经里有个故事一个女人犯了奸淫罪， 被拖到广场大家都说要扔石头处罚他，基督说你们谁是没有罪的，就可以处罚他，就没人动了；真正的宽恕只有一种，就是明白自己的不完美，犯错的人同样不完美，我就没脸怪罪你；你现在犯的错可能是我犯的错间接导致的，我们都是同罪；如果每一个人都有这种同罪感，认为自己并不无辜也不高人一等，我们就能宽恕他人一切的罪责，因为宽恕他人就是宽恕自己，爱他人就是爱自己\\n\\n“爱生活，甚于爱生活的意义“\\n\\n\\n\\nAs Wittgenstein remarks somewhere: if there is such a thing as eternal life, it must be here and now. It is the present moment which is an image of eternity, not an infinite succession of such moments\\n\\n\\n\\nWhat we need is a form of life which is completely pointless, just as the jazz performance is pointless. Rather than serve some utilitarian purpose or earnest metaphysical end, it is a delight in itself. It needs no justification beyond its own existence. In this sense, the meaning of life is interestingly close to meaninglessness.\\n\\n\\nThis kind of activity is known as agape, or love, and has nothing to do with erotic or even affectionate feelings. The command to love is purely impersonal: the prototype of it is loving strangers, not those you desire or admire. It is a practice or way of life, not a state of mind. It has no connection with warm glows or personal intimacies. Is love, then, the meaning of life? It has certainly\\n\\n\\nreason for this is that happiness is not in fact some beaming, bovine contentment, but (for Aristotle, at least) the condition of well-being which springs from the free flourishing of one's powers and capacities. And love, it can be claimed, is the same condition viewed in relational terms - the state in which the flourishing of one individual comes about through the flourishing of others.\\n\\n\\n\\n与其说时间带来了变化，不如说变化造成了时间；意识与外界的互动，才能意识到变化的发生，才有时间流逝的感觉；\\n稳定自我和真实自我存在的预设，和不断变化的自我感受之间的鸿沟，成为人的痛苦来源\\n\\n\\n\\nIt had not registered until now that he would not step foot on his native shore again for many years, if ever. He wasn’t sure what to make of this fact. The word loss was inadequate. Loss just meant a lack, meant something was missing, but it did not encompass the totality of this severance, this terrifying un-anchoring from all that he’d ever known\\n\\n\\n加缪在这里沿用了一个非常古老的结论，也就是柏拉图所谓的“哲学始于惊奇”。在柏拉图看来，在日常生活当中忽然对某些事物感到好奇，由此产生追问的兴趣，就是哲学的起点。\\n\\n加缪所表现的也是这样一个瞬间，一个从日常生活的惯性当中突然松脱的瞬间。好似无缘由地问了一个“为什么”的问题，这个问题一问出口就发现，原来没有答案，这就是加缪所谓^带点惊奇味道的厌倦”。\\n\\n加缪把这样的态度称为对“数量”的重视，不是比谁活得最好，而是比谁活得更多。英译本的表达是“most living”。但是大家请不要误解，加缪的意思不是像消费主义宣称的那样，鼓励你去尝试不一样的人生。\\n\\n相反，他的意思是，我就牢牢地站在我的生活里，在我的轨道上，所有遭遇的事情，我就这么目不斜视、心无挂碍地走进去，经历、体验，不为了累积可供玩味的回忆，也不在里面建设虚幻的海市蜃楼。\\n\\n如果每天重复，毫无内容的变化该怎么办？加缪说，那正好，我一边继续前进，一边被这个单调提醒着，别多想，生命本来就给不了你什么。\\n\\n面对荒谬的世界，加缪给了我们三种武器。\\n\\n第一，我们清醒地认识到荒谬，但是没有屈服也没有崩溃，太阳出来了，该吃早餐吃早餐， 这就是我们的反抗；第二，我自愿走进我的生活，在这条路上既没有苦求意义也没有寄托于虚幻的彼岸，这样的我就是自由的；第三，这样的早起吃早餐，这样的一次又一次走出家门，我居然还有精气神，没有被荒谬压垮，没有觉得这一杯牛奶索然无味，这样的我就保持着属于我的激情。\\n\\n只要有这三点作为保障，哪怕是每天出门推石头的西西弗，也是幸福的。\\n\\n\\n\\n\\n\\n\\nthere are tyranny from the state and tyranny from your own presopositions,\\n\\n\\n\\nHappiness is sometimes seen as a state of mind. But this is not how Aristotle regards it. ‘Well-being’, as we usually translate his term for happiness, is what we might call a state of soul, which for him involves not just an interior condition of being, but a disposition to behave in certain ways. As Ludwig Wittgenstein once remarked, the best image of the soul is the body. If you want to observe someone’s ‘spirit’, look at what they do\\n\\nHappiness is part of a practical way of life, not some private inner contentment. On this theory, you could look at someone’s conduct over a period of time and exclaim ‘He’s happy!’\\n\\n It is a mood or state\\nof consciousness rather than a way of life. It is, in fact, exactly the kind of modern concept of happiness which Aristotle might well have found unintelligible, or at least objectionable. For him, you could not be happy sitting in a machine all your life – not just because your experience would be a matter of simulation rather than reality, but because well-being involves a practical, social form of life. Happiness for Aristotle is not an inward disposition that might then issue in certain actions, but a way of acting which creates certain dispositions.\\n\\n For most people, in practice if not always in theory, life\\nis made meaningful by their relationships with those closest to them, such as partners and children\\n\\n\\n**所谓的无我就是类似初级动物没有self consciousness** \\n\\n道德经中没有努力的概念，正是因为努力是人头脑中的小我作怪，\\n\\n定个目标，然后为之努力奋斗已经成了现代人的常态，\\n\\n顺利实现目标只是短暂的时刻，\\n\\n更多时间里，都是朝着那个目标进行所谓的努力，\\n\\n这个过程业力累积全都来了，\\n\\n目标实现欢欣鼓舞，目标无法实现消沉抑郁，\\n\\n王德峰在另外一段视频中说，他很希望人们像屎壳郎一样活着，\\n\\n说你看纪录片中的情节，推着粪球突然卡住了，\\n\\n小虫就换各种方向角度尝试从困境中解脱出来，\\n\\n终于找到方向，得到解脱，小虫就继续推着粪球赶路，\\n\\n整个过程中，遇到了坎坷它也没什么情绪，\\n\\n终于从困境中脱离出来，它也没什么可庆祝，\\n\\n如果用佛教的理论来解释，\\n\\n涅槃境界指的是不再入轮回，不再“存在”，归于空境，\\n\\n最重要的一点就是佛学中常常提到的消业，\\n\\n业力因果会形成循环，\\n\\n我觉得这些当成迷信也好，信仰也罢，\\n\\n都是无法证实也无法证伪的事情，\\n\\n多一个视角看待人生也未尝不可，\\n\\n努力的前提是你要有个为之奋斗的目标，\\n\\n而这个目标的确立常常源于小我的执着，\\n\\n这种对于目标的追求过程，正是造业的过程，\\n\\n业力有了就要消，\\n\\n消的过程如果没有智慧又会生出新的业力，\\n\\n最终在轮回中反反复复，\\n\\n而且业力在的话，人不得自由，\\n\\n你想，都不知道从哪来的愿望，且深深的扎根在心中，\\n\\n一生只会为这个目标忙碌，\\n\\n哪来自由，\\n\\n当然，这种情况用另外一套方式来解释，\\n\\n也是幸福之事，那就是找到你内心深处的声音，\\n\\n找到为之奋斗终身的事业，也是一件幸事。\\n\\n老子显然不把这事看成幸事，\\n\\n佛陀也好，老子也罢，\\n\\n他们那个级别的智者，\\n\\n要的是不入轮回，\\n\\n试想一下，此生消去所有业障，\\n\\n而不生新的业力，在死亡到来之时，\\n\\n正负归零，没有新的业力等着下辈子去消解，\\n\\n也就没有理由再次来到这尘世，\\n\\n涅槃境界达成。\\n\\n基督教，佛教经常强调人有罪，\\n\\n怎么见得？难道一生行善也有罪么？\\n\\n佛教解释相当牛，大概意思说，你来了这尘世，\\n\\n就说明有罪，有业力需要消，\\n\\n没有罪，没有业力，你根本不会出现在这世界上，\\n\\n以上都当我瞎扯吧，\\n\\n反正“努力”这个概念确实不怎么样，\\n\\n“自律”也是一样，都是无意义的概念。\\n\\n看起来努力的人用不着努力，\\n\\n看起来自律的人用不着自律，\\n \\n \\n A statement like ‘The meaning of life is suffering’ suggests not that suffering is the whole of life, or the point and purpose of life, but that it is the most significant or fundamental feature of it.\\n\\n\\nIn Genesis 2: 19, ‘the Lord God formed every beast of the field and every bird of the air, and brought them to the man to see what he would call them; and whatever the man called every living creature, that was its name’ (RSV). Since the act of naming in ancient Judaic culture is always a creative\\nor performative one, this suggests that it is humanity which is the source of meaning, while Yahweh is the source of being.\\n\\n\\n\\nif you believe that every individual has its own value, what does it mean that you believe it? You act as if its true \\n\\n\\n“I am by nature an outsider, by profession an onlooker, by inclination a loner, and I have spent my life looking at things and happenings, and observing their effect upon my own particular sensibility. ”\\n\\n\\nfaith means the willingness to take risk \\nu have to decide certain things as pre conditions for action, independent in real sense of evidence, thats the faith - 克尔郭凯尔的观点\\n\\nLet us ask not what an ‘inherent’ meaning might look like, but what it means to claim that meanings are what we ‘construct’ the world to be.\\n\\n\\n固化的meaning是存在的，但是需要人的参与才能unfold it \\nOn this view, men and women are not just the puppets of some grandiose Truth, as they are for Schopenhauer. There is such a Truth in these cases; but without men and women’s active participation in it, it will\\nnot unfold. It is part of Oedipus’s tragic fate that he actively, if blindly, helps to bring on his own catastrophe. For Christian\\nfaith, the kingdom of God will not arrive unless human beings co-operate in its creation, even though the fact that they do this\\nis already reckoned into the very idea of the kingdom. For Hegel, Reason realizes itself in history only through the genuinely free acti\\n\\n\\nons of individuals; indeed, it is at its most real when they are at their most free. All of these grand narratives dismantle the distinction between freedom and necessity – between forging your own meanings and being receptive to one already installed in the world.\\n\\nAll meanings are human performances, and ‘inherent’ meanings are just those performances which manage to capture something of the truth of the matter\\n\\nShakespeare is well aware of the parallels between value and meaning. His plays brood constantly on the question of whether meanings are innate or relative. He lived, after all, at a point of historical transition from a faith in\\nthe former to a belief in the latter; and his drama relates this momentous shift to an economic shift from ‘intrinsic’ values to the ‘exchange-values’ generated by market forces\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='40f38f33-0e9f-4445-8d3a-eb6a3d24c00c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nBe All You Can Be\\n—U.S. Army recruiting slogan, 1980-2001\\n\\nThe next generation of Idealist philosophers, most significantly Immanuel Kant and G. W. F. Hegel, also saw reality as a product of man’s perceptive faculties—but our senses, they argued, were limited in their ability to perceive the true nature of things. The mind was finally experiencing itself, Kant and Hegel reasoned, and not ultimate reality. Like Berkeley, Kant and Hegel also believed, more or less, in a fixed nature or set of universal laws, within which an awakened person could serve as an extraordinary actor but not as an agent of creation.\\n\\nSome mid- to late-nineteenth-century modernists, such as Arthur Schopenhauer and Friedrich Nietzsche, extolled the powers of human will and spoke of an inner-self that formed an invisible seat of power.\\xa0But, again, such views did not elevate the mind as the author of reality. Indeed, all of the major Idealist philosophers and their offspring, from Kant to Emerson to Nietzsche, held that natural man could\\xa0*ally*\\xa0himself with universal forces, and thus attain a kind of greatness or at least a right way of living\\n\\nEach went on to write landmark books whose titles and bylines became synonymous with success:\\xa0*Think and Grow Rich*\\xa0by Napoleon Hill and\\xa0*How to Win Friends and Influence People*\\xa0by Dale Carnegie.\\n\\nHill was more clearly a product of the mind-power culture. His first book, in 1928, was an eight-volume opus called\\xa0*The Law of Success*, a\\xa0title borrowed from Prentice Mulford. Like Mulford, Hill believed that the mind possessed clairvoyant energies and forces. In particular Hill emphasized the existence of a “Master Mind,” an over-mind of shared human consciousness, which reveals itself to us in moments of intuition, in hunches, or in prophetic dreams. Tapping into the Master Mind became the centerpiece of his work.\\n\\nHill jettisoned any vestige of the mind-power movement’s earlier social consciousness. “I gave a beggar a dime,” he wrote, “with the suggestion that he invest it in a copy of Elbert Hubbard’s\\xa0*Message to Garcia*.” Hill was referring to Hubbard’s famous 1899 essay about an American soldier who displayed remarkable drive in carrying a message behind enemy lines to a Cuban rebel leader during the Spanish-American War.\\xa0*Message to Garcia*\\xa0was Hubbard’s paean to self-will and personal accountability. Hubbard’s social outlook, however, wasn’t quite what Hill’s tribute implied.\\n\\nHill was able to score the ultimate “get”: an interview with the steel magnate Andrew Carnegie.\\nHill described his first encounters with Carnegie—“the richest man that the richest nation on earth ever produced”\\n\\nHill was able to score the ultimate “get”: an interview with the steel magnate Andrew Carnegie.\\nHill described his first encounters with Carnegie—“the richest man that the richest nation on earth ever produced”\\n\\nA similar outlook prevailed in the work of Dale Carnegie. Like Hill, Carnegie possessed an innate grasp of how to get men in power to open up to him: Just ask them how they overcame their early hardships. But this method of gaining access—in which the questioner always dotes and never challenges—also left Carnegie with the perspective that corporate chieftains are always ready with a\\xa0*put-’er-there-pal*\\xa0handshake and an abundance of helpful advice. The question of corruption or backbiting\\xa0never seemed to enter Carnegie’s mind. He believed that the men on top deserved to be there—and his level of introspection on the matter went no further.\\n\\nYet Carnegie, like Hill, proved a pioneering observer of human nature, and a genius of communication. He had a key message and he understood how to convey it to a vast range of people. It was this:\\xa0*Agreeable people win*.\\n\\nThe Christian Science and New Thought views share commonalities with the mystical tradition—but only up to a point. While Christian Science specifically denies the reality of illness, it also promises a\\xa0*change in condition from illness*; or, as Christian Scientists put it,\\xa0*a revelation of truth*.\\xa0Christian Science testifies that if you realize higher truth, in understanding the absolute permeation of all things by the beneficence of God, you will experience that goodness as freedom and true healing. In Christian Science this often involves sickness yielding to health; in New Thought it typically means poverty transforming to plenty. This\\xa0*change in outer circumstance*\\xa0is the central promise of the psycho-metaphysical philosophies. It is what distinguishes them from mysticism, Transcendentalism, and various strands of existential or meaning-based psychology.\\n\\nNew Thought is neither\\xa0monolithic\\xa0nor\\xa0doctrinaire, in general, modern-day adherents of New Thought share some core beliefs:\\n\\n1. God or Infinite Intelligence is \"supreme, universal, and everlasting\";\\n2. divinity\\xa0dwells within each person, that all people are spiritual beings;\\n3. \"the highest spiritual principle [is] loving one another unconditionally... and teaching and healing one another\"; and\\n4. \"our\\xa0mental states\\xa0are carried forward into manifestation and become our experience in daily living\".\\n    \\n    [5]\\n    \\n    [6]\\n    \\n\\nWilliam James\\xa0used the term \"New Thought\" as synonymous with the \"Mind cure movement\", in which he included many sects with diverse origins, such as\\xa0idealism\\xa0and Hinduism\\n\\nEmerson actually sought to deal with this problem. He took account of both aspects of human existence—man’s great potential and unthinkable smallness—in his 1860 essay, “Fate.” Making the kind of pronouncement that later was selectively quoted by mind-power acolytes, Emerson wrote, “But the soul contains the event that shall befall it; for the event is only the actualization of its thoughts, and what we pray to ourselves for is always granted.” Yet Emerson also insisted that man’s creative faculties are not all that he lives under. He added that there existed just “one key, one solution to the mysteries of human condition.” And that was to acknowledge that man exists under\\xa0*both*\\xa0self-direction and nature’s will. And the will of nature contains purposes we cannot know, but can only bow to, and thus take our place in creation. “So when a man is victim of his fate,” Emerson continued, “…\\xa0he is to rally on his relation to the Universe, which his ruin benefits. Leaving the daemon who suffers, he is to take sides with the Deity who secures universal benefit by his pain.”\\n\\nTo consider which of these approaches works best, and at what cost or benefit, it is helpful to break down and critique what I see as the four primary schools of positive thinking. They are:\\n1.\\xa0\\xa0\\xa0\\xa0The Magical Thinking or Divine Thought School\\n\\nIt informs the outlook of\\xa0*The Secret*’s Rhonda Byrne. The Magical Thinking perspective sees the individual as a kind of holy channel for a higher power. It could also be called the Law of Attraction School. It is the least “provable” approach, yet the most popular and enduring.\\n\\nEach of the aforementioned thinkers professed a different version of the philosophy—Peale was a conservative Christian who used Scripture to support his ideas about “prayer power”; Murphy was a New Age mystic who told of an all-powerful subconscious mind that represented an inner God; Wattles, Holmes, and Byrne adhered to an occult science that considers man a transmitter of an infinite power, or a “thinking\\xa0stuff,” as Wattles put it. Each denizen of this approach, whatever his or her individual wrinkle, sees the mind as a vessel and ignition engine constantly out-picturing all of our thoughts into reality. Seen from their perspective, the mind, with proper awareness, can function as a tool to dispatch every right desire.\\n\\nAnd as rationalist philosopher David Hume wrote of Christianity: “Mere reason is insufficient to convince us of its veracity.”\\n\\n2.\\xa0\\xa0\\xa0\\xa0The Conditioning or Reprogramming School\\n\\nwhich sees the mind as a complex, conditioned machine capable of reprogramming. In this view,\\xa0*conditioning is destiny*. The Conditioning School prescribes affirmations, visualizations, behavior modeling, and guided meditations to reprogram our self-image, and thus improve our functioning. Many inquirers who attempt this approach are surprised by the rigor demanded from such programs—at least an hour a day of visualizations and guided meditations can be required in Maltz’s program, for example.\\n\\n3.\\xa0\\xa0\\xa0\\xa0The Conversion School\\n\\nPhilosopher William James and psychologist Carl Jung shared a key idea: that a conversion experience, or religious awakening, could objectively alter the circumstances of a person’s life.\\n\\nThe Conversion School sees man as a psycho-spiritual being who is capable of experiencing dramatic, visible life changes through a consuming experience of faith, which reorders a person’s priorities and perspective. “Conversion,” wrote Bill Wilson, “does alter motivation, and does semi-automatically enable a person to be and to do the formerly impossible.”\\n\\nthe self-affirming beliefs of mind-power can generate tremendous enthusiasm and reorientation. The problem is in sustaining that experience. For individuals with a defined and well-ordered aim, such as staying sober, support groups such as AA do provide a sustaining structure. But most people discover positive-thinking philosophy through books, such as\\xa0*The Secret*\\xa0or\\xa0*The Power of Your Subconscious Mind*. And their initial excitement—along with the self-validation they may experience—is not generally\\xa0sustainable. Hence, positive-thinking bestsellers and seminars can attract droves of excited newcomers—but the movement is like a great revolving door through which the curious quickly come and go (another issue to which we will return).\\n\\n4.\\xa0\\xa0\\xa0\\xa0The Meaning-Based School\\n\\nThis approach is found within the spiritual ideas of Rabbi Joshua Loth Liebman and the existential philosophy of psychologists such as Erich Fromm and Viktor E. Frankl. Frankl—writing after he survived Auschwitz—drew upon his wartime experiences to reach stark conclusions about the depths of human indecency, but also about the very real possibilities of an inner grace appearing from within a person even under the most horrific conditions.\\n\\nFrankl and his contemporaries saw man as a being of great potential—but one who is trapped in a state of psychical slumber. In a crisis, Frankl reasoned, man can awaken to his higher self. The key is to locate some meaning in life, to find personal terms in which suffering or travails amount to some worth in the world; this revelation can dramatically alter a person’s viewpoint and provide new possibilities.\\n\\nself help 是否是统治集团的阴谋？\\n\\nInformation is the most profitable product in the world,” intoned MVH, with funereal seriousness. He then told us the story of Chicken Soup. The first book,\\xa0*Chicken Soup for the Soul*, was a collection of feel-good stories about individual success and triumph over adversity. Since then, the Chicken Soup book buyer has been offered Chicken Soup stories for every conceivable collective: moms, scuba divers, stamp collectors, abuse survivors\\n\\nA nurse works in a hospital caring for “cripples” when a crane smashes through the window and paralyzes her; now she lobbies against the government and is famous and rich. Yes, the ways in which these stories manipulate our emotional response are evident, yet, despite such knowledge, the formula works. These stories are tiny units of powerful emotional incitement. Even for a cynical reader, is there not something potentially chord-touching in these stories? Do you hate babies and cripples?\\n\\nMVH employed the same technique used in his books to emotionally ignite and detonate his audience.\\n\\nIn 1882, William Mather, a British professor, lawyer, and journalist, wrote of “success” books, “From the general spirit of these appeals, one would suppose the writers to believe that every human being at birth is potentially a Shakespeare or a Newton, and that, provided he is educated properly, and labors long and hard enough, he may astonish the world with ‘Hamlets’ and ‘Principias.’\\u200a”\\nThe inspirational story is so ubiquitous it has subgenres: rags-to-riches tales, Horatio Alger Jr. stories, Cinderella stories, ugly duckling stories. Your classic I.S. looks like this:\\nHuman in desperate, objectively bad situation—Makes accidental yet somehow fated discovery—Has epiphany—Pursues line of activity based on epiphany—Becomes wildly successful—Overcomes desperate, objectively bad situation\\n\\nOur hunger for inspirational stories is nothing new. Ancient Egypt had a genre called “Sebayt,” an instructional literature on life (Sebayt means “teachings”). Stoic philosophers Seneca, Marcus Aurelius, and Epictetus frequently mixed anecdotes with maxims on how to live, and works by these three authors can still be found in the self-help section of your local [[]]\\n\\xa0Sections of the Bible like the book of Job, Proverbs, and Ecclesiastes use parables to guide the reader’s behavior (Tyndale House Publishers even offers a Bible “self-help edition”). During the Early Middle Ages, Middle Ages, and Renaissance, mirror-of-princes books told stories of kings whose behavior should be imitated or avoided. Conduct books, which told men how to behave in polite society, were popular in Italy, France, and England during the seventeenth and eighteenth centuries. Horatio Alger Jr.’s popular fictions in the late nineteenth century featured stories of upward mobility and personal\\xa0triumph, where merit was prized over inherited privilege, and a person of modest or no means could succeed through ingenuity, education, talent. Titles like\\xa0*Jed, the Poorhouse Boy*\\xa0and\\xa0*Ragged Dick*\\xa0cued the reader toward his characters’ low beginnings.\\n\\nAs modern living became chaotic and baffling, Victorians searched for structure and order. Self-help provided this, as well as a way to organize and disseminate knowledge.\\n\\nLike\\xa0*Chicken Soup for the Soul, Self-Help*\\xa0was initially rejected (by Routledge in 1855). Smiles self-published using his own money in 1859, and the book became a sweeping best seller, moving 20,000 units. By 1900 it had sold more than a quarter of a million copies.\\xa0*Self-Help*\\xa0was published in the same year as\\xa0*The Origin of Species*; Smiles outsold Darwin. The only book\\xa0*Self-Help*\\xa0didn’t outsell that year was the Bible.\\n\\nDale Carnegie was born on a Missouri farm into a poor family. At college he could not afford room and board and had to ride horseback from home to his classes daily. First drawn to sales, he then made a brief attempt at a career in acting. In 1912, he found himself jobless in New York City, where he talked the local YMCA into letting him lecture on\\xa0public speaking. Carnegie did minimal speaking himself, asking the students to perform impromptu lectures instead. Soon he was publishing how-to books on public speaking. He approached Simon & Schuster many times about publishing his techniques, and they finally acquiesced in 1936.\\n\\nThe reliance on formulas, both for writing self-help books and for the transformations of their readers, is crucial to the inspirational story. Formulas explain to the reader how individual success can translate into a universally replicable salvation. An exemplar of the link between formulas and self-betterment comes in Benjamin Franklin’s\\xa0*Autobiography*,\\n\\nFranklin could be the poster child for the American myth of self-creation. He was a boy who, without inherited wealth, formal education, or family connections, rose to a position of wealth and influence, using only his hard work, pluck, and ingenuity\\n\\n!Untitled\\n\\nOne consistent criticism of self-help is that it is too pat, too regulatory, too mechanical. Franklin’s list of virtues, for instance, does not attempt to plumb the depths of human experience, preferring to skim the pragmatic surface of daily living\\n\\nIt’s enough to drive anyone to the brink. One feels there has to be some order, even if it is self-imposed. Self-help is like a balm for this particular wound. Buying a book can make you feel better because it makes you feel like you are in control. I have started, it says. I am about to change something. It is said that many people buy self-help books and do not read them, or start to read them but do not finish them. (Most of the self-help books I bought for research were secondhand, and were heavily underlined and annotated for the first twenty pages.) Yet the temptation to buy these books remains, offering the illusory promise of structure in the face of chaos.\\n\\n**We take ourselves seriously whether we lead serious lives or\\nnot and whether we are concerned primarily with fame, pleas-\\nure,\\n virtues, luxury, triumph, beauty, justice, knowledge, salva-\\ntion,\\n  or  mere survival.  If we  take other people  seriously  and\\ndevote  ourselves  to  them,  that  only  multiplies  the  problem.\\nHuman life is full of effort, plans, calculation, success and failure:\\nwe\\n pursue\\n our lives, with varying degrees of sloth and energy.**\\n\\n**The  things  we  do  or  want  without   reasons,  and  without\\nrequiring  reasons -  the things  that  define  what  is a reason  for  us\\nand  what  is not  -  are  the  starting  points  of our  skepticism.**', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e0cc6888-0fa0-4b96-bbf4-bde7f2c999cb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n**Economic Crisis and Consumerism**\\n\\n**经济危机与消费主义**\\n\\n多年以来，在资本主义世界强有力的叙事下，08年全球金融危机都被描绘为是一场单纯的周期性金融体系崩溃。然而，在更多的社会科学家和人文学家看来，它更代表着消费资本主义体系所不可避免的一种内爆。的确，在当下，由于广告、市场营销、明星产业和其他的文化规训，人们被教导、被说服，去消费远远超过他们的需要的产品，并常常为了维持理想的生活方式而负债累累。最后，各种次级抵押贷款、即那些将钱贷给无力偿还的消费者的贷款，必将因应这些需求、泛滥于世。事实上，美国的次级抵押贷款坏账危机，也正是08危机最主要的导火索之一。\\n\\n消费主义和次级抵押贷款的泡沫悲剧之间显见的联系向我们证明了，08危机不是一场偶然的周期性调整，而是一个结构性的恶行、即消费资本主义制度所带来的审判。它告诉我们，如果消费文化、尤其是负债购物的消费文化没有得到管理和控制，其中隐含着的自我毁灭的力量，最终导致可怕的社会和经济混乱、创伤和痛苦。然而，令人无奈的事实是，在2020年，美国的家庭消费债务总额比08年时期的13万亿美金不增反降，来到了14.3万亿美金的历史高位。就连中国，在2019年末，我们的家庭消费债务总额也总计来到了47.9万亿人民币，约等于7.2万亿美金的位置，相当于美国的50%。似乎，无论事情曾如何的坏，我们都没有从08危机所代表的结构性消费文化恶性循环里吸取应有的教训。\\n\\n的确，直至今日，无论是互联网上的关于消费主义的各种视频，亦或是各种流通于世面的类似《断舍离》这样劝诫消费主义的通俗读物，都没有涉及到消费主义的本质。大多数创作者都没有意识到，消费主义从来不是一个单独的概念或浪潮，更与个人的自律精神无关。在本质上，它是资本主义的一个强大变种、是资本主义的一种表现形式——它，是一种全新的、扭曲的宗教，通过拜物的形式应许一切的幸福和美好，以维持和压制资本主义内生的矛盾和危机。\\n\\n正因此，大量作品不谈资本、不谈资本主义而讨论现代消费行为的意涵、流变和解决方案，就不仅是词不达意的、更是不切实际的。关键点，从来不在商品身上，不在消费身上，更不在所谓“消费力所能及的、真正喜欢的事物”身上——毕竟，当你的需求被时刻伪造时、当你的劳动成果都被时刻剥削时、当消费行为被花样百出的资本运作成经济危机时，你又该如何定义“力所能及”、如何定义“真正喜欢”呢？\\n\\n消费的关键，永远在那个不可言说的背景板，那个无远弗届的资本主义制度身上。一切把消费主义视为消费者的主观个人选择和所谓道德自由，而无视资本主义文化操控之整体性的行为，本身就是其文化操控的重要组成部分。只有理智解析这种文化背后所蕴含的复杂制度性问题、而非明贬暗褒地鼓励维持现状，才是我们走出这个危机纪元的真正道路。\\n\\n这里是学院派Academia，我们致力于和教授一起、把世界顶尖大学的硬核知识漂洋过海带给你。欢迎来到硬核社会学第二期，马克思为什么是对的（下）：资本不死、消费永生，消费资本主义是如何成为控制世界的新宗教的。\\n\\n**Defining Consumer Culture**\\n\\n**定义消费文化**\\n\\n在二战后，人类掀起了两波主要的经济浪潮，一波是以股份制上市改革和金融革命为代表的资本运作全民化浪潮；一波是以扬弃本土重工业和劳动密集型产业，转以广告营销、服务业和文化业等价值链上游产业为代表的消费主义浪潮。这两波浪潮，在本质上息息相关、互为表里，构成了资本主义摆脱其在重工业时代和殖民时代致命危机、更加隐蔽地剥削世界的救命符。\\n\\n英国社会科学家迈克•费瑟斯通(Mike Featherstone)曾写了一本名为《消费者文化与后现代主义》(Consumer Cultures & Postmodernism)的学术著作，在其中，他指出，关于消费者文化的社会学研究有三个互相关联的、但都可以追溯回马克思的主要流派：\\n\\n第一个学派认为，消费文化是以资本主义商品生产的扩张为前提的，资本主义商品生产以消费品和购买、消费场所的形式积累了大量的物质文化。这导致休闲和消费活动在当代西方社会日益突出。尽管这些活动被一些人视为导致更大的平等主义和个人自由，但本质上增加了通过意识形态操纵个人自由引导社会人口服从“固有社会关系安排”的能力。\\n\\n第二个学派认为，从商品中获得的满意度与其在零和博弈中的社会结构性获取有关。在这种博弈中，满意度和地位取决于人们如何在通货膨胀条件下显示和维持地位差异。这里的关注点在于人们使用商品以创造社会联系和维持社会阶级的不同方式。\\n\\n第三个学派则更接近精神分析学派对消费的情感愉悦的探究，其旨在研究消费者文化意象和特定消费场所所赞颂的梦想和欲望，以及它们如何在不同程度上产生直接的身体兴奋和审美愉悦。\\n\\n换言之，这三个学派分别关心消费主义的三个不同侧面，即资本如何在物质上通过消费控制人民，人类如何通过消费实现阶级差异，以及消费主义如何塑造出愉悦的精神幻梦。而这，也将是我们接下来的三个主要线索。同时，我们要注意，排除这三者在方法论和研究视角上的差异，他们都一致承认，在现代资本主义社会之下，个人消费永远都只是消费文化的冰山一角。我们的个人购买行为永远不可能是一种个人行为，而与我们的社会身份和社会文化息息相关。而想了解这种身份和文化是被如何塑造出来的，我们就必须回到这三个学派的祖师爷，即马克思那里去。\\n\\n**Marx and Consumerism**\\n\\n**马克思与消费主义**\\n\\n从历史的角度来看，在现代化大规模生产之前，人类在长时间内是不具备消费主义的基础能力的。因此，无论在哪种文化里，禁欲主义和最小化消费的概念几乎都是社会意识形态的主流，尽管这种主流并不是在每个时代都被有能力的精英完美地实践。可以说，在现代社会之前的人类社会里，几乎没有任何一个族群的精英、无论他是儒家的士大夫还是基督教的加尔文主义牧师，会将过度消费作为一件值得夸耀的事情。因此，资本主义社会在面临大规模生产所产生的大规模剩余时，他们要做的第一件事情，就是破坏过去的意识形态和宗教信仰，改变人类对消费和生产的看法。而这，也正是马克思在十九世纪所洞察到的未来世界的格局改变。\\n\\n正如他所说：\\n\\n在资本主义下，每一个人都在思考如何在另一个人身上创造一种新的需求，以迫使他做出新的牺牲，使他处于一种新的、类似于奴隶的依赖状态，并诱使他相信自己获得了一种新的快乐，从而慢慢走入经济崩溃。每个人都试图在别人之上建立一种外来的力量，以便在那里找到自己的利己主义需要的满足。\\n\\n在马克思看来，在资本主义之下，处于统治阶级和拥有生产资料的人力图在人们身上创造新的“需求”，并试图将传统的需求，如亲情、友情、爱情、对工作的掌控感和自豪感异化，甚至赋予它们一个资本价值。换言之，统治者们希望把一切需求消费化，只有这样，他才能够通过自己所掌握的生产资料来源源不断地创造新需求、并且重新定义所有需求。而这些行为最终的目的，就是为了让人民处于一种“新奴隶状态”，以维持社会的阶级分化。同时，这种消费主义世界观，最终将服务于统治阶级转移人民注意力的目的，阻止他们思考变革的可能性。\\n\\n在这种系统性的操纵手段中，“异化”就起到了至关重要的作用。我们上一期视频已经着重谈过资本主义社会下的异化问题，我们这里就简单地再说一说。在资本主义社会中，工人最大的问题不光是经济利益和剩余价值被剥削，更重要的是他们与自己真正的兴趣被疏远和分离，工作只是为了创造经济效益、而不创造任何精神成就感和利他主义的正外部性。\\n\\n正如马克思所解释的那样：\\n\\n“工人只有在闲暇时才会感觉自己在家，而在工作时，他会觉得无家可归。他的工作不是自愿的，而是强迫性的。它不是一种需求的满足，而只是一种满足其他需求的手段。它的异化性质清楚地表现在这样一个事实上，那就是一旦没有物理上的或其他层面的强迫，劳动就会像是瘟疫一样被避免。工人在其产品中的异化不仅意味着他的劳动和劳动产物成为了一个客体，具有自己的存在，而且意味着它独立地存在于工人之外，对工人来说是陌生的。他所赋予生命的物体成为了一种敌对的外来力量，作为一种自治的力量站在了他的对立面。”\\n\\n于是，在资本主义社会下，人们开始变得疏远和异化。疏远和异化他们的不光是他们的工作，还有他们的朋友，他们的亲人，他们自己，和他们的生活旨趣——因为，这些传统价值在消费品的价值面前、几乎一文不值。\\n\\n最后，人们甚至将这种被异化的、与社会其他价值和关系皆疏远开来的生活状态为自豪。人们畏惧关系、恐惧社交，甚至连消费时都要尽力避免与店员的谈话，更讽刺的是，当资本家以这种脱节的社会关系为卖点时，人们甚至甘之如饴。是的，在这个时代，享受完整的、彻底的、不被打断的被异化的权利，甚至都成为了一种消费品。\\n\\n至此，商品开始获得了“自我意识和自我行为”、而人却恰好丧失了这两者。\\n\\n当然，在资本主义社会中被异化的远不仅仅是工人。在马克思看来，资本主义社会中的每个人都被异化了，因为那些统治阶级的成员也被异化了，他们将永远担心任何形式的变革将影响到他们的统治地位。当他们看到火苗时，他们寝食难安。于是，他们便需要更多的、象征着自己的异化状态的军功章，比如一副拥有腹肌的、由高档健身房健身教练和药品打造出来的皮囊，或者斥资千万打造出来却从不踏入、最大的功能就是找个看房网红来拍视频的私人图书馆。换言之，他要告诉全世界，我之所以成功、之所以可以决定你们的异化状态和模式，是因为我比你们更异化、更扭曲、更付出，甚至仅仅是睡的更少。\\n\\n进而，异化不仅不再被视作异化，它更被视作一种荣誉、一种规范。而无法承受这种规范和荣誉的人，不仅将被视为失败者、更将寻找不到任何形式的人生意义和社会认可。于是，这种全方位的异化和压迫最终将工人阶级赶入了消费文化，就好像沙漠里寻找清泉的人臣服于海市蜃楼的幻象引沙止渴一般。人们逐渐发现，只有当他们购买东西时，他们才能找到短暂的满足感，才能摆脱困扰他们的异化状态。于是，一个类似毒品的恶性循环被创造了出来。\\n\\n在资本主义国家，工人工作的越多、被异化的就越多。这导致他们必须通过购物和参与消费文化来逃避这种异化的感觉，但这往往需要更多的金钱。于是，他们就必须越来越努力地工作、越来越努力地被异化。在当下，这种痛苦不光是个恶性循环，它更像是一个死循环。\\n\\n然而，如何确保工人能在消费上获得足够多的虚假满足、而不去思考这个系统本身呢？这就要提到广告、公共关系、市场营销，甚至整个泛文化行业为消费资本主义所起到的润滑剂作用了。\\n\\n对于资本主义社会而言，广告销售的不光是产品，更是一种普遍参与的、无法被规避也不允许被规避的消费性社会秩序。广告通过产生人们的不满和焦虑来发挥作用，并以弥漫在资本主义社会中的异化感为食。它通过把注意力集中在个人消费上来缓和这些感觉。你可以躲避消费，但你无法躲避广告。至此，人们不再关注统治阶级统治资本主义社会的方式，也不再关注财富分配的严重不平等。一切，都只与自己当下的、片面的需求有关。\\n\\n此外，广告的另一个作用就是最大程度地促进自恋。它要求人们把注意力集中在自己的个人需求或欲望上，而忽视社会和政治问题以及他人的需求，更重要的是，忽视那些没有被广告推送的需求。广告切断了人对同阶层人民的团结意愿和同情，它让社会的中低层劳动者永远无法真正联合起来。于是，我们便浑浑噩噩地相互斗争、像被驯养的鬣狗一样互相残杀以渴求主人今天恩赐的饭食。然而，主人们往往还对鬣狗负有感情和义务，而对劳动者而言，他们只有一个身份，那就是屠夫。\\n\\n想要详细分析广告对消费主义的作用，以及广告消费主义是如何为人类营造出永不间断的虚假满足感、最后得以让我们忘记自己被剥削和异化的事实的话，我们就不得不提到另一尊大神，那就是继承了马克思精神的、上世纪最重要的后现代哲学家和社会学家之一：鲍德里亚。\\n\\n**Baudrillard and the World of Ad Simulacra**\\n\\n**鲍德里亚与广告性拟像世界**\\n\\n让·鲍德里亚是二十世纪法国最重要的社会科学家和哲学家之一。由于广告在消费文化中的重要性，他所写的《物体系》一书中的最后一部分就致力于分析广告的意义。\\n\\n他曾这么写道：\\n\\n任何对客体系统的分析最终必然意味着对客体的话语分析——也就是说，对促销“信息”(包括图像和话语)的分析。广告不仅仅是物体系统的附属物；它不能脱离物体系统，但也不能限于它的适当功能，因为广告的作用从来都不是严格提供信息的。广告整体上造就了一个无用和不必要的宇宙。它是纯粹的、被编织的“意义”。它对生产或对事物的直接实际应用没有任何贡献，但它在物体系统中发挥着不可分割的作用，不仅因为它与消费有关，而且因为它本身就已经成为消费的对象。\\n\\n换言之，在鲍德里亚看来，在现代社会广告负责促进消费，而在后现代社会，广告本身就是消费的对象。再过去，广告是寄生在消费品的实际作用上的产出物，而在现在，消费品必须依循广告、尤其是由广告的整体系统性（而非单一广告）所营造出来的文化幻象的意志来设计自己。\\n\\n于是，越来越多的广告，尤其是那些大公司的广告，比如豪华汽车和电子产品，更不必说奢侈品行业，他们的广告通常与产品本身的细节毫无关系，而致力于埋下草蛇灰线的情感线索。\\n\\n因此，这个社会从“功能”和“实体”，走向了“符号”和“拟像”。也只有符号和拟像，能超脱于现实世界，为资本创造出无穷无尽的对更美好生活的幻象。\\n\\n同时，我们还要注意，广告所诉诸的不是现代性的核心、即理性精神，而恰恰是后现代性的核心、那就是对无限阐释和无限意义生成的反理性精神的商业性构建。换言之，广告所追求的，是完全剥夺工人的理性思考能力，将其降格为可以由不断被权力所重塑和再定义的话语、阐释和符号所操纵的木偶。\\n\\n诚如伦敦大学社会学家 David Walsh所说：\\n\\n“当代社会是一个后现代社会，它不再是由符合其需要的个人生产所构成，而是由各种各样的符号交换所构成。因此，后现代社会是一个由不同群体组成的异质性社会，在话语、生活方式、身体、性、交际等层面上都有独特的日常生活规范和实践，它涉及到对主导着现代主义社会生产逻辑及其工具理性的否定。但可悲的是，这种不同群体的异质性和独特生活方式都被广泛地符号化和商业化了。现在，资本主义创造出了一个以属性和符号为尊的消费社会。这是一个基于新的技术和文化形式的模拟社会。符号现在有了自己的生命，成为社会经验的主要决定因素。符号和代码取代了现实，这个世界是通过各种符号的模拟再生来体验的，以至于与符号原本寄生的、本质的真实消失了。一个超现实的世界被创造出来，在这个世界里，世界上的一切都被模拟出来，在某种意义上，由符号创建的模型取代了真实。”\\n\\n然而，消费性文化符号的本质是消费过去确实存在过的文化属性，比如摇滚文化、巴洛克主义、军事文化，以及其他更传统的艺术创造形式，比如诗歌、绘画、文学等等，它本身并未创造出任何的新兴文化。这也是为什么时尚界永恒的定律是“复古”，因为“复古”本质上就是对过去的无限抄袭和复制，反映的恰恰是整个广告消费业本质创造力的虚空。\\n\\n换言之，广告消费主义永远都在不断消耗过去，而没有创造出新的事物。正因此，鲍德里亚提出，广告消费主义最后会消耗掉人类所有现存的文化符号。\\n\\n但广告消费主义依托于消费资本主义的底层运作逻辑，资本不可能允许它就这么死去，因为它背后背负的甚至不光是几十个百亿级别企业的生死，更是整个资本主义得以欺骗人民而存续的龙脉。因此，鲍德里亚提出，一种不反应现实生活意义，只与系统内的符号互动的、符号的符号出现了。他将其称之为，拟像。\\n\\n在鲍德里亚看来，拟像与符号最大的不同在于，符号仍然致力于模拟某种程度的真实，而由于拟像本身所模拟的就是不真实的符号，它就可以完全逃脱出真实世界的引力，为自己创造一个纯粹的权力-话语体系。而这种体系，则完全是为资本所服务的，因为它无拘于任何形式的真实。就拿这几年很火的球鞋文化为例来说：\\n\\n在最开始，球鞋仅仅是球鞋，它的功能性和实体价值是最重要的，哪怕是广告，所描绘的也更多是球鞋在面料和工艺上能为实战所带来的帮助。\\n\\n然而，在球鞋的第二阶段，即Air Jordan品牌出现之后，Nike开始大规模地和各大球星、球队等文化符号所挂钩，进入一个大营销时代，这个阶段也被大量的商业学术研究列为全球价值链提升的典型。至此，球鞋进入符号模拟阶段，消费者对球鞋的购买欲望已经从这鞋好穿好用转移到了这鞋乔丹穿过。然而，即使如此，这时的符号模拟仍然反应了某种层面的文化现实，比如某双鞋见证了某个著名的绝杀、或者被协会禁穿的轶事。因此，它也确实具有某种“真实”文化意义。这种文化意义广泛地存在于人类社会的所有阶段中，就好像所有教堂里的十字架一样，尽管并非本真原初之物，但也反应某种文化现实。\\n\\n但是，在球鞋文化的第三阶段，即近几年出现的所谓“炒鞋”文化里，各种针对符号、而非符号所模拟的现实的二次模拟、即拟像，也就出现了，那就是所谓的“球鞋系列复刻”和“经典球鞋再设计”。所谓的复刻和再设计球鞋，他们所模拟的对象已经不是当年的某场具体比赛所代表的文化属性，而是球鞋这个物体系统本身所具备的价值。如果说AJ1初代禁穿和扣碎的价值来自于那几场赛事，那接下来所有的复刻和再设计鞋款，他们的价值就只来自于禁穿和扣碎这些本身就是符号模拟的鞋子本身。初代球鞋的意义来自于它所模拟的事件，而复刻球鞋的意义却来自于初代球鞋。前者是符号模拟，而后者是纯粹的拟像，因为前者仍受限于基本的真实性，而后者则可以被无限复制。大家自己去看看现在改个色印个图标联个名就能卖到上万的球鞋就知道什么是拟像了。\\n\\n至此，广告消费主义找到了解决文化符号短缺的出路。而拟像的生产，由于其完全脱离真实世界本身，便可以无限地根据商业利益被再塑造、再定义。今天可以流行巴黎的奢侈，明天就可以流行美国的摇滚，今天可以捏造岁月静好，明天就可以鼓吹丧即是真。于是整个大规模生产系统再也不愁缺少可以操纵人心和制造幻梦的各种文化符号。\\n\\n然而，对鲍德里亚而言，这种拟像消费相比于符号消费而言更次等、更无法真正满足人类的需求，因为他们消费的产品只是“虚假的物品或者幸福的特征标志”，并没有任何真正的力量赋予拥有者幸福。至此，空虚、不快乐的消费者别无选择，只能消费更多的产品，希望找到满足感。这便是又一个永无止境的死循环。\\n\\n于是，在最后，所有的事物都沦为了消费品，而所有的消费品也都丧失了它的原本现实意义，逐渐成为一种文化符号。而人类社会所存在的唯一意义，也便沦为了创造和交易各种各样新兴的文化符号。资本和商品于此刻，终于跳脱出了凡胎肉身，以符号为神、文化为魂，踏上了漫漫的登神长阶——也是从此刻开始，我们要面对的，就不再是某一个单一的个体或者群体，而是一种超越了单一意识、无远弗届般存在的宗教秩序。\\n\\n而这，也解释了消费主义和资本主义为何如此难以反抗。因为，我们不是在屠龙——而是要弑神。龙是一个敌人，比如大洋彼岸的某个国家，哪怕它再强大，它的本质也是肉体凡躯。你可以在经济、军事、政治上打败它、消灭它。这没有问题，这可以做到。然而，神是一种秩序、是一种广泛的意志、是一种超越的共谋。它无法在物质上被消灭，它永远可以转世重生。能取代神的，只有新神。一神教秩序曾是一种最强大的神，哪怕你把欧洲所有的教堂烧掉，你也无法浇灭他们的信仰。它无法被消灭，只能被继承，曾经，它的继承者叫国家、叫殖民主义，现在，它的继承者叫消费、叫资本主义。龙是神的宿主，也是它的表象。如果你不能弑神，屠龙者或许只是下一个容器。\\n\\n**Consumerism as Individual Freedom, Cosmopolitan Culture, and Class Politics**\\n\\n**作为个人自由、全球文明和阶层政治的消费主义**\\n\\n的确，目前，世界上任何一个主要城市的街景都可以证明消费文化的宗教性地位。和遍布欧洲各地的教堂一样，今天的商场们在外形和精神上也出奇的相似。没错，正如马克思所指出的，资产阶级通过对世界市场的剥削和控制，赋予了所有国家的产品和消费以世界性的特征。这个世界既不是通过政治信念也不是通过任何其他的共识凝结在一起的，统合这个世界的事物，本质上就是消费品和市场利润。\\n\\n因此，与之前所有的时代不同，这个时代的自由只有一个，那就是购物自由、消费自由。这就是消费主义第一个近乎神性的宗教承诺，那就是个体的自由和解放。\\n\\n这种对自由的再定义，事实上也是那么多人像狗一样努力工作的原因。现代社会的雇佣制度把人类社会切割成了工作和生活两端。人们忍受着工作的负担，这样他们就可以买到他们想要的东西。我们工作是为了生活，但生活需要由我们拥有的东西来定义。\\n\\n和宗教的承诺一样，消费主义的力量说服我们，让我们确信可以买到所有我们自己不具备的东西，例如，美貌、名望、朋友，这诱使着我们自愿地被资本主义夺舍肉身、来为它延续生命。\\n\\n同时，对我们来说，购物、消费和拥有东西是如此“自然”，我们并不认为这是一种特权或文化属性，就好像中世纪的欧洲人不会质疑信仰的必要性一样——他们只会争辩一根针头上能站几个天使，就好像我们争辩口红有多少色号一样。消费和信仰一样，它就是我们的存在、是我们自由的根基。这正是意识形态在日常生活中的力量：它最大的力量不光在于它定义了我们是谁，驱使了我们去做我们想做的事情——更重要的是，它让我们更少地去提出问题。我们既不思考它是否是一种特权，更不思考它是否是一种镣铐。它仿佛是一个思想上的中立地带或真空领域，但又无远弗届地存在着。\\n\\n此外，我们很少想知道购买的冲动来自哪里，或者商品是如何生产的。只有当一个最喜欢的品牌从货架上消失时，我们才会想，究竟是什么不自然的事情导致了它的神秘消失。没错，商品的出现是自然的，我们不会问他为什么出现；而商品的消失反倒是不自然的，它象征着关于我们的一部分定义被抽走了。没错，在我们的生活中，商品不仅是被期待和垂涎的存在，它更是我们的日常社会存在的锚定物。它使得资本主义如此诱人，更使得批判资本主义如此困难，哪怕是在智力层面批判它都仿佛是天方夜谭。\\n\\n我们被消费迷住了，因为消费对我们来说才是真实的。我们更可能去购物，而不是去阅读、参与讨论，或者是单纯地思考。时至今日，当你和你的朋友出门时，你几乎难以找到一个与消费不挂钩的活动。公共假日也是一个很好的例子，比如那最讽刺的五一劳动节。在这些日子里，我们不会思考劳动力的价值，我们只会关心什么东西打折了、打了多少折。\\n\\n事实上，当代资本主义最大的陷阱就是，当你完成一天沉重的工作时，消费就会像毒品一样催促着你完成最快捷的多巴胺分泌。你的工作越累、越不自由，你就越需要在消费中寻找付款的轻松和选择的自由。也因此，你就越不会思考为什么你要工作的这么辛苦。没错，你需要消费来帮你忘记工作中的辛苦。于是，那些工作中最辛苦的人，反而成为了最不可能改变这个剥削他们的制度的人。当变革太过宏大、太过遥不可及，而透支性享受又近在咫尺时，你永远无法说服、更不具备这个道德高低来说服任何人选择变革、而放弃享受。事实上，这正是他当年未曾设想到的未来，在这个时代里，人人都是工贼——却又不得已成为工贼。\\n\\n由于选择的自由在日常生活中如此规范化，我们幸福地没有意识到构成我们购物自由基础的社会关系，即与商品生产的社会关系，以及由此而来的工人和资本家与资本和利润的不平等关系。这，也正是“商品拜物教”的作用和由来。\\n\\n的确，我们如此执着于商品本身，以至于我们不知道它到底是什么。正如资本主义的其他方面一样，我们把商品具象孤立化，好像它们是有自己生命的东西，好像它们似乎神秘地独立于生产和消费的社会组织，因此我们可以无思考、无负担地使用它。\\n\\n此外，消费的诱惑进一步抑制了阶级意识的发展；如果我们都可以去购物中心，消费资本主义生产的商品，我们为什么要担心有些人比其他人有更多的东西可以消费？毕竟我们都享有购物的自由，我们都享有在资本主义社会生产的商品。因此，错误的意识不仅仅意味着我们自由地同意在劳动力市场上推销自己，以至于我们的社会地位和存在不仅弱于我们自身所创造的事物，更弱于我们所购买的事物——没错，在这个时代，物要远比人重要得多。\\n\\n因此，我们不得不欺骗自己，如果我们买更多的东西，我们就会更有价值。过去，他曾说，“我们除了枷锁什么都不会失去”。然而，现实情况是，就像柏拉图的洞穴隐喻所说的那样，当你的镣铐待久了之后，你就会对跪着这件事情上瘾。因此，消费和消费意识形态将我们与资本主义联系在一起——它们成为了全球文明的新标志。\\n\\n于是，这种所谓的“全球文明”为一切的不平等作了最精巧的辩护。现代消费社会不仅有目的地系统地生产产品，而且还有目的地进行差异化。这样做的目的是将人与人之间真正的差异缩减为他们所拥有的消费品之间的差异。进而，人和人之间在权力上真正的差距，被缩减为了他们所拥有的消费品之间的差距。换言之，在这个时代，当人们看向资本家时，人们看不到他们手上所掌握的权力，看到的是他们的各种奢侈消费享受。这带来了两个结果：一、资本家真正的权力被遮蔽了，这削减了冲突的可能；二、消费层次的差距创造了一种虚假的目标感和自由感，仿佛普通人只要也能消费起这样的东西，你也就获得了和资本家一样的社会阶层和权力地位一样。\\n\\n换言之，在消费资本主义之下，人和人的差异和差距似乎都可以用购买的商品不同来表征。如果说差异的商品化仅仅是让所有人被虚假的个性所蒙骗，那么差距的商品化就是让人们心甘情愿地接受不平等的现状，因为他们意识不到一切除了商品之外的、实质的权力差距。只要他们拿起最新款的iPhone，他们仿佛也成为了和那些生产出iPhone的硅谷精英一样的人。然而，我们都知道，猎物和猎手，是不可能同日而语的。\\n\\n**The Religionization of Consumerism**\\n\\n**消费主义的宗教化**\\n\\n没错，当消费主义开始为自己树立自由、个性、文明和平等的圣碑时，它便正式开始接过耶稣的权杖、成为一个新型的宗教。\\n\\n在鲍德里亚之后，我们可以发现，后工业时代的消费已经彻底脱离了马克思时代所经历的功能性消费，而走向了彻底的符号消费和拟像消费。人们从消费中所获得的快感已经从重工业时代所标志的各种物理感官享受，走向了更好被资本主义操控、更无穷无尽的伪精神文化享受。这种后现代的消费主义相比于马克思时代的现代消费主义更危险之处在于，它几乎完全脱离了人类的物质现实，因此可以被无穷无尽地生产和无穷无尽地消费。因此，它是一种更好地缓解工作异化的精神鸦片，也是一种更好的、解决资本主义大规模生产剩余和行业永动性的工具。\\n\\n诚如鲍德里亚所说，广告管理我们的需求，它教会我们从消费每一件物品中期望得到什么。换句话说，它告诉我们每个物体给我们什么，或者一个物体给我们带来了什么样的结果。所以说，广告不仅仅鼓励我们购买产品。它还把我们带到了一个世界，在那里我们学会了如何将特殊的事物与特殊的社会概念和意义联系起来。\\n\\n因此，即使广告不能说服我们购买某种特定的产品，它仍然是一个赢家，因为它的主要目标是迫使我们接受广告语言或对象的意义及其与我们生活的关系。简而言之，广告的主要目的就是让我们相信并遵守它赐予我们的规则。今天，“我们不再通过产品本身来消费产品，但是我们通过广告来消费产品的含义”。于是，广告成为了一种新型的、架设在虚空里的宗教，它用一种全新的规则关系教育我们，让我们相信我们生活在一个关心我们、解决我们的问题、满足我们所有需求的社会。在过去，无论任何信仰，天堂都是一道窄门，你只有不断削减自身的外力、还原自己的本质，你才能穿的进去。而现在，天堂似乎是为重度肥胖症患者设计的加护病房，你只有不断地增肥、不断地增肥，让外物充斥你血脉和灵魂的每一寸，你才能走进这个极乐世界。\\n\\n于是，在这种消费的宗教化下，拥有产品不是主要目标。相反，其主要目标是通过消费完成“自我实现”。消费社会通过广告，使自我实现成为了一件切实的操作。你拥有了一件汉服，你就是汉文化最正统的捍卫者。你拥有了一件Lolita，你就是19世纪的英国小公主。你拥有了每季最新的电子产品，你就是美国硅谷的极客精英。而且，你的裙子越正、你的电脑越贵，你的自我成就感就越高、纯度就越浓，哪怕你压根没读过四书五经、哪怕你对近代的欧洲历史一窍不通、哪怕你不剪辑视频也不打3A大作，只要你消费了，你就是你所相信的那个人，而余下的我们，也会鼓掌认同。\\n\\n正如鲍德里亚所说，“我们的内部冲突或深层倾向在消费过程中被动员和异化，就像劳动力在生产过程中被异化一样”。广告系统创造了一个地位和声望的等级制度，并迫使我们接受它们。每一个商品都是一个符号，在这个系统中具有特殊的意义和地位，它可以将一个人定位在一个特殊的权力和地位上。\\n\\n现在，这个商品可以作为人类特征的指示器，如性格、举止、人性、情感、勇敢等等。不管人们喜欢与否，他们生活在一个物体系统中，其他人根据这个系统的规则来评判和对待他们。\\n\\n于是，通过拥有一张可以让所有的家庭成员都可以挨着坐的餐桌，我们可以表明家庭的团结和统一，即使他们从来没有聚在一起吃东西；通过在起居室里放一件乐器，我们可以表明一个人对音乐的热情，即使他不知道如何演奏它，甚至对音乐不感兴趣。\\n\\n情感、忠诚和人性本来是必须在现实中进行表达的概念关系和生活经历；但在现代物体系统中，这些概念通常是通过商品来表达的。\\n\\n诚如本杰明·巴伯所说，消费主义成为了一种新颖的身份政治，在这种身份政治中，企业自身扮演着打造有利于买卖的身份的角色。身份在这里成为“生活方式”的反映，这种生活方式与商业品牌及其标签的产品，以及与我们在哪里购物、如何购物、吃什么、穿什么等与消费相关的态度和行为密切相关。这些属性反过来又与收入、阶级和其他经济力量相关联，这些力量看似允许选择，但实际上在很大程度上受到统治阶级的影响，超出了个体消费者的控制范围。品牌生活方式不仅仅是深层身份认同上的表象，而且在某种程度上必须成为替代身份认同、并获得个性的形式。它们取代了传统的种族和文化特征，压倒了我们为自己选择的身份认同的自愿方面。\\n\\n至此，消费便彻底宗教化了。\\n\\n在这种普世的宗教下，我们相信消费社会能满足我们所有的需求，我们需要做的仿佛只是更加努力就好了。于是，即使一个人生活在最糟糕的条件下，即使他从这个社会中得不到任何好处，他也从不怀疑或批评消费者社会的系统，因为他或她相信这个系统是完美无缺的。如果你的需求得不到满足，这不是系统的问题，而是你个人的问题，因为有足够的产品和服务供每个人使用。消费社会是为每个人的财富和幸福而建立的。所以，如果你不能得到它们，那是你的问题：你应该更努力地工作来改变这个现状。\\n\\n这种观念是个人处于被动的主要原因之一，也是制度得以生存的主要原因。\\n\\n消费的意识形态试图为我们创造这样一种信念：创造性地劳动以获得意义的时代已经结束，我们生活在消费的时代。在这个时代，史无前例的，仿佛一切都准备好满足我们所有的需求和欲望。\\n\\n的确，我们生活在一个几乎一切都依赖于消费的社会。我们所有的需求和需求，无论是精神上的还是生物上的，都必须通过消费社会所规定的那种消费来满足。此外，所有通往幸福和成功的道路都已经在消费社会中定义和确定。我们唯一要做的就是跟随他们。消费社会中的人们比以往任何时候都更有希望，因为他们只需要等待着通过购买好运和幸福的标志来获得好运和幸福——在这个时代，救赎似乎触手可得。\\n\\n仿佛天堂近在咫尺，仿佛你我皆是信徒。\\n\\n还有比这更可怕的世界吗？\\n\\n**How to Break Consumerism Dilemma\\xa0in the Age of AI**\\n\\n**AI弑神：消费主义的破局之道，人类最危险的时代**\\n\\n的确，我们这个世界已经接近走到了最危险的境地。08年的经济危机是一次警告，但社会大众似乎完全无视了它的教训。哪怕这个社会中有一部分人意识到了消费主义的真相，理解了消费主义是资本为了剥削人民，并让人对工作异化和生产剥削无感的一种手段，他们似乎也无法走出消费主义的陷阱，更无法说服他人加入他们。\\n\\n靠民众的自我意识反抗消费意识形态的操纵是一条出路，但经济基础决定上层建筑，在高压的资本社会工作制度之下，在经历了一天的劳累、并明知自己无法在未来逃脱出这种境地的绝望之后，你真的能选择不投入消费主义的怀抱吗？依赖民众的意识觉醒固然是一条道路，但这条道路太不稳定，不稳定到只要有一点甜头，就会出现无数的反叛者和工贼。这注定将是一个脆弱的联盟。\\n\\n然而，如果消费者无法达成联盟——那么生产者是否可以呢？为了寻求这个问题的答案，我们回到了更宏观的经济史和人类学视角上。这次，我们发现了新的东西。\\n\\n在现代资本社会之前，消费和生产并不是分开的事物，它的核心、也是它们的平衡点，是交换。为什么前现代社会不存在消费主义，因为消费主义的一个核心前提就是对生产的抽离。而在前现代社会里，每一个物品都是被相对单一、相对能见、相对个体化的生产出来的。你消费的不光是一个物品，你还能相对感知这个物品的完整生产过程。同时，当你自己销售你的生产物时，你也能被消费者所感知到。这种互相感知，最终成为了彼此对对方价值的承认，也保证了交换的天平两端不会失衡。此时，消费的意义从消费这件孤立的行为，转化为了对对方生产价值的认可。这与黑格尔-科耶夫的“承认理论”也是暗合的。\\n\\n然而，当全球资本化击穿了具有能见度的本体交换模式，并用全球化的大规模工业生产消费品时，个人是意识不到消费品背后的生产过程和生产属性的。甚至连购买一件商品的过程，我们都希望他可以尽量地“非人化”，比如店员不要找你说话、快递员把东西放门口就好。于是，当我们在消费一个产品时，我们不会花心思去思考它是怎么被“人类”制作出来的，中间有多少辛苦、努力、巧思和剥削。我们只需要使用它就好了。这种效应被当代社会心理学家称之为资本主义的集体失忆症。而这种失忆症在劳动过程被普遍机械化之后就愈发强烈了，因为真正的人类离切实的生产过程越来越远了，每个人都成为了螺丝钉，没有人是真正的一件产品的生产者。\\n\\n也正因此，相对应的，在全球资本和工业的劳动和工作过程中，我们也往往预设，我们的劳动成果不会得到他人的尊重和认可，哪怕具有尊重和认可，这份情感也不会转嫁到我的身上，而会加之在已经被拟人化、甚至被宗教神像化的“消费品”和“公司”身上，或者某些资本家身上。而我，也再也无法在生产中寻找到价值依归。\\n\\n同时，由于扭曲的工作制度和生产制度，劳动者在工作过程中被剥削的时间、精力和劳动所得完全落入资本家的掌握之中。人们越来越辛苦，消费地就越来越多。最后，工作不光成为不了归属，反而成为了瘟疫。\\n\\n换言之，正因资本主义供应和生产网络的发达、正因资本家在劳动过程中的剥削，以及商品和符号拜物教的神化，人类再也无法从“生产”、“工作”和“劳动”这件在马克思看来决定了人性本质的事物获得实体价值和精神价值。也正因此，交换的天平两端被打破了，消费成为了唯一的、让我们获得价值依归的事物。我们从“生产决定了我们的身份”，走向了“消费决定了我们的身份”。\\n\\n这种消费资本主义会带来一个主要结果和两个伴生结果。其主要结果是：\\n\\n因为消费成为了价值依归，也因为人类在资本主义制度下生产过程极其痛苦且异化，人类会逐渐遗忘生产的价值，进而认为生产对于自己的价值实现是无关紧要的。于是，干什么职业也是无关紧要的，只要能赚钱、能通过消费“止痛”就行。进而，人类会开始丧失自己的工作能力，以至于完全放弃生产的个人性，完全投入消费。\\n\\n这个主要结果将带来两个极其可怕的伴生结果：\\n\\n1、由于人类抽离于商品生产，商品中的“人性”，也就是那些反应生产者个人特质、经历和情感的事物会逐渐降低，最后走向最彻底的沃尔玛化和麦当劳化。而这种无人性的造物，又将在以消费为主的世界中被捧上神坛，最终，整个社会将慢慢滑向鲍德里亚所说的符号和拟像的深渊，成为真实性的荒漠。\\n\\n2、由于工作者对于“生产”的重视逐渐降低，资本家将加快机械化和AI化的进程。换言之，当所有人都只看中自己消费者的身份，并以此逃避资本主义在生产端的异化时，资本家就会彻底抛弃人类劳动力。同时，由于工作者对生产的关注度下降，他将离具有决定性地位的生产技术和资料越来越远。最后，当资本主义决定全面机械化和AI化时，底层劳动者将在任何维度上都没有与之抗衡的能力和技术。注意，是任何维度上。至于有几个维度，大家可以自己想象。\\n\\n也因此，想要解决消费问题，以及消费问题所带来的生产问题，我们就不能只靠呼吁民众意识觉醒来解决消费问题，而是更要通过更复杂的途径来解决生产的异化问题。我们需要让人类在工业体系和泛金融化世界秩序下，获得对工作生产更多的掌控权，让人类更少地在工作中被异化、更少地被剥削。只要生产不被异化和剥削、工人可以掌权，只要人类在生产中能获得更多的精神和物质价值，消费的价值自然就会被降低，而我们也能拥有更多有灵魂的、有情感的劳动结晶。而消费和生产也将从对立回归统一，回归到其作为“交换”的天平两端。\\n\\n而这个过程，就更加地复杂，因为这涉及到人类持续了两百多年的工作制度该如何被改善的问题。它不光与人文学科和社会科学有关，更与经济学甚至神经科学和人工智能领域相关。AI究竟是会帮助资本家彻底消灭整个劳动力阶级、以实现消费主义的利润最大化，还是会赋能普通工人、让人能够跳脱繁琐的螺丝钉劳动，以机器所不能及的人性光辉发挥出独特价值，这不光是一个科技问题，而将是人类在本世纪所将面临的最复杂的问题。AI也许不会自己成神、消灭所有人类，但只要我们在人文、社科、科学、工程，以及广泛的社会合作和制度安排上犯一个最小的错误，最后的结果都有可能是AI成为消灭普通人类的最好工具。\\n\\n当消费主义消灭了生产的意义、并将生产交托于AI之手时，它必侵蚀了人类最基础的人性，剥夺了人类所有的创造力，剥削了人类所有的劳动成果。\\n\\n人类，便已到了最危险的时刻。\\n\\n就如我们前面所说的，能杀死神的只有新神，屠龙战争只不过是外在的表象。而新神往往吞噬着旧神的尸体而生。欧洲现代国家得以建立、殖民主义得以伸张，与基督教的世界格局和宗教精神紧密相关，但它在成功后，几乎彻底消灭了宗教在政治中的话语权；消费资本主义得以成型，则与现代国家的殖民主义和帝国主义息息相关，但它最后成为了跨越国境、一统人类的凶兽。哪怕是它最忠诚的信徒美利坚，也会被他的贪婪残忍吞食。那么，寄托于消费资本主义而生的AI，是否又会彻底消灭其父神的格局呢？而这种格局的破碎，究竟是重生、还是毁灭？这与我们每个人息息相关。\\n\\n这场弑神的战争，没有人可以逃脱。\\n\\n是选择劳动AI化和资本全球化下的真实的荒漠，还是选择人人都能在生产和消费两端获得更大自由和快乐的新世界——未来的命运仍在我们手中。然而，我们想要的未来需要卓绝的努力和牺牲才能得到，为此，我们必须学贯文理、不问中西。而我们不想要的未来，其实只需要我们一直堕落和愚昧下去就可以了。\\n\\n怎么选，看你自己。', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f2a06e6c-132f-46c5-9d11-ef43a175174f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nmachine learning system design\\n\\n!Untitled\\n\\n!Untitled\\n\\n!Untitled\\n\\n**Experimental Stage**\\n\\n1. 1.**Data store:**\\xa0The data store refers to wherever data relevant to data analysis and model development is stored. An example of a data store could be using Hadoop to store large volumes of data, which can be used by multiple model development teams. In this example, data scientists can pull raw data from this data store to start performing experiments and conducting data analysis.\\xa0\\n2. 2.**Process raw data:**\\xa0As\\xa0previously\\xa0mentioned, raw data must be processed in order to collect the relevant information. From there, it must also be purged of faults and corrupted data. When a company collects massive volumes of data every day, some of it is bound to be corrupted or faulty in some way eventually, and it’s important to get rid of these points because they can harm the data analysis and model development processes. For example, one null value entry can completely destroy the training process of a neural network used for a regression (value prediction) task.\\xa0\\n3. 3.**Data analysis:**\\xa0This step involves\\xa0analyzing\\xa0all aspects of the data. The general gist of it was discussed earlier, but in the context of updating the model, data scientists want to see if there are any new trends or variety in data that they think the model should be updated on. Since the initial training process can be thought of as a small representation of the real-world setting, there is a fair chance that the model will need to be updated soon after the initial deployment. This does depend on how many characteristics of the true user base the original training set captured however, but user bases change over time, and so must the models. By “user base,” we refer to the actual customers using the prediction services\\xa0of\\xa0the model.\\xa0\\n4. 4.**Model building stage:**\\xa0This stage is more or less the\\xa0same\\xa0as what we discussed earlier. The second time around, when updating the model, it could turn out that slight adjustments to the model layers may be needed. In some of the worst cases, the current model architecture being used cannot achieve a high enough performance even with new data or architectural tweaks. An entirely new model may have to be built, trained, and validated. If there are no such issues, then the model would just be further trained, tested, validated, and pushed to the code repository upon meeting some performance criteria.\\n    ◦ An important thing to note about this experimental stage is that it is quite popular for experiments to be conducted using Jupyter notebook. When model development teams reach a target level of performance, they must work on building a workable model that can be called by other code. For example, this can be done by creating a model class with various functions that provide functionality such as\\xa0load_weights,\\xa0predict, and perhaps even\\xa0evaluate\\xa0to allow\\xa0for\\xa0easier gathering of performance metrics. Since the true label can’t be known in real-time settings, evaluation metrics can simply be something like a root-mean-squared error.\\n\\n**Deployment Stage:**\\n\\n1. 5.**Model deployment:**\\xa0In this case, this step is\\xa0where\\xa0software engineers must manually integrate the model into the system/application they are developing. Whenever the model development team finishes with their experiments, builds a workable model, and pushes it to the code repository, the engineering team must manually integrate it again. Although the process may not be that bad the second time around, there is still the issue of fixing any potential bugs that may arise from the new model. Additionally, engineering teams must also handle testing of not only the model once it is integrated into the application, but also of the rest of the application.\\n\\n1. 6.**Model services:**\\xa0This step is where the model is finally deployed and is interacting with the\\xa0user\\xa0base in real time. This is also where the operational team steps in to help maintain the functionality of the software. For example, if there are any issues with some aspect of the model functionality, the operational team must record the bug and forward it to the model development team.\\xa0\\n2. 7.**Data collection:**\\xa0The operational team can also collect raw data and performance metrics. This data\\xa0is\\xa0crucial for the company to operate since that is how it makes its decisions. For example, the company might want to know what service is most popular with the user base, or how well the machine learning models are performing so far. This job can be performed by the application as well, storing all the relevant data in some specific data store related to the application.\\xa0\\n3. 8.**Data forwarded to data store:**\\xa0This step is where the operational team sends the data to the\\xa0data store. Because there could be massive volumes of data collected, it’s fair to assume some degree of automation on behalf of the operational team on this end. Additionally, the application itself could also be in charge of forwarding data it collects to the relevant data store.\\n\\n!Untitled\\n\\n1. .**Feature store:**\\xa0This is a data storage bin that\\xa0takes\\xa0the place of the data store in the previous example. The reason for this is that all data can now be standardized to a common definition that all processes can use in this instance. For example, the processes in the experimental stage will be using the same input data as the deployed training pipeline because all of the data is held to the same definition. What is meant by\\xa0**common definition**\\xa0is that raw data is cleansed and processed in a procedural way that applies to all relevant raw data. These processed features are then held in the feature store for pipelines to draw from, and it is ensured that every pipeline uses features processed according to this standard. This way, any perceived differences in trends between two different pipelines won’t be attributed to deviances in processing\\xa0procedures.\\nPresume for an instance that you are trying to provide an object detection service that detects and identifies various animals in a national park. All video feed from the trail cameras (a video can be thought of as a sequence of frames) can be stored as raw data, but it can be possible that different trail cameras have different resolutions. Instead of repeating the same data processing procedure, you can simply apply the same procedure (normalizing, scaling, and batching the frames, for example) to the raw videos and store the features that you know all pipelines will\\xa0use.\\xa0\\n2. 2.**Data analysis:**\\xa0In this step,\\xa0data analysis\\xa0is still performed to give data scientists and machine learning engineers an idea of what the data looks like, how it’s distributed, and so on, just like in the manual setup. Similarly, this step can determine whether or not to proceed with construction of a new model or just update the current model.\\xa0\\n3. 3.**Automated model building and analysis:**\\xa0In this step, data\\xa0scientists\\xa0and machine learning engineers can select a model, set any specific hyperparameters, and let the pipeline automate the entire process. The pipeline will automatically process the data according to the specifications of this model (take the case where the features are 331x331x3 images but this particular model only accepts images that are 224x224x3), build the model, train it, evaluate it, and validate it. During validation, the pipeline may automatically tune the hyperparameters as well optimize performance. It is possible that manual intervention may be required in some cases (debugging, for example, when the model is particularly large and complex, or if the model has a novel architecture), but automation should otherwise take care of producing an optimal model. Once this occurs, modularized code is automatically created so that this pipeline can be easily deployed.\\nEverything in this stage is set up so that the experimental stage goes very smoothly, requiring only that the model is built. Depending on the level of automation implemented, perhaps all that is required is that the model\\xa0architecture\\xa0is selected with some hyperparameters specified, and the automation takes care of the rest. Either way, the development process in the experimental stage is sped up massively. With this stage going faster, more experiments can be performed too, leading to possible boosts in overall efficiency as productivity is increased and optimal solutions can be found quicker.\\n\\n1. 4.**Modularized code:**\\xa0The experimental stage is set up so that the pipeline and its components are modularized. In this specific\\xa0context, the data scientist/machine learning engineer defines and builds some model, and the data is standardized to some definition. Basically, the pipeline should be able to accept any constructed model and perform the corresponding steps given some data without hardcoding anything. (Meaning there isn’t any code that will only work for a specific model and specific data. The code works with generalized cases of models and data.)\\nThis is\\xa0**modularization**\\xa0, when the whole system is divided into individual components that each have their own function, and these components can be switched out depending on variable inputs. Thanks to the modularized code, when the pipeline is deployed, it will be able to accept any new feature data as needed in order to update the deployed model. Furthermore, this structure also lets it swap out models as needed, so there’s no need to construct the entire pipeline for every new model architecture.\\nThink of it this way: the pipeline is a puzzle piece, and the models along with their feature data are various puzzle pieces that can all fit within the pipeline. They all have their own “image” on the piece and the other sides can have variable shapes, but what is important is that they fit with the pipeline and can easily be swapped out for others.\\xa0\\n2. 5.**Deploy pipeline:**\\xa0In\\xa0this\\xa0step, the pipeline is manually deployed and is retrieved from the source code. Thanks to its modularization, the pipeline setup is able to operate independently and automatically train the deployed model on any new data if needed, and the application is built around the code structure of the pipeline so all components will work with each other correspondingly. The engineering team has to build parts of the application around the pipeline and its modularized components the first time around, but after that, the pipelines should work seamlessly with the applications so as long as the structure remains the same. Models are simply swapped, unlike before when the model had to be manually integrated into the application. This time, the pipeline must be integrated into the application, and the models are simply swapped out.\\nHowever, it is important to mention that pipeline structures can change depending on the model. The main takeaway here is that pipelines should be able to handle many more\\xa0modelsbefore having to be restructured compared to the setup before where “swapping” models meant you only loaded updated weights. Now, if several architectures all have common training, testing, and validation code, they can all be used under the same pipeline.\\xa0\\n3. 6.**Automated training pipeline:**\\xa0This pipeline contains the model that provides its services\\xa0and\\xa0is set up to automatically fetch new features upon activation of the trigger. The conditions for trigger activation will be discussed in item 10. When the pipeline finishes updating a trained model, the model is saved to a model registry, a type of storage unit that holds trained models for ease of access.\\xa0\\n4. 7.**Model registry:**\\xa0This\\xa0is\\xa0a storage unit that specifically holds model classes and/or weights. The purpose of this unit is to hold trained models for easy retrieval by an application, for example, and it is a good component to add to an automation setup. Without the model registry, the model classes and weights would just be saved to whatever source code repository is established, but this way, we make the process simpler by providing a centralized area of storage for these models. It also serves to bridge the gap between model development teams, software development teams, and operational teams since it is accessible by everyone, which is ultimately what we want in an ideal automation setup.\\nThis registry along with the automated training pipeline assures\\xa0**continuous delivery of model services**\\xa0since models can frequently be updated, pushed to this registry, and deployed without having to go through the entire experimental stage.\\n\\n1. 8.**Model services:**\\xa0Here the application pulls the latest,\\xa0best\\xa0performing model from the model registry and makes use of its prediction services. This action then goes on to provide the desired functionality in the application.\\xa0\\n2. 9.**Performance and user data collection:**\\xa0New data is collected as usual along with performance metrics related to the\\xa0model. This data goes to the feature store, where the new data is processed and standardized so that it can be used in both the experimental stage and the deployment stage and there are no discrepancies between the data used by either stage. Performance data is stored so that data scientists can tell how the model is performing once deployed. Based on that data, important decisions such as whether or not to build a new model with a new architecture can be made.\\xa0\\n3. 10.**Training pipeline trigger:**\\xa0This trigger, upon activation, initiates the automated\\xa0training\\xa0pipeline for the deployed model and allows for feature retrieval by the pipeline from the feature store. The trigger can have any of the following conditions, although it is not limited to them:\\n    ◦ **Manual trigger:**\\xa0Perhaps the model is to be trained only if the process is manually initiated. For example, data science teams can choose to start this process after reviewing performance and data and concluding that the deployed model needs to train on fresh batches of data.\\n    ◦ **Scheduled training:**\\xa0Perhaps the model is set to train on a specific schedule. This can be a certain time on the weekend, every night during hours of lowest traffic, every month, and so on.\\n    ◦ **Performance issues:**\\xa0Perhaps performance data indicates that the model’s performance has dipped below a certain benchmark. This can automatically activate the training process to attempt to get the performance back up to par. If this is not possible or is taking too many resources, data scientists and machine learning engineers can choose to build and deploy a new model.\\n    ◦ **Changes in**\\xa0**data patterns:**\\xa0Perhaps changes in the trends of the data have been noticed while creating the features in the feature store. Of course, the feature store isn’t the only possible place that can analyze data and identify any new trends or changes in the data. There can be a separate process/program dedicated to this task, which can decide whether or not to activate the trigger.\\nThis would also be a good condition to begin the training process, since the new trends in the data are likely to lead to performance degradation. Instead of waiting for the performance hit to activate the trigger, the model can begin training on new data immediately upon sufficient detection of such changes in the data, allowing for the company to minimize any potential losses from such a scenario.\\n\\n1. .\\n    \\n    **Feature store:**\\xa0The feature store contains standardized data processed into features. Features can be pulled by data scientists for offline data analysis. Upon activation of the trigger, features can also be sent to the automated training pipeline to further train the deployed model.\\n    \\n2. 2.\\n    \\n    **Data analysis:**\\xa0This\\xa0step is performed by data scientists on features pulled from the feature store. The results from the analysis can determine whether or not to build a new model or adjust the architecture of an existing model and retrain it from there.\\n    \\n3. 3.\\n    \\n    **Automated model building and analysis:**\\xa0This step is performed by the model development\\xa0team. Models can be built by the team and passed into the pipeline, assuming that they are compatible with the training, testing, and validation code, and the entire process is automatically conducted with a model analysis report generated at the end. In the case where the team wants to implement some of the latest machine learning architectures, models will have to be created from scratch with integration into pipelines in mind to maintain modularity. Parts of the pipeline code may have to change as well, which is acceptable because the new components of this setup can handle this automatically.\\n    \\n4. 4.\\n    \\n    **Modularized code:**\\xa0Once the model reaches a\\xa0minimum\\xa0level of performance in the validation step, the pipeline, its components, and the model are all ready to be modularized and stored in a source repository.\\n    \\n5. 5.\\n    \\n    **Source repository:**\\xa0The\\xa0source repository\\xa0holds all of the packaged pipeline and model code for different pipelines and different models. Teams can create multiples at once for different purposes and store them all here. In the old setup, pipelines and models would be pulled from here and manually integrated and deployed by software engineering teams. In this setup, the modularized code must now be tested to make sure all of the components will work correctly.\\n    \\n    !Untitled\\n    \\n    !Untitled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_docs = [doc for doc in documents1_clean if len(doc.text)>10000]\n",
    "long_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n**Experiment Design**\\n\\n- What do you mean by A/B testing?\\n    - https://dimensionless.in/data-science-interview-questions-with-answers/\\n    - As well as being perhaps the most accurate tool for estimating effect size (and therefore ROI), it is also able to provide us with causality, a very elusive thing in data science! With causality we can finally lay to rest the “correlation vs causation” argument, and prove that our new product actually works.\\n    - https://towardsdatascience.com/data-science-you-need-to-know-a-b-testing-f2f12aff619a\\n    - Type I error\\u200a—\\u200aor falsely concluding that your intervention was successful (which here might be falsely concluding that layout B is better than Layout A). Also known as a false positive result.\\n    - Type II error\\u200a—\\u200afalsely concluding that your intervention was not successful. Also known as a false negative result.\\n    - http://www.cs.cornell.edu/courses/cs578/2006fa/design.html\\n    - https://www.analyticsvidhya.com/blog/2017/05/41-questions-on-statisitics-data-scientists-analysts/\\n- Which of the following measures of central tendency will always change if a single value in the data changes?\\n    - Mean, median, mode, or all?\\n    - A - The mean of the dataset would always change if we change any value of the data set. Since we are summing up all the values together to get it, every value of the data set contributes to its value. Median and mode may or may not change with altering a single value in the dataset.\\n- What does it mean for a result to be statistically significant?\\n    - https://www.quora.com/What-kind-of-A-B-testing-questions-should-I-expect-in-a-data-scientist-interview-and-how-should-I-prepare-for-such-questions\\n    - Statistically significant is the likelihood that a relationship between two or more variables is caused by something other than chance.\\n    - Statistical hypothesis testing is used to determine whether the result of a data set is statistically significant.\\n    - This test provides a p-value, representing the probability that random chance could explain the result; in general, a p-value of 5% or lower is considered to be statistically significant.\\n    - Statistical significance is used to accept or reject the null hypothesis, which hypothesizes that there is no relationship between measured variables. A data set is statistically significant when the set is large enough to accurately represent the phenomenon or population sample being studied. A data set is typically deemed to be statistically significant if the probability of the phenomenon being random is less than 1/20, resulting in a p-value of 5%. When the test result exceeds the p-value, the null hypothesis is accepted. When the test result is less than the p-value, the null hypothesis is rejected.\\n- What is a confidence interval?\\n    - In statistics, a confidence interval (CI) is a type of interval estimate, computed from the statistics of the observed data **that might contain the true value of an unknown population parameter.**\\n    - The interval has an associated confidence level that, loosely speaking, quantifies the level of confidence that the parameter lies in the interval. More strictly speaking, the confidence level represents the frequency (i.e. the proportion) of possible confidence intervals that contain the true value of the unknown population parameter.\\n    - In other words, if confidence intervals are constructed using a given confidence level from an infinite number of independent sample statistics, the proportion of those intervals that contain the true value of the parameter will be equal to the confidence level.\\n    - https://en.wikipedia.org/wiki/Confidence_interval\\n    - \\n    - [[diff in diff and instrumental variables IV]]\\n- What are instrumental variables and why are they important for experiment design?\\n    - https://www.quora.com/What-are-some-data-science-interview-questions-Do-they-include-canonical-algorithm-questions-such-as-search-graphs-data-structures-etc\\n    - In statistics, econometrics, epidemiology and related disciplines, the method of instrumental variables (IV) is used to estimate causal relationships when controlled experiments are not feasible or when a treatment is not successfully delivered to every unit in a randomized experiment.[1] Intuitively, IVs are used when an explanatory variable of interest is correlated with the error term, in which case ordinary least squares and ANOVA give biased results. A valid instrument induces changes in the explanatory variable but has no independent effect on the dependent variable, allowing a researcher to uncover the causal effect of the explanatory variable on the dependent variable.\\n    - Instrumental Variables regression (IV) basically splits your explanatory variable into two parts: one part that could be correlated with ε and one part that probably isn’t. By isolating the part with no correlation, it’s possible to estimate β in the regression equation:Yi = β0 + β1Xi + εi.\\n    - https://www.statisticshowto.datasciencecentral.com/experimental-design/confounding-variable/\\n- When building a recommender system, how do you know if your model is working? Is there a rigorous way to test your model?\\n    - Is there a more rigorous —> number associating with match?\\n        - Manually rate random matches then see how ranks compare\\n            - Compare to random guessing\\n            - Benchmark is almost always random guessing\\n        - Take a list of 10 sentences then 1 test sentence —> then rank those sentences —> “is the best match pair of sentences in the top 3 of what the model picked out” —> the top x and top n in recommender systems\\n            - Getting creative with measuring the success of your model?\\n            - As a user what matters to me!\\n\\n- What proportion is more than 2.0 standard deviations from the mean?\\n    - 95.45%\\n- What proportion is between 1.25 and 2.1 standard deviations above the mean?\\n    - \\n        \\n        https://lh3.googleusercontent.com/PaFTE14_OF-SQGzoRpiJxtA2k8-dza1aisdEhj9arcJlCerAK19KQe2DLDYd7olEdTuU9kqrPBAbb6VRv9_9ZRCu5U4qTMgOIhsXhR9iYmIh5vJ3GGWd-JEYkumX1h_V9nmx_K6T\\n        \\n- What term refers to the standard deviation of the sampling distribution?\\n    - The standard error (of the mean) is the standard deviation of the sampling distribution from the sample mean.\\n- What is the shape of the sampling distribution of r? In what way does the shape depend on the size of the population correlation?\\n    - The shape of the sampling distribution of r is usually (negatively) skewed, unless r is 0. The reason for this is that r cannot take on values greater than 1.0 or less than -1.0, so its distribution cannot extend as far in one direction as it can in the other direction. The greater the value of p, the more pronounced the skew of r’s distribution. To convert Pearson’s r to a value that’s normally distributed, we can use a transformation called Fisher’s z transformation.\\n    - The shape of the sampling distribution of r is negatively skewed. Higher the size of the population correlation, more pronounced the skew.\\n    - http://onlinestatbook.com/2/sampling_distributions/samp_dist_r.html\\n- When solving problems where you need the sampling distribution of r, what is the reason for converting from r to z\\'? (relevant section)\\n    - The reason we convert r to z’ is to transform the variables into ones that have a normal distribution. Without a normal distribution, it would be very difficult to compute probabilities and the standard errors of the sampling distribution of r. Thus, by using the Fisher method, we can transform r into a variable that is normally distributed and that has a known standard error of and is thus much easier to work with.\\n- What is the p-value?\\n    - When we execute a hypothesis test in statistics, a p-value helps us in determine the significance of our results. These Hypothesis tests are nothing but to test the validity of a claim that is made about a population. A null hypothesis is a situation when the hypothesis and the specified population is with no significant difference due to sampling or experimental error.\\n    - https://www.educba.com/statistics-interview-questions/\\n- When should you use a t-test vs a z-test ?\\n    - https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/hypothesis-testing/t-score-vs-z-score/\\n    - Use a t-test when you have a smaller sample, N<30 , and a z-test when you have a larger sample\\n    - Both are used for hypothesis testing, but for a z-test, you don’t need to know the SD of your population.\\n    - \\n        \\n        https://lh3.googleusercontent.com/Ce7-NfnjU3Vl8PuHHqP0jJFNMySGyZp7OlUKzya7B-573ZKUPOBvtrY3_xLHURLGXnwmYB2dRp2svXLtjRCFbf33_0w9gvJJgcBdevlND2Pm48Cb7A7JmDGHdnawjvqV2j9w8BCF\\n        \\n- How is power defined in hypothesis testing?\\n    - power is the probability of correctly rejecting the null hypothesis. We’re typically only interested in the power of a test when the null is in fact false. This definition also makes it more clear that power is a conditional probability: the null hypothesis makes a statement about parameter values, but the power of the test is conditional upon what the values of those parameters really are.\\n    - https://apcentral.collegeboard.org/courses/ap-statistics/classroom-resources/power-in-tests-of-significance\\n    - \\n        \\n        https://lh5.googleusercontent.com/52nYhqos80JydcvTvawu5s2B-RIbgAocPU9v7pFYWdsM9RT-MJwCCWnNbyvyZNeaiBoxRjewjiCth27SZnNvYJMSj4VmCKKoF69bD3QNz9rwKWz7aF7K-2DMy__W0VblV5IiHv09\\n        \\n    - Power is the probability of rejecting the null hypothesis when in fact it is false.\\n    - Power is the probability of making a correct decision (to reject the null hypothesis) when the null hypothesis is false.\\n    - Power is the probability that a test of significance will pick up on an effect that is present.\\n    - Power is the probability that a test of significance will detect a deviation from the null hypothesis, should such a deviation exist.\\n    - Power is the probability of avoiding a Type II error.\\n- What is the difference between data science, ML and AI?\\n    - In computer science, **artificial intelligence** (**AI**), sometimes called **machine intelligence**, is intelligence demonstrated by machines, in contrast to the **natural intelligence** displayed by humans and other animals. Computer science defines AI research as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.[1] More specifically, Kaplan and Haenlein define AI as “a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation”.[2]Colloquially, the term \"artificial intelligence\" is used to describe machines that mimic \"cognitive\" functions that humans associate with other human minds, such as \"learning\" and \"problem solving\".[3]\\n    - Understanding the nature of intelligence in general\\n    - https://en.wikipedia.org/wiki/Artificial_intelligence\\n    \\n    !Untitled\\n    \\n    为了绕开观测数据因果推断的问题，我们引入了准实验。从目前因果推断整体的分析框架中可以看到准实验所处的位置，左图包含实验数据和观测数据的因果推断。其中，在观测数据的因果推断中，我们会优先看数据是否满足DID（Differences In Difference，双重差分）、工具变量和断点回归的前提要求。如果满足，会优先使用这三种方法；如果不满足，才会使用PSM（Propensity Score Matching，倾向评分匹配）和混淆PSM方法。这种优先级的原因是相比于PSM，前三种方法绕开了混杂因子，这是唯一的也是最重要的区别。因此它们依赖的假设在业务层面更容易得到满足，同时也很容易被检验，这样的结论也更容易被信服。我们把上面的三种方法称为准实验方法\\n    \\n    DID在腾讯看点中是一个常用的方法，我们用DID发现了在极端天气下，天气资讯对用户留存的影响。去年8月6号，是台风黑格比经过的时间，我们希望在这样极端的天气下，推送天气的咨询是否能提升用户留存。\\n    \\n    对于这个问题，我们首先想到如下实验：\\n    \\n    - 实验组：8月6号曝光天气的用户\\n    - 对照组：8月6号未曝光天气的用户\\n    \\n    结论：曝光天气的用户次留相比于未曝光天气的用户次留高了20%。\\n    \\n    事实上，这个结论肯定是错误的。因为曝光天气和未曝光天气这两组用户本身就不平衡，因为我们通常是给活跃用户曝光。因此，这样得到的结论是带有混淆偏差的。\\n    \\n    因此，我们又想到如下实验：\\n    \\n    - 实验组：前期未曝光天气，8月6号曝光天气的用户作为实验组\\n    - 对照组：前期未曝光天气，8月6号未曝光天气的用户作为对照组\\n    \\n    结论：曝光天气的用户相比于未曝光天气的用户在受到干预之后，次留扩大了1.4%\\n    \\n    基于上述结论，我们判断天气内容的曝光对次留是有因果效应的。为什么说这就是因果效应呢？双重差分中，第一层差分指的是实验组和对照组在实验前后的差异，我们在右上图看到了实验前的平行性是满足的，可以认为混淆变量对实验组和对照组的第一重差分是相等的，那么影响第二重差分（实验组和对照组差分的差分）的因素就只有干预本身了。因此，我们可以通过二次差分得到一个因果效应，也就是这里的1.4%。\\n    \\n    !Untitled\\n    \\n    针对这类问题，我们提出一套通用的观测数据因果推断分析方式来给出答案。我们主要关注三个问题，第一个问题启动重置对下一次的使用有没有影响？\\xa0第二个问题 一段时间的启动重置下来对用户的未来的打开次数，活跃，收入是否有影响？\\xa0前两个问题解决后，我们关心是否存在部分人群能够既不影响体验，又不影响收入增加和其他功能的导流。这三个问题又称为短期影响、长期影响和用户异质性分析。\\n    \\n    考虑前面给出的分析框架，我们发现都有相应的解法。\\n    \\n    **①\\xa0短期影响**\\n    \\n    由于用户是否被启动重置，只取决于用户的访问时间在40分钟右侧还是左侧，那么对于这类问题很适合用断点回归的方式解决。\\n    \\n    **②\\xa0长期影响**\\n    \\n    长期影响依赖于很多混淆变量，它适合用PSM、混淆控制的方式处理。前面提到，如果我们考虑PSM和匹配方法有一个难题——它的结论很容易被挑战，因为不存在遗漏的混杂因子是无法被证明的。如何解决这个问题是个难点。\\n    \\n    **③\\xa0用户异质性**\\n    \\n    异质性分析的前提是实验数据，或者说准实验数据，如何去获并分析短期和长期干预的准实验数据呢？同时在我们的场景中，我们关注多个指标和解释性，异质性没有一个直接可以满足的方法。那么现有的下钻分析和uplift能满足这样的目标吗？\\n    \\n    针对这三个问题，我们分别进行阐述\\n    \\n    !Untitled\\n    \\n    !Untitled\\n    \\n    针对长期问题，可以画出如上因果图，考虑一段时间启动重置累积后对用户的影响。长期问题的难点是无法绕开遗漏的混杂因子。比如，我们通过混淆控制的方法去解决这个长期问题，我们先尝试控制用户的活跃度，使其在一段时间内的访问次数都是21，发现击中比例越高的用户的访问天数越多。如果访问次数已经囊括了所有的混淆变量的话，这个结论就是正确的。事实上我们发现，当访问次数都是21的时候，击中比例越高的用户，相当于他们的间隔都比较长，也就是他们是低频高日活型的用户，而击中比例越低的用户，他们正好是高频低日活型的用户。也就是说，我们控制了访问次数，却没有控制住用户的访问模式。这样得出的结论也是错误的。\\n    \\n    当然，我们可以用PSM把这些所有可能的混淆变量一步步都考虑进去。但同样会存在两个问题，一是局部性问题，PSM匹配的样本只是样本中的一小部分，无法代表整体样本，二是遗漏的混杂因子的问题依然无法解决。下面给出我们的解决办法。\\n    \\n    !Untitled\\n    \\n    **准实验：**\\n    \\n    在短期的断点回归中，我们可以看到因为访问间隔会随机地落在40分钟的左右两侧，因此在40分钟邻域构成一个准实验。从业务的视角看，这个准实验是用户无法感知这次访问距离上一次是过了39分钟还是41分钟，他是无法感知到这个差异的，这导致来访的用户的各种变量也是随机分配到这个区间的。那么这个邻域是否能一定程度地扩大呢，能否扩大到30到50分钟或者20到60分钟呢？\\n    \\n    **邻域选择：**\\n    \\n    邻域的选择是置信度和随机性的折中。当范围越大的时候，我们覆盖的样本就越多，但随机性会变差。当范围越小的时候，随机性很好，但覆盖的样本很少，从而置信度会受到质疑。最终，选择了20到60分钟这个区间。我们还通过特征平衡性来证明这两个区间的样本在各项重要特征上都是比较接近的。\\n    \\n    **构造变量：**\\n    \\n    因为我们已经证明了用户的访问行为落在20到40分钟和40到60分钟是一个几乎随机的事件，那么我们可以基于这个事件去构造一个长期的随机变量，就是用户在一段时间内落在40到60分钟的次数除以落在20到40分钟的次数，用这个比例作为长期的准实验变量\\n    \\n    !Untitled\\n    \\n    我们用上表按照长期的击中比例来分组，我们发现两组用户在两周内各项数据都没有明显差异。也就是说，我们的长期Rate比例是与各种混淆因子独立的，也就是T独立于X。那么我们可以证明，Rate和活跃天数Y的因果性是等于相关性的。在右图做了大量证明，我们说明了准实验变量的相关性是等于因果性的，我们就可以直接去观测T和Y的关系，也就是我们构造出来的Rate和活跃天数的关系。\\n    \\n    !Untitled\\n    \\n    !Untitled\\n    \\n    如果说整体上的结论，短期整体和长期整体的结论是显而易见而且直觉的，那么第三个问题细分人群的结论就不是那么显而易见了，异质性分析的前提是实验或者准实验。前面，我们已经构造了准实验变量，创造了无偏样本。下面，我们希望通过异质性分析找到不同人群在不同干预措施下的不同效果，然后去去改善策略。\\n    \\n    比如，我们发现主动打开为主的活跃用户在被启动重置打断后的活跃度和收入都出现了下降，那么对于这类用户我们就应该下架策略。又比如，我们发现启动重置打断不仅会增加频繁打开信息流用户的活跃度和信息流的时长，还不影响他们的搜索时长，那么对于这类用户我们就可以执行启动重置策略。\\n    \\n    这里的难点是我们的目标指标有多个，包括搜索时长、信息流时长、收入。同时，用户的标签维度很高，包括主动打开、频繁打开信息流等。同时，我们要把这样的结论通过算法解释并满足通用性。需要同时满足这四个要求是个难点\\n    \\n    !Untitled\\n    \\n    通过调研发现，这四个要求是很难同时满足的。从前面的分析框架中，我们可以看到，异质性分析主要包括下钻分析和Uplift分析。在下钻分析和Uplift分析的调研中，我们发现了解释性、通用性和细粒度之间矛盾。下钻分析有比较好的解释性但通用性比较差，因为它不太适合处理连续变量，而且它一旦遭遇维度比较高的问题会有搜索效率的问题。Uplift在通用性和研究粒度上没有问题，但是它的解释性较差，比较适合高维和复杂业务。可以看到，在我们的问题中Uplift更加满足要求。\\n    \\n    我们继续调研Uplift发现，Transform outcome的方法是更满足我们的要求的，它相比Meta-learner有更高的准确性，同时相比于Direct uplift model有更低的实验成本，但问题是，它只适合于单指标的建模。那么多指标的uplift的建模，我们目前了解到的只有Mr-uplift方法。它的实验方法是用多个outcome组成一个新的outcome，然后对新的变量建模。这个转换是不可逆的，也就是说我们的变量对原始的outcome的uplift是无法被复原的。因此我们发现，只有Transform outcome最满足我们的要求，下面我们对其进行改造\\n    \\n    !Untitled\\n    \\n    我们的算法目标有3个：\\n    \\n    - 多指标的实验uplift拟合\\n    - 模型可解释\\n    - 算法通用、可处理高维度\\n    \\n    下面，我们用伪代码来呈现我们是怎么达到以上目标的，主要是四个步骤：\\n    \\n    Step1：在数据处理后，先通过Transform outcome去转换我们原始的Y和G。新的变量会被称为Y*和G*。然后对新的变量分别用CatBoost拟合模型。\\n    \\n    Step2：输出模型的重要特征，并选择出现次数最多的，用前15个或前10个解释细分人群。\\n    \\n    Step3：通过两个模型预测的uplift的正负值划分四个象限，比较不同象限的人群在Step2中得到的重要特征的均值差异，得到一个定性的结论。\\n    \\n    Step4：通过Step3的定性结论做一个单维度的搜索，得到定量的结论。然后输出每个维度子人群的uplift的绝对量值以及置信度。\\n    \\n    !Untitled\\n    \\n    我们通过Gini Score来评估这4种模型方案的准确性，黑线代表的是随机实验的效果，蓝线代表的是当前模型的效果，与黑线构成的面积越大效果越好。红线是理论上能够达到的最大值，但是它不能说明是最优效果，只能说是一个量高。我们发现Transform outcome加CatBoost的模型效果最好，Gini面积达到了0.1387，比单模型方法的效果好两倍\\n    \\n    !Untitled\\n    \\n    拿短期异质性来举例，我们希望知道不同上下文的访问行为在被启动重置打断后，在搜索使用时长和总使用时长上有没有什么不同的表现。首先，我们根据算法画4个象限图，我们根据总时长和搜索使用时长分别建立一个uplift模型，横轴为搜索使用时长的uplift，纵轴为总使用时长的uplift，每个点表示一次不同上下文的访问行为。那么第一象限代表被启动重置后，其总时长和搜索使用时长都会有提升，第三象限代表被启动重置后，其总时长和搜索时长都会有明显的下降。\\n    \\n    接着，我们得到这两个模型的重要特征，然后对比四象限的人群在这些重要特征的均值上的差异。对比第一象限和第二象限，我们发现第一象限的人群搜索时长相比于第二象限的人群搜索时长的占比更低，这说明启动重置策略对搜索时长占比较高的用户可能会下降搜索意愿。对比第一象限和第三象限，我们发现第一象限的打开方式有多种，而第三象限的打开方式主要是主动打开，这说明对主动打开的用户，启动重置策略会引起反感，不仅会降低搜索意愿并且对信息流导流不感兴趣。对于这类用户，我们需要采用下架策略。这样的定性结论到底是正确还是错误，我们还需要定量验证。\\n    \\n    对前两次打开方式做一个细分，每一种上下文我们都区分实验组和对照组。通过对比这四种细分上下文实验组和对照组的总时长和搜索时长的差异，得到真实的离线数据的总时长uplift和搜索时长uplift。最终来确定量化的uplift和置信度。\\n    \\n    这是短期异质性的四象限分析算法效果，长期异质性也是一致的\\n    \\n    !Untitled\\n    \\n    最终我们得到，整体上短期和长期的启动重置策略都有副作用。但区分用户看，可以发现可以对搜索活跃度较低的用户保持现有策略，对搜索活跃度相对较高的用户下架现有策略。更加精细化地，我们可以区分不同session上下文的行为。到这里，我们已经说完了异质性的结论。在分析过程中，我们发现启动重置对搜索用户的影响更大。因此，我们特别对产品平台上做了建议，就是在搜索用户搜索完退出再返回时，切换时增加一个动画，提醒用户之前的上下文已经被收纳到这个窗口里了，让用户主动选择是继续之前的上下文还是来到新的信息流页面。\\n    \\n    到这里，我们就已经解决了之前提出的3个问题，我们用断点分析解决短期影响，用uplift解决长期影响，用改良的准实验构造解决用户异质性。'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_docs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lengths = [len(doc_element.text) for doc_element in documents1_clean]\n",
    "plt.hist(lengths, bins=range(min(lengths), max(lengths) + 2), edgecolor='black')\n",
    "plt.xlabel(\"String Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Doc Lengths\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nDrawing\\n```json\\n{\\n\\t\"type\": \"excalidraw\",\\n\\t\"version\": 2,\\n\\t\"source\": \"https://github.com/zsviczian/obsidian-excalidraw-plugin/releases/tag/1.9.9\",\\n\\t\"elements\": [\\n\\t\\t{\\n\\t\\t\\t\"type\": \"freedraw\",\\n\\t\\t\\t\"version\": 36,\\n\\t\\t\\t\"versionNonce\": 67521280,\\n\\t\\t\\t\"isDeleted\": false,\\n\\t\\t\\t\"id\": \"n5qri4pZQTumqhSLwVflx\",\\n\\t\\t\\t\"fillStyle\": \"hachure\",\\n\\t\\t\\t\"strokeWidth\": 1,\\n\\t\\t\\t\"strokeStyle\": \"solid\",\\n\\t\\t\\t\"roughness\": 1,\\n\\t\\t\\t\"opacity\": 100,\\n\\t\\t\\t\"angle\": 0,\\n\\t\\t\\t\"x\": -215.12890625,\\n\\t\\t\\t\"y\": -162.390625,\\n\\t\\t\\t\"strokeColor\": \"#1e1e1e\",\\n\\t\\t\\t\"backgroundColor\": \"transparent\",\\n\\t\\t\\t\"width\": 194.9375,\\n\\t\\t\\t\"height\": 78.2109375,\\n\\t\\t\\t\"seed\": 2001151232,\\n\\t\\t\\t\"groupIds\": [],\\n\\t\\t\\t\"frameId\": null,\\n\\t\\t\\t\"roundness\": null,\\n\\t\\t\\t\"boundElements\": [],\\n\\t\\t\\t\"updated\": 1690073550610,\\n\\t\\t\\t\"link\": null,\\n\\t\\t\\t\"locked\": false,\\n\\t\\t\\t\"points\": [\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0.2421875,\\n\\t\\t\\t\\t\\t-0.4921875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0.2421875,\\n\\t\\t\\t\\t\\t-0.73828125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t3.015625,\\n\\t\\t\\t\\t\\t-4.21484375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t10.390625,\\n\\t\\t\\t\\t\\t-13.4375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t14.48046875,\\n\\t\\t\\t\\t\\t-19.578125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t25.64453125,\\n\\t\\t\\t\\t\\t-31.4453125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t41.68359375,\\n\\t\\t\\t\\t\\t-47.48828125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t62.78125,\\n\\t\\t\\t\\t\\t-62.68359375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t73.23046875,\\n\\t\\t\\t\\t\\t-68.21875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t92.23828125,\\n\\t\\t\\t\\t\\t-74.3046875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t113.328125,\\n\\t\\t\\t\\t\\t-78.2109375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t126.0390625,\\n\\t\\t\\t\\t\\t-78.2109375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t148.49609375,\\n\\t\\t\\t\\t\\t-77.41015625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t154.87890625,\\n\\t\\t\\t\\t\\t-75.9375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t171.40625,\\n\\t\\t\\t\\t\\t-70.91015625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t179.14453125,\\n\\t\\t\\t\\t\\t-66.48828125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t185.77734375,\\n\\t\\t\\t\\t\\t-60.41015625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t190.56640625,\\n\\t\\t\\t\\t\\t-53.4921875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t192.28125,\\n\\t\\t\\t\\t\\t-49.19921875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t194.40625,\\n\\t\\t\\t\\t\\t-41.21875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t194.9375,\\n\\t\\t\\t\\t\\t-33.23828125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t194.9375,\\n\\t\\t\\t\\t\\t-27.59375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t194.19921875,\\n\\t\\t\\t\\t\\t-25.01953125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t192.7265625,\\n\\t\\t\\t\\t\\t-22.8125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t190.8828125,\\n\\t\\t\\t\\t\\t-20.97265625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t184.7421875,\\n\\t\\t\\t\\t\\t-16.8828125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t177.2890625,\\n\\t\\t\\t\\t\\t-13.69140625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t172.9921875,\\n\\t\\t\\t\\t\\t-11.9765625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t162.890625,\\n\\t\\t\\t\\t\\t-8.4140625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t151.20703125,\\n\\t\\t\\t\\t\\t-5.34375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t142.359375,\\n\\t\\t\\t\\t\\t-3.1328125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t138.0625,\\n\\t\\t\\t\\t\\t-2.27734375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t134.953125,\\n\\t\\t\\t\\t\\t-2.27734375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t134.953125,\\n\\t\\t\\t\\t\\t-2.27734375\\n\\t\\t\\t\\t]\\n\\t\\t\\t],\\n\\t\\t\\t\"lastCommittedPoint\": null,\\n\\t\\t\\t\"simulatePressure\": true,\\n\\t\\t\\t\"pressures\": []\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\"type\": \"freedraw\",\\n\\t\\t\\t\"version\": 50,\\n\\t\\t\\t\"versionNonce\": 678685440,\\n\\t\\t\\t\"isDeleted\": false,\\n\\t\\t\\t\"id\": \"MSJXLz6mp9TO2BptJMSt-\",\\n\\t\\t\\t\"fillStyle\": \"hachure\",\\n\\t\\t\\t\"strokeWidth\": 1,\\n\\t\\t\\t\"strokeStyle\": \"solid\",\\n\\t\\t\\t\"roughness\": 1,\\n\\t\\t\\t\"opacity\": 100,\\n\\t\\t\\t\"angle\": 0,\\n\\t\\t\\t\"x\": -153.6328125,\\n\\t\\t\\t\"y\": -153.70703125,\\n\\t\\t\\t\"strokeColor\": \"#1e1e1e\",\\n\\t\\t\\t\"backgroundColor\": \"transparent\",\\n\\t\\t\\t\"width\": 265.28515625,\\n\\t\\t\\t\"height\": 96.640625,\\n\\t\\t\\t\"seed\": 1587012864,\\n\\t\\t\\t\"groupIds\": [],\\n\\t\\t\\t\"frameId\": null,\\n\\t\\t\\t\"roundness\": null,\\n\\t\\t\\t\"boundElements\": [],\\n\\t\\t\\t\"updated\": 1690073551890,\\n\\t\\t\\t\"link\": null,\\n\\t\\t\\t\"locked\": false,\\n\\t\\t\\t\"points\": [\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0.2421875,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0.484375,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t1.015625,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t4.6953125,\\n\\t\\t\\t\\t\\t1.6328125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t11.078125,\\n\\t\\t\\t\\t\\t4.0859375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t19.3671875,\\n\\t\\t\\t\\t\\t7.40234375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t30.05859375,\\n\\t\\t\\t\\t\\t10.96484375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t59.08203125,\\n\\t\\t\\t\\t\\t20.03125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t96.54296875,\\n\\t\\t\\t\\t\\t29.140625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t119,\\n\\t\\t\\t\\t\\t33.1484375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t128.74609375,\\n\\t\\t\\t\\t\\t34.8671875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t149.8359375,\\n\\t\\t\\t\\t\\t38.76953125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t159.58203125,\\n\\t\\t\\t\\t\\t41.63671875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t160.44140625,\\n\\t\\t\\t\\t\\t41.921875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t160.44140625,\\n\\t\\t\\t\\t\\t42.1640625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t160.1953125,\\n\\t\\t\\t\\t\\t42.1640625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t159.66015625,\\n\\t\\t\\t\\t\\t42.6953125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t157.921875,\\n\\t\\t\\t\\t\\t44.08203125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t154.0546875,\\n\\t\\t\\t\\t\\t46.65625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t151.33203125,\\n\\t\\t\\t\\t\\t48.59765625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t146.625,\\n\\t\\t\\t\\t\\t52.359375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t138.59375,\\n\\t\\t\\t\\t\\t58.6640625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t126.40625,\\n\\t\\t\\t\\t\\t67.46484375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t114.58984375,\\n\\t\\t\\t\\t\\t74.68359375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t101.72265625,\\n\\t\\t\\t\\t\\t82.8046875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t91.26953125,\\n\\t\\t\\t\\t\\t88.94921875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t81.76171875,\\n\\t\\t\\t\\t\\t94.29296875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t74.59765625,\\n\\t\\t\\t\\t\\t96.3359375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t73.3671875,\\n\\t\\t\\t\\t\\t96.640625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t73.12109375,\\n\\t\\t\\t\\t\\t96.39453125\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t73.12109375,\\n\\t\\t\\t\\t\\t96.1484375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t73.12109375,\\n\\t\\t\\t\\t\\t95.90234375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t73.65234375,\\n\\t\\t\\t\\t\\t95.3671875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t75.4921875,\\n\\t\\t\\t\\t\\t93.5234375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t77.69921875,\\n\\t\\t\\t\\t\\t91.6796875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t82.1953125,\\n\\t\\t\\t\\t\\t88.9765625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t94.6640625,\\n\\t\\t\\t\\t\\t82.41015625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t106.73828125,\\n\\t\\t\\t\\t\\t77.32421875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t137.36328125,\\n\\t\\t\\t\\t\\t68.04296875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t152.2578125,\\n\\t\\t\\t\\t\\t64.65625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t182.1875,\\n\\t\\t\\t\\t\\t59.2109375\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t200.671875,\\n\\t\\t\\t\\t\\t57.73046875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t230.6015625,\\n\\t\\t\\t\\t\\t55.9140625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t238.58203125,\\n\\t\\t\\t\\t\\t55.37890625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t257.06640625,\\n\\t\\t\\t\\t\\t55.37890625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t262.7109375,\\n\\t\\t\\t\\t\\t55.37890625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t265.28515625,\\n\\t\\t\\t\\t\\t55.37890625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t265.28515625,\\n\\t\\t\\t\\t\\t55.37890625\\n\\t\\t\\t\\t]\\n\\t\\t\\t],\\n\\t\\t\\t\"lastCommittedPoint\": null,\\n\\t\\t\\t\"simulatePressure\": true,\\n\\t\\t\\t\"pressures\": []\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\"id\": \"iMby4zSS\",\\n\\t\\t\\t\"type\": \"text\",\\n\\t\\t\\t\"x\": -100.75,\\n\\t\\t\\t\"y\": 24.2578125,\\n\\t\\t\\t\"width\": 13.760000228881836,\\n\\t\\t\\t\"height\": 25,\\n\\t\\t\\t\"angle\": 0,\\n\\t\\t\\t\"strokeColor\": \"#1e1e1e\",\\n\\t\\t\\t\"backgroundColor\": \"transparent\",\\n\\t\\t\\t\"fillStyle\": \"hachure\",\\n\\t\\t\\t\"strokeWidth\": 1,\\n\\t\\t\\t\"strokeStyle\": \"solid\",\\n\\t\\t\\t\"roughness\": 1,\\n\\t\\t\\t\"opacity\": 100,\\n\\t\\t\\t\"groupIds\": [],\\n\\t\\t\\t\"frameId\": null,\\n\\t\\t\\t\"roundness\": null,\\n\\t\\t\\t\"seed\": 1986857474,\\n\\t\\t\\t\"version\": 2,\\n\\t\\t\\t\"versionNonce\": 1886984130,\\n\\t\\t\\t\"isDeleted\": false,\\n\\t\\t\\t\"boundElements\": null,\\n\\t\\t\\t\"updated\": 1690077945135,\\n\\t\\t\\t\"link\": null,\\n\\t\\t\\t\"locked\": false,\\n\\t\\t\\t\"text\": \"0\",\\n\\t\\t\\t\"rawText\": \"0\",\\n\\t\\t\\t\"fontSize\": 20,\\n\\t\\t\\t\"fontFamily\": 1,\\n\\t\\t\\t\"textAlign\": \"left\",\\n\\t\\t\\t\"verticalAlign\": \"top\",\\n\\t\\t\\t\"baseline\": 17,\\n\\t\\t\\t\"containerId\": null,\\n\\t\\t\\t\"originalText\": \"0\",\\n\\t\\t\\t\"lineHeight\": 1.25\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\"id\": \"KrVz1ElIGpyKrxog9to-N\",\\n\\t\\t\\t\"type\": \"freedraw\",\\n\\t\\t\\t\"x\": 50.962063033404434,\\n\\t\\t\\t\"y\": -4.4973520730414975,\\n\\t\\t\\t\"width\": 73.69803630675119,\\n\\t\\t\\t\"height\": 35.85309874382489,\\n\\t\\t\\t\"angle\": 0,\\n\\t\\t\\t\"strokeColor\": \"#1e1e1e\",\\n\\t\\t\\t\"backgroundColor\": \"transparent\",\\n\\t\\t\\t\"fillStyle\": \"hachure\",\\n\\t\\t\\t\"strokeWidth\": 1,\\n\\t\\t\\t\"strokeStyle\": \"solid\",\\n\\t\\t\\t\"roughness\": 1,\\n\\t\\t\\t\"opacity\": 100,\\n\\t\\t\\t\"groupIds\": [],\\n\\t\\t\\t\"frameId\": null,\\n\\t\\t\\t\"roundness\": null,\\n\\t\\t\\t\"seed\": 1092009310,\\n\\t\\t\\t\"version\": 37,\\n\\t\\t\\t\"versionNonce\": 105578974,\\n\\t\\t\\t\"isDeleted\": false,\\n\\t\\t\\t\"boundElements\": null,\\n\\t\\t\\t\"updated\": 1690077948976,\\n\\t\\t\\t\"link\": null,\\n\\t\\t\\t\"locked\": false,\\n\\t\\t\\t\"points\": [\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-0.9959194095507655,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0.9959194095506518,\\n\\t\\t\\t\\t\\t-1.9918388191013605\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t3.983677638202721,\\n\\t\\t\\t\\t\\t-3.983677638202778\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t9.95919409550686,\\n\\t\\t\\t\\t\\t-7.967355276405556\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t15.934710552810998,\\n\\t\\t\\t\\t\\t-11.951032914608277\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t21.910227010115136,\\n\\t\\t\\t\\t\\t-16.930629962361763\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t28.88166287697004,\\n\\t\\t\\t\\t\\t-20.91430760056454\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t35.85309874382483,\\n\\t\\t\\t\\t\\t-21.910227010115193\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t40.83269579157832,\\n\\t\\t\\t\\t\\t-22.9061464196659\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t44.81637342978104,\\n\\t\\t\\t\\t\\t-21.910227010115193\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t48.80005106798387,\\n\\t\\t\\t\\t\\t-20.91430760056454\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t50.791889887085176,\\n\\t\\t\\t\\t\\t-18.922468781463124\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t51.78780929663594,\\n\\t\\t\\t\\t\\t-16.930629962361763\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t50.791889887085176,\\n\\t\\t\\t\\t\\t-12.946952324158985\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t48.80005106798387,\\n\\t\\t\\t\\t\\t-7.967355276405556\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t45.8122928393318,\\n\\t\\t\\t\\t\\t-4.97959704775343\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t42.82453461067962,\\n\\t\\t\\t\\t\\t-0.9959194095507087\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t39.83677638202755,\\n\\t\\t\\t\\t\\t1.9918388191013605\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t38.8408569724769,\\n\\t\\t\\t\\t\\t3.983677638202778\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t37.84493756292625,\\n\\t\\t\\t\\t\\t4.9795970477534865\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t38.8408569724769,\\n\\t\\t\\t\\t\\t3.983677638202778\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t41.82861520112897,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t46.808212248882455,\\n\\t\\t\\t\\t\\t-2.987758228652069\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t52.783728706186594,\\n\\t\\t\\t\\t\\t-5.975516457304138\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t57.76332575394008,\\n\\t\\t\\t\\t\\t-7.967355276405556\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t61.7470033921428,\\n\\t\\t\\t\\t\\t-7.967355276405556\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t64.73476162079487,\\n\\t\\t\\t\\t\\t-5.975516457304138\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t66.72660043989629,\\n\\t\\t\\t\\t\\t-3.983677638202778\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t67.72251984944694,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t64.73476162079487,\\n\\t\\t\\t\\t\\t5.975516457304138\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t63.73884221124422,\\n\\t\\t\\t\\t\\t10.955113505057625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t62.74292280169345,\\n\\t\\t\\t\\t\\t12.946952324158985\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t65.73068103034552,\\n\\t\\t\\t\\t\\t11.951032914608277\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t71.70619748764977,\\n\\t\\t\\t\\t\\t8.963274685956208\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t72.70211689720043,\\n\\t\\t\\t\\t\\t7.967355276405556\\n\\t\\t\\t\\t]\\n\\t\\t\\t],\\n\\t\\t\\t\"pressures\": [\\n\\t\\t\\t\\t0.07999999821186066,\\n\\t\\t\\t\\t0.07999999821186066,\\n\\t\\t\\t\\t0.14249108731746674,\\n\\t\\t\\t\\t0.16876882314682007,\\n\\t\\t\\t\\t0.17254318296909332,\\n\\t\\t\\t\\t0.1880166381597519,\\n\\t\\t\\t\\t0.17874497175216675,\\n\\t\\t\\t\\t0.18084609508514404,\\n\\t\\t\\t\\t0.17159868776798248,\\n\\t\\t\\t\\t0.16680264472961426,\\n\\t\\t\\t\\t0.15366637706756592,\\n\\t\\t\\t\\t0.148329496383667,\\n\\t\\t\\t\\t0.14571578800678253,\\n\\t\\t\\t\\t0.15379293262958527,\\n\\t\\t\\t\\t0.16824303567409515,\\n\\t\\t\\t\\t0.17132166028022766,\\n\\t\\t\\t\\t0.1722126454114914,\\n\\t\\t\\t\\t0.16361545026302338,\\n\\t\\t\\t\\t0.1489483267068863,\\n\\t\\t\\t\\t0.14314965903759003,\\n\\t\\t\\t\\t0.1397760659456253,\\n\\t\\t\\t\\t0.1379593163728714,\\n\\t\\t\\t\\t0.14086319506168365,\\n\\t\\t\\t\\t0.14766232669353485,\\n\\t\\t\\t\\t0.1495460569858551,\\n\\t\\t\\t\\t0.15658633410930634,\\n\\t\\t\\t\\t0.15987975895404816,\\n\\t\\t\\t\\t0.152239128947258,\\n\\t\\t\\t\\t0.13482604920864105,\\n\\t\\t\\t\\t0.12245431542396545,\\n\\t\\t\\t\\t0.11606164276599884,\\n\\t\\t\\t\\t0.12594124674797058,\\n\\t\\t\\t\\t0.10662301629781723,\\n\\t\\t\\t\\t0.06340881437063217,\\n\\t\\t\\t\\t0.010039123706519604,\\n\\t\\t\\t\\t0\\n\\t\\t\\t],\\n\\t\\t\\t\"simulatePressure\": false,\\n\\t\\t\\t\"lastCommittedPoint\": [\\n\\t\\t\\t\\t72.70211689720043,\\n\\t\\t\\t\\t7.967355276405556\\n\\t\\t\\t]\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\"id\": \"rnDA67ZopFTU3uxsbuEzA\",\\n\\t\\t\\t\"type\": \"freedraw\",\\n\\t\\t\\t\"x\": 56.93757949070857,\\n\\t\\t\\t\"y\": 52.27005427134793,\\n\\t\\t\\t\"width\": 12.946952324158929,\\n\\t\\t\\t\"height\": 4.97959704775343,\\n\\t\\t\\t\"angle\": 0,\\n\\t\\t\\t\"strokeColor\": \"#1e1e1e\",\\n\\t\\t\\t\"backgroundColor\": \"transparent\",\\n\\t\\t\\t\"fillStyle\": \"hachure\",\\n\\t\\t\\t\"strokeWidth\": 1,\\n\\t\\t\\t\"strokeStyle\": \"solid\",\\n\\t\\t\\t\"roughness\": 1,\\n\\t\\t\\t\"opacity\": 100,\\n\\t\\t\\t\"groupIds\": [],\\n\\t\\t\\t\"frameId\": null,\\n\\t\\t\\t\"roundness\": null,\\n\\t\\t\\t\"seed\": 423957250,\\n\\t\\t\\t\"version\": 10,\\n\\t\\t\\t\"versionNonce\": 1285035806,\\n\\t\\t\\t\"isDeleted\": false,\\n\\t\\t\\t\"boundElements\": null,\\n\\t\\t\\t\"updated\": 1690077949850,\\n\\t\\t\\t\"link\": null,\\n\\t\\t\\t\"locked\": false,\\n\\t\\t\\t\"points\": [\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0.9959194095506518,\\n\\t\\t\\t\\t\\t-0.9959194095507087\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t1.9918388191013037,\\n\\t\\t\\t\\t\\t-0.9959194095507087\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t2.987758228652069,\\n\\t\\t\\t\\t\\t-1.9918388191014174\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t5.975516457304138,\\n\\t\\t\\t\\t\\t-2.987758228652069\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t8.963274685956208,\\n\\t\\t\\t\\t\\t-2.987758228652069\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t11.951032914608277,\\n\\t\\t\\t\\t\\t-2.987758228652069\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t12.946952324158929,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t11.951032914608277,\\n\\t\\t\\t\\t\\t1.9918388191013605\\n\\t\\t\\t\\t]\\n\\t\\t\\t],\\n\\t\\t\\t\"pressures\": [\\n\\t\\t\\t\\t0.07999999821186066,\\n\\t\\t\\t\\t0.07999999821186066,\\n\\t\\t\\t\\t0.09253799170255661,\\n\\t\\t\\t\\t0.10853799432516098,\\n\\t\\t\\t\\t0.13899774849414825,\\n\\t\\t\\t\\t0.15639950335025787,\\n\\t\\t\\t\\t0.14868274331092834,\\n\\t\\t\\t\\t0.05903305113315582,\\n\\t\\t\\t\\t0\\n\\t\\t\\t],\\n\\t\\t\\t\"simulatePressure\": false,\\n\\t\\t\\t\"lastCommittedPoint\": [\\n\\t\\t\\t\\t11.951032914608277,\\n\\t\\t\\t\\t1.9918388191013605\\n\\t\\t\\t]\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\"id\": \"h3__rzCeg5dKTzJ7OMIb_\",\\n\\t\\t\\t\"type\": \"freedraw\",\\n\\t\\t\\t\"x\": 44.98654657610018,\\n\\t\\t\\t\"y\": 80.15579773876726,\\n\\t\\t\\t\"width\": 50.791889887085176,\\n\\t\\t\\t\"height\": 64.73476162079498,\\n\\t\\t\\t\"angle\": 0,\\n\\t\\t\\t\"strokeColor\": \"#1e1e1e\",\\n\\t\\t\\t\"backgroundColor\": \"transparent\",\\n\\t\\t\\t\"fillStyle\": \"hachure\",\\n\\t\\t\\t\"strokeWidth\": 1,\\n\\t\\t\\t\"strokeStyle\": \"solid\",\\n\\t\\t\\t\"roughness\": 1,\\n\\t\\t\\t\"opacity\": 100,\\n\\t\\t\\t\"groupIds\": [],\\n\\t\\t\\t\"frameId\": null,\\n\\t\\t\\t\"roundness\": null,\\n\\t\\t\\t\"seed\": 2045702594,\\n\\t\\t\\t\"version\": 61,\\n\\t\\t\\t\"versionNonce\": 1622118978,\\n\\t\\t\\t\"isDeleted\": false,\\n\\t\\t\\t\"boundElements\": null,\\n\\t\\t\\t\"updated\": 1690077950987,\\n\\t\\t\\t\"link\": null,\\n\\t\\t\\t\"locked\": false,\\n\\t\\t\\t\"points\": [\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-0.9959194095506518,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-1.9918388191013037,\\n\\t\\t\\t\\t\\t0.9959194095506518\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-1.9918388191013037,\\n\\t\\t\\t\\t\\t1.9918388191014174\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-0.9959194095506518,\\n\\t\\t\\t\\t\\t1.9918388191014174\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t2.987758228652069,\\n\\t\\t\\t\\t\\t-0.9959194095506518\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t9.959194095506973,\\n\\t\\t\\t\\t\\t-2.987758228652069\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t16.930629962361763,\\n\\t\\t\\t\\t\\t-4.9795970477534865\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t21.91022701011525,\\n\\t\\t\\t\\t\\t-5.975516457304138\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t24.89798523876732,\\n\\t\\t\\t\\t\\t-5.975516457304138\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t25.89390464831797,\\n\\t\\t\\t\\t\\t-4.9795970477534865\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t25.89390464831797,\\n\\t\\t\\t\\t\\t-1.9918388191014174\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t22.9061464196659,\\n\\t\\t\\t\\t\\t4.9795970477534865\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t15.934710552811111,\\n\\t\\t\\t\\t\\t12.946952324159042\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t6.971435866854904,\\n\\t\\t\\t\\t\\t20.914307600564484\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t1.9918388191014174,\\n\\t\\t\\t\\t\\t25.89390464831797\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-0.9959194095506518,\\n\\t\\t\\t\\t\\t25.89390464831797\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-1.9918388191013037,\\n\\t\\t\\t\\t\\t21.91022701011525\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-0.9959194095506518,\\n\\t\\t\\t\\t\\t15.934710552811111\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t2.987758228652069,\\n\\t\\t\\t\\t\\t10.955113505057625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t7.967355276405556,\\n\\t\\t\\t\\t\\t6.971435866854904\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t12.946952324159042,\\n\\t\\t\\t\\t\\t5.975516457304138\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t16.930629962361763,\\n\\t\\t\\t\\t\\t6.971435866854904\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t19.918388191013833,\\n\\t\\t\\t\\t\\t11.951032914608277\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t20.914307600564598,\\n\\t\\t\\t\\t\\t12.946952324159042\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t22.9061464196659,\\n\\t\\t\\t\\t\\t14.938791143260346\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t23.902065829216667,\\n\\t\\t\\t\\t\\t14.938791143260346\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t24.89798523876732,\\n\\t\\t\\t\\t\\t13.942871733709694\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t25.89390464831797,\\n\\t\\t\\t\\t\\t10.955113505057625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t26.889824057868736,\\n\\t\\t\\t\\t\\t9.959194095506973\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t27.88574346741939,\\n\\t\\t\\t\\t\\t5.975516457304138\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t30.873501696071457,\\n\\t\\t\\t\\t\\t1.9918388191014174\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t32.865340515172875,\\n\\t\\t\\t\\t\\t-0.9959194095506518\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t33.86125992472353,\\n\\t\\t\\t\\t\\t-0.9959194095506518\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t32.865340515172875,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t32.865340515172875,\\n\\t\\t\\t\\t\\t0.9959194095506518\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t32.865340515172875,\\n\\t\\t\\t\\t\\t1.9918388191014174\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t33.86125992472353,\\n\\t\\t\\t\\t\\t2.987758228652069\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t34.85717933427429,\\n\\t\\t\\t\\t\\t3.983677638202721\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t35.853098743824944,\\n\\t\\t\\t\\t\\t2.987758228652069\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t36.849018153375596,\\n\\t\\t\\t\\t\\t1.9918388191014174\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t37.84493756292636,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t39.836776382027665,\\n\\t\\t\\t\\t\\t-3.983677638202721\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t40.83269579157843,\\n\\t\\t\\t\\t\\t-8.963274685956208\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t42.824534610679734,\\n\\t\\t\\t\\t\\t-14.938791143260346\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t42.824534610679734,\\n\\t\\t\\t\\t\\t-20.914307600564484\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t43.8204540202305,\\n\\t\\t\\t\\t\\t-25.89390464831797\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t44.81637342978115,\\n\\t\\t\\t\\t\\t-28.88166287697004\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t46.80821224888257,\\n\\t\\t\\t\\t\\t-31.86942110562211\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t47.80413165843322,\\n\\t\\t\\t\\t\\t-31.86942110562211\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t47.80413165843322,\\n\\t\\t\\t\\t\\t-30.8735016960714\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t48.80005106798387,\\n\\t\\t\\t\\t\\t-27.88574346741933\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t47.80413165843322,\\n\\t\\t\\t\\t\\t-20.914307600564484\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t46.80821224888257,\\n\\t\\t\\t\\t\\t-5.975516457304138\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t46.80821224888257,\\n\\t\\t\\t\\t\\t0.9959194095506518\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t45.8122928393318,\\n\\t\\t\\t\\t\\t13.942871733709694\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t44.81637342978115,\\n\\t\\t\\t\\t\\t24.89798523876732\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t44.81637342978115,\\n\\t\\t\\t\\t\\t31.86942110562211\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t46.80821224888257,\\n\\t\\t\\t\\t\\t32.865340515172875\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t46.80821224888257,\\n\\t\\t\\t\\t\\t31.86942110562211\\n\\t\\t\\t\\t]\\n\\t\\t\\t],\\n\\t\\t\\t\"pressures\": [\\n\\t\\t\\t\\t0.07999999821186066,\\n\\t\\t\\t\\t0.07999999821186066,\\n\\t\\t\\t\\t0.07999999821186066,\\n\\t\\t\\t\\t0.0922422781586647,\\n\\t\\t\\t\\t0.1250520646572113,\\n\\t\\t\\t\\t0.14411777257919312,\\n\\t\\t\\t\\t0.15346874296665192,\\n\\t\\t\\t\\t0.1688670665025711,\\n\\t\\t\\t\\t0.17451907694339752,\\n\\t\\t\\t\\t0.1685582846403122,\\n\\t\\t\\t\\t0.1554294377565384,\\n\\t\\t\\t\\t0.1426158994436264,\\n\\t\\t\\t\\t0.12471560388803482,\\n\\t\\t\\t\\t0.09776122868061066,\\n\\t\\t\\t\\t0.08797091990709305,\\n\\t\\t\\t\\t0.08787216246128082,\\n\\t\\t\\t\\t0.10124929994344711,\\n\\t\\t\\t\\t0.10661477595567703,\\n\\t\\t\\t\\t0.10849160701036453,\\n\\t\\t\\t\\t0.13088442385196686,\\n\\t\\t\\t\\t0.15850116312503815,\\n\\t\\t\\t\\t0.17213410139083862,\\n\\t\\t\\t\\t0.18254505097866058,\\n\\t\\t\\t\\t0.1739979237318039,\\n\\t\\t\\t\\t0.16745318472385406,\\n\\t\\t\\t\\t0.14641784131526947,\\n\\t\\t\\t\\t0.11774850636720657,\\n\\t\\t\\t\\t0.0873769223690033,\\n\\t\\t\\t\\t0.07325021177530289,\\n\\t\\t\\t\\t0.08028052002191544,\\n\\t\\t\\t\\t0.08954119682312012,\\n\\t\\t\\t\\t0.09533966332674026,\\n\\t\\t\\t\\t0.10918218642473221,\\n\\t\\t\\t\\t0.1555793136358261,\\n\\t\\t\\t\\t0.1618737131357193,\\n\\t\\t\\t\\t0.16119931638240814,\\n\\t\\t\\t\\t0.15661807358264923,\\n\\t\\t\\t\\t0.14226864278316498,\\n\\t\\t\\t\\t0.12615841627120972,\\n\\t\\t\\t\\t0.11801333725452423,\\n\\t\\t\\t\\t0.10189080983400345,\\n\\t\\t\\t\\t0.0855678990483284,\\n\\t\\t\\t\\t0.06881728768348694,\\n\\t\\t\\t\\t0.0602109357714653,\\n\\t\\t\\t\\t0.05756954848766327,\\n\\t\\t\\t\\t0.062178224325180054,\\n\\t\\t\\t\\t0.07693600654602051,\\n\\t\\t\\t\\t0.09691091626882553,\\n\\t\\t\\t\\t0.12771853804588318,\\n\\t\\t\\t\\t0.13554473221302032,\\n\\t\\t\\t\\t0.15531519055366516,\\n\\t\\t\\t\\t0.16409802436828613,\\n\\t\\t\\t\\t0.17790883779525757,\\n\\t\\t\\t\\t0.21996301412582397,\\n\\t\\t\\t\\t0.2279004156589508,\\n\\t\\t\\t\\t0.22937725484371185,\\n\\t\\t\\t\\t0.2008998692035675,\\n\\t\\t\\t\\t0.14772313833236694,\\n\\t\\t\\t\\t0.01260598748922348,\\n\\t\\t\\t\\t0\\n\\t\\t\\t],\\n\\t\\t\\t\"simulatePressure\": false,\\n\\t\\t\\t\"lastCommittedPoint\": [\\n\\t\\t\\t\\t46.80821224888257,\\n\\t\\t\\t\\t31.86942110562211\\n\\t\\t\\t]\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\"id\": \"nmh56ErqLie4gPWc2FAL_\",\\n\\t\\t\\t\"type\": \"freedraw\",\\n\\t\\t\\t\"x\": 125.65601874970628,\\n\\t\\t\\t\"y\": 46.29453781404379,\\n\\t\\t\\t\"width\": 4.9795970477534865,\\n\\t\\t\\t\"height\": 18.922468781463124,\\n\\t\\t\\t\"angle\": 0,\\n\\t\\t\\t\"strokeColor\": \"#1e1e1e\",\\n\\t\\t\\t\"backgroundColor\": \"transparent\",\\n\\t\\t\\t\"fillStyle\": \"hachure\",\\n\\t\\t\\t\"strokeWidth\": 1,\\n\\t\\t\\t\"strokeStyle\": \"solid\",\\n\\t\\t\\t\"roughness\": 1,\\n\\t\\t\\t\"opacity\": 100,\\n\\t\\t\\t\"groupIds\": [],\\n\\t\\t\\t\"frameId\": null,\\n\\t\\t\\t\"roundness\": null,\\n\\t\\t\\t\"seed\": 1903897310,\\n\\t\\t\\t\"version\": 10,\\n\\t\\t\\t\"versionNonce\": 1520560386,\\n\\t\\t\\t\"isDeleted\": false,\\n\\t\\t\\t\"boundElements\": null,\\n\\t\\t\\t\"updated\": 1690077951439,\\n\\t\\t\\t\"link\": null,\\n\\t\\t\\t\"locked\": false,\\n\\t\\t\\t\"points\": [\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t0.9959194095506518\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t1.9918388191013605\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t4.97959704775343\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-0.9959194095507655,\\n\\t\\t\\t\\t\\t7.967355276405499\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-1.9918388191014174,\\n\\t\\t\\t\\t\\t11.95103291460822\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-2.987758228652069,\\n\\t\\t\\t\\t\\t15.934710552811055\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-3.9836776382028347,\\n\\t\\t\\t\\t\\t18.922468781463124\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-4.9795970477534865,\\n\\t\\t\\t\\t\\t18.922468781463124\\n\\t\\t\\t\\t]\\n\\t\\t\\t],\\n\\t\\t\\t\"pressures\": [\\n\\t\\t\\t\\t0.07999999821186066,\\n\\t\\t\\t\\t0.13944903016090393,\\n\\t\\t\\t\\t0.18283599615097046,\\n\\t\\t\\t\\t0.19746887683868408,\\n\\t\\t\\t\\t0.17992430925369263,\\n\\t\\t\\t\\t0.15612246096134186,\\n\\t\\t\\t\\t0.09196163713932037,\\n\\t\\t\\t\\t0,\\n\\t\\t\\t\\t0\\n\\t\\t\\t],\\n\\t\\t\\t\"simulatePressure\": false,\\n\\t\\t\\t\"lastCommittedPoint\": [\\n\\t\\t\\t\\t-4.9795970477534865,\\n\\t\\t\\t\\t18.922468781463124\\n\\t\\t\\t]\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\"id\": \"5rOqHAGH7nkc-i7xqwI1m\",\\n\\t\\t\\t\"type\": \"freedraw\",\\n\\t\\t\\t\"x\": 114.70090524464865,\\n\\t\\t\\t\"y\": 64.22108718595626,\\n\\t\\t\\t\"width\": 60.75108398259215,\\n\\t\\t\\t\"height\": 39.836776382027665,\\n\\t\\t\\t\"angle\": 0,\\n\\t\\t\\t\"strokeColor\": \"#1e1e1e\",\\n\\t\\t\\t\"backgroundColor\": \"transparent\",\\n\\t\\t\\t\"fillStyle\": \"hachure\",\\n\\t\\t\\t\"strokeWidth\": 1,\\n\\t\\t\\t\"strokeStyle\": \"solid\",\\n\\t\\t\\t\"roughness\": 1,\\n\\t\\t\\t\"opacity\": 100,\\n\\t\\t\\t\"groupIds\": [],\\n\\t\\t\\t\"frameId\": null,\\n\\t\\t\\t\"roundness\": null,\\n\\t\\t\\t\"seed\": 126787614,\\n\\t\\t\\t\"version\": 92,\\n\\t\\t\\t\"versionNonce\": 938087810,\\n\\t\\t\\t\"isDeleted\": false,\\n\\t\\t\\t\"boundElements\": null,\\n\\t\\t\\t\"updated\": 1690077953127,\\n\\t\\t\\t\"link\": null,\\n\\t\\t\\t\"locked\": false,\\n\\t\\t\\t\"points\": [\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t-0.9959194095507655\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-0.9959194095507655,\\n\\t\\t\\t\\t\\t-0.9959194095507655\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-0.9959194095507655,\\n\\t\\t\\t\\t\\t-1.9918388191014174\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-0.9959194095507655,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-1.9918388191014174,\\n\\t\\t\\t\\t\\t3.983677638202721\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-1.9918388191014174,\\n\\t\\t\\t\\t\\t6.97143586685479\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-1.9918388191014174,\\n\\t\\t\\t\\t\\t7.967355276405442\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-0.9959194095507655,\\n\\t\\t\\t\\t\\t7.967355276405442\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t1.9918388191013037,\\n\\t\\t\\t\\t\\t7.967355276405442\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t5.975516457304138,\\n\\t\\t\\t\\t\\t5.975516457304138\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t10.955113505057625,\\n\\t\\t\\t\\t\\t1.9918388191013037\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t14.938791143260346,\\n\\t\\t\\t\\t\\t-0.9959194095507655\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t18.922468781463067,\\n\\t\\t\\t\\t\\t-2.987758228652183\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t18.922468781463067,\\n\\t\\t\\t\\t\\t-1.9918388191014174\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t17.926549371912415,\\n\\t\\t\\t\\t\\t0.9959194095506518\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t15.934710552810998,\\n\\t\\t\\t\\t\\t4.979597047753373\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t13.942871733709694,\\n\\t\\t\\t\\t\\t8.963274685956208\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t11.951032914608277,\\n\\t\\t\\t\\t\\t12.946952324158929\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t9.95919409550686,\\n\\t\\t\\t\\t\\t16.93062996236165\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t6.97143586685479,\\n\\t\\t\\t\\t\\t19.91838819101372\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t6.97143586685479,\\n\\t\\t\\t\\t\\t20.914307600564484\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t7.967355276405556,\\n\\t\\t\\t\\t\\t18.922468781463067\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t8.963274685956208,\\n\\t\\t\\t\\t\\t16.93062996236165\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t10.955113505057625,\\n\\t\\t\\t\\t\\t15.934710552810998\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t12.946952324158929,\\n\\t\\t\\t\\t\\t14.938791143260346\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t14.938791143260346,\\n\\t\\t\\t\\t\\t14.938791143260346\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t15.934710552810998,\\n\\t\\t\\t\\t\\t14.938791143260346\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t15.934710552810998,\\n\\t\\t\\t\\t\\t15.934710552810998\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t15.934710552810998,\\n\\t\\t\\t\\t\\t17.926549371912415\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t14.938791143260346,\\n\\t\\t\\t\\t\\t20.914307600564484\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t13.942871733709694,\\n\\t\\t\\t\\t\\t23.902065829216554\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t12.946952324158929,\\n\\t\\t\\t\\t\\t24.897985238767205\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t11.951032914608277,\\n\\t\\t\\t\\t\\t25.89390464831797\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t10.955113505057625,\\n\\t\\t\\t\\t\\t26.889824057868623\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t10.955113505057625,\\n\\t\\t\\t\\t\\t27.885743467419275\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t10.955113505057625,\\n\\t\\t\\t\\t\\t28.88166287697004\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t9.95919409550686,\\n\\t\\t\\t\\t\\t30.873501696071344\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t9.95919409550686,\\n\\t\\t\\t\\t\\t32.86534051517276\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t9.95919409550686,\\n\\t\\t\\t\\t\\t34.85717933427418\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t9.95919409550686,\\n\\t\\t\\t\\t\\t35.85309874382483\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t10.955113505057625,\\n\\t\\t\\t\\t\\t35.85309874382483\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t12.946952324158929,\\n\\t\\t\\t\\t\\t34.85717933427418\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t15.934710552810998,\\n\\t\\t\\t\\t\\t32.86534051517276\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t17.926549371912415,\\n\\t\\t\\t\\t\\t28.88166287697004\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t20.914307600564484,\\n\\t\\t\\t\\t\\t21.910227010115136\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t23.902065829216554,\\n\\t\\t\\t\\t\\t14.938791143260346\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t26.889824057868623,\\n\\t\\t\\t\\t\\t10.955113505057511\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t29.877582286520692,\\n\\t\\t\\t\\t\\t5.975516457304138\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t30.873501696071457,\\n\\t\\t\\t\\t\\t3.983677638202721\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t30.873501696071457,\\n\\t\\t\\t\\t\\t1.9918388191013037\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t30.873501696071457,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t31.86942110562211,\\n\\t\\t\\t\\t\\t-0.9959194095507655\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t32.86534051517276,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t32.86534051517276,\\n\\t\\t\\t\\t\\t0.9959194095506518\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t32.86534051517276,\\n\\t\\t\\t\\t\\t1.9918388191013037\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t33.86125992472353,\\n\\t\\t\\t\\t\\t3.983677638202721\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t33.86125992472353,\\n\\t\\t\\t\\t\\t6.97143586685479\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t32.86534051517276,\\n\\t\\t\\t\\t\\t10.955113505057511\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t31.86942110562211,\\n\\t\\t\\t\\t\\t16.93062996236165\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t29.877582286520692,\\n\\t\\t\\t\\t\\t23.902065829216554\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t28.88166287697004,\\n\\t\\t\\t\\t\\t26.889824057868623\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t28.88166287697004,\\n\\t\\t\\t\\t\\t27.885743467419275\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t28.88166287697004,\\n\\t\\t\\t\\t\\t26.889824057868623\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t28.88166287697004,\\n\\t\\t\\t\\t\\t24.897985238767205\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t28.88166287697004,\\n\\t\\t\\t\\t\\t20.914307600564484\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t28.88166287697004,\\n\\t\\t\\t\\t\\t14.938791143260346\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t28.88166287697004,\\n\\t\\t\\t\\t\\t8.963274685956208\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t30.873501696071457,\\n\\t\\t\\t\\t\\t3.983677638202721\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t33.86125992472353,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t36.849018153375596,\\n\\t\\t\\t\\t\\t-1.9918388191014174\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t39.836776382027665,\\n\\t\\t\\t\\t\\t-2.987758228652183\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t42.824534610679734,\\n\\t\\t\\t\\t\\t-3.9836776382028347\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t46.808212248882455,\\n\\t\\t\\t\\t\\t-3.9836776382028347\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t49.795970477534524,\\n\\t\\t\\t\\t\\t-3.9836776382028347\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t51.78780929663594,\\n\\t\\t\\t\\t\\t-3.9836776382028347\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t51.78780929663594,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t49.795970477534524,\\n\\t\\t\\t\\t\\t5.975516457304138\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t46.808212248882455,\\n\\t\\t\\t\\t\\t13.94287173370958\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t44.81637342978104,\\n\\t\\t\\t\\t\\t21.910227010115136\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t43.820454020230386,\\n\\t\\t\\t\\t\\t25.89390464831797\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t43.820454020230386,\\n\\t\\t\\t\\t\\t27.885743467419275\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t45.8122928393318,\\n\\t\\t\\t\\t\\t28.88166287697004\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t47.80413165843311,\\n\\t\\t\\t\\t\\t29.877582286520692\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t49.795970477534524,\\n\\t\\t\\t\\t\\t28.88166287697004\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t52.783728706186594,\\n\\t\\t\\t\\t\\t27.885743467419275\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t54.77556752528801,\\n\\t\\t\\t\\t\\t24.897985238767205\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t57.76332575394008,\\n\\t\\t\\t\\t\\t19.91838819101372\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t58.75924516349073,\\n\\t\\t\\t\\t\\t17.926549371912415\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t58.75924516349073,\\n\\t\\t\\t\\t\\t13.94287173370958\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t58.75924516349073,\\n\\t\\t\\t\\t\\t12.946952324158929\\n\\t\\t\\t\\t]\\n\\t\\t\\t],\\n\\t\\t\\t\"pressures\": [\\n\\t\\t\\t\\t0.07999999821186066,\\n\\t\\t\\t\\t0.07999999821186066,\\n\\t\\t\\t\\t0.06320296972990036,\\n\\t\\t\\t\\t0.10915341228246689,\\n\\t\\t\\t\\t0.1811860054731369,\\n\\t\\t\\t\\t0.18042361736297607,\\n\\t\\t\\t\\t0.15798428654670715,\\n\\t\\t\\t\\t0.1465887427330017,\\n\\t\\t\\t\\t0.14284701645374298,\\n\\t\\t\\t\\t0.1309598982334137,\\n\\t\\t\\t\\t0.12439867854118347,\\n\\t\\t\\t\\t0.12318631261587143,\\n\\t\\t\\t\\t0.12910830974578857,\\n\\t\\t\\t\\t0.15082882344722748,\\n\\t\\t\\t\\t0.1758720725774765,\\n\\t\\t\\t\\t0.18134182691574097,\\n\\t\\t\\t\\t0.18000583350658417,\\n\\t\\t\\t\\t0.16857904195785522,\\n\\t\\t\\t\\t0.1520618349313736,\\n\\t\\t\\t\\t0.12154809385538101,\\n\\t\\t\\t\\t0.09022567421197891,\\n\\t\\t\\t\\t0.08023639023303986,\\n\\t\\t\\t\\t0.07979943603277206,\\n\\t\\t\\t\\t0.09863708168268204,\\n\\t\\t\\t\\t0.13005325198173523,\\n\\t\\t\\t\\t0.1643577218055725,\\n\\t\\t\\t\\t0.18455109000205994,\\n\\t\\t\\t\\t0.18869909644126892,\\n\\t\\t\\t\\t0.17340891063213348,\\n\\t\\t\\t\\t0.15141043066978455,\\n\\t\\t\\t\\t0.1262923926115036,\\n\\t\\t\\t\\t0.12135089188814163,\\n\\t\\t\\t\\t0.11789422482252121,\\n\\t\\t\\t\\t0.13694052398204803,\\n\\t\\t\\t\\t0.1450185924768448,\\n\\t\\t\\t\\t0.1304534673690796,\\n\\t\\t\\t\\t0.12285357713699341,\\n\\t\\t\\t\\t0.13362182676792145,\\n\\t\\t\\t\\t0.14701925218105316,\\n\\t\\t\\t\\t0.15868359804153442,\\n\\t\\t\\t\\t0.16355951130390167,\\n\\t\\t\\t\\t0.15933364629745483,\\n\\t\\t\\t\\t0.1502024233341217,\\n\\t\\t\\t\\t0.14284588396549225,\\n\\t\\t\\t\\t0.13931000232696533,\\n\\t\\t\\t\\t0.14129230380058289,\\n\\t\\t\\t\\t0.1379031389951706,\\n\\t\\t\\t\\t0.1314718872308731,\\n\\t\\t\\t\\t0.126432865858078,\\n\\t\\t\\t\\t0.11115927249193192,\\n\\t\\t\\t\\t0.0861562192440033,\\n\\t\\t\\t\\t0.04663082957267761,\\n\\t\\t\\t\\t0.021255889907479286,\\n\\t\\t\\t\\t0.09039077907800674,\\n\\t\\t\\t\\t0.1072980985045433,\\n\\t\\t\\t\\t0.1327335238456726,\\n\\t\\t\\t\\t0.15709514915943146,\\n\\t\\t\\t\\t0.1928785741329193,\\n\\t\\t\\t\\t0.22216199338436127,\\n\\t\\t\\t\\t0.22787213325500488,\\n\\t\\t\\t\\t0.21927663683891296,\\n\\t\\t\\t\\t0.20952355861663818,\\n\\t\\t\\t\\t0.15570572018623352,\\n\\t\\t\\t\\t0.10941454768180847,\\n\\t\\t\\t\\t0.1000286266207695,\\n\\t\\t\\t\\t0.09653158485889435,\\n\\t\\t\\t\\t0.10343005508184433,\\n\\t\\t\\t\\t0.12496615946292877,\\n\\t\\t\\t\\t0.1418415904045105,\\n\\t\\t\\t\\t0.15852588415145874,\\n\\t\\t\\t\\t0.17555394768714905,\\n\\t\\t\\t\\t0.1838352084159851,\\n\\t\\t\\t\\t0.17751239240169525,\\n\\t\\t\\t\\t0.16881689429283142,\\n\\t\\t\\t\\t0.149704709649086,\\n\\t\\t\\t\\t0.13695739209651947,\\n\\t\\t\\t\\t0.13103735446929932,\\n\\t\\t\\t\\t0.13976992666721344,\\n\\t\\t\\t\\t0.1574166864156723,\\n\\t\\t\\t\\t0.17939318716526031,\\n\\t\\t\\t\\t0.17816564440727234,\\n\\t\\t\\t\\t0.17347152531147003,\\n\\t\\t\\t\\t0.1427186280488968,\\n\\t\\t\\t\\t0.1203295886516571,\\n\\t\\t\\t\\t0.10864010453224182,\\n\\t\\t\\t\\t0.09333465248346329,\\n\\t\\t\\t\\t0.10012930631637573,\\n\\t\\t\\t\\t0.10919537395238876,\\n\\t\\t\\t\\t0.09066644310951233,\\n\\t\\t\\t\\t0.020901642739772797,\\n\\t\\t\\t\\t0\\n\\t\\t\\t],\\n\\t\\t\\t\"simulatePressure\": false,\\n\\t\\t\\t\"lastCommittedPoint\": [\\n\\t\\t\\t\\t58.75924516349073,\\n\\t\\t\\t\\t12.946952324158929\\n\\t\\t\\t]\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\"id\": \"uDs_VncJChAcEIS8Jh0UL\",\\n\\t\\t\\t\"type\": \"freedraw\",\\n\\t\\t\\t\"x\": 187.40302214184908,\\n\\t\\t\\t\"y\": 48.28637663314515,\\n\\t\\t\\t\"width\": 162.33486375676273,\\n\\t\\t\\t\"height\": 63.738842211244275,\\n\\t\\t\\t\"angle\": 0,\\n\\t\\t\\t\"strokeColor\": \"#1e1e1e\",\\n\\t\\t\\t\"backgroundColor\": \"transparent\",\\n\\t\\t\\t\"fillStyle\": \"hachure\",\\n\\t\\t\\t\"strokeWidth\": 1,\\n\\t\\t\\t\"strokeStyle\": \"solid\",\\n\\t\\t\\t\"roughness\": 1,\\n\\t\\t\\t\"opacity\": 100,\\n\\t\\t\\t\"groupIds\": [],\\n\\t\\t\\t\"frameId\": null,\\n\\t\\t\\t\"roundness\": null,\\n\\t\\t\\t\"seed\": 596474782,\\n\\t\\t\\t\"version\": 79,\\n\\t\\t\\t\"versionNonce\": 2121621854,\\n\\t\\t\\t\"isDeleted\": false,\\n\\t\\t\\t\"boundElements\": null,\\n\\t\\t\\t\"updated\": 1690077954708,\\n\\t\\t\\t\"link\": null,\\n\\t\\t\\t\"locked\": false,\\n\\t\\t\\t\"points\": [\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-0.9959194095506518,\\n\\t\\t\\t\\t\\t0.9959194095507087\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-0.9959194095506518,\\n\\t\\t\\t\\t\\t3.983677638202778\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t4.97959704775343\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t5.975516457304138\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t1.9918388191014174,\\n\\t\\t\\t\\t\\t7.967355276405499\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t2.987758228652069,\\n\\t\\t\\t\\t\\t8.963274685956208\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t3.9836776382028347,\\n\\t\\t\\t\\t\\t9.95919409550686\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t4.9795970477534865,\\n\\t\\t\\t\\t\\t9.95919409550686\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t4.9795970477534865,\\n\\t\\t\\t\\t\\t7.967355276405499\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t6.971435866854904,\\n\\t\\t\\t\\t\\t4.97959704775343\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t10.955113505057625,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t16.930629962361763,\\n\\t\\t\\t\\t\\t-4.9795970477534865\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t23.902065829216554,\\n\\t\\t\\t\\t\\t-7.967355276405556\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t28.88166287697004,\\n\\t\\t\\t\\t\\t-7.967355276405556\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t29.877582286520806,\\n\\t\\t\\t\\t\\t-7.967355276405556\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t28.88166287697004,\\n\\t\\t\\t\\t\\t-3.983677638202778\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t22.9061464196659,\\n\\t\\t\\t\\t\\t2.987758228652069\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t11.951032914608277,\\n\\t\\t\\t\\t\\t11.951032914608277\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t2.987758228652069,\\n\\t\\t\\t\\t\\t21.91022701011525\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-2.987758228652069,\\n\\t\\t\\t\\t\\t26.889824057868623\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-5.975516457304138,\\n\\t\\t\\t\\t\\t26.889824057868623\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-6.97143586685479,\\n\\t\\t\\t\\t\\t21.91022701011525\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-5.975516457304138,\\n\\t\\t\\t\\t\\t13.942871733709694\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-0.9959194095506518,\\n\\t\\t\\t\\t\\t7.967355276405499\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t4.9795970477534865,\\n\\t\\t\\t\\t\\t3.983677638202778\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t9.959194095506973,\\n\\t\\t\\t\\t\\t2.987758228652069\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t13.942871733709694,\\n\\t\\t\\t\\t\\t1.9918388191013605\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t16.930629962361763,\\n\\t\\t\\t\\t\\t2.987758228652069\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t17.926549371912415,\\n\\t\\t\\t\\t\\t5.975516457304138\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t14.938791143260346,\\n\\t\\t\\t\\t\\t10.955113505057625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t7.967355276405556,\\n\\t\\t\\t\\t\\t19.918388191013833\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-2.987758228652069,\\n\\t\\t\\t\\t\\t28.88166287697004\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-12.946952324158929,\\n\\t\\t\\t\\t\\t36.849018153375596\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-21.910227010115136,\\n\\t\\t\\t\\t\\t41.82861520112908\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-30.873501696071457,\\n\\t\\t\\t\\t\\t43.820454020230386\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-38.8408569724769,\\n\\t\\t\\t\\t\\t43.820454020230386\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-47.80413165843322,\\n\\t\\t\\t\\t\\t42.824534610679734\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-54.77556752528801,\\n\\t\\t\\t\\t\\t40.83269579157832\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-59.7551645730415,\\n\\t\\t\\t\\t\\t38.84085697247701\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-63.73884221124422,\\n\\t\\t\\t\\t\\t36.849018153375596\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-64.73476162079487,\\n\\t\\t\\t\\t\\t33.86125992472353\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-65.73068103034564,\\n\\t\\t\\t\\t\\t30.873501696071457\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-65.73068103034564,\\n\\t\\t\\t\\t\\t27.88574346741939\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-68.7184392589977,\\n\\t\\t\\t\\t\\t26.889824057868623\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-76.68579453540326,\\n\\t\\t\\t\\t\\t27.88574346741939\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-91.6245856786636,\\n\\t\\t\\t\\t\\t31.86942110562211\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-103.57561859327188,\\n\\t\\t\\t\\t\\t38.84085697247701\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-113.53481268877874,\\n\\t\\t\\t\\t\\t43.820454020230386\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-121.5021679651843,\\n\\t\\t\\t\\t\\t46.808212248882455\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-128.4736038320392,\\n\\t\\t\\t\\t\\t47.80413165843322\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-132.45728147024192,\\n\\t\\t\\t\\t\\t47.80413165843322\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-131.46136206069127,\\n\\t\\t\\t\\t\\t46.808212248882455\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-122.49808737473506,\\n\\t\\t\\t\\t\\t38.84085697247701\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-105.5674574123733,\\n\\t\\t\\t\\t\\t25.89390464831797\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-81.66539158315663,\\n\\t\\t\\t\\t\\t12.946952324158929\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-55.77148693483866,\\n\\t\\t\\t\\t\\t1.9918388191013605\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-30.873501696071457,\\n\\t\\t\\t\\t\\t-6.971435866854847\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-7.967355276405556,\\n\\t\\t\\t\\t\\t-9.959194095506916\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t8.963274685956208,\\n\\t\\t\\t\\t\\t-9.959194095506916\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t17.926549371912415,\\n\\t\\t\\t\\t\\t-6.971435866854847\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t18.92246878146318,\\n\\t\\t\\t\\t\\t-0.9959194095507087\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t12.946952324159042,\\n\\t\\t\\t\\t\\t5.975516457304138\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0.9959194095506518,\\n\\t\\t\\t\\t\\t14.938791143260346\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-16.930629962361763,\\n\\t\\t\\t\\t\\t22.9061464196659\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-38.8408569724769,\\n\\t\\t\\t\\t\\t29.877582286520692\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-59.7551645730415,\\n\\t\\t\\t\\t\\t37.84493756292625\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-77.68171394495391,\\n\\t\\t\\t\\t\\t44.81637342978115\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-94.61234390731568,\\n\\t\\t\\t\\t\\t49.795970477534524\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-105.5674574123733,\\n\\t\\t\\t\\t\\t52.783728706186594\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-111.54297386967744,\\n\\t\\t\\t\\t\\t53.77964811573736\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-110.54705446012667,\\n\\t\\t\\t\\t\\t51.78780929663594\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-99.59194095506916,\\n\\t\\t\\t\\t\\t46.808212248882455\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-78.67763335450456,\\n\\t\\t\\t\\t\\t40.83269579157832\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-54.77556752528801,\\n\\t\\t\\t\\t\\t35.85309874382483\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-30.873501696071457,\\n\\t\\t\\t\\t\\t28.88166287697004\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-11.951032914608277,\\n\\t\\t\\t\\t\\t20.914307600564484\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t-6.97143586685479,\\n\\t\\t\\t\\t\\t18.92246878146318\\n\\t\\t\\t\\t]\\n\\t\\t\\t],\\n\\t\\t\\t\"pressures\": [\\n\\t\\t\\t\\t0.07999999821186066,\\n\\t\\t\\t\\t0.07999999821186066,\\n\\t\\t\\t\\t0.1090639978647232,\\n\\t\\t\\t\\t0.11759048700332642,\\n\\t\\t\\t\\t0.127610445022583,\\n\\t\\t\\t\\t0.1494656354188919,\\n\\t\\t\\t\\t0.13932812213897705,\\n\\t\\t\\t\\t0.1264503449201584,\\n\\t\\t\\t\\t0.07199783623218536,\\n\\t\\t\\t\\t0.07556182891130447,\\n\\t\\t\\t\\t0.09087585657835007,\\n\\t\\t\\t\\t0.11792977899312973,\\n\\t\\t\\t\\t0.15368500351905823,\\n\\t\\t\\t\\t0.17648300528526306,\\n\\t\\t\\t\\t0.17781402170658112,\\n\\t\\t\\t\\t0.18531586229801178,\\n\\t\\t\\t\\t0.14834906160831451,\\n\\t\\t\\t\\t0.12049063295125961,\\n\\t\\t\\t\\t0.07403720170259476,\\n\\t\\t\\t\\t0.06293389946222305,\\n\\t\\t\\t\\t0.08030901849269867,\\n\\t\\t\\t\\t0.10696588456630707,\\n\\t\\t\\t\\t0.09294155985116959,\\n\\t\\t\\t\\t0.09678619354963303,\\n\\t\\t\\t\\t0.0900837704539299,\\n\\t\\t\\t\\t0.12043420225381851,\\n\\t\\t\\t\\t0.16941504180431366,\\n\\t\\t\\t\\t0.19701339304447174,\\n\\t\\t\\t\\t0.20666848123073578,\\n\\t\\t\\t\\t0.20913344621658325,\\n\\t\\t\\t\\t0.19611236453056335,\\n\\t\\t\\t\\t0.14732769131660461,\\n\\t\\t\\t\\t0.09921478480100632,\\n\\t\\t\\t\\t0.06488580256700516,\\n\\t\\t\\t\\t0.059745848178863525,\\n\\t\\t\\t\\t0.08588150143623352,\\n\\t\\t\\t\\t0.10220865160226822,\\n\\t\\t\\t\\t0.11737481504678726,\\n\\t\\t\\t\\t0.12219372391700745,\\n\\t\\t\\t\\t0.11954028159379959,\\n\\t\\t\\t\\t0.1177532970905304,\\n\\t\\t\\t\\t0.11425948888063431,\\n\\t\\t\\t\\t0.11477605998516083,\\n\\t\\t\\t\\t0.11529559642076492,\\n\\t\\t\\t\\t0.11550057679414749,\\n\\t\\t\\t\\t0.09895990043878555,\\n\\t\\t\\t\\t0.05332806333899498,\\n\\t\\t\\t\\t0.05016760155558586,\\n\\t\\t\\t\\t0.05966867133975029,\\n\\t\\t\\t\\t0.0985812395811081,\\n\\t\\t\\t\\t0.1396407186985016,\\n\\t\\t\\t\\t0.17888760566711426,\\n\\t\\t\\t\\t0.21238180994987488,\\n\\t\\t\\t\\t0.21392938494682312,\\n\\t\\t\\t\\t0.22956372797489166,\\n\\t\\t\\t\\t0.22757042944431305,\\n\\t\\t\\t\\t0.215678870677948,\\n\\t\\t\\t\\t0.18348000943660736,\\n\\t\\t\\t\\t0.173598513007164,\\n\\t\\t\\t\\t0.1730438768863678,\\n\\t\\t\\t\\t0.18371808528900146,\\n\\t\\t\\t\\t0.22672131657600403,\\n\\t\\t\\t\\t0.2308312952518463,\\n\\t\\t\\t\\t0.19925054907798767,\\n\\t\\t\\t\\t0.11754519492387772,\\n\\t\\t\\t\\t0.08259066939353943,\\n\\t\\t\\t\\t0.09804791212081909,\\n\\t\\t\\t\\t0.14002612233161926,\\n\\t\\t\\t\\t0.1726100742816925,\\n\\t\\t\\t\\t0.19601498544216156,\\n\\t\\t\\t\\t0.21659402549266815,\\n\\t\\t\\t\\t0.22422964870929718,\\n\\t\\t\\t\\t0.2436126470565796,\\n\\t\\t\\t\\t0.2614494264125824,\\n\\t\\t\\t\\t0.23918844759464264,\\n\\t\\t\\t\\t0.16303803026676178,\\n\\t\\t\\t\\t0.060110319405794144,\\n\\t\\t\\t\\t0\\n\\t\\t\\t],\\n\\t\\t\\t\"simulatePressure\": false,\\n\\t\\t\\t\"lastCommittedPoint\": [\\n\\t\\t\\t\\t-6.97143586685479,\\n\\t\\t\\t\\t18.92246878146318\\n\\t\\t\\t]\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\"id\": \"0G6qQG2Td9dv0i4veDcZw\",\\n\\t\\t\\t\"type\": \"freedraw\",\\n\\t\\t\\t\"x\": 210.30916856151498,\\n\\t\\t\\t\"y\": -171.81181287755766,\\n\\t\\t\\t\"width\": 94.61234390731568,\\n\\t\\t\\t\"height\": 99.59194095506916,\\n\\t\\t\\t\"angle\": 0,\\n\\t\\t\\t\"strokeColor\": \"#1e1e1e\",\\n\\t\\t\\t\"backgroundColor\": \"transparent\",\\n\\t\\t\\t\"fillStyle\": \"hachure\",\\n\\t\\t\\t\"strokeWidth\": 1,\\n\\t\\t\\t\"strokeStyle\": \"solid\",\\n\\t\\t\\t\"roughness\": 1,\\n\\t\\t\\t\"opacity\": 100,\\n\\t\\t\\t\"groupIds\": [],\\n\\t\\t\\t\"frameId\": null,\\n\\t\\t\\t\"roundness\": null,\\n\\t\\t\\t\"seed\": 1460824962,\\n\\t\\t\\t\"version\": 35,\\n\\t\\t\\t\"versionNonce\": 958261058,\\n\\t\\t\\t\"isDeleted\": false,\\n\\t\\t\\t\"boundElements\": null,\\n\\t\\t\\t\"updated\": 1690077956094,\\n\\t\\t\\t\"link\": null,\\n\\t\\t\\t\"locked\": false,\\n\\t\\t\\t\"points\": [\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t0,\\n\\t\\t\\t\\t\\t0\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t1.9918388191014174,\\n\\t\\t\\t\\t\\t-0.9959194095506518\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t3.9836776382028347,\\n\\t\\t\\t\\t\\t-1.9918388191013605\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t7.967355276405556,\\n\\t\\t\\t\\t\\t-2.987758228652069\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t11.951032914608277,\\n\\t\\t\\t\\t\\t-3.983677638202778\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t24.89798523876732,\\n\\t\\t\\t\\t\\t-8.963274685956208\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t36.849018153375596,\\n\\t\\t\\t\\t\\t-13.942871733709694\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t50.79188988708529,\\n\\t\\t\\t\\t\\t-19.918388191013833\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t64.73476162079498,\\n\\t\\t\\t\\t\\t-23.902065829216554\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t76.68579453540326,\\n\\t\\t\\t\\t\\t-26.88982405786868\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t86.64498863091012,\\n\\t\\t\\t\\t\\t-26.88982405786868\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t90.62866626911295,\\n\\t\\t\\t\\t\\t-23.902065829216554\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t90.62866626911295,\\n\\t\\t\\t\\t\\t-16.930629962361763\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t84.65314981180882,\\n\\t\\t\\t\\t\\t-6.971435866854847\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t73.69803630675119,\\n\\t\\t\\t\\t\\t3.983677638202778\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t57.76332575394008,\\n\\t\\t\\t\\t\\t15.934710552811055\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t41.82861520112908,\\n\\t\\t\\t\\t\\t26.88982405786868\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t25.89390464831797,\\n\\t\\t\\t\\t\\t35.85309874382489\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t16.930629962361763,\\n\\t\\t\\t\\t\\t41.828615201129026\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t15.934710552811111,\\n\\t\\t\\t\\t\\t41.828615201129026\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t22.9061464196659,\\n\\t\\t\\t\\t\\t37.844937562926305\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t30.873501696071457,\\n\\t\\t\\t\\t\\t31.86942110562211\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t42.824534610679734,\\n\\t\\t\\t\\t\\t22.9061464196659\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t52.78372870618671,\\n\\t\\t\\t\\t\\t17.926549371912472\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t66.72660043989629,\\n\\t\\t\\t\\t\\t14.938791143260403\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t77.68171394495391,\\n\\t\\t\\t\\t\\t14.938791143260403\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t88.63682745001154,\\n\\t\\t\\t\\t\\t19.918388191013833\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t93.61642449776502,\\n\\t\\t\\t\\t\\t29.87758228652075\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t94.61234390731568,\\n\\t\\t\\t\\t\\t40.832695791578374\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t91.6245856786636,\\n\\t\\t\\t\\t\\t51.78780929663594\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t85.64906922135947,\\n\\t\\t\\t\\t\\t62.74292280169357\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t80.66947217360598,\\n\\t\\t\\t\\t\\t70.71027807809912\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t81.66539158315675,\\n\\t\\t\\t\\t\\t72.70211689720048\\n\\t\\t\\t\\t],\\n\\t\\t\\t\\t[\\n\\t\\t\\t\\t\\t84.65314981180882,\\n\\t\\t\\t\\t\\t70.71027807809912\\n\\t\\t\\t\\t]\\n\\t\\t\\t],\\n\\t\\t\\t\"pressures\": [\\n\\t\\t\\t\\t0.07999999821186066,\\n\\t\\t\\t\\t0.07999999821186066,\\n\\t\\t\\t\\t0.13836000859737396,\\n\\t\\t\\t\\t0.14543798565864563,\\n\\t\\t\\t\\t0.15853051841259003,\\n\\t\\t\\t\\t0.1723373681306839,\\n\\t\\t\\t\\t0.17169290781021118,\\n\\t\\t\\t\\t0.17217181622982025,\\n\\t\\t\\t\\t0.17130127549171448,\\n\\t\\t\\t\\t0.16030846536159515,\\n\\t\\t\\t\\t0.1522316038608551,\\n\\t\\t\\t\\t0.14880673587322235,\\n\\t\\t\\t\\t0.1445712000131607,\\n\\t\\t\\t\\t0.16054610908031464,\\n\\t\\t\\t\\t0.16493242979049683,\\n\\t\\t\\t\\t0.15076379477977753,\\n\\t\\t\\t\\t0.15401895344257355,\\n\\t\\t\\t\\t0.1494321972131729,\\n\\t\\t\\t\\t0.15311475098133087,\\n\\t\\t\\t\\t0.16898642480373383,\\n\\t\\t\\t\\t0.15161310136318207,\\n\\t\\t\\t\\t0.1442108154296875,\\n\\t\\t\\t\\t0.13550527393817902,\\n\\t\\t\\t\\t0.13199694454669952,\\n\\t\\t\\t\\t0.1559293270111084,\\n\\t\\t\\t\\t0.1685316413640976,\\n\\t\\t\\t\\t0.1701705902814865,\\n\\t\\t\\t\\t0.16214044392108917,\\n\\t\\t\\t\\t0.16235555708408356,\\n\\t\\t\\t\\t0.1736508160829544,\\n\\t\\t\\t\\t0.18561020493507385,\\n\\t\\t\\t\\t0.19134090840816498,\\n\\t\\t\\t\\t0.10439816117286682,\\n\\t\\t\\t\\t0\\n\\t\\t\\t],\\n\\t\\t\\t\"simulatePressure\": false,\\n\\t\\t\\t\"lastCommittedPoint\": [\\n\\t\\t\\t\\t84.65314981180882,\\n\\t\\t\\t\\t70.71027807809912\\n\\t\\t\\t]\\n\\t\\t}\\n\\t],\\n\\t\"appState\": {\\n\\t\\t\"theme\": \"light\",\\n\\t\\t\"viewBackgroundColor\": \"#ffffff\",\\n\\t\\t\"currentItemStrokeColor\": \"#1e1e1e\",\\n\\t\\t\"currentItemBackgroundColor\": \"transparent\",\\n\\t\\t\"currentItemFillStyle\": \"hachure\",\\n\\t\\t\"currentItemStrokeWidth\": 1,\\n\\t\\t\"currentItemStrokeStyle\": \"solid\",\\n\\t\\t\"currentItemRoughness\": 1,\\n\\t\\t\"currentItemOpacity\": 100,\\n\\t\\t\"currentItemFontFamily\": 1,\\n\\t\\t\"currentItemFontSize\": 20,\\n\\t\\t\"currentItemTextAlign\": \"left\",\\n\\t\\t\"currentItemStartArrowhead\": null,\\n\\t\\t\"currentItemEndArrowhead\": \"arrow\",\\n\\t\\t\"scrollX\": 652.1570401093837,\\n\\t\\t\"scrollY\": 455.6488445995047,\\n\\t\\t\"zoom\": {\\n\\t\\t\\t\"value\": 1.0040973098929256\\n\\t\\t},\\n\\t\\t\"currentItemRoundness\": \"round\",\\n\\t\\t\"gridSize\": null,\\n\\t\\t\"currentStrokeOptions\": null,\\n\\t\\t\"previousGridSize\": null\\n\\t},\\n\\t\"files\": {}\\n}\\n```\\n%%'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents1_clean[1167].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tt = [documents1[600], documents1[300]]\n",
    "# myindex = VectorStoreIndex.from_documents(tt, embed_model=Settings.embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_alldoc = VectorStoreIndex.from_documents(documents1_clean, embed_model=Settings.embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# initialize the vector store\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# client = qdrant_client.QdrantClient(\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#     path=\"./qdrant_data\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m QdrantVectorStore(client\u001b[38;5;241m=\u001b[39m\u001b[43mclient\u001b[49m, collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtweets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m storage_context \u001b[38;5;241m=\u001b[39m StorageContext\u001b[38;5;241m.\u001b[39mfrom_defaults(vector_store\u001b[38;5;241m=\u001b[39mvector_store)\n\u001b[1;32m      8\u001b[0m llm \u001b[38;5;241m=\u001b[39m Ollama(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTinydolphin\u001b[39m\u001b[38;5;124m\"\u001b[39m, request_timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m120.0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "# initialize the vector store\n",
    "# client = qdrant_client.QdrantClient(\n",
    "#     path=\"./qdrant_data\"\n",
    "# )\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"tweets\")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "llm = Ollama(model=\"Tinydolphin\", request_timeout=120.0)\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "# Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
    "# Settings.num_output = 512\n",
    "# Settings.context_window = 3900\n",
    "\n",
    "query_engine = index_alldoc.as_query_engine()\n",
    "chat_engine = index_alldoc.as_chat_engine()\n",
    "retriever = index_alldoc.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " So, based on the given context and my understanding from the provided context information, I will provide a summary of all the relevant points related to social perception.\n",
      "\n",
      "- The study of social perception is complex and has evolved over time. The traditional model of attraction based on actor, partner, and relationship effects has been replaced by theories that include actor, partner, and interactive effects.\n",
      "- The presence of social perception theories has been documented in various studies. These theories have attempted to provide a general conceptual framework for the attraction process. Some of the key factors mentioned in these theories are personality, intelligence, and situational context.\n",
      "- Kenny did not treat liking as the same as the personality judgement task. Instead, he referred to liking as an intrinsic attribute that cannot be changed or altered by external factors. He believed that people can do things differently but maintain their core personality traits.\n",
      "- The present review will explore four key topics in social perception: (a) the types of responses defined as indicators of attraction; (b) the stimulus variables identified as antecedents of attraction; (c) the consequences of attraction (i.e., behaviors mediated by attraction); and (d) theories that attempt to provide a general conceptual framework for the attraction process.\n",
      "- The discussion will be organized around four main areas: (a) the types of responses defined as indicators of attraction, (b) the stimulus variables identified as antecedents of attraction, (c) the consequences of attraction (i.e., behaviors mediated by attraction), and (d) theories that attempt to provide a general conceptual framework for the attraction process.\n",
      "- The main findings from these studies will be presented in the review, providing a summary of the current state of knowledge about social perception.\n",
      "\n",
      "In conclusion, while there is not much in the way of empirically established theories regarding the specifics of how people see others or themselves, it is clear that the conceptual framework proposed by Kenny and his team is an important step towards understanding the nuances of these relationships. This review aims to provide a comprehensive overview of the field and its ongoing research.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Summary of all context about social perception\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Nice Guys is a comedy troupe that prides itself on making fun of the everyday foibles and stupidity of the human race. They perform in front of live audiences and have gained a dedicated following thanks to their unique blend of humor, wit, and relentless banter. Some people may know them from their popular sketch \"The Creepy Crush\" or their other comedy sketches on various social media platforms.\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"Nice Guys\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NodeWithScore(node=TextNode(id_='680921c3-9cd9-4c27-ba74-a43eecbc19fa', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='87d41701-0b40-4ae8-9a71-2152d9d04ad4', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='df69577a5282086dcc2f4be280fd6f81b223c12f8071e9a2683d6006b5658730'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='80839621-2b32-4f4c-a8c3-78dceb7af4a7', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='5c2adc3688d64bdef72312a35c3db18c777aa7f4495e23744b2727c6e569f3a5')}, text=\"杂 86 nice guy\\n\\n**Being integrated means being able to accept all aspects of one's self.** An integrated man is able to\\nembrace everything that makes him uniquely male: his power, his assertiveness, his courage, and his\\npassion as well as his imperfections, his mistakes, and his dark side.\\n\\nAn integrated male possesses many of the following attributes:\\n\\n- ●\\xa0**He has a strong sense of self. He likes himself just as he is.**\\n- ●\\xa0**He takes responsibility for getting his own needs met.**\\n- ●\\xa0**He is comfortable with his masculinity and his sexuality.**\\n- ●\\xa0**He has integrity. He does what is right, not what is expedient.**\\n- ●\\xa0**He is a leader. He is willing to provide for and protect those he cares about.**\\n- ●\\xa0**He is clear, direct, and expressive of his feelings.**\\n- ●\\xa0**He can be nurturing and giving without caretaking or problem-solving.**\\n- ●\\xa0**He knows how to set boundaries and is not afraid to work through conflict.**\\n\\n**Breaking Free Activity #3**\\n\\nIt is impossible to cover every factor that might cause a young boy to try to hide his perceived\\nflaws and seek approval from others. I don't believe it is essential for Nice Guys to uncover *every*\\nexperience that ever made them feel unsafe or bad. But I have found that some understanding of\\nwhere a life script originated is helpful in changing that script.\\n\\nReread the stories of Alan, Jason, and Jose. Think about how these stories are similar to your own\\nchildhood experiences. On a separate piece of paper or journal, write down or illustrate the\\nmessages you received in your family that seemed to imply that it wasn't OK for you to be who\\nyou were, just as you were. Share these experiences with a safe person. As you do, make note of your feelings. Do you feel sad, angry, lonely, numb? Share this information as well.\\n\\nThe purpose of this assignment is to name, rather than blame. Blaming will keep you stuck.\\nNaming the childhood experiences that led you to believe that it was not a safe or acceptable thing for you to be just who you were will allow you replace these messages with more accurate ones and help you change your Nice Guy script.\\n\\n**Breaking Free Activity #6**\\n\\nLook over the lists above. Write down examples of situations in which you have tried to hide or\\ndistract attention from any of these perceived flaws. How effective do you think you are in keeping\\nthese things hidden from the people you love?\\n****\\n\\n**Breaking Free Activity #9**\\n\\nBegin with the list above and add good things that you can do for yourself. Put the list up where\\nyou will see it and choose at least one thing per day and do it for yourself.\\n\\n**Making It Difficult For Others To Give To Them Prevents Nice Guys From Getting Their\\nNeeds Met**\\n\\nIn addition to using ineffective strategies to get their needs met, **Nice Guys are terrible receivers.** Since\\ngetting their needs met contradicts their childhood paradigms, Nice Guys are extremely uncomfortable when they actually do get what they want. Though most Nice Guys have a difficult time grasping this concept, they are terrified of getting what they really want and will go to extreme measures to make sure they don't. Nice Guys carry out this unconscious agenda by connecting with needy or unavailable people, operating from an unspoken agenda, being unclear and indirect, pushing people away, and sabotaging.\\n\\n**Caretaking**\\n\\n**1) Gives to others what the giver needs to give.**\\n\\n**2) Comes from a place of within emptiness within\\nthe giver.**\\n\\n**3) Always has unconscious strings attached.**\\n\\n**Caring**\\n\\n**1) Gives to others what the receiver needs.**\\n\\n**2) Comes from a place of abundance the giver.**\\n\\n**3) Has no strings attached.**\\n\\n**victim triangle.** The victim triangle\\nconsists of three predictable sequences:\\n\\n1) The Nice Guy gives to others hoping to get something in return.\\n\\n2) When it doesn't seem that he is getting as much as he gives or he isn't getting what he expected, he feels frustrated and resentful. Remember, the Nice Guy is the one keeping score and he isn't totally objective.\\n\\n3) When this frustration and resentment builds up long enough, it spills out in the form of rage attacks, passive-aggressive behavior, pouting, tantrums, withdrawing, shaming, criticizing, blaming, even physical abuse. Once the cycle has been completed, it usually just begins all over again.\\n\\n**Since Nice Guys learned to sacrifice themselves in order to survive, recovery must center on learning to put themselves first and making their needs a priority.\", start_char_idx=2, end_char_idx=4492, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7412231138238312), NodeWithScore(node=TextNode(id_='fd6ff6f3-7a69-498e-b013-dc81e1f97710', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='87d41701-0b40-4ae8-9a71-2152d9d04ad4', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='df69577a5282086dcc2f4be280fd6f81b223c12f8071e9a2683d6006b5658730'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='f9a205d1-fc38-47e8-a045-2fc1f9962b76', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='f3cb3cf42fbe16566b3d78652eb1f3ee34468fb7851b0009c5ff597de3d17527'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='07052890-4764-4666-9c63-962a2f69604c', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='b3daaead11d831ffa4bd4ac04eee768ee248f59d45b4b032c05029d498137903')}, text='Without this masculine energy we would have all become extinct eons ago. Masculinity\\nempowers a man to create and produce. It also empowers him provide for and protect those who are\\nimportant to him.\\n\\nMost Nice Guys believe that by repressing the darker side of their masculine energy they will win the approval of women. As Nice Guys try to avoid the dark side of their masculinity, they also repress many other aspects of this male energy force. As a result, they often lose their sexual assertiveness, competitiveness, creativity, ego, thirst for experience, boisterousness, exhibitionism, and power.\\n\\nThe process involves coming to believe that it really is a good\\nthing to be a man and embracing all of their masculine traits. Reclaiming one\\'s masculinity involves:\\n\\n- ●\\xa0Connecting with other men.\\n- ●\\xa0Getting strong.\\n- ●\\xa0Finding healthy male role models.\\n- ●\\xa0Reexamining one\\'s relationship with one\\'s father.\\n\\nThe best thing you can do for your relationship with your girlfriend or wife is to have male friends\\n\\n**They realize that if their lives are a reaction to Dad, then Dad is still in control.**\\n\\nIn relationships, a life-and-death struggle is played out to balance their fear of vulnerability with their fear of isolation. Vulnerability means someone may get too close to them and see how bad they are. Nice Guys are convinced that when others make this discovery, these people will hurt them, shame them, or leave them\\n\\nThe first is through\\nbecoming overly involved in an intimate relationship at the expense of one\\'s self and other outside\\ninterests. The second is through being emotionally unavailable to a primary partner while playing the\\nNice Guy role outside of the relationship. I call the first type of Nice Guy an **enmesher** and the second\\ntype an **avoider**.\\n\\nThe enmeshing Nice Guy makes his partner his emotional center. His world revolves around her. She is\\nmore important than his work, his buddies, his hobbies. He will do whatever it takes to make her happy.\\nHe will give her gifts, try to fix her problems, and arrange his schedule to be with her. He will gladly\\nsacrifice his wants and needs to win her love. He will even tolerate her bad moods, rage attacks,\\naddictions, and emotional or sexual unavailability — all because he \"loves her so much.\"\\n\\nI sometimes refer to enmeshing Nice Guys as **table dogs**. They are like little dogs who hover beneath\\nthe table just in case a scrap happens to fall their way. Enmeshing Nice Guys do this same hovering\\nroutine around their partner just in case she happens to drop him a scrap of sexual interest, a scrap of her\\ntime, a scrap of a good mood, or a scrap of her attention.\\n\\nOn the surface it may appear that the enmeshing Nice Guy desires, and is available for an intimate relationship, but this is an illusion. The Nice Guy\\'s pursuing and enmeshing behavior is an attempt to hook up an emotional hose to his partner. This hose is used to suck the life out of her and fill an empty place inside of him. The Nice Guy\\'s partner unconsciously picks up on this agenda and works like hell to make sure the Nice Guy can\\'t get close enough to hook up the hose. Consequently, the Nice Guy\\'s partner is often seen as the one preventing the closeness the Nice Guy desires.\\n\\nThe people who like them just as they are will hang around. The people who don\\'t, won\\'t. This is the only way to have a healthy relationship\\n\\nWhen trying to decide how to deal with a behavior they have deemed unacceptable, I encourage Nice\\nGuys to apply the **Healthy Male Rule.** Following this rule of thumb, they simply ask themselves, \"How\\nwould a healthy male handle this situation?\" For some reason, just asking this question connects them\\nwith their intuitive wisdom and helps them access the power they need to respond appropriately.\\n\\nOnce the Nice Guy knows he can set a boundary any time he needs to, he can let people move toward him, get close, have feelings, be sexual, and so on. He can let these things happen because he is confident that at any point, if he begins to feel uncomfortable, he can say \"stop,\" \"no,\" or \"slow down,\" or can remove himself. He can do whatever he needs to do to take care of himself.\\n\\n**Breaking Free Activity #34**\\n\\n**Are there any areas in your personal relationships in which you avoid setting appropriate\\nboundaries? Do you:**\\n\\n- ●\\xa0**Tolerate intolerable behavior.**\\n- ●\\xa0**Avoid dealing with a situation because it might cause conflict.**\\n- ●\\xa0**Not ask for what you want.**\\n- ●\\xa0**Sacrifice yourself to keep the peace.', start_char_idx=11204, end_char_idx=15716, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7332056748301413)]\n"
     ]
    }
   ],
   "source": [
    "ret1 = retriever.retrieve(\"Nice Guys\")\n",
    "print(ret1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
