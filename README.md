# Cairo's Personal Chatbot 
#### - This project used ollama and llamaindex to create a chatbot based on all my personal data, including obsidian notes, dairy stored at Lifelight, and weChat conversation history. It is an ambitious project to organize my digital life.
#### - The chatbot can utilize both locally hosted small models (1.1B tinyllama or tinydolphin) and connects to big LLM models via Chatgpt or Claude API

#### - Current state: allows 4 types of interactions with my chatbot and retrieve, organize, and summarize useful information from my knowledge base

#### Example usage
Options:
1. Chat: focus more on conversation history/context than the knowledge base
2. Query: return relevant content combined and summarized by AI; Focus more on the knowledge base but also combines information from conversation history/context
3. Search notes: only return most relevant content without narrative processing from AI
4. Exit
Debug: User selected option: '1'

**Processing chat query**: summarize my notes about transformers

**Response**:  The context provided in your notes revolves around transformers which are a family of neural network architectures and models that have been widely adopted for natural language processing tasks such as machine translation, summarization, and sentiment analysis. Transformers are composed of multiple layers, each with its own set of learnable parameters or 'heads'. In this context, we are discussing the transformer architecture and how it is used to perform language translation tasks.

The question you posed is about a transformer's ability to perform well on complex natural language understanding tasks, such as summarization and sentiment analysis. Transformers have shown to outperform traditional machine learning approaches in these domains by being able to learn complex relationships between words, sentences, and even entire phrases, which can then be used for prediction, generation, or classification tasks.

Transformers are known for their ability to learn long-term dependencies and can generate new information based on the knowledge already stored within them. They are widely used in natural language processing due to their ability to capture complex relationships between words and their meaning. Transformers also have the advantage of being able to handle both short and long sequences, which is beneficial for tasks such as natural language understanding and generation.

In summary, transformers have the potential to perform well on a variety of natural language processing tasks due to their ability to learn complex relationships between words and their meanings, making them more powerful than traditional machine learning approaches in many domains.
